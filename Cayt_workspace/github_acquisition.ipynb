{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f42beb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import json\n",
    "from typing import Dict, List, Optional, Union, cast\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup as BSoup\n",
    "\n",
    "from env import github_token, github_username"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeffe540",
   "metadata": {},
   "source": [
    "### Create function to generate URLs to scrape\n",
    "- need a LIST in te format of \"gocodeup/codeup-setup-script\"\n",
    "- search criteria: https://github.com/search?q=oceanography&type=repositories\n",
    "- page 2 is: https://github.com/search?p=2&q=oceanography&type=Repositories\n",
    "- type parameter changes capitalization....?p=1 exists, so just use that\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e012977b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://github.com/search?p=1&q=oceanography&type=Repositories'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers = {\"Authorization\": f\"token {github_token}\", \"User-Agent\": github_username}\n",
    "p=1\n",
    "url = f'https://github.com/search?p={p}&q=oceanography&type=Repositories'\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34720971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#grab the first page\n",
    "response = requests.get(url, headers=headers)\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a440636c",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BSoup(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a671e34",
   "metadata": {},
   "source": [
    "**now try and get all the repo urls on the page**\n",
    "- the repo search results page has each repo in its own li\n",
    "- in that list there are two divs.  The one we want is div class='mt-n1 flex-auto>\n",
    "- then another div class=\"d-flex\"\n",
    "- then another div class=\"f4 text-normal\"\n",
    "- then:\n",
    "\n",
    "a: \n",
    "\n",
    "class=\"v-align-middle\" \n",
    "\n",
    "data-hydro-click= *lots of text*\n",
    "\n",
    "data-hydro-click-hmac= *alphanumeric string* \n",
    "\n",
    "href=\"/rabernat/intro_to_physical_oceanography\">rabernat/intro_to_physical_\n",
    "<em>oceanography</em> /a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f0e659c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(soup.find_all('div', {'class':'f4 text-normal'})) \n",
    "#GOOD - it gets 10 (which is our num results per page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c39b380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<div class=\"f4 text-normal\">\n",
       " <a class=\"v-align-middle\" data-hydro-click='{\"event_type\":\"search_result.click\",\"payload\":{\"page_number\":1,\"per_page\":10,\"query\":\"oceanography\",\"result_position\":1,\"click_id\":41701566,\"result\":{\"id\":41701566,\"global_relay_id\":\"MDEwOlJlcG9zaXRvcnk0MTcwMTU2Ng==\",\"model_name\":\"Repository\",\"url\":\"https://github.com/rabernat/intro_to_physical_oceanography\"},\"originating_url\":\"https://github.com/search?p=1&amp;q=oceanography&amp;type=Repositories\",\"user_id\":null}}' data-hydro-click-hmac=\"2429d6b0fa54733ee57c2aedde47122dabd955a1ee2d1f8e6f5bb61c633959b3\" href=\"/rabernat/intro_to_physical_oceanography\">rabernat/intro_to_physical_<em>oceanography</em></a>\n",
       " </div>,\n",
       " <div class=\"f4 text-normal\">\n",
       " <a class=\"v-align-middle\" data-hydro-click='{\"event_type\":\"search_result.click\",\"payload\":{\"page_number\":1,\"per_page\":10,\"query\":\"oceanography\",\"result_position\":2,\"click_id\":570570,\"result\":{\"id\":570570,\"global_relay_id\":\"MDEwOlJlcG9zaXRvcnk1NzA1NzA=\",\"model_name\":\"Repository\",\"url\":\"https://github.com/dankelley/oce\"},\"originating_url\":\"https://github.com/search?p=1&amp;q=oceanography&amp;type=Repositories\",\"user_id\":null}}' data-hydro-click-hmac=\"a07f1a10b0a5301ca8436cb14eadb9ffcda96117c883b18637fa2c448922ab85\" href=\"/dankelley/oce\">dankelley/oce</a>\n",
       " </div>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's look at the first 2\n",
    "soup.find_all('div', {'class':'f4 text-normal'})[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "836385e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.Tag"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#seems to grab what we want, so now we just need the href of each one\n",
    "type(soup.find_all('div', {'class':'f4 text-normal'})[0]) #tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "830f648b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"v-align-middle\" data-hydro-click='{\"event_type\":\"search_result.click\",\"payload\":{\"page_number\":1,\"per_page\":10,\"query\":\"oceanography\",\"result_position\":1,\"click_id\":41701566,\"result\":{\"id\":41701566,\"global_relay_id\":\"MDEwOlJlcG9zaXRvcnk0MTcwMTU2Ng==\",\"model_name\":\"Repository\",\"url\":\"https://github.com/rabernat/intro_to_physical_oceanography\"},\"originating_url\":\"https://github.com/search?p=1&amp;q=oceanography&amp;type=Repositories\",\"user_id\":null}}' data-hydro-click-hmac=\"2429d6b0fa54733ee57c2aedde47122dabd955a1ee2d1f8e6f5bb61c633959b3\" href=\"/rabernat/intro_to_physical_oceanography\">rabernat/intro_to_physical_<em>oceanography</em></a>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('div', {'class':'f4 text-normal'})[0].select_one('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0516670c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.Tag"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(soup.find_all('div', {'class':'f4 text-normal'})[0].select_one('a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bcbf18fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/rabernat/intro_to_physical_oceanography'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('div', {'class':'f4 text-normal'})[0].select_one('a')['href']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ab9f54",
   "metadata": {},
   "source": [
    "so that ^^ works.  but maybe I can go straight to the a tag?  let's see if that class is unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "644fe99e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(soup.find_all('a',{'class':'v-align-middle'}))\n",
    "#YAY - only 10!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "90915b69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/rabernat/intro_to_physical_oceanography'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Okay, so the shorter way is:\n",
    "soup.find_all('a',{'class':'v-align-middle'})[0]['href']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a9131e",
   "metadata": {},
   "source": [
    "**Now figure out next page** - seems straight forward, here's the anchor tag:\n",
    "\n",
    "class=\"next_page\" \n",
    "\n",
    "rel=\"next\" \n",
    "\n",
    "href=\"/search?p=2&amp;q=oceanography&amp;type=Repositories\">Next /a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "597b8930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"next_page\" href=\"/search?p=2&amp;q=oceanography&amp;type=Repositories\" rel=\"next\">Next</a>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('a',{'class':'next_page'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "80dffade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/search?p=2&q=oceanography&type=Repositories'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('a',{'class':'next_page'})['href']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b6317a",
   "metadata": {},
   "source": [
    "**Test logic for when next page doesn't exist:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0ea219e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is the last of many pages\n",
    "response2 = requests.get('https://github.com/search?p=90&q=oceanography&type=Repositories',headers=headers)\n",
    "response2.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f2d716f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup2 = BSoup(response2.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e94582a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "if not soup2.find('a',{'class':'next_page'}): print('hi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d569cdf",
   "metadata": {},
   "source": [
    "**okay so we need to check if none prior to grabbing the href**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9a69f673",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"Authorization\": f\"token {github_token}\", \"User-Agent\": github_username}\n",
    "\n",
    "def get_repo_links(soup):\n",
    "    '''\n",
    "    Given the soup of a repo search page, get a list urls for all the repos in the results\n",
    "    \n",
    "    Output: Returns LIST of partial URLs\n",
    "    Parameters: (R) soup: Beautiful Soup object of the page contents\n",
    "    '''\n",
    "    pg_repos = []\n",
    "    #loop over all our 'a' tags\n",
    "    for a in soup.find_all('a',{'class':'v-align-middle'}):\n",
    "        #grab the href and strip first character >> append to repo list\n",
    "        pg_repos.append(a['href'][1:])\n",
    "    #return the list \n",
    "    return pg_repos\n",
    "        \n",
    "def get_oceanography_repos():\n",
    "    '''\n",
    "    Gets a list of oceanography repos from Github.  \n",
    "    Must have your own github token and username in a local env file. \n",
    "    Output: Returns LIST of strings - with section of URL in format 'user/repo'\n",
    "    '''\n",
    "    #set starting point\n",
    "    #we know there are multiple pages, so including query parameter 'p'\n",
    "    p=1\n",
    "    url = f'https://github.com/search?p={p}&q=oceanography&type=Repositories'\n",
    "    \n",
    "    #FIRST PAGE\n",
    "    #get page info\n",
    "    response = requests.get(url, headers=headers)\n",
    "    #SOUP, there it is!\n",
    "    soup = BSoup(response.text)\n",
    "    \n",
    "    #grab links for first page\n",
    "    repo_list = get_repo_links(soup)\n",
    "    \n",
    "    #NEXT PAGE - \n",
    "    #set domain\n",
    "    domain = 'https://github.com'\n",
    "    #get first next page\n",
    "    np_tag = soup.find('a',{'class':'next_page'})\n",
    "    #IF that element exists, go to and repeat!\n",
    "    while np_tag:\n",
    "        #create url for next page\n",
    "        url = domain + np_tag['href']\n",
    "        \n",
    "        #get new soup\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BSoup(response.text)\n",
    "        \n",
    "        #add new links to our list\n",
    "        repo_list += get_repo_links(soup)\n",
    "        \n",
    "        #break loop if we have > X links\n",
    "        if len(repo_list) >= 300: break\n",
    "        \n",
    "        #grab next page\n",
    "        np_tag = soup.find('a',{'class':'next_page'})\n",
    "        time.sleep(7)\n",
    "        \n",
    "    return repo_list\n",
    "    \n",
    "def create_REPOS():\n",
    "    filename = 'repos.csv'\n",
    "    #see if file exists\n",
    "    if os.path.isfile(filename):\n",
    "        df = pd.read_csv(filename,header=None)\n",
    "        return df.iloc[:0].tolist()\n",
    "    else:\n",
    "        #get new list of repos\n",
    "        mylist = get_oceanography_repos()\n",
    "        #save to CSV\n",
    "        pd.DataFrame(mylist).to_csv(filename,header=False,index=False)\n",
    "        return mylist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f78681d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://github.com/search?p=2&q=oceanography&type=Repositories\n",
      "200\n",
      "20\n",
      "<a class=\"next_page\" href=\"/search?p=3&amp;q=oceanography&amp;type=Repositories\" rel=\"next\">Next</a>\n",
      "https://github.com/search?p=3&q=oceanography&type=Repositories\n",
      "200\n",
      "30\n",
      "<a class=\"next_page\" href=\"/search?p=4&amp;q=oceanography&amp;type=Repositories\" rel=\"next\">Next</a>\n",
      "https://github.com/search?p=4&q=oceanography&type=Repositories\n",
      "200\n",
      "40\n",
      "<a class=\"next_page\" href=\"/search?p=5&amp;q=oceanography&amp;type=Repositories\" rel=\"next\">Next</a>\n",
      "https://github.com/search?p=5&q=oceanography&type=Repositories\n",
      "200\n",
      "50\n",
      "<a class=\"next_page\" href=\"/search?p=6&amp;q=oceanography&amp;type=Repositories\" rel=\"next\">Next</a>\n",
      "https://github.com/search?p=6&q=oceanography&type=Repositories\n",
      "200\n",
      "60\n",
      "<a class=\"next_page\" href=\"/search?p=7&amp;q=oceanography&amp;type=Repositories\" rel=\"next\">Next</a>\n",
      "https://github.com/search?p=7&q=oceanography&type=Repositories\n",
      "200\n",
      "70\n",
      "<a class=\"next_page\" href=\"/search?p=8&amp;q=oceanography&amp;type=Repositories\" rel=\"next\">Next</a>\n",
      "https://github.com/search?p=8&q=oceanography&type=Repositories\n",
      "200\n",
      "80\n",
      "<a class=\"next_page\" href=\"/search?p=9&amp;q=oceanography&amp;type=Repositories\" rel=\"next\">Next</a>\n",
      "https://github.com/search?p=9&q=oceanography&type=Repositories\n",
      "200\n",
      "90\n",
      "<a class=\"next_page\" href=\"/search?p=10&amp;q=oceanography&amp;type=Repositories\" rel=\"next\">Next</a>\n",
      "https://github.com/search?p=10&q=oceanography&type=Repositories\n",
      "200\n",
      "100\n",
      "<a class=\"next_page\" href=\"/search?p=11&amp;q=oceanography&amp;type=Repositories\" rel=\"next\">Next</a>\n",
      "https://github.com/search?p=11&q=oceanography&type=Repositories\n",
      "200\n",
      "110\n",
      "<a class=\"next_page\" href=\"/search?p=12&amp;q=oceanography&amp;type=Repositories\" rel=\"next\">Next</a>\n",
      "https://github.com/search?p=12&q=oceanography&type=Repositories\n",
      "200\n",
      "120\n",
      "<a class=\"next_page\" href=\"/search?p=13&amp;q=oceanography&amp;type=Repositories\" rel=\"next\">Next</a>\n",
      "https://github.com/search?p=13&q=oceanography&type=Repositories\n",
      "200\n",
      "130\n",
      "<a class=\"next_page\" href=\"/search?p=14&amp;q=oceanography&amp;type=Repositories\" rel=\"next\">Next</a>\n",
      "https://github.com/search?p=14&q=oceanography&type=Repositories\n",
      "200\n",
      "140\n",
      "<a class=\"next_page\" href=\"/search?p=15&amp;q=oceanography&amp;type=Repositories\" rel=\"next\">Next</a>\n",
      "https://github.com/search?p=15&q=oceanography&type=Repositories\n",
      "200\n",
      "150\n",
      "<a class=\"next_page\" href=\"/search?p=16&amp;q=oceanography&amp;type=Repositories\" rel=\"next\">Next</a>\n",
      "https://github.com/search?p=16&q=oceanography&type=Repositories\n",
      "200\n",
      "160\n",
      "<a class=\"next_page\" href=\"/search?p=17&amp;q=oceanography&amp;type=Repositories\" rel=\"next\">Next</a>\n",
      "https://github.com/search?p=17&q=oceanography&type=Repositories\n",
      "200\n",
      "170\n",
      "<a class=\"next_page\" href=\"/search?p=18&amp;q=oceanography&amp;type=Repositories\" rel=\"next\">Next</a>\n",
      "https://github.com/search?p=18&q=oceanography&type=Repositories\n",
      "200\n",
      "180\n",
      "<a class=\"next_page\" href=\"/search?p=19&amp;q=oceanography&amp;type=Repositories\" rel=\"next\">Next</a>\n",
      "https://github.com/search?p=19&q=oceanography&type=Repositories\n",
      "200\n",
      "190\n",
      "<a class=\"next_page\" href=\"/search?p=20&amp;q=oceanography&amp;type=Repositories\" rel=\"next\">Next</a>\n",
      "https://github.com/search?p=20&q=oceanography&type=Repositories\n",
      "200\n",
      "200\n",
      "<a class=\"next_page\" href=\"/search?p=21&amp;q=oceanography&amp;type=Repositories\" rel=\"next\">Next</a>\n",
      "https://github.com/search?p=21&q=oceanography&type=Repositories\n",
      "200\n",
      "210\n",
      "<a class=\"next_page\" href=\"/search?p=22&amp;q=oceanography&amp;type=Repositories\" rel=\"next\">Next</a>\n",
      "https://github.com/search?p=22&q=oceanography&type=Repositories\n",
      "200\n",
      "220\n",
      "<a class=\"next_page\" href=\"/search?p=23&amp;q=oceanography&amp;type=Repositories\" rel=\"next\">Next</a>\n",
      "https://github.com/search?p=23&q=oceanography&type=Repositories\n",
      "200\n",
      "230\n",
      "<a class=\"next_page\" href=\"/search?p=24&amp;q=oceanography&amp;type=Repositories\" rel=\"next\">Next</a>\n",
      "https://github.com/search?p=24&q=oceanography&type=Repositories\n",
      "200\n",
      "240\n",
      "<a class=\"next_page\" href=\"/search?p=25&amp;q=oceanography&amp;type=Repositories\" rel=\"next\">Next</a>\n",
      "https://github.com/search?p=25&q=oceanography&type=Repositories\n",
      "200\n",
      "250\n",
      "<a class=\"next_page\" href=\"/search?p=26&amp;q=oceanography&amp;type=Repositories\" rel=\"next\">Next</a>\n",
      "https://github.com/search?p=26&q=oceanography&type=Repositories\n",
      "200\n",
      "260\n",
      "<a class=\"next_page\" href=\"/search?p=27&amp;q=oceanography&amp;type=Repositories\" rel=\"next\">Next</a>\n",
      "https://github.com/search?p=27&q=oceanography&type=Repositories\n",
      "200\n",
      "270\n",
      "<a class=\"next_page\" href=\"/search?p=28&amp;q=oceanography&amp;type=Repositories\" rel=\"next\">Next</a>\n",
      "https://github.com/search?p=28&q=oceanography&type=Repositories\n",
      "200\n",
      "280\n",
      "<a class=\"next_page\" href=\"/search?p=29&amp;q=oceanography&amp;type=Repositories\" rel=\"next\">Next</a>\n",
      "https://github.com/search?p=29&q=oceanography&type=Repositories\n",
      "200\n",
      "290\n",
      "<a class=\"next_page\" href=\"/search?p=30&amp;q=oceanography&amp;type=Repositories\" rel=\"next\">Next</a>\n",
      "https://github.com/search?p=30&q=oceanography&type=Repositories\n",
      "200\n",
      "300\n",
      "<a class=\"next_page\" href=\"/search?p=31&amp;q=oceanography&amp;type=Repositories\" rel=\"next\">Next</a>\n",
      "https://github.com/search?p=31&q=oceanography&type=Repositories\n",
      "200\n",
      "310\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['haloissufo/TOOLBOXES',\n",
       " 'panmits86/thesis_1',\n",
       " 'clase69/ocesat',\n",
       " 'jbarthelotte/DFO-Moorings',\n",
       " 'jklymak/Eos314']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing = get_oceanography_repos()\n",
    "testing[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "01215ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "87ac9abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(testing).to_csv('testing.csv',header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5fefab4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rabernat/intro_to_physical_oceanography</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dankelley/oce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wafo-project/pywafo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ocefpaf/python4oceanographers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>matplotlib/cmocean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>haloissufo/TOOLBOXES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>panmits86/thesis_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>clase69/ocesat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>jbarthelotte/DFO-Moorings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>jklymak/Eos314</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>310 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           0\n",
       "0    rabernat/intro_to_physical_oceanography\n",
       "1                              dankelley/oce\n",
       "2                        wafo-project/pywafo\n",
       "3              ocefpaf/python4oceanographers\n",
       "4                         matplotlib/cmocean\n",
       "..                                       ...\n",
       "305                     haloissufo/TOOLBOXES\n",
       "306                       panmits86/thesis_1\n",
       "307                           clase69/ocesat\n",
       "308                jbarthelotte/DFO-Moorings\n",
       "309                           jklymak/Eos314\n",
       "\n",
       "[310 rows x 1 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('testing.csv',header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1868a62c",
   "metadata": {},
   "source": [
    "After adding some troubleshooting code, it looks like github is limiting my scrapping as I get a 429 response code (too many requests), so I added a pause."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aced6854",
   "metadata": {},
   "source": [
    "##### NOW TEST THIS!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f5844040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rabernat/intro_to_physical_oceanography',\n",
       " 'dankelley/oce',\n",
       " 'wafo-project/pywafo',\n",
       " 'ocefpaf/python4oceanographers',\n",
       " 'matplotlib/cmocean']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myrepos = create_REPOS()\n",
    "myrepos[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "91264b7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(myrepos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3e70e3",
   "metadata": {},
   "source": [
    "### Functions for scraping repo data after URLs were generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64808770",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A module for obtaining repo readme and language data from the github API.\n",
    "Before using this module, read through it, and follow the instructions marked\n",
    "TODO.\n",
    "After doing so, run it like this:\n",
    "    python acquire.py\n",
    "To create the `data.json` file that contains the data.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# TODO: Make a github personal access token.\n",
    "#     1. Go here and generate a personal access token https://github.com/settings/tokens\n",
    "#        You do _not_ need select any scopes, i.e. leave all the checkboxes unchecked\n",
    "#     2. Save it in your env.py file under the variable `github_token`\n",
    "# TODO: Add your github username to your env.py file under the variable `github_username`\n",
    "# TODO: Add more repositories to the `REPOS` list below.\n",
    "\n",
    "REPOS = []\n",
    " \n",
    "\n",
    "headers = {\"Authorization\": f\"token {github_token}\", \"User-Agent\": github_username}\n",
    "\n",
    "if headers[\"Authorization\"] == \"token \" or headers[\"User-Agent\"] == \"\":\n",
    "    raise Exception(\n",
    "        \"You need to follow the instructions marked TODO in this script before trying to use it\"\n",
    "    )\n",
    "\n",
    "\n",
    "def github_api_request(url: str) -> Union[List, Dict]:\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response_data = response.json()\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\n",
    "            f\"Error response from github api! status code: {response.status_code}, \"\n",
    "            f\"response: {json.dumps(response_data)}\"\n",
    "        )\n",
    "    return response_data\n",
    "\n",
    "\n",
    "def get_repo_language(repo: str) -> str:\n",
    "    url = f\"https://api.github.com/repos/{repo}\"\n",
    "    repo_info = github_api_request(url)\n",
    "    if type(repo_info) is dict:\n",
    "        repo_info = cast(Dict, repo_info)\n",
    "        return repo_info.get(\"language\", None)\n",
    "    raise Exception(\n",
    "        f\"Expecting a dictionary response from {url}, instead got {json.dumps(repo_info)}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def get_repo_contents(repo: str) -> List[Dict[str, str]]:\n",
    "    url = f\"https://api.github.com/repos/{repo}/contents/\"\n",
    "    contents = github_api_request(url)\n",
    "    if type(contents) is list:\n",
    "        contents = cast(List, contents)\n",
    "        return contents\n",
    "    raise Exception(\n",
    "        f\"Expecting a list response from {url}, instead got {json.dumps(contents)}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def get_readme_download_url(files: List[Dict[str, str]]) -> str:\n",
    "    \"\"\"\n",
    "    Takes in a response from the github api that lists the files in a repo and\n",
    "    returns the url that can be used to download the repo's README file.\n",
    "    \"\"\"\n",
    "    for file in files:\n",
    "        if file[\"name\"].lower().startswith(\"readme\"):\n",
    "            return file[\"download_url\"]\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def process_repo(repo: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Takes a repo name like \"gocodeup/codeup-setup-script\" and returns a\n",
    "    dictionary with the language of the repo and the readme contents.\n",
    "    \"\"\"\n",
    "    contents = get_repo_contents(repo)\n",
    "    readme_contents = requests.get(get_readme_download_url(contents)).text\n",
    "    return {\n",
    "        \"repo\": repo,\n",
    "        \"language\": get_repo_language(repo),\n",
    "        \"readme_contents\": readme_contents,\n",
    "    }\n",
    "\n",
    "\n",
    "def scrape_github_data() -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Loop through all of the repos and process them. Returns the processed data.\n",
    "    \"\"\"\n",
    "    return [process_repo(repo) for repo in REPOS]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = scrape_github_data()\n",
    "    json.dump(data, open(\"data2.json\", \"w\"), indent=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
