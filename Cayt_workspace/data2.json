[
 {
  "repo": "rabernat/intro_to_physical_oceanography",
  "language": "Jupyter Notebook",
  "readme_contents": "# Intro to Physical Oceanography\n\nThis repository contains course materials for EESC4925. The lecture notes are in the form of interactive Jupyter Notebooks.\n\n**New** The class notes are now compiled as a Jupyter Book: http://rabernat.github.io/intro_to_physical_oceanography\n\nGithub source: https://github.com/rabernat/intro_to_physical_oceanography\n"
 },
 {
  "repo": "dankelley/oce",
  "language": "R",
  "readme_contents": "\n\n\n# oce <img src=\"https://raw.githubusercontent.com/dankelley/oce/develop/oce-logo-3.png\" align=\"right\" height=\"95\" />\n\n<!-- badges: start -->\n\n[![Project Status: Active \u2013 The project has reached a stable, usable\nstate and is being actively\ndeveloped.](http://www.repostatus.org/badges/latest/active.svg)](https://www.repostatus.org/)\n[![status](https://joss.theoj.org/papers/10.21105/joss.03594/status.svg)](https://joss.theoj.org/papers/10.21105/joss.03594)\n[![R-CMD-check](https://github.com/dankelley/oce/workflows/R-CMD-check/badge.svg)](https://github.com/dankelley/oce/actions)\n[![Codecov test\ncoverage](https://codecov.io/gh/dankelley/oce/branch/develop/graph/badge.svg)](https://app.codecov.io/gh/dankelley/oce?branch=develop)\n[![CRAN\\_Status\\_Badge](https://www.r-pkg.org/badges/version/oce)](https://cran.r-project.org/package=oce)\n![RStudio CRAN mirror\ndownloads](https://cranlogs.r-pkg.org/badges/last-month/oce) ![RStudio\nCRAN mirror downloads](https://cranlogs.r-pkg.org/badges/last-week/oce)\n![RStudio CRAN mirror\ndownloads](https://cranlogs.r-pkg.org/badges/last-day/oce)\n<!-- badges: end -->\n\n## Why use R for oceanographic analysis?\n\nThe R language is popular in many branches of science, and Oceanography is no\nexception. With its broad statistical support, R is a natural choice for\noceanographers in the biological, chemical and geological sub-disciplines.\nHowever, some physical oceanographers have remained attached to Matlab, which\nwas widely adopted during the 1990s. Lately, this has been changing, as\noceanographers turn to open-source systems such as Python and R. A particular\nstrength of R is its provision of many powerful and well-vetted packages for\nhandling specialized calculations. The oce package is a prime example.\n\n## About oce\n\nThe oce package handles a wide variety of tasks that come up in the analysis of\nOceanographic data. In addition to the present README file, a brief sketch of\nthe package has been written by the core developers (Kelley Dan E., Clark\nRichards and Chantelle Layton, 2022. [oce: an R package for Oceanographic\nAnalysis](https://doi.org/10.21105/joss.03594). Journal of Open Source\nSoftware, 7(71), 3594), and the primary developer uses the package extensively\nin his book about the place of R in oceanographic analysis\n(Kelley, Dan E., 2018.\n[Oceanographic Analysis with R](https://link.springer.com/book/10.1007/978-1-4939-8844-0).\nNew York. Springer-Verlag ISBN 978-1-4939-8844-0).  Details of oce functions\nare provided within the R help system, and in the package\n[webpage](https://dankelley.github.io/oce/).\n\n## Installing oce\n\nStable versions of oce are available from CRAN, and may be installed\nfrom within R, in the same way as other packages. However, the CRAN\nversion is only updated a few times a year (pursuant to policy), so many\nusers install the `\"develop\"` branch instead. This branch may be updated\nseveral times per day, as the authors fix bugs or add features that are\nmotivated by day-to-day usage. This is the branch favoured by users who\nneed new features or who would wish to contribute to Oce development.\n\nThe easy way to install the `\"develop\"` branch is to execute the\nfollowing commands in R.\n\n    remotes::install_github(\"dankelley/oce\", ref=\"develop\")\n\nand most readers should also install Ocedata, with\n\n    remotes::install_github(\"dankelley/ocedata\", ref=\"main\")\n\n## Evolution of oce\n\nOce is emphatically an open-source system, and so the participation of\nusers is very important. This is why Git is used for version control of\nthe Oce source code, and why GitHub is the host for that code. Users are\ninvited to take part in the development process, by suggesting features,\nby reporting bugs, or just by watching as others do such things.\nOceanography is a collaborative discipline, so it makes sense that the\nevolution of Oce be similarly collaborative.\n\n## Examples using built-in datasets\n\n### CTD\n\n    library(oce)\n    data(ctd)\n    plot(ctd, which=c(1,2,3,5), type=\"l\", span=150)\n\n![Sample CTD plot.](https://raw.githubusercontent.com/dankelley/oce/develop/oce-demo-1.png)\n\n\n### Acoustic Doppler profiler\n\n    library(oce)\n    data(adp)\n    plot(adp)\n\n![Sample adp plot.](https://raw.githubusercontent.com/dankelley/oce/develop/oce-demo-2.png)\n\n### Sealevel and tides\n\n    library(oce)\n    data(sealevel)\n    m <- tidem(sealevel)\n    par(mfrow=c(2, 1))\n    plot(sealevel, which=1)\n    plot(m)\n\n![Sample sealevel plot.](https://raw.githubusercontent.com/dankelley/oce/develop/oce-demo-3.png)\n\n### Echosounder\n\n    library(oce)\n    data(echosounder)\n    plot(echosounder, which=2, drawTimeRange=TRUE, drawBottom=TRUE)\n\n![Sample echosounder plot.](https://raw.githubusercontent.com/dankelley/oce/develop/oce-demo-4.png)\n\n### Map\n\n    library(oce)\n    par(mar=rep(0.5, 4))\n    data(endeavour, package=\"ocedata\")\n    data(coastlineWorld, package=\"oce\")\n    mapPlot(coastlineWorld, col=\"gray\")\n    mapPoints(endeavour$longitude, endeavour$latitude, pch=20, col=\"red\")\n\n![Sample map plot.](https://raw.githubusercontent.com/dankelley/oce/develop/oce-demo-5.png)\n\n### Landsat image\n\n    library(ocedata)\n    library(oce)\n    data(landsat)\n    plot(landsat)\n\n![Sample landsat image plot.](https://raw.githubusercontent.com/dankelley/oce/develop/oce-demo-6.png)\n\n"
 },
 {
  "repo": "wafo-project/pywafo",
  "language": "Jupyter Notebook",
  "readme_contents": "\n|wafo_logo|\n==========================================\nWave Analysis for Fatigue and Oceanography\n==========================================\n\n|pkg_img| |tests_img| |docs_img| |health_img| |coverage_img| |versions_img| |downloads_img|\n\n\nDescription\n===========\n\nWAFO is a toolbox Python routines for statistical analysis and simulation of\nrandom waves and random loads. WAFO is freely redistributable software, see WAFO\nicence, cf. the GNU General Public License (GPL) and contain tools for:\n\nFatigue Analysis\n----------------\n\n- Fatigue life prediction for random loads\n- Theoretical density of rainflow cycles\n\nSea modelling\n-------------\n\n- Simulation of linear and non-linear Gaussian waves\n- Estimation of seamodels (spectrums)\n- Joint wave height, wave steepness, wave period distributions\n\nStatistics\n------------\n\n- Extreme value analysis\n- Kernel density estimation\n- Hidden markov models\n\nClasses\n-------\nA short description of the main classes found in WAFO:\n\n\n* TimeSeries:\n    Data analysis of time series. Example: extraction of turning points,\n    estimation of spectrum and covariance function. Estimation transformation\n    used in transformed Gaussian model.\n\n* CovData:\n    Computation of spectral functions, linear and non-linear time series\n    simulation.\n\n* SpecData:\n    Computation of spectral moments and covariance functions, linear and\n    non-linear time series simulation. Ex: common spectra implemented,\n    directional spectra, bandwidth measures, exact distributions for wave\n    characteristics.\n\n* CyclePairs:\n    Cycle counting, discretization, and crossings, calculation of damage.\n    Simulation of discrete Markov chains, switching Markov chains,\n    harmonic oscillator. Ex:  Rainflow cycles and matrix, discretization of\n    loads. Damage of a rainflow count or matrix, damage matrix, S-N plot.\n\n\nSubpackages\n-----------\nA short descriptions the subpackages of WAFO:\n\n* TRANSFORM\n    Modelling with linear or transformed Gaussian waves.\n* STATS\n    Statistical tools and extreme-value distributions. Ex: generation of random\n    numbers, estimation of parameters, evaluation of pdf and cdf\n* KDETOOLS\n    Kernel-density estimation.\n* MISC\n    Miscellaneous routines.\n* DOCS\n    Documentation of toolbox, definitions. An overview is given in the routine\n    wafomenu.\n* DATA\n    Measurements from marine applications.\n\nWAFO homepage: <http://www.maths.lth.se/matstat/wafo/>\nOn the WAFO home page you will find:\n- The WAFO Tutorial\n- List of publications related to WAFO.\n\nInstallation\n============\n\nWAFO contains some Fortran and C extensions that require a properly configured\ncompiler and NumPy/f2py.\n\nCreate a binary wheel package and place it in the dist folder as follows::\n\n    python setup.py bdist_wheel -d dist\n\nAnd install the wheel package with::\n\n    pip install dist/wafo-X.Y.Z+abcd123-os_platform.whl\n\nGetting started\n===============\n\nA quick introduction to some of the many features of wafo can be found in the Tutorial IPython notebooks in the\n`tutorial scripts folder`_:\n\n* Chapter 1 - `Some applications of WAFO`_\n\n* Chapter 2 - `Modelling random loads and stochastic waves`_\n\n* Chapter 3 - `Demonstrates distributions of wave characteristics`_\n\n* Chapter 4 - `Fatigue load analysis and rain-flow cycles`_\n\n* Chapter 5 - `Extreme value analysis`_\n\n-- _tutorial scripts folder: http://nbviewer.jupyter.org/github/wafo-project/pywafo/tree/master/src/wafo/doc/tutorial_scripts/\n\n.. _Some applications of WAFO: http://nbviewer.jupyter.org/github/wafo-project/pywafo/blob/master/src/wafo/doc/tutorial_scripts/WAFO%20Chapter%201.ipynb\n\n.. _Modelling random loads and stochastic waves: http://nbviewer.jupyter.org/github/wafo-project/pywafo/blob/master/src/wafo/doc/tutorial_scripts/WAFO%20Chapter%202.ipynb\n\n.. _Demonstrates distributions of wave characteristics: http://nbviewer.jupyter.org/github/wafo-project/pywafo/blob/master/src/wafo/doc/tutorial_scripts/WAFO%20Chapter%203.ipynb\n\n.. _Fatigue load analysis and rain-flow cycles: http://nbviewer.jupyter.org/github/wafo-project/pywafo/blob/master/src/wafo/doc/tutorial_scripts/WAFO%20Chapter%204.ipynb\n\n.. _Extreme value analysis: http://nbviewer.jupyter.org/github/wafo-project/pywafo/blob/master/src/wafo/doc/tutorial_scripts/WAFO%20Chapter%205.ipynb\n\n\nUnit tests\n==========\n\nTo test if the toolbox is working paste the following in an interactive\npython session::\n\n   import wafo as wf\n   wf.test(coverage=True, doctests=True)\n\n\n.. |wafo_logo| image:: https://github.com/wafo-project/pywafo/blob/master/src/wafo/data/wafoLogoNewWithoutBorder.png\n    :target: https://github.com/wafo-project/pywafo\n\n\n.. |pkg_img| image:: https://badge.fury.io/py/wafo.png\n    :target: https://pypi.python.org/pypi/wafo/\n\n.. |tests_img| image:: https://travis-ci.org/wafo-project/pywafo.svg?branch=master\n    :target: https://travis-ci.org/wafo-project/pywafo\n\n.. |docs_img| image:: https://readthedocs.org/projects/pip/badge/?version=latest\n    :target: http://pywafo.readthedocs.org/en/latest/\n\n.. |health_img| image:: https://codeclimate.com/github/wafo-project/pywafo/badges/gpa.svg\n   :target: https://codeclimate.com/github/wafo-project/pywafo\n   :alt: Code Climate\n\n.. |coverage_img| image:: https://coveralls.io/repos/wafo-project/pywafo/badge.svg?branch=master\n   :target: https://coveralls.io/github/wafo-project/pywafo?branch=master\n\n.. |versions_img| image:: https://img.shields.io/pypi/pyversions/wafo.svg\n   :target: https://github.com/wafo-project/pywafo\n\n\n.. |downloads_img| image:: https://img.shields.io/pypi/dm/wafo.svg\n   :alt: PyPI - Downloads\n\n"
 },
 {
  "repo": "ocefpaf/python4oceanographers",
  "language": "Jupyter Notebook",
  "readme_contents": "python4oceanographers\n=====================\n\nLearn python with examples applied to oceanography.\n"
 },
 {
  "repo": "matplotlib/cmocean",
  "language": "Python",
  "readme_contents": "cmocean\n=======\n\nDocumentation available: [http://matplotlib.org/cmocean/](http://matplotlib.org/cmocean/).\n\nWe have a paper with guidelines to colormap selection for your application and a description of the `cmocean` colormaps:\nThyng, K. M., Greene, C. A., Hetland, R. D., Zimmerle, H. M., & DiMarco, S. F. (2016). True colors of oceanography. Oceanography, 29(3), 10.\nlink: [http://tos.org/oceanography/assets/docs/29-3_thyng.pdf](http://tos.org/oceanography/assets/docs/29-3_thyng.pdf)\n\nBesides Python, the cmocean colormaps are also available:\n\n* For [MATLAB](http://www.mathworks.com/matlabcentral/fileexchange/57773-cmocean-perceptually-uniform-colormaps) by [Chad Greene](http://www.chadagreene.com/)\n* For [R cmocean](https://github.com/aitap/cmocean), which includes [ggplot2](ggplot2.tidyverse.org/) compatible functions. Also included in [Oce](http://dankelley.github.io/oce/): an oceanographic analysis package by [Dan Kelley](http://www.dal.ca/faculty/science/oceanography/people/faculty/daniel-e-kelley.html) and [Clark Richards](http://clarkrichards.org/).\n* For Julia, included in [Plots.jl](https://github.com/JuliaPlots/Plots.jl) and [Makie.jl](https://github.com/JuliaPlots/Makie.jl)\n* For [Ocean Data Viewer](https://github.com/kthyng/cmocean-odv)\n* For Generic Mapping Tools (GMT)  at [cpt-city](http://soliton.vm.bytemark.co.uk/pub/cpt-city/cmocean/index.html) and on [github](https://github.com/kthyng/cmocean-gmt)\n* For [Paraview](https://github.com/kthyng/cmocean-paraview) inspired by [Phillip Wolfram](https://github.com/pwolfram)\n* In [Plotly](https://plot.ly/python/cmocean-colorscales/)\n* Chad Greene's [Antarctic Mapping Tools](http://www.mathworks.com/matlabcentral/fileexchange/47638-antarctic-mapping-tools) in Matlab uses `cmocean`\n* For [Tableau](https://www.tableau.com) as a preferences file on [github](https://github.com/shaunwbell/cmocean_tableau)\n* For [ImageJ](https://imagej.nih.gov/ij/) as [LUTs](https://github.com/mikeperrins/cmocean-LUT-ImageJ)\n* For [ncview](http://meteora.ucsd.edu/~pierce/ncview_home_page.html) via [ncmaps](https://github.com/TomLav/ncmaps).\n* For [SeaDAS](https://github.com/gertvd-stanford/cmocean_seadas), and should work with BEAM/SNAP as well.\n\n\nTo install:\n``pip install cmocean``\n\nTo install with Anaconda:\n``conda install -c conda-forge cmocean``\n\nIf you want to be able to use the `plots` submodule, you can instead install with:\n\n`pip install \"cmocean[plots]\"`\n\nwhich will also install `viscm` and `colorspacious`.\n"
 },
 {
  "repo": "stoqs/stoqs",
  "language": "Jupyter Notebook",
  "readme_contents": "Spatial Temporal Oceanographic Query System\n-------------------------------------------\n\n[![Build Status](https://travis-ci.org/stoqs/stoqs.svg)](https://travis-ci.org/stoqs/stoqs/branches)\n[![Requirements Status](https://requires.io/github/stoqs/stoqs/requirements.svg?branch=master)](https://requires.io/github/stoqs/stoqs/requirements/?branch=master)\n[![PyUp](https://pyup.io/repos/github/MBARIMike/stoqs/shield.svg)](https://pyup.io/account/repos/github/MBARIMike/stoqs/)\n[![DOI](https://zenodo.org/badge/20654/stoqs/stoqs.svg)](https://zenodo.org/badge/latestdoi/20654/stoqs/stoqs)\n \nSTOQS is a geospatial database and web application designed to give oceanographers\nefficient integrated access to *in situ* measurement and *ex situ* sample data.\nSee http://www.stoqs.org.\n\n#### Getting started with a STOQS development system \n\nFirst, install [Vagrant](https://www.vagrantup.com/) and and [VirtualBox](https://www.virtualbox.org/)\n&mdash; there are standard installers for Mac, Windows, and Linux. (You will also need \n[X Windows System](doc/instructions/XWINDOWS.md) sofware on your computer.) Then create an empty folder off your \nhome directory such as `Vagrants/stoqsvm`, open a command prompt window, cd to that folder, and enter these \ncommands:\n\n```bash\ncurl \"https://raw.githubusercontent.com/stoqs/stoqs/master/Vagrantfile\" -o Vagrantfile\ncurl \"https://raw.githubusercontent.com/stoqs/stoqs/master/provision.sh\" -o provision.sh\nvagrant plugin install vagrant-vbguest\nvagrant up --provider virtualbox\n```\nThe Vagrantfile and provision.sh will provision a development system with an NFS mounted\ndirectory from your host operating system. If your host doesn't support serving files via\nNFS (most Windows hosts don't support NFS file serving) then you'll need to edit these files \nbefore executing `vagrant up`. Look for the `support NFS file serving` comments in these \nfiles for the lines you need to change.\n\nThe `vagrant up` command takes an hour or so to provision and setup a complete CentOS 7 \nSTOQS Virtual Machine that also includes MB-System, InstantReality, and all the Python data science \ntools bundled in packages such as Anaconda.  You will be prompted for your admin password\nfor configuring a shared folder from the VM (unless you've disabled the NFS mount).  All connections to this VM are \nperformed from the the directory you installed it in; you must cd to it (e.g. `cd\n~/Vagrants/stoqsvm`) before logging in with the `vagrant ssh -- -X` command.  After \ninstallation finishes log into your new VM and test it:\n\n```bash\nvagrant ssh -- -X                        # Wait for [vagrant@localhost ~]$ prompt\nexport STOQS_HOME=/vagrant/dev/stoqsgit  # Use STOQS_HOME=/home/vagrant/dev/stoqsgit if not using NFS mount\ncd $STOQS_HOME && source venv-stoqs/bin/activate\nexport DATABASE_URL=postgis://stoqsadm:CHANGEME@127.0.0.1:5438/stoqs\n./test.sh CHANGEME load noextraload\n```\n\nIn another terminal window start the development server (after a `cd ~/Vagrants/stoqsvm`):\n\n```bash\nvagrant ssh -- -X                        # Wait for [vagrant@localhost ~]$ prompt\nexport STOQS_HOME=/vagrant/dev/stoqsgit  # Use STOQS_HOME=/home/vagrant/dev/stoqsgit if not using NFS mount\ncd $STOQS_HOME && source venv-stoqs/bin/activate\nexport DATABASE_URL=postgis://stoqsadm:CHANGEME@127.0.0.1:5438/stoqs\nstoqs/manage.py runserver 0.0.0.0:8000 --settings=config.settings.local\n```\n\nVisit your server's STOQS User Interface using your host computer's browser:\n\n    http://localhost:8008\n\nMore instructions are in the doc/instructions directory &mdash; see [LOADING](doc/instructions/LOADING.md) \nfor specifics on loading your own data. For example, you may create your own database of an archived MBARI campaign:\n\n    cd stoqs\n    ln -s mbari_campaigns.py campaigns.py\n    loaders/load.py --db stoqs_cce2015\n\nYou are encouraged to contribute to the STOQS project! Please see [CONTRIBUTING](CONTRIBUTING.md)\nfor how to share your work.  Also, see example \n[Jupyter Notebooks](http://nbviewer.jupyter.org/github/stoqs/stoqs/blob/master/stoqs/contrib/notebooks)\nthat demonstrate specific analyses and visualizations that go beyond the capabilities of the STOQS User Interface.\nVisit the [STOQS Wiki pages](https://github.com/stoqs/stoqs/wiki) for updates and links to presentations.\nThe [stoqs-discuss](https://groups.google.com/forum/#!forum/stoqs-discuss) list in Google Groups is also \na good place to ask questions and engage in discussion with the STOQS user and developer communities.\n\nSupported by the David and Lucile Packard Foundation, STOQS undergoes continual development\nto help support the mission of the Monterey Bay Aquarium Research Institue.  If you have your\nown server you will occasionally want to get new features with:\n\n```bash\ngit pull\n```\n\n#### Production Deployment with Docker\n\nFirst, install [Docker](https://www.docker.com/) and [docker-compose](https://docs.docker.com/compose/install/)\non your system.  Then clone the repository; in the docker directory copy the `template.env` file to `.env` \nand edit it for your specific installation, then execute `docker-compose up`:\n\n```bash\ngit clone https://github.com/stoqs/stoqs.git stoqsgit\ncd stoqsgit/docker\ncp template.env .env\nchmod 600 .env      # You must then edit .env and change settings for your environment\ndocker-compose up\n```\nIf the directory set to the STOQS_VOLS_DIR variable in your .env file doesn't exist then the \nexecution of `docker-compose up` will create the postgresql database cluster, load a default \nstoqs database, and execute the unit and functional tests of the stoqs application.  If you\ndon't see these tests being executed (they will take several minutes) then check for error\nmessages.\n\nOnce you see `... [emperor] vassal /etc/uwsgi/django-uwsgi.ini is ready to accept requests`\nyou can visit the site at https://localhost &mdash; it uses a self-signed certificate, so your\nbrowser will complain and you will need to add an exception. (The nginx service also delivers \nthe same app at http://localhost:8000 without the certificate issue.)\n\nThe default settings in `template.env` will run a production nginx/uwsgi/stoqs server configured\nfor https://localhost in a Vagrant virtual machine. To configure a server for intranet or public serving of\nyour data follow the instructions provided in the comments for the settings in your `.env` file.\nAfter editing your `.env` file you will need to rebuild the images and restart the Docker \nservices, this time with the `-d` option to run the containers in the background:\n\n```bash\ndocker-compose build\ndocker-compose up -d\n```\n\nThe above commands should also be done following a `git pull` in order to deploy updated\nsoftware on your server.\n\nOne thing that's good to do is monitor logs and check for error messages, this can be done with:\n\n```\ndocker-compose logs -f\n```\n\n#### Using STOQS in Docker\n\nYou can execute Python code in the stoqs server from your host by prefacing it with `docker-compose exec stoqs`\n(Use `docker-compose run stoqs` to launch another container for long-running processes), for \nexample to load some existing MBARI campaign data:\n\n```bash\ndocker-compose run stoqs stoqs/loaders/load.py --db stoqs_simz_aug2013\n```\n\n(To load MBARI Campaigns you will need to have uncommented the `CAMPAIGNS_MODULE=stoqs/mbari_campaigns.py` \nline in your .env file. Make sure that you do not have a symbolic link named `campaigns.py` in the stoqs \ndirectory. This is needed only for a Vagrant development machine &mdash; it's best to keep the directory used\nfor a Docker deployment separate from one used for Vagrant.)\n\nIn another window monitor its output:\n\n```bash\ndocker-compose run stoqs tail -f /srv/stoqs/loaders/MolecularEcology/loadSIMZ_aug2013.out\n# Or (The stoqs code is bound as a volume in the container from the GitHub cloned location)\ntail -f stoqsgit/stoqs/loaders/MolecularEcology/loadSIMZ_aug2013.out\n```\n\nYou may also use `pg_restore` to more quickly load an existing Campaign database on your system.\nFor instructions click on the Campaign name in the top bar of a Campaign on another STOQS server, \nfor example on [MBARI's Public STOQS Server](https://stoqs.mbari.org).\n\n\n\nIf you use STOQS for your research please cite this publication:\n\n> McCann, M.; Schramm, R.; Cline, D.; Michisaki, R.; Harvey, J.; Ryan, J., \"Using STOQS (The spatial \n> temporal oceanographic query system) to manage, visualize, and understand AUV, glider, and mooring data,\" \n> in *Autonomous Underwater Vehicles (AUV), 2014 IEEE/OES*, pp.1-10, 6-9 Oct. 2014\n> doi: 10.1109/AUV.2014.7054414\n\n![STOQS logo](stoqs/static/images/STOQS_logo_gray1_689.png)\n\n"
 },
 {
  "repo": "aodn/imos-toolbox",
  "language": "MATLAB",
  "readme_contents": "# IMOS Toolbox\n\nThe IMOS Toolbox aims at **converting oceanographic instrument files into quality-controlled IMOS compliant NetCDF files**. \n\nThe toolbox process instruments deployed on moorings (**time series**) or during casts (**profiles**). The processing, including quality control (**QC**) of several oceanographic variables, can be done in **batch mode** or **interactive**.\n\nFinally, the package allows deployment metadata to be ingested into the files from any JDBC supported database (including MS-access), CSV files, or added manually through a graphical user interface (GUI). Manual **QC** is also possible.\n\nSee our [wiki page](https://github.com/aodn/imos-toolbox/wiki) page for more details.\n\n# Distribution\n\nThe **stable** releases may be obtained [here](https://github.com/aodn/imos-toolbox/releases). The releases contain both the source code and binary applications (executables).\n\n## Licensing\nThis project is licensed under the terms of the GNU GPLv3 license.\n# Requirements\n\nWe support Windows and Linux. The toolbox may be used as a **Matlab stand-alone library** or **stand-alone application** (**No Matlab is required**).\n\nFor a **stand-alone library** usage, **Matlab R2018b** or newer is required ( since version 2.6.1).\n\nFor a **stand-alone application**, you will need the Matlab Component Runtime (v95).\n\nSee [installation instructions](https://github.com/aodn/imos-toolbox/wiki/ToolboxInstallation) for further information.\n\n# Usage\n\nThe toolbox can connect to a deployment database to collect the relevant metadata attached to each dataset. We provide an MS-Access database file template and an underlying schema, but several types of databases can be used. The Java code interface can use any JDBC API to query the deployment database. By default, [UCanAccess](http://ucanaccess.sourceforge.net/site.html) is used to query the MS-Access file.\n\nPlease read the [wiki](https://github.com/aodn/imos-toolbox/wiki) for more information on how to **use** the toolbox. \n\nThis project is designed and maintained by [ANMN](http://imos.org.au/facilities/nationalmooringnetwork/) and [AODN](http://imos.org.au/facilities/aodn/).\n\n# License\n\nThe toolbox is copyrighted and licensed under the terms of the GNU GPLv3. For more details, click [here](https://raw.githubusercontent.com/aodn/imos-toolbox/master/license.txt).\n"
 },
 {
  "repo": "castelao/CoTeDe",
  "language": "Python",
  "readme_contents": "======\nCoTeDe\n======\n\n.. image:: https://joss.theoj.org/papers/10.21105/joss.02063/status.svg\n   :target: https://doi.org/10.21105/joss.02063\n\n.. image:: https://zenodo.org/badge/10284681.svg\n   :target: https://zenodo.org/badge/latestdoi/10284681\n\n.. image:: https://readthedocs.org/projects/cotede/badge/?version=latest\n   :target: https://cotede.readthedocs.io/en/latest/?badge=latest\n   :alt: Documentation Status\n\n.. image:: https://img.shields.io/travis/castelao/CoTeDe.svg\n   :target: https://travis-ci.org/castelao/CoTeDe\n\n.. image:: https://codecov.io/gh/castelao/CoTeDe/branch/master/graph/badge.svg\n   :target: https://codecov.io/gh/castelao/CoTeDe\n\n.. image:: https://img.shields.io/pypi/v/cotede.svg\n   :target: https://pypi.python.org/pypi/cotede\n\n.. image:: https://mybinder.org/badge_logo.svg\n   :target: https://mybinder.org/v2/gh/castelao/CoTeDe/master?filepath=docs%2Fnotebooks\n\n\n`CoTeDe <http://cotede.castelao.net>`_ is an Open Source Python package to quality control (QC) oceanographic data such as temperature and salinity.\nIt was designed to attend individual scientists as well as real-time operations on large data centers.\nTo achieve that, CoTeDe is highly customizable, giving the user full control to compose the desired set of tests including the specific parameters of each test, or choose from a list of preset QC procedures.\n\nI believe that we can do better than we have been doing with more flexible classification techniques, which includes machine learning. My goal is to minimize the burden on manual expert QC improving the consistency, performance, and reliability of the QC procedure for oceanographic data, especially for real-time operations.\n\nCoTeDe is the result from several generations of quality control systems that started in 2006 with real-time QC of TSGs and were later expanded for other platforms including CTDs, XBTs, gliders, and others.\n\n\n----------\nWhy CoTeDe\n----------\n\nCoTeDe contains several QC procedures that can be easily combined in different ways:\n\n- Pre-set standard tests according to the recommendations by GTSPP, EGOOS, XBT, Argo or QARTOD;\n- Custom set of tests, including user defined thresholds;\n- Two different fuzzy logic approaches: as proposed by Timms et. al 2011 & Morello et. al. 2014, and using usual defuzification by the bisector;\n- A novel approach based on Anomaly Detection, described by `Castelao 2021 <https://doi.org/10.1016/j.cageo.2021.104803>`_ (available since 2014 `<http://arxiv.org/abs/1503.02714>`_).\n\nEach measuring platform is a different realm with its own procedures, metadata, and meaningful visualization. \nSo CoTeDe focuses on providing a robust framework with the procedures and lets each application, and the user, to decide how to drive the QC.\nFor instance, the `pySeabird package <http://seabird.castelao.net>`_ is another package that understands CTD and uses CoTeDe as a plugin to QC.\n\n-------------\nDocumentation\n-------------\n\nA detailed documentation is available at http://cotede.readthedocs.org, while a collection of notebooks with examples is available at\nhttp://nbviewer.ipython.org/github/castelao/CoTeDe/tree/master/docs/notebooks/\n\n--------\nCitation\n--------\n\nIf you use CoTeDe, or replicate part of it, in your work/package, please consider including the reference:\n\nCastel\u00e3o, G. P., (2020). A Framework to Quality Control Oceanographic Data. Journal of Open Source Software, 5(48), 2063, https://doi.org/10.21105/joss.02063\n\n::\n\n  @article{Castelao2020,\n    doi = {10.21105/joss.02063},\n    url = {https://doi.org/10.21105/joss.02063},\n    year = {2020},\n    publisher = {The Open Journal},\n    volume = {5},\n    number = {48},\n    pages = {2063},\n    author = {Guilherme P. Castelao},\n    title = {A Framework to Quality Control Oceanographic Data},\n    journal = {Journal of Open Source Software}\n  }\n\nFor the Anomaly Detection techinique specifically, which was implemented in CoTeDe, please include the reference:\n\nCastel\u00e3o, G. P. (2021). A Machine Learning Approach to Quality Control Oceanographic Data. Computers & Geosciences, https://doi.org/10.1016/j.cageo.2021.104803\n\n::\n\n  @article{Castelao2021,\n    doi = {10.1016/j.cageo.2021.104803},\n    url = {https://doi.org/10.1016/j.cageo.2021.104803},\n    year = {2021},\n    publisher = {Elsevier},\n    author = {Guilherme P. Castelao},\n    title = {A Machine Learning Approach to Quality Control Oceanographic Data},\n    journal = {Computers and Geosciences}\n  }\n\nIf you are concerned about reproducibility, please include the DOI provided by Zenodo on the top of this page, which is associated with a specific release (version).\n"
 },
 {
  "repo": "FinalTheory/oceanography-numerical-calculations",
  "language": "HTML",
  "readme_contents": "Calculation of Marine and Hydrologic Factors\n============================================\n\n\n\n## \u4e00\u3001[\u6f6e\u6c50\u7684\u6700\u5c0f\u4e8c\u4e58\u8c03\u548c\u5206\u6790](http://nbviewer.jupyter.org/github/FinalTheory/oceanography-numerical-calculations/blob/master/Harmonic%20Analysis/HomeWork1.ipynb)\n\n- \u57fa\u4e8eFortran\u4e0ePython\u7684\u6df7\u5408\u7f16\u7a0b\n- \u4f7f\u7528Numpy\u7684Array\u5bf9\u8c61\u4f5c\u4e3a\u901a\u7528\u7684\u6570\u636e\u5b58\u50a8\u5bb9\u5668\n- \u5728IPython Notebook\u4e0a\u5b8c\u6210\u57fa\u672c\u7a0b\u5e8f\u7684\u7f16\u5199\u548c\u7f16\u8bd1\n- \u6700\u540e\u8f93\u51fa\u4e3aipynb\u6587\u4ef6\uff0c\u65b9\u4fbf\u5404\u79cd\u5171\u4eab\u4ee5\u53ca\u91cd\u73b0\u7ed3\u679c\n- \u6240\u6709\u6d89\u53ca\u7684\u6570\u636e\u6587\u4ef6\u548c\u4ee3\u7801\u5747\u53ef\u4ee5\u4ece\u8be5Repo\u4e2d\u83b7\u53d6\uff0c\u4e5f\u53ef\u4ee5\u76f4\u63a5\u6253\u5305\u4e0b\u8f7d\n\n\n## \u4e8c\u3001[\u5317\u592a\u5e73\u6d0b\u6d77\u533aSST\u6570\u636e\u7684EOF\u5206\u89e3\u4ee5\u53ca\u5206\u6790](http://nbviewer.jupyter.org/github/FinalTheory/oceanography-numerical-calculations/blob/master/EOF%20Analysis/HomeWork2.ipynb)\n\n- \u4f7f\u7528Basemap\u7ed8\u56fe\u5e93\u5b8c\u6210\u4f5c\u56fe\n- \u5206\u6790\u5199\u5f97\u76f8\u5f53\u6c34\uff0c\u8fd8\u8bf7\u89c1\u8c05\n- \u4f7f\u7528eofs\u5de5\u5177\u8fdb\u884cEOF\u5206\u89e3\uff0c\u6548\u7387\u66f4\u9ad8\uff1b\u5728\u505a\u5927\u89c4\u6a21\u8ba1\u7b97\u65f6\uff0c\u76f8\u5bf9\u4e0ematlab\u7a0b\u5e8f\u53ef\u4ee5\u51cf\u5c11\u4e00\u534a\u7684\u5185\u5b58\u5360\u7528\uff0c\u8282\u7701\u5341\u500d\u7684\u65f6\u95f4\n\n## \u4e09\u3001\u4e8c\u7ef4\u6f6e\u6ce2\u6570\u503c\u6a21\u62df\u7a0b\u5e8f\n\n- \u4f7f\u7528Fortran\u8bed\u8a00\u5b8c\u6210\u547d\u4ee4\u884c\u8fd0\u7b97\u7a0b\u5e8f\u7684\u7f16\u5199\n- \u8be5\u8fd0\u7b97\u6838\u5fc3\u63a5\u53d717\u4e2a\u8f93\u5165\u53c2\u6570\uff0c\u7528\u4ee5\u51b3\u5b9a\u662f\u5426\u5ffd\u7565\u8fd0\u52a8\u65b9\u7a0b\u4e2d\u7684\u67d0\u4e9b\u9879\uff0c\u4ee5\u53ca\u5404\u4e2a\u7cfb\u6570\u7684\u53d6\u503c\u7b49\n- \u4f7f\u7528Python\u7684Tkinter\u6a21\u5757\u7f16\u5199\u56fe\u5f62\u754c\u9762\uff0c\u7528numpy\u8bfb\u53d6\u5e76\u5904\u7406\u6570\u636e\uff0c\u5e76\u4e0eMatPlotLib\u8fdb\u884c\u6574\u5408\u4ee5\u5b9e\u73b0\u7ed8\u56fe\u3001\u52a8\u753b\u7b49\u529f\u80fd\n- \u8fd0\u884c\u6548\u679c\u5982\u4e0b\uff1a\n<img src=\"https://github.com/FinalTheory/oceanography-numerical-calculations/raw/master/Numerical%20Simulation/demonstration.gif\" width=1130>\n\n## \u56db\u3001\u4e8c\u7ef4\u6f6e\u6ce2\u4f34\u968f\u540c\u5316\u6a21\u578b\n"
 },
 {
  "repo": "introocean/introocean-en",
  "language": "PostScript",
  "readme_contents": "[![License: CC BY-NC-SA 4.0](https://img.shields.io/badge/License-CC%20BY--NC--SA%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by-nc-sa/4.0/)\n[![Latest Release](https://img.shields.io/badge/-Latest%20Release-lightgrey.svg)](https://github.com/introocean/introocean-en/releases/latest)\n\n\n# Introduction to Physical Oceanography, open-source edition\n**Based on the work of Robert Stewart**\n\nRobert Stewart's \"Introduction to Physical Oceanography\" was \nthe first oceanography textbook for many students. \nIt was always the free high-quality source of basic \noceanographic knowledge. Robert Stewart retired in \n2009 and did not update the book after that. \nWith his permission, we now publish the latest \nversion of the book's LaTeX source code on GitHub. \nDr. Stewart gives his permission to update the book\nby fixing errors and adding new information. He has \nonly two conditions:\n\n> Don\u2019t let the book grow into an encyclopedia. \nIt is an INTRODUCTION, leaving details to more\nexpert and focused publications (which could also\nbe on the web server). Keep the book focused on the\nmost important issues students need to know about.\nToday, satellite oceanography needs to be emphasized\nas most of our knowledge comes from satellites and \ndrifters. \n\n> My only request is that the book not be offered\nfor sale. It should be open source and available \nto everyone at no cost.\n\nWe are going to update the book through the usual\nGitHub procedure with PRs and code review. Any\nhelp and contributions from the oceanographic \ncommunity will be greatly appreciated.\n"
 },
 {
  "repo": "OceanLabPy/OceanLab",
  "language": "Python",
  "readme_contents": "# OceanLab\n\nPackage of Python scripts for Oceanography  (Python +3.6)\n\n## Code Example\n\nCheck `examples` folder in our [github repository](../../tree/master/examples).\n\n## Installation\n\n`pip install OceanLab`\n\n## Modules\n\n- **OA**\n  - *vectoa()*: Objective analysis for vectorial fields;\n  - *scaloa()*: Objective analysis for scalar fields;\n- **DYN**\n  - *dyn_amp()*: Makes the projection of every dynamical mode to velocity to obtain its amplitude;\n  - *zeta()*: Calculates the vorticity field by velocity field;\n  - *psi2uv()*: Calculates the velocity field by stream function scalar field;\n  - *vmodes()*: Calculates the QG pressure modes from N2 profile;\n  - *eqmodes()*: Calculates the equatorial pressure and vertical velocity modes from N2 profile;\n- **EOF**\n  - *eoft()*: Calculates the Empirical Orthogonal Functions;\n  - *my_eof_interp()*: Fillgaps on matrix based on EOFs (translated from Cesar Rocha Matlab version);\n  - *ceof()*: Performs the Complex (or Hilbert) Empirical Orthogonal Functions decomposition;\n  - *reconstruct_ceof()*: Reconstructs the CEOF modes individually;\n- **UTILS**\n  - *argdistnear()*: Searchs the position of the closest points in an array to a reference point;\n  - *meaneddy()*: Performs an eddy-mean decomposition with a low-pass filter;\n\n\n## Contributors\n\nEveryone can contribute to this code. Some of functions were based on Filipe Fernandes or Cesar Rocha functions and some of them were created with help of Dante C. Napolitano, H\u00e9lio M. R. Almeida and Wandrey Watanabe at Ocean Dynamics Lab of University of S\u00e3o Paulo (USP).\n"
 },
 {
  "repo": "TEOS-10/GSW-Fortran",
  "language": "Fortran",
  "readme_contents": "==========================================================================\r\n Gibbs SeaWater (GSW) Oceanographic Toolbox of TEOS-10 (Fortran)\r\n==========================================================================\r\n\r\n This is a subset of functions contained in the Gibbs SeaWater (GSW)\r\n Oceanographic Toolbox of TEOS-10.\r\n\r\n Version 1.0 written by David Jackett\r\n Modified by Paul Barker (version 3.02)\r\n Modified by Glenn Hyland (version 3.04+)\r\n\r\n For help with this Oceanographic Toolbox email: help@teos-10.org\r\n\r\n This software is available from http://www.teos-10.org\r\n\r\n==========================================================================\r\n\r\n gsw_data_v3_0.nc\r\n NetCDF file that contains the global data set of Absolute Salinity Anomaly\r\n Ratio, the global data set of Absolute Salinity Anomaly atlas, and check\r\n values and computation accuracy values for use in gsw_check_function.\r\n The data set gsw_data_v3_0.nc must not be tampered with.\r\n\r\n gsw_check_function.f90\r\n Contains the check functions. We suggest that after downloading, unzipping\r\n and installing the toolbox the user runs this program to ensure that the\r\n toolbox is installed correctly and there are no conflicts. This toolbox has\r\n been tested to compile and run with gfortran.\r\n\r\n==========================================================================\r\n\r\n Build using Make:\r\n cd test\r\n make\r\n ./gsw_check\r\n ./poly_check\r\n\r\n==========================================================================\r\n\r\n Build using CMake:\r\n   static and shared libraries\r\n   build 'gsw_check_functions' and 'gsw_poly_check' on demand\r\n   provides a package configuration\r\n   Git submodules support\r\n   Window (Visual Studio), Mac and Linux support\r\n\r\n Windows (using a terminal):\r\n   mkdir build; cd build\r\n   cmake ..\r\n   Open gsw.sln in Visual Studio\r\n\r\n Mac, Linux:\r\n   mkdir build; cd build\r\n   cmake ..\r\n   make\r\n   ctest\r\n\r\n Use a Git submodule (using extern/gsw as target folder):\r\n   git submodule add https://github.com/TEOS-10/GSW-Fortran extern/gsw\r\n   git submodule init\r\n   git submodule update\r\n    ... add e.g. target_link_libraries(my_target STATIC gsw_static)\r\n    ... in your CMakeLists.txt. It is not necessary to compile and install\r\n    ... the TEOS-10/GSW library - the compilation and linking will be done\r\n    ... due to the target_link_libraries() statement.\r\n\r\n Note that gfortran is the name of the GNU Fortran project, developing a\r\n free Fortran 95/2003/2008 compiler for GCC, the GNU Compiler Collection.\r\n It is available from http://gcc.gnu.org/fortran/\r\n\r\n==========================================================================\r\n"
 },
 {
  "repo": "njwilson23/narwhal",
  "language": "Python",
  "readme_contents": "# Narwhal\n\n[![Build Status](https://travis-ci.org/njwilson23/narwhal.svg?branch=master)](https://travis-ci.org/njwilson23/narwhal)\n\n## Oceanographic data analysis in Python\n\nNarwhal is a Python module built on [pandas](http://pandas.pydata.org/) and\n[matplotlib](http://matplotlib.org/). Narwhal is designed for manipulating and\nvisualizing oceanographic data.\n\nData are organized into self-describing `Cast` and `CastCollection` data\nstructures. Convenience methods and functions are included for:\n\n- interpolation\n- density and depth calculation\n- buoyancy frequency estimation\n- baroclinic mode analysis\n- water type fraction inversion\n\n### Quickly visualize results\n\nThe `narwhal.plotting` submodule contains convenience methods for creating T-S\ndiagrams, cast plots, and section plots. Here's some data from off the coast of\nnortheastern Greenland:\n\n![T-S diagram](https://github.com/njwilson23/narwhal/raw/gh-pages/images/ts-plot2_v7.png)\n\n![Section diagram](https://github.com/njwilson23/narwhal/raw/gh-pages/images/sill_velocity.png)\n\n### Python wrapper for the thermodynamic equation of state\n\nNarwhal provides a *ctypes* wrapper for the\n[Gibbs Seawater Toolbox](http://www.teos-10.org/pubs/gsw/html/gsw_contents.html)\nin the `narwhal.gsw` submodule, making things like the following possible:\n\n    density = narwhal.gsw.rho(cast[\"sa\"], cast[\"ct\"], cast[\"p\"])\n\nCurrently, GSW 3.05 is packaged with Narwhal.\n\n### Data should not be tied to software\n\nFor storage, data is serialized to JSON or HDF files. These common formats are\nopen and easily imported into other analysis packages (such as MATLAB), or\nvisualization libraries (such as D3).\n\n## Installation\n\n    git clone https://github.com/njwilson23/narwhal.git\n    pip install -r narwhal/requirements.txt\n    pip install narwhal\n\n### Dependencies\n\n- Python 2.7+ or Python 3.4+\n- pandas\n- matplotlib\n- scipy\n- requests\n- dateutil\n- six\n- C-compiler (for GSW)\n- h5py (optional, required for HDF read/write)\n\nIf [Karta](https://github.com/fortyninemaps/karta) is installed, it will be used\nfor fast and accurate geographical calculations.\n\nNarwhal is experimental. See also\n[python-oceans](https://github.com/ocefpaf/python-oceans) and\n[oce](https://github.com/dankelley/oce) (R).\n"
 },
 {
  "repo": "MITgcm/xmitgcm",
  "language": "Python",
  "readme_contents": "xmitgcm: Read MITgcm mds binary files into xarray\n=================================================\n\n|pypi| |Build Status| |codecov| |docs| |DOI|\n\nxmitgcm is a python package for reading MITgcm_ binary MDS files into\nxarray_ data structures. By storing data in dask_ arrays, xmitgcm enables\nparallel, out-of-core_ analysis of MITgcm output data.\n\nLinks\n-----\n\n-  HTML documentation: https://xmitgcm.readthedocs.org\n-  Issue tracker: https://github.com/MITgcm/xmitgcm/issues\n-  Source code: https://github.com/MITgcm/xmitgcm\n\nInstallation\n------------\n\nRequirements\n^^^^^^^^^^^^\n\nxmitgcm is compatible with python >=3.7. It requires xarray_\n(>= version 0.14.1) and dask_ (>= version 1.0).\nThese packages are most reliably installed via the\n`conda <https://conda.pydata.org/docs/>`_ environment management\nsystem, which is part of the Anaconda_ python distribution. Assuming you have\nconda available on your system, the dependencies can be installed with the\ncommand::\n\n    conda install xarray dask\n\nIf you are using earlier versions of these packages, you should update before\ninstalling xmitgcm.\n\nInstallation via pip\n^^^^^^^^^^^^^^^^^^^^\n\nIf you just want to use xmitgcm, the easiest way is to install via pip::\n\n    pip install xmitgcm\n\nThis will automatically install the latest release from\n`pypi <https://pypi.python.org/pypi>`_.\n\nInstallation from github\n^^^^^^^^^^^^^^^^^^^^^^^^\n\nxmitgcm is under active development. To obtain the latest development version,\nyou may clone the `source repository <https://github.com/MITgcm/xmitgcm>`_\nand install it::\n\n    git clone https://github.com/MITgcm/xmitgcm.git\n    cd xmitgcm\n    python setup.py install\n\nUsers are encouraged to `fork <https://help.github.com/articles/fork-a-repo/>`_\nxmitgcm and submit issues_ and `pull requests`_.\n\nQuick Start\n-----------\n\nFirst make sure you understand what an xarray_ Dataset object is. Then find\nsome MITgcm MDS data. If you don't have any data of your own, you can download\nthe xmitgcm\n`test repositories <https://figshare.com/articles/xmitgcm_test_datasets/4033530>`_\nTo download the some test data, run the shell commands::\n\n    $ curl -L -J -O https://ndownloader.figshare.com/files/6494718\n    $ tar -xvzf global_oce_latlon.tar.gz\n\nThis will create a directory called ``global_oce_latlon`` which we will use\nfor the rest of these examples. If you have your own data, replace this with\nthe path to your mitgcm files.\n\nTo open MITgcm MDS data as an xarray.Dataset, do the following in python::\n\n    from xmitgcm import open_mdsdataset\n    data_dir = './global_oce_latlon'\n    ds = open_mdsdataset(data_dir)\n\n``data_dir``, should be the path (absolute or relative) to an\nMITgcm run directory. xmitgcm will automatically scan this directory and\ntry to determine the file prefixes and iteration numbers to read. In some\nconfigurations, the ``open_mdsdataset`` function may work without further\nkeyword arguments. In most cases, you will have to specify further details.\n\nConsult the `online documentation <https://xmitgcm.readthedocs.org>`_ for\nmore details.\n\n.. |DOI| image:: https://zenodo.org/badge/70649781.svg\n   :target: https://zenodo.org/badge/latestdoi/70649781\n.. |Build Status| image:: https://travis-ci.org/MITgcm/xmitgcm.svg?branch=master\n   :target: https://travis-ci.org/MITgcm/xmitgcm\n   :alt: travis-ci build status\n.. |codecov| image:: https://codecov.io/github/MITgcm/xmitgcm/coverage.svg?branch=master\n   :target: https://codecov.io/github/MITgcm/xmitgcm?branch=master\n   :alt: code coverage\n.. |pypi| image:: https://badge.fury.io/py/xmitgcm.svg\n   :target: https://badge.fury.io/py/xmitgcm\n   :alt: pypi package\n.. |docs| image:: https://readthedocs.org/projects/xmitgcm/badge/?version=stable\n   :target: https://xmitgcm.readthedocs.org/en/stable/?badge=stable\n   :alt: documentation status\n\n.. _dask: https://dask.pydata.org\n.. _xarray: https://xarray.pydata.org\n.. _Comodo: https://pycomodo.forge.imag.fr/norm.html\n.. _issues: https://github.com/MITgcm/xmitgcm/issues\n.. _`pull requests`: https://github.com/MITgcm/xmitgcm/pulls\n.. _MITgcm: http://mitgcm.org/public/r2_manual/latest/online_documents/node277.html\n.. _out-of-core: https://en.wikipedia.org/wiki/Out-of-core_algorithm\n.. _Anaconda: https://www.continuum.io/downloads\n"
 },
 {
  "repo": "kthyng/python4geosciences",
  "language": "Roff",
  "readme_contents": "# python4geosciences\nPython for the Geosciences, a class offered at Texas A&M University in the Oceanography department.\n\n**Course Topics, Calendar of Activities, Major Assignment Dates (subject to change)**  \nHomework is typically due every Friday night at midnight\n\n\nWeek 0\u20132 (Aug 27/29, Sep 3/5/10/12):  Course intro; Python basics \u2014 Core language  \n*Homework 00 due Aug 30, hw01 due Sep 6, hw02 due Sep 13*  \nUsing Jupyter notebooks and JupyterHub.  Overview of the standard Python programming language, standard data containers (lists, tuples, dictionaries, etc), importing packages, for/while loops, functions, and object oriented programming (objects as containers for data and associated functions).  \nMaterials: \n- [Intro](https://redfish.geos.tamu.edu:8000/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fkthyng%2Fpython4geosciences&subPath=materials%2F0_intro.ipynb&app=notebook)\n- [Core](https://redfish.geos.tamu.edu:8000/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fkthyng%2Fpython4geosciences&subPath=materials%2F1_core.ipynb&app=notebook)   \n\nWeek 3\u20134 (Sep 17/19/24/26):  Numerical Python   \n*hw03 due Sep 20, hw04 due Sep 27*  \nNumpy, vector operations, data types, and array broadcasting.  \nMaterials: \n- [numpy](https://redfish.geos.tamu.edu:8000/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fkthyng%2Fpython4geosciences&subPath=materials%2F2_numpy.ipynb&app=notebook)\n\nWeek 5 (Oct 1): Review  \n*hw05 due Oct 4*  \nReview core language and numpy.\n\nWeek 5\u20137 (Oct 3/8/10/15):  Basic plotting in Python with matplotlib  \n*hw06 due Oct 11, hw07 due Oct 18, Email project plan by Oct 18*  \nOverview of the matplotlib plotting package: 1D (line plots, histograms), 2D (contours, pcolor).  \nMaterials: \n- [matplotlib](https://redfish.geos.tamu.edu:8000/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fkthyng%2Fpython4geosciences&subPath=materials%2F3_matplotlib.ipynb&app=notebook)\n- [interpolation](https://redfish.geos.tamu.edu:8000/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fkthyng%2Fpython4geosciences&subPath=materials%2FST_interpolation.ipynb&app=notebook)\n\nWeek 7\u20139 (Oct 17/22/24/29):  1D time series analysis  \n*hw08 due Oct 25, hw09 due Nov 1*  \npandas, indexing, averaging.  \nMaterials: \n- [pandas, 1d](https://redfish.geos.tamu.edu:8000/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fkthyng%2Fpython4geosciences&subPath=materials%2F4_pandas.ipynb&app=notebook)\n\nWeek 9\u201310 (Oct 31, Nov 5):  Review  \n*hw10 due Nov 8*  \nReview core, numpy, matplotlib, and pandas.\n\nWeek 10-12 (Nov 7/12/14/19): 2D geospatial plotting  \n*hw11 due Nov 15*  \nCartopy mapping package and shapefiles. xarray: reading and writing NetCDF files locally and over the internet.  \nMaterials: \n- [maps](https://redfish.geos.tamu.edu:8000/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fkthyng%2Fpython4geosciences&subPath=materials%2F5_maps.ipynb&app=notebook)\n- [shapefiles on maps](https://redfish.geos.tamu.edu:8000/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fkthyng%2Fpython4geosciences&subPath=materials%2F7_shapefiles.ipynb&app=notebook)\n- [netCDF and xarray](https://redfish.geos.tamu.edu:8000/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fkthyng%2Fpython4geosciences&subPath=materials%2F6_xarray.ipynb&app=notebook)\n\nWeek 12-13 (Nov 21/26, No class Nov 28): Python beyond the notebook  \nAnaconda package installer, iPython for terminal window, writing scripts, Jupyterlab; debugging, unit testing.  \nMaterials: \n- [beyond the notebook](https://redfish.geos.tamu.edu:8000/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fkthyng%2Fpython4geosciences&subPath=materials%2FST_beyond_notebook.ipynb&app=notebook)\n\nWeek 14 (Dec 3): Share projects in groups  \n*hw12/project due Dec 3 (undergrads and grads)*  \nShare projects in groups and get feedback. Attendance is required.\n"
 },
 {
  "repo": "ngs-docs/2016-metagenomics-sio",
  "language": "HTML",
  "readme_contents": "2016 / October / Metagenomics\n=============================\n\nThis workshop was given on October 12th and 13th, 2016,\nby Harriet Alexander and C. Titus Brown, at the Scripps Institute of\nOceanography.\n\nSee https://2016-metagenomics-sio.readthedocs.io/en/latest/ for the\nrendered version of this site.\n\n----\n\nTo look into:\n\n* https://github.com/chrisquince/DESMAN for binning\n* https://github.com/DRL/blobtools - blobtools viz\n"
 },
 {
  "repo": "SciTools/python-stratify",
  "language": "Jupyter Notebook",
  "readme_contents": "# Stratify\n\nInterpolation for restratification, particularly useful for Nd vertical interpolation of atmospheric and oceanographic datasets\n\n[![cirrus-ci](https://api.cirrus-ci.com/github/SciTools-incubator/python-stratify.svg)](https://cirrus-ci.com/github/SciTools-incubator/python-stratify)\n[![conda-forge](https://img.shields.io/conda/vn/conda-forge/python-stratify?color=orange&label=conda-forge&logo=conda-forge&logoColor=white)](https://anaconda.org/conda-forge/python-stratify)\n[![PyPI](https://img.shields.io/pypi/v/stratify?color=orange&label=pypi&logo=python&logoColor=white)](https://pypi.org/project/stratify/)\n[![codecov](https://codecov.io/gh/SciTools-incubator/python-stratify/branch/master/graph/badge.svg?token=v1R1bJ4kYr)](https://codecov.io/gh/SciTools-incubator/python-stratify)\n[![pre-commit.ci status](https://results.pre-commit.ci/badge/github/SciTools-incubator/python-stratify/master.svg)](https://results.pre-commit.ci/latest/github/SciTools-incubator/python-stratify/master)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n[![License](https://img.shields.io/github/license/SciTools-incubator/python-stratify?style=plastic)](https://github.com/SciTools-incubator/python-stratify/blob/master/LICENSE)\n[![Contributors](https://img.shields.io/github/contributors/SciTools-incubator/python-stratify?style=plastic)](https://github.com/SciTools-incubator/python-stratify/graphs/contributors)\n[![Binder](http://mybinder.org/badge.svg)](http://mybinder.org:/repo/scitools-incubator/python-stratify)\n\n## Introduction\n\nDiscover the capabilities of `stratify` with this introductory [Jupyter Notebook](https://github.com/SciTools-incubator/python-stratify/blob/master/index.ipynb).\n\n![](https://SciTools-incubator.github.io/python-stratify/summary.png)\n\n## Installation\n\n```shell\nconda install -c conda-forge python-stratify\n```\n```shell\npip install python-stratify\n```\n\n## License\nStratify is licenced under a [BSD 3-Clause License](LICENSE).\n\n"
 },
 {
  "repo": "obidam/ds2-2022",
  "language": "Jupyter Notebook",
  "readme_contents": "# DS2 Class 2022, Big Data & Cloud Computing for Oceanography\n\nHome of the 2022 ISblue Big Data & Cloud Computing for Oceanography class (IMT-A, ENSTA, IUEM) given by:\n\n- Pierre Tandeo - pierre.tandeo@imt-atlantique.fr\n- Fr\u00e9d\u00e9ric Paul - frederic.paul@ifremer.fr\n- Sally Close - sally.close@univ-brest.fr\n- Guillaume Maze - Guillaume.Maze@ifremer.fr\n- Carlos\u00a0Granero Belinchon\u00a0- carlos.granero-belinchon@imt-atlantique.fr\n\nThis repo is a place holder for the class practice session and for projects developed by students.\n\n## Practice notebooks\n\nSee https://github.com/obidam/ds2-2022/blob/main/practice/README.md\n\n## Projects\n\nSee https://github.com/obidam/ds2-2022/blob/main/project/README.md\n\n***\n<img src=\"https://github.com/obidam/ds2-2022/raw/main/logo_isblue.jpg\">"
 },
 {
  "repo": "TEOS-10/GSW-Matlab",
  "language": "MATLAB",
  "readme_contents": "# GSW-Matlab (Code only)\nGibbs-SeaWater (GSW) Oceanographic Toolbox in Matlab - Code only !!!\n\nThis is a work in progress repository, thie files contained in this repository are only the code file (.m files)  contained in GSW(Matlab). It is intended to be a method of assisting those who are translating the code into their prefered language.\n\nDo not download this code and treat it as an distributed release of the GSW code - it is not under any circumstance.\n\nThe ONLY location to download software is from the TEOS-10 website http://www.TEOS-10.org/\n\nPaul and Trevor.\n"
 },
 {
  "repo": "hainegroup/oceanspy",
  "language": "Jupyter Notebook",
  "readme_contents": ".. _readme:\n\n======================================================================================\nOceanSpy - A Python package to facilitate ocean model data analysis and visualization.\n======================================================================================\n\n|OceanSpy|\n\n|version| |conda forge| |docs| |CI| |pre-commit| |codecov| |black| |license| |doi| |JOSS| |binder|\n\n.. admonition:: Interactive Demo\n\n   Check out the interactive demonstration of OceanSpy at `www.bndr.it/nakt7 <https://bndr.it/nakt7>`_\n\nFor publications, please cite the following paper:  \n\nAlmansi, M., R. Gelderloos, T. W. N. Haine, A. Saberi, and A. H. Siddiqui (2019). OceanSpy: A Python package to facilitate ocean model data analysis and visualization. *Journal of Open Source Software*, 4(39), 1506, doi: https://doi.org/10.21105/joss.01506 .\n\nThis material is based upon work supported by the National Science Foundation under Grant Numbers 1835640, 124330, 118123, and 1756863. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.\n\nWhat is OceanSpy?\n-----------------\n**OceanSpy** is an open-source and user-friendly Python package that enables scientists and interested amateurs to analyze and visualize ocean model datasets. \nOceanSpy builds on software packages developed by the Pangeo_ community, in particular xarray_, dask_, and xgcm_. \nThe integration of dask facilitates scalability, which is important for the petabyte-scale simulations that are becoming available. \n\nWhy OceanSpy?\n-------------\nSimulations of ocean currents using numerical circulation models are becoming increasingly realistic.\nAt the same time, these models generate increasingly large volumes of model output data, making the analysis of model data harder.\nUsing OceanSpy, model data can be easily analyzed in the way observational oceanographers analyze field measurements.\n\nHow to use OceanSpy?\n--------------------\nOceanSpy can be used as a standalone package for analysis of local circulation model output, or it can be run on a remote data-analysis cluster, such as the Johns Hopkins University SciServer_ system, which hosts several simulations and is publicly available (see `SciServer Access`_, and `Datasets`_).\n\n.. note::\n\n   OceanSpy has been developed and tested using MITgcm output. However, it is designed to work with any (structured grid) ocean general circulation model. OceanSpy's architecture allows to easily implement model-specific features, such as different grids, numerical schemes for vector calculus, budget closures, and equations of state. We actively seek input and contributions from users of other ocean models (`feedback submission`_).\n\n\n\n\n.. _Pangeo: http://pangeo-data.github.io\n.. _xarray: http://xarray.pydata.org\n.. _dask: https://dask.org\n.. _xgcm: https://xgcm.readthedocs.io\n.. _SciServer: http://www.sciserver.org\n.. _`SciServer Access`: https://oceanspy.readthedocs.io/en/latest/sciserver.html\n.. _Datasets: https://oceanspy.readthedocs.io/en/latest/datasets.html\n.. _`feedback submission`: https://github.com/hainegroup/oceanspy/issues\n\n.. |OceanSpy| image:: https://github.com/hainegroup/oceanspy/raw/main/docs/_static/oceanspy_logo_blue.png\n   :alt: OceanSpy image\n   :target: https://oceanspy.readthedocs.io\n\n.. |version| image:: https://img.shields.io/pypi/v/oceanspy.svg?style=flat\n    :alt: PyPI\n    :target: https://pypi.python.org/pypi/oceanspy\n\n.. |conda forge| image:: https://anaconda.org/conda-forge/oceanspy/badges/version.svg\n   :alt: conda-forge\n   :target: https://anaconda.org/conda-forge/oceanspy\n\n.. |docs| image:: http://readthedocs.org/projects/oceanspy/badge/?version=latest\n    :alt: Documentation\n    :target: http://oceanspy.readthedocs.io/en/latest/?badge=latest\n\n.. |CI| image:: https://img.shields.io/github/workflow/status/hainegroup/oceanspy/CI?logo=github\n    :alt: CI\n    :target: https://github.com/hainegroup/oceanspy/actions\n    \n.. |codecov| image:: https://codecov.io/github/hainegroup/oceanspy/coverage.svg?branch=main\n    :alt: Coverage\n    :target: https://codecov.io/github/hainegroup/oceanspy?branch=main\n\n.. |black| image:: https://img.shields.io/badge/code%20style-black-000000.svg\n    :alt: black\n    :target: https://github.com/psf/black\n\n.. |license| image:: https://img.shields.io/github/license/mashape/apistatus.svg\n   :alt: License\n   :target: https://github.com/hainegroup/oceanspy\n   \n.. |doi| image:: https://zenodo.org/badge/DOI/10.5281/zenodo.3270646.svg\n   :alt: doi\n   :target: https://doi.org/10.5281/zenodo.3270646\n   \n.. |JOSS| image:: http://joss.theoj.org/papers/10.21105/joss.01506/status.svg\n   :alt: JOSS\n   :target: https://doi.org/10.21105/joss.01506\n\n.. |binder| image:: https://mybinder.org/badge_logo.svg\n   :alt: binder\n   :target: https://mybinder.org/v2/gh/hainegroup/oceanspy.git/main?filepath=binder\n\n.. |pre-commit| image:: https://results.pre-commit.ci/badge/github/hainegroup/oceanspy/main.svg\n   :target: https://results.pre-commit.ci/latest/github/hainegroup/oceanspy/main\n   :alt: pre-commit.ci status\n"
 },
 {
  "repo": "pyoceans/sea-py",
  "language": "CSS",
  "readme_contents": "# sea-py\n\n[![Travis-CI](https://travis-ci.org/pyoceans/sea-py.svg?branch=master)](https://travis-ci.org/pyoceans/sea-py)\n\nA collaborative effort to organize and distribute Python tools for the Oceanographic Community\n\nSee the rendered page here:\nhttp://pyoceans.github.io/sea-py/\n"
 },
 {
  "repo": "OceanMixingCommunity/Standard-Mixing-Routines",
  "language": "Matlab",
  "readme_contents": "# Standard-Mixing-Routines\nRepository for standard software used by oceanographic mixing community.\n\nThis repository is inteded to be a repo for \"best-practices\" software used to perform standard calculations in the Oceanographic mixing community (such as Thorpe scales, finescale parameterizations etc). Benefits include:\n* Enable reproducibility of analyses.\n* Enable comparison of different datasets using same code.\n* Allow easy re-calculations if bug is found later in code, or \u201cbest-practices\u201d change.\n* Allow testing of one\u2019s own code against other versions.\n* Easy, well-documented citations of techniques for publication\n\nWe are just starting this repo and getting input from the community about how it should be set up. If you have any suggestions, we would love to hear them. Email andypicke (at) gmail (dot) com . Thanks!\n\n![Vintage photo of Amy Waterhouse](/ifildpkheoobnopb.jpg?raw=true)\n\n\n## How to Contribute\n* You can upload code to <https://www.dropbox.com/request/s0pWpFVFkdijwEQQMl0K>\n* Eventually we envision that everyone in the community will contribute and help maintain the repository. But at first we think it will be easier for a small group to upload and organize things.\n* Find a bug or problem with codes? Open an 'issue' to notify team members and create an official record.\n* Want to make changes? First, 'fork' the repository (make your own separate copy), then make changes, then open a 'pull request'. Once approved, it can be merged into the master branch.\n* Don't use git often and don't want to remember all the terminal commands? Download GitHub for desktop <https://desktop.github.com/>, a nice and easy to use front-end.\n\n## Possible topics\n* Thorpe Scales / overturns\n* Finescale Parameterizations (shear/strain)\n* Calculation of stratification\n* Theoretical spectra (Batchelor/Kraichnan)\n* Small sample datasets that can be used to test code and compare results vs other versions.\n* Summaries of sensitivity studies, method comparisons etc.\n\n## Links\n* Climate Process Team: <http://www-pord.ucsd.edu/~jen/cpt/>\n* OSU Ocean Mixing Group: <http://mixing.coas.oregonstate.edu/>\n* Guide to GitHub workflow: <https://guides.github.com/introduction/flow/>\n* CVmix: <http://cvmix.github.io/>\n* Amy's Microstructure Database : <http://microstructure.ucsd.edu> ( see <https://github.com/OceanMixingCommunity/Standard-Mixing-Routines/blob/master/Examine_mixing_data.ipynb> for example of working w/ data in python)\n* pycurrents (python utils for oceanographic data analysis) <http://currents.soest.hawaii.edu/hgstage/pycurrents/file/9f335da750d2>\n"
 },
 {
  "repo": "gcosne/OceanographyProject",
  "language": "Jupyter Notebook",
  "readme_contents": "# Coupling Oceanic Observation Systems to Study Mesoscale Ocean Dynamics\n\nThe paper related to the code implemented can be found here : [arxiv](https://arxiv.org/abs/1910.08573).\n\n\n**Abstract** : Understanding local currents in the North Atlantic region of the ocean is a key part of modelling heat transfer and global climate patterns. Satellites provide a surface signature of the temperature of the ocean with a high horizontal resolution while in situ autonomous probes supply high vertical resolution, but horizontally sparse, knowledge of the ocean interior thermal structure. The objective of this paper is to develop a methodology to combine these complementary ocean observing systems measurements to obtain a three-dimensional time series of ocean temperatures with high horizontal and vertical resolution. Within an observation-driven framework, we investigate the extent to which mesoscale ocean dynamics in the North Atlantic region may be decomposed into a mixture of dynamical modes, characterized by different local regressions between Sea Surface Temperature (SST), Sea Level Anomalies (SLA) and Vertical Temperature fields. Ultimately we propose a Latent-class regression method to improve prediction of vertical ocean temperature.\n\n**LATEST VERSION OF THE NOTEBOOK can be found on colaboratory here** : [colab](https://colab.research.google.com/drive/1xX_XcPrx6cdHfIDTYd7K7BpJnu5LliDv)\n"
 },
 {
  "repo": "euroargodev/argopy",
  "language": "Python",
  "readme_contents": "|<img src=\"https://raw.githubusercontent.com/euroargodev/argopy/master/docs/_static/argopy_logo_long.png\" alt=\"argopy logo\" width=\"200\"/><br>``argopy`` is a python library dedicated to Argo data access, visualisation and manipulation for regular users as well as Argo experts and operators|\n|:---------:|\n|[![JOSS](https://img.shields.io/badge/DOI-10.21105%2Fjoss.02425-brightgreen)](//dx.doi.org/10.21105/joss.02425) ![CI](https://github.com/euroargodev/argopy/actions/workflows/pytests.yml/badge.svg) [![codecov](https://codecov.io/gh/euroargodev/argopy/branch/master/graph/badge.svg)](https://codecov.io/gh/euroargodev/argopy) [![Documentation Status](https://img.shields.io/readthedocs/argopy?logo=readthedocs)](https://argopy.readthedocs.io/en/latest/?badge=latest) [![PyPI](https://img.shields.io/pypi/v/argopy)](//pypi.org/project/argopy/)|\n\n### Documentation\n\nThe official documentation is hosted on ReadTheDocs.org: https://argopy.readthedocs.io\n\n### Install\n\nBinary installers for the latest released version are available at the [Python Package Index (PyPI)](https://pypi.org/project/argopy/) and on [Conda](https://anaconda.org/conda-forge/argopy).\n\n```bash\n# conda\nconda install -c conda-forge argopy\n````\n```bash\n# or PyPI\npip install argopy\n````\n\n``argopy`` is continuously tested to work under most OS (Linux, Mac, Windows) and with python versions 3.7 and 3.8.\n\n### Usage\n\n[![badge](https://img.shields.io/static/v1.svg?logo=Jupyter&label=Binder&message=Click+here+to+try+argopy+online+!&color=blue&style=for-the-badge)](https://mybinder.org/v2/gh/euroargodev/binder-sandbox/main?urlpath=git-pull%3Frepo%3Dhttps%253A%252F%252Fgithub.com%252Feuroargodev%252Fargopy%26urlpath%3Dlab%252Ftree%252Fargopy%252Fdocs%252Ftryit.ipynb%26branch%3Dmaster)\n\n```python\n# Import the main fetcher:\nfrom argopy import DataFetcher as ArgoDataFetcher\n```\n```python\n# Define what you want to fetch... \n# a region:\nArgoSet = ArgoDataFetcher().region([-85,-45,10.,20.,0,10.])\n# floats:\nArgoSet = ArgoDataFetcher().float([6902746, 6902747, 6902757, 6902766])\n# or specific profiles:\nArgoSet = ArgoDataFetcher().profile(6902746, 34)\n```\n```python\n# then fetch and get data as xarray datasets:\nds = ArgoSet.load().data\n# or\nds = ArgoSet.to_xarray()\n```\n```python\n# you can even plot some information:\nArgoSet.plot('trajectory')    \n```\n\nThey are many more usages and fine-tuning to allow you to access and manipulate Argo data:\n- [filters at fetch time](https://argopy.readthedocs.io/en/latest/user_mode.html) (standard vs expert users, automatically select QC flags or data mode, ...)\n- [select data sources](https://argopy.readthedocs.io/en/latest/data_sources.html) (erddap, ftp, local, ...)\n- [manipulate data](https://argopy.readthedocs.io/en/latest/data_manipulation.html) (points, profiles, interpolations, binning, ...)\n- [visualisation](https://argopy.readthedocs.io/en/latest/visualisation.html) (trajectories, topography, histograms, ...)\n- [tools for Quality Control](https://argopy.readthedocs.io/en/latest/data_quality_control.html) (OWC, figures, ...)\n- [improve performances](https://argopy.readthedocs.io/en/latest/performances.html) (caching, parallel data fetching)\n\nJust check out [the documentation for more](https://argopy.readthedocs.io) ! \n\n## Development and contributions \n\nSee our development roadmap here: https://github.com/euroargodev/argopy/milestone/3\n\nCheckout [the contribution page](https://argopy.readthedocs.io/en/latest/contributing.html) if you want to get involved and help maintain or develop ``argopy``.\n"
 },
 {
  "repo": "ioos/colocate",
  "language": "Jupyter Notebook",
  "readme_contents": "# colocate\nEasy access to oceanographic data co-located in space and time via Python\n\n## Preview/Run:\n\n#### ERDDAP colocate.ipynb:\n[![binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/ioos/colocate/master?filepath=notebooks%2Fcolocate.ipynb)\n[![nbviewer](https://raw.githubusercontent.com/jupyter/design/master/logos/Badges/nbviewer_badge.svg)](https://nbviewer.jupyter.org/github/ioos/colocate/blob/master/notebooks/colocate.ipynb)\n\n#### ERDDAP colocate-dev.ipyb:\n[![binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/ioos/colocate/master?filepath=notebooks%2Fcolocate-dev.ipynb)\n[![nbviewer](https://raw.githubusercontent.com/jupyter/design/master/logos/Badges/nbviewer_badge.svg)](https://nbviewer.jupyter.org/github/ioos/colocate/blob/master/notebooks/colocate-dev.ipynb)\n\n<img src=\"img/colocate_datashader.png\" alt=\"colocate example map plot\" style=\"width: 200px, align: center\" />\n\n## Installation:\n\n### Install with conda:\n```\ngit clone https://github.com/ioos/colocate.git\ncd colocate\nconda env create -f environment.yml\nconda activate colocate\n```\n\n### Run via command line:\n```\nerddap-co-locate\n```\n\n### Run in Jupyter notebook:\n```\njupyter notebook &\n```\n\n### Run in JupyterLab:\n\nThis step may be necessary for ipyleaflet and HoloViz to run correctly in JupyterLab.  Run the following on the command line with the 'colocate' conda environment active:\n```\njupyter labextension install @jupyter-widgets/jupyterlab-manager\njupyter labextension install jupyter-leaflet\njupyter labextension install @pyviz/jupyterlab_pyviz\n```\nThen, start JupyterLab:\n```\njupyter-lab &\n```\n\n### Run with voila:\n```\nvoila colocate.ipynb --enable_nbextensions=True --VoilaConfiguration.file_whitelist=\"['.*']\"\n```\n\n## Local Development:\nIf you want to develop locally, clone from GitHub, `cd` to the cloned repository root directory, and run `pip install` as follows:\n```\ngit clone https://github.com/ioos/colocate.git\ncd colocate\npip install -e . --no-deps --force-reinstall\n```\n\n\n# OceanHackWeek 2019 'co-locators' Project\nDescription of the original OceanHackWeek 2019 project that led to the development of this module.  \n\n## The problem\nCo-locate oceanographic data (served via ERDDAP) by establishing constraints to use as data server query filters.  \n\nSubmit the identical filter criteria to all ERDDAP servers indexed by the [awesome-erddap project](https://github.com/IrishMarineInstitute/awesome-erddap) and visualize the results.\n\n### Application example\nA user is interested in all the available oceanographic data in a region where an eddy just formed. They provide the geospatial bounds of the region and a temporal range and get an aggregated response of all available data.\n\n## Collaborators:\n\n| Name | Year |\n|------------|----|\n|Mathew Biddle|2019|\n|Sophie Chu|2019|\n|Yeray Santana Falcon|2019|\n|Molly James|2019|\n|Pedro Maga\u00f1a|2019|\n|Jazlyn Natalie|2019|\n|Laura Gomez Navarro|2019|\n|Shikhar Rai|2019|\n|Micah Wengren|2019|\n|Jacqueline Tay|2020|\n|Mike Morley|2020|\n|Yuta Norden|2020|\n\n\n### Specific tasks\n- [x] Collect temporal bounds.\n- [x] Collect spatial bounds.\n- [x] _Collect keywords?_\n- [x] Build query url.\n- [x] Do the search.\n- [ ] Evaluate the response.\n- [ ] Manage response.\n- [ ] Geospatial plotting.\n- [ ] Temporal plotting.\n- [ ] Link back to dataset on erddap server.\n- [ ] _Aggregated download?_\n\n\n### Existing methods\n- The Irish Marine Institute has developed a [keyword search across existing ERDDAP\n  servers](https://github.com/IrishMarineInstitute/search-erddaps)\n  - [Here is the list of all ERDDAP's they use.](https://github.com/IrishMarineInstitute/search-erddaps/blob/master/erddaps.json)\n- OHW18 built a [search interface for one ERDDAP server](https://github.com/oceanhackweek/ohw18_erddap-explorer)\n- yodapy: https://github.com/cormorack/yodapy\n\n\n### Proposed methods/tools\n- Jupyter\n- Python\n  - [erddapy](https://github.com/ioos/erddapy)\n  - [Pandas](https://pandas.pydata.org/pandas-docs/stable/getting_started/overview.html)\n\n### Background reading\n\nOptional: links to manuscripts or technical documents for more in-depth analysis.\n"
 },
 {
  "repo": "lnferris/ocean_data_tools",
  "language": "MATLAB",
  "readme_contents": "# ocean_data_tools: a MATLAB toolbox for interacting with bulk freely-available oceanographic data\n\n<img src=\"https://user-images.githubusercontent.com/24570061/85356569-4664ba80-b4dd-11ea-9ec7-8ec26df76dcf.png\" width=\"500\">\n\n[![GitHub license](https://img.shields.io/github/license/lnferris/ocean_data_tools)](https://github.com/lnferris/ocean_data_tools/blob/master/LICENSE) [![GitHub stars](https://img.shields.io/github/stars/lnferris/ocean_data_tools)](https://github.com/lnferris/ocean_data_tools/stargazers) [![GitHub forks](https://img.shields.io/github/forks/lnferris/ocean_data_tools)](https://github.com/lnferris/ocean_data_tools/network) \n[![View ocean_data_tools on File Exchange](https://www.mathworks.com/matlabcentral/images/matlab-file-exchange.svg)](https://www.mathworks.com/matlabcentral/fileexchange/80047-ocean_data_tools) [![DOI](https://joss.theoj.org/papers/10.21105/joss.02497/status.svg)](https://doi.org/10.21105/joss.02497)\n\n**Copyright (c) 2020 lnferris** \n\nocean_data_tools simplifies the process of extracting, formatting, and visualizing freely-available oceanographic data. While a wealth of oceanographic data is accessible online, some end-users may be dissuaded from utilizing this data due to the overhead associated with obtaining and formatting it into usable data structures. ocean_data_tools solves this problem by allowing the user to transform common oceanographic data sources into uniform structs, call generalized functions on these structs, easily perform custom calculations, and make graphics.\n\nFind a bug, have a question, or want to chat about contributing? Open an issue or email lnferris@alum.mit.edu.\n\n### [Getting Started](#getting-started-1)\n\n### [Dependencies](#dependencies-1)\n\n### [Accessing Help](#accessing-help-1)\n\n### [How to Contribute](#how-to-contribute-1)\n\n### [Contents](#contents-1)\n\n### [Finding Data](#finding-data-1)\n\n### [Citing ODT](#citing-odt-1)\n\n## Getting Started\n\n1. Download [bathymetry](#bathymetry).\n2. Download [nctoolbox](https://github.com/nctoolbox/nctoolbox). You will need to run the command ``setup_nctoolbox`` at the beginning of each MATLAB session.\n3. Add ocean_data_tools and nctoolbox to the path. Specifically, the following folders must be added to the [path](https://www.mathworks.com/help/matlab/matlab_env/what-is-the-matlab-search-path.html):\n\n- ocean_data_tools/ocean_data_tools\n- ocean_data_tools/ocean_data_tools/utilities\n- nctoolbox/\n\n4. Run each demonstration in **demos/demos.m**, which contains example usages for all functions. All required test data is included in **data/**.\n\nFunctions are named using a two-part system. The prefix (``argo_``, ``bathymetry_``, ``general_``, etc.) indicates the appropriate data source, while the suffix (``\\_build``, ``\\_profiles``, ``\\_section``, etc.) indicates the action performed. Functions with the ``\\_build`` suffix load raw data into uniform structs (e.g. ``argo``, ``cruise``, ``hycom``, ``mercator``, ``woa``, ``wod``). Uniform structs created by ``\\_build`` functions are compatable with any ``general_`` function.\n\nData sources currently supported:\n| Data Source | DOI, Product Code, or Link    |\n|:--  |:--|\n| Argo floats | [doi:10.17882/42182](https://doi.org/10.17882/42182) |\n| Smith & Sandwell bathymetry | [doi:10.1126/science.277.5334.1956](https://doi.org/10.1126/science.277.5334.1956) |\n| IOOS Glider DAC | https://gliders.ioos.us/ |\n| MOCHA Climatology | [doi:10.7282/T3XW4N4M](https://doi.org/10.7282/T3XW4N4M) |\n| HYbrid Coordinate Ocean Model | https://hycom.org |\n| CMEMS Global Ocean 1/12\u00b0 Physics Analysis and Forecast | GLOBAL_ANALYSIS_FORECAST_ PHY_001_024 |\n| CMEMS Global Ocean Waves Multi Year | GLOBAL_REANALYSIS_WAV_001_032 |\n| GO-SHIP hydrographic cruises | https://www.go-ship.org/ |\n| World Ocean Atlas 2018 | https://www.ncei.noaa.gov/products/world-ocean-atlas |\n| World Ocean Database | https://www.ncei.noaa.gov/products/world-ocean-database |\n\nMain functions are located in **ocean_data_tools/**. Demonstrations are located in **demos/**. Test datas are located in **data/**. Shell scripts for batch downloading data are located in **shell_scripts/**. While shell scripts can be run directly in a macOS Terminal, running them in Windows requires [Cygwin](https://www.cygwin.com/) (and perhaps slight modification of commands). Python syntax examples are located in **python/**, which may be grow to become a module in the future.\n\n## Dependencies\n\nThe only true dependency is [nctoolbox](https://github.com/nctoolbox/nctoolbox).\n\nIt is recommended to also download [Gibbs-SeaWater (GSW) Oceanographic Toolbox](http://www.teos-10.org/software.htm#1). A benefit of ocean_data_tools is that neatly packs data into uniform structs; at which point a user can easily apply custom calculations or functions from other toolboxes such as GSW. See an [example](docs/gsw_example.md).\n\n## Accessing Help\n\nTo access help, run the command ``doc ocean_data_tools``.\n\n## How to Contribute\n\n* Want to make changes or add a new function? (1) Fork the repository (make your own separate copy), (2) make changes, and (3) open a 'pull request'. Once approved, it can be merged into the master branch. If you wish to chat beforehand about your contribution, open an issue or email lnferris@alum.mit.edu. \n* Don't use git often and don't want to remember all the terminal commands? Download [GitHub Desktop](https://desktop.github.com/).\n* Find a bug in the code? Open an 'issue' to notify contributors and create an official record.\n\nBefore contributing, please see [Contents](#contents-1) and consider how your function fits into ocean_data_tools and its ethos of structure arrays. At a minimum, functions must be well-documented and address a specific freely-available oceanographic data source which can be accessed by anyone online.\n\nAdding a new function isn't the only way to contribute. Python, Julia, etc. translations of existing Matlab functions are also welcomed!\n\nIf you are interested in becoming a formal collaborator (e.g. have direct access and co-manage this repository), please reach out.\n\n## Contents\n\n#### [Building uniform structs from data sources](#building-uniform-structs-from-data-sources-1)\n\n#### [General functions for subsetting and plotting uniform structs](#general-functions-for-subsetting-and-plotting-uniform-structs-1)\n\n#### [Plotting gridded data without building structs](#plotting-gridded-data-without-building-structs-1)\n\n#### [Adding bathymetry to existing plots](#adding-bathymetry-to-existing-plots-1)\n\n#### [Additional functions for inspecting Argo data](#additional-functions-for-inspecting-argo-data-1)\n\n#### [Miscellaneous utilities](#miscellaneous-utilities-1)\n\n\n### Building uniform structs from data sources\n\n**[argo_build](docs/argo_build.md)** searches the locally-stored Argo profiles matching the specified region & time period and builds a uniform struct\n\n**[glider_build](docs/glider_build.md)** loads an archived glider survey (downloaded from gliders.ioos.us/erddap) and builds a uniform struct\n\n**[mocha_build_profiles](docs/mocha_build_profiles.md)** builds a uniform struct of profiles from the MOCHA Mid-Atlantic Bight climatology\n\n**[model_build_profiles](docs/model_build_profiles.md)**  builds a uniform struct of profiles from HYCOM or Operational Mercator CMEMS GLOBAL_ANALYSIS_FORECAST_PHY_001_024\n\n<img src=\"https://user-images.githubusercontent.com/24570061/88250150-ac776580-cc74-11ea-8c72-cea7cc50b4d9.png\" width=\"700\">\n\n**waves_build** builds a uniform struct of timeseries from CMEMS Global Ocean Waves Multi Year product GLOBAL_REANALYSIS_WAV_001_032\n\n**[whp_cruise_build](docs/whp_cruise_build.md)** builds a uniform struct of profiles from GO-SHIP cruise data in WHP-Exchange Format\n\n**[woa_build_profiles](docs/woa_build_profiles.md)** builds a uniform struct of profiles from World Ocean Atlas 2018 Statistical Mean for All Decades, Objectively Analyzed Mean Fields\n\n**[wod_build](docs/wod_build.md)** builds a uniform struct of profiles from World Ocean Database data\n\n*Don't see a function yet for your preferred data source? Email lnferris@alum.mit.edu to request or [contribute](#how-to-contribute-1).*\n\n### General functions for subsetting and plotting uniform structs\n\n**[general_depth_subset](docs/general_depth_subset.md)** subsets a uniform struct by depth\n\n**[general_map](docs/general_map.md)** plots coordinate locations in a uniform struct, with optional bathymetry contours\n\n**[general_profiles](docs/general_profiles.md)** plots vertical profiles in a uniform struct\n\n**[general_region_subset](docs/general_region_subset.md)** subsets a uniform struct by polygon region\n\n<img src=\"https://user-images.githubusercontent.com/24570061/88250944-358f9c00-cc77-11ea-9b0d-2d582ad186dd.png\" width=\"700\">\n\n**[general_remove_duplicates](docs/general_remove_duplicates.md)** removes spatially (or spatially and temporally) non-unique profiles from a uniform struct\n\n**[general_section](docs/general_section.md)** plots a data section from a uniform struct\n\n\n### Plotting gridded data without building structs\n\n**[mocha_domain_plot](docs/mocha_domain_plot.md)** plots a 3-D domain from the MOCHA Mid-Atlantic Bight climatology\n\n**[mocha_simple_plot](docs/mocha_simple_plot.md)** plots a 2-D layer from the MOCHA Mid-Atlantic Bight climatology\n\n**[model_domain_plot](docs/model_domain_plot.md)** plots a 3-D domain from HYCOM or Operational Mercator CMEMS GLOBAL_ANALYSIS_FORECAST_PHY_001_024\n\n**[model_simple_plot](docs/model_simple_plot.md)** plots a 2-D layer from HYCOM or Operational Mercator CMEMS GLOBAL_ANALYSIS_FORECAST_PHY_001_024\n\n<img src=\"https://user-images.githubusercontent.com/24570061/88250403-8900ea80-cc75-11ea-8a5d-8a474d2e5c3f.png\" width=\"700\">\n\n**[woa_domain_plot](docs/woa_domain_plot.md)** plots a 3-D domain from World Ocean Atlas 2018 Statistical Mean for All Decades, Objectively Analyzed Mean Fields\n\n**[woa_simple_plot](docs/woa_simple_plot.md)** plots a 2-D layer from World Ocean Atlas 2018 Statistical Mean for All Decades, Objectively Analyzed Mean Fields\n\n### Adding bathymetry to existing plots\n\n**[bathymetry_extract](docs/bathymetry_extract.md)** extracts a region of Smith & Sandwell Global Topography and outputs as arrays\n\n**[bathymetry_plot](docs/bathymetry_plot.md)** adds bathymetry to 2-D (latitude vs. longitude) or 3-D (latitude vs. longitude vs. depth) data plots\n\n<img src=\"https://user-images.githubusercontent.com/24570061/88251161-ed24ae00-cc77-11ea-87d6-0e3b4484764d.jpg\" width=\"700\">\n\n**[bounding_region](docs/bounding_region.md)** finds the rectangular region around a uniform struct and/or list of coordinates to pass as an argument for other bathymetry functions\n\n**[bathymetry_section](docs/bathymetry_section.md)** adds Smith & Sandwell Global Topography to a section from plot using bathymetry data nearest to specified coordinates\n\n<img src=\"https://user-images.githubusercontent.com/24570061/88250660-3d027580-cc76-11ea-808c-f51d5105e420.png\" width=\"700\">\n\n### Additional functions for inspecting Argo data\n\n**[argo_platform_map](docs/argo_platform_map.md)** plots locations of Argo profiles in a uniform struct, coloring markers by platform (individual Argo float)\n\n<img src=\"https://user-images.githubusercontent.com/24570061/88250439-a2099b80-cc75-11ea-9516-ad3d1f65fdf9.jpg\" width=\"700\">\n\n**[argo_platform_subset](docs/argo_platform_subset.md)** subsets a uniform struct of Argo data to one platform (individual Argo float)\n\n**[argo_profiles_map](docs/argo_profiles_map.md)** plots coordinate locations of Argo profiles in uniform struct argo, using colors corresponding to argo_profiles called on the same struct\n\n**[argo_profiles](docs/argo_profiles.md)** plots vertical Argo profiles in uniform struct argo, using colors corresponding to argo_profiles_map called on the same struct\n\n\n### Miscellaneous utilities\n\n**[region_select](docs/region_select.md)** creates coordinate list (which represents vertices of a polygon region) by clicking stations on a plot\n\n**[transect_select](docs/transect_select.md)** creates a coordinate list (which represents a virtual transect) by clicking stations on a plot\n\n<img src=\"https://user-images.githubusercontent.com/24570061/88250639-2b20d280-cc76-11ea-9c94-3ce16300f735.png\" width=\"700\">\n\n\n## Finding Data\n\nThere two types of datasets: those that need to be downloaded manually<sup>1</sup> and those that can be accessed remotely<sup>2</sup> through OpenDAP (e.g. the data can be accessed directly on the the internet using a url). \n\n#### argo<sup>1</sup>\n\nDownload [Argo data](https://argo.ucsd.edu/) directly from GDAC FTP servers using either the [Coriolis selection tool](http://www.argodatamgt.org/Access-to-data/Argo-data-selection), or the [US GDAC](https://nrlgodae1.nrlmry.navy.mil/cgi-bin/argo_select.pl). See the [Argo User's Manual](http://www.argodatamgt.org/Documentation) for more information.\n\nAlternatively run **shell_scripts/download_argo** to download data via File Transfer Protocol.\n\n#### bathymetry<sup>1</sup>\n\nTo get bathymetry data (for ``bathymetry_dir``), download Smith & Sandwell under [Global Topography V19.1](https://topex.ucsd.edu/marine_topo/) in netcdf form (topo_20.1.nc).\n\n#### glider<sup>1</sup>\n\nVist [gliders.ioos.us/erddap](https://gliders.ioos.us/erddap/index.html). Click \"View a List of All 779 Datasets\" or use the \"Advanced Search\". After choosing a dataset, navigate to the [Data Access Form](https://gliders.ioos.us/erddap/tabledap/ce_311-20170725T1930-delayed.html). To get started, select these variables:\n\n<img src=\"https://user-images.githubusercontent.com/24570061/94058620-419af580-fdaf-11ea-859a-616c8b5b1433.png\" width=\"700\">\n\nScroll to \"File type:\". In the drop-down menu, select \".nc\". Click \"Submit\".\n\n#### mocha<sup>2</sup>\n\nThe url for MOCHA Mid-Atlantic Bight climatology is embedded. See [Rutgers Marine catalog](http://tds.marine.rutgers.edu/thredds/catalog.html).\n\n#### model<sup>1,2</sup>\n\nHYCOM data may be accessed remotely using OpenDAP. Get the data url by visiting the [HYCOM website](https://www.hycom.org/dataserver/gofs-3pt1/analysis). For example, click Access Data Here -> GLBv0.08/expt_57.7 (Jun-01-2017 to Sep-30-2017)/ -> Hindcast Data: Jun-01-2017 to Sep-30-2017. Click on the\u00a0OpenDAP link. Copy the url as and use this as the ``source`` in ``model_build_profiles``.\n\nAlteratively, download subsetted HYCOM data using NCSS. Get the data url by visiting the [HYCOM website](https://www.hycom.org/dataserver/gofs-3pt1/analysis). For example, click Access Data Here -> GLBv0.08/expt_57.7 (Jun-01-2017 to Sep-30-2017)/ -> Hindcast Data: Jun-01-2017 to Sep-30-2017. Click on the\u00a0NetcdfSubset\u00a0link. Set constraints and copy the NCSS Request URL at the bottom of the page. Run **shell_scripts/download_hycom_lite**. To download multiple months or years, run **shell_scripts/download_hycom_bulk_daily** (partition files by day) or **shell_scripts/download_hycom_bulk_monthly** (partition files by month). Please use responsibly.\n\nFor Mercator, download Copernicus Marine data directly from FTP servers. First make a [Copernicus account](http://marine.copernicus.eu/services-portfolio/access-to-products/). Use the selection tool to download GLOBAL_ANALYSIS_FORECAST_PHY_001_024. Alternatively run **shell_scripts/download_mercator**. Before running the script, follow the instructions for modifying your ~/.netrc file in the comments of the script.\n\n#### waves<sup>1</sup>\n\nFirst make a [Copernicus account](http://marine.copernicus.eu/services-portfolio/access-to-products/). Use the selection tool to download CMEMS Global Ocean Waves Multi Year product GLOBAL_REANALYSIS_WAV_001_032.\n\n#### whp_cruise<sup>1</sup>\n\nFor [GO-SHIP data](https://usgoship.ucsd.edu/hydromap/), get CTD data (for ``ctdo_dir``) by choosing a [GO-SHIP cruise](https://cchdo.ucsd.edu/search?q=GO-SHIP) and downloading the\u00a0CTD\u00a0data in\u00a0whp_netcdf\u00a0format. More information about whp_netcdf parameters is available [here](https://exchange-format.readthedocs.io/en/latest/index.html#). Get LADCP data (for ``uv_dir``, ``wke_dir``) [here](https://currents.soest.hawaii.edu/go-ship/ladcp/). There is information about LACDP processing [here](https://www.ldeo.columbia.edu/~ant/LADCP.html).\n\n#### woa<sup>2</sup>\n\nFunctions build the World Ocean Atlas url at maximum resolution based on arguments, but coarser resolutions and seasonal climatologies are available at the [NODC website](https://www.nodc.noaa.gov/OC5/woa18/woa18data.html). Note NCEI is scheduled to update data urls in the near future. Functions will be updated as such.\n\n#### wod<sup>1</sup>\n\nSearch the [World Ocean Database](https://www.nodc.noaa.gov/OC5/SELECT/dbsearch/dbsearch.html) and select products.\n\n## Citing ODT\n\nFerris, L., (2020).  ocean_data_tools:  A MATLAB toolbox for interacting with bulk freely-available oceanographic data. Journal of Open Source Software, 5(54), 2497. https://doi.org/10.21105/joss.02497\n\n"
 },
 {
  "repo": "SciTools/iris",
  "language": "Python",
  "readme_contents": "<h1 align=\"center\">\n  <a href=\"https://scitools-iris.readthedocs.io/en/latest/\">\n   <img src=\"https://scitools-iris.readthedocs.io/en/latest/_static/iris-logo-title.svg\" alt=\"Iris\" width=\"300\"></a><br>\n</h1>\n\n\n<h4 align=\"center\">\n    Iris is a powerful, format-agnostic, community-driven Python package for\n    analysing and visualising Earth science data\n</h4>\n\n<p align=\"center\">\n<a href=\"https://github.com/SciTools/iris/actions/workflows/ci-tests.yml\">\n<img src=\"https://github.com/SciTools/iris/actions/workflows/ci-tests.yml/badge.svg?branch=main\"\n     alt=\"ci-tests\"></a>\n<a href=\"https://scitools-iris.readthedocs.io/en/latest/?badge=latest\">\n<img src=\"https://readthedocs.org/projects/scitools-iris/badge/?version=latest\"\n     alt=\"Documentation Status\"></a>\n<a href=\"https://results.pre-commit.ci/latest/github/SciTools/iris/main\">\n<img src=\"https://results.pre-commit.ci/badge/github/SciTools/iris/main.svg\"\n     alt=\"pre-commit.ci status\"></a>\n</p>\n\n<p align=\"center\">\n<a href=\"https://anaconda.org/conda-forge/iris\">\n<img src=\"https://img.shields.io/conda/v/conda-forge/iris?color=orange&label=conda-forge%7Ciris&logo=conda-forge&logoColor=white\"\n     alt=\"conda-forge\"></a>\n<a href=\"https://pypi.org/project/scitools-iris\">\n<img src=\"https://img.shields.io/pypi/v/scitools-iris?color=orange&label=pypi%7Cscitools-iris&logo=python&logoColor=white\"\n     alt=\"pypi\"></a>\n<a href=\"https://github.com/SciTools/iris/releases\">\n<img src=\"https://img.shields.io/github/v/release/scitools/iris\"\n     alt=\"latest release\"></a>\n<a href=\"https://github.com/SciTools/iris/commits/main\">\n<img src=\"https://img.shields.io/github/commits-since/SciTools/iris/latest.svg\"\n     alt=\"Commits since last release\"></a>\n<a href=\"https://zenodo.org/badge/latestdoi/5312648\">\n<img src=\"https://zenodo.org/badge/5312648.svg\"\n     alt=\"zenodo\"></a>\n<a href=\"https://github.com/psf/black\">\n<img src=\"https://img.shields.io/badge/code%20style-black-000000.svg\"\n     alt=\"black\"></a>\n<a href=\"https://github.com/SciTools/iris/discussions\">\n<img src=\"https://img.shields.io/badge/github-discussions-yellow?style=social&logo=github&style=plastic\"\n     alt=\"github discussions\"></a>\n<a href=\"https://twitter.com/scitools_iris\">\n<img src=\"https://img.shields.io/twitter/follow/scitools_iris?color=yellow&label=twitter%7Cscitools_iris&logo=twitter&style=plastic\"\n     alt=\"twitter scitools_iris\"></a>\n</p>\n\n<p align=\"center\">\nFor documentation see the \n<a href=\"https://scitools-iris.readthedocs.io/en/latest/\">latest</a>  \ndeveloper version or the most recent released\n<a href=\"https://scitools-iris.readthedocs.io/en/stable/\">stable</a> version.\n</p>\n"
 },
 {
  "repo": "sea-mat/sea-mat",
  "language": "HTML",
  "readme_contents": "# sea-mat\n\n[![Build Status](https://travis-ci.org/sea-mat/sea-mat.svg?branch=master)](https://travis-ci.org/sea-mat/sea-mat)\n\nA collaborative effort to organize and distribute Python tools for the Oceanographic Community\n\nSee the rendered page here:\nhttp://sea-mat.github.io/sea-mat/\n"
 },
 {
  "repo": "Energy-Pathways-Group/GLOceanKit",
  "language": "MATLAB",
  "readme_contents": "GLOceanKit\n===========\nGLOceanKit is a collection of models and analysis tools for physical oceanography.\n\nThe code is in written in two different languages: Matlab and Objective-C, but not all models or analysis tools are available in both languages.\n\n[Matlab](Matlab/)\n-------\nThe Matlab directory contains the following subdirectories of models and tools,\n- [Advection-Diffusion Estimation](Matlab/AdvectionDiffusionEstimation) Tools for computing estimating velocity field parameters (strain, vorticity, divergence). From Oscroft, Sykulski & Early (2021).\n- [Advection-Diffusion Models](Matlab/AdvectionDiffusionModels) Code for generating particles in advection diffusion models with boundaries.\n- [Boussinesq2D](Matlab/Boussinesq2D) 2D nonlinear spectral Boussinesq model.\n- [Diffusivity](Matlab/Diffusivity) A collection of analysis tools for computing relative diffusivity from particles.\n- [InternalModes](Matlab/InternalModes) Tools solving the vertical mode eigenvalue problem with very high accuracy. From Early, Lelong & Smith (2020).\n- [InternalWaveModel](Matlab/InternalWaveModel) A linear internal wave model.\n- [InternalWaveSpectrum](Matlab/InternalWaveSpectrum) Tools for computing the Garrett-Munk spectrum and its approximations.\n- [OceanBoundaryLayer](Matlab/OceanBoundaryLayer) A few simple ocean boundary layer models taken from Elipot and Gille (2009).\n- [Quasigeostrophy](Matlab/Quasigeostrophy) Tools for analyzing the output of the Quasigeostrophic model.\n\n[Objective-C](GLOceanKit/)\n-------\nContains internal modes routines, internal wave model, and a QG model.\n\ngit-lfs\n--------\nThis repo links to the lfs for some precomputed internal wave modes, but does not download them by default. To override these settings, see [this comment](https://github.com/git-lfs/git-lfs/issues/2717). I think, that if you just do,\n`git config lfs.fetchexclude \"\"`\nthen it'll remove the exclusion and you can started to download those big files.\n"
 },
 {
  "repo": "ArgoCanada/argoFloats",
  "language": "R",
  "readme_contents": "if (!interactive()) png(\"exampleTS.png\", width=7, height=3.5, unit=\"in\", res=120, pointsize=8)\nlibrary(argoFloats)\nlibrary(oce)\n## 1. Get worldwide float-profile index, saving to ~/data/argo by default.\nindexAll <- getIndex()\n## 2. Narrow to a 30km-radius circle centred on Abaco Island, The Bahamas.\nindex <- subset(indexAll,\n                circle=list(longitude=-77.06,latitude=26.54,radius=30))\n## 3. Get NetCDF files for these profiles, saving to ~/data/argo by default.\nprofiles  <- getProfiles(index)\n## 4. Read the NetCDF files.\nargos <- readProfiles(profiles)\n## 5. Examine QC flags, and set questionable data to NA.\nargosClean <- applyQC(argos)\npar(mfrow=c(1, 2))                     # want two-panel plot\npar(mar=c(3.5, 3.5, 2.0, 2.0))         # tighten margins\n## 6. Plot a map with bathymetry, indicating number of profiles.\nplot(index, which=\"map\")\npoints(-77.06, 26.54, pch=\"*\", cex=3)  # show centre of focus\nmtext(paste(argosClean[[\"length\"]], \"profiles\"))\n## 7. Plot a TS diagram\nplot(argosClean, which=\"TS\")\nif (!interactive()) dev.off()\n\n"
 },
 {
  "repo": "metarelate/metOcean",
  "language": "Python",
  "readme_contents": "metOcean\n=========\n\nmetOcean mapping is a knowledge repository providing information on translating meteorological and oceanographic metadata.\n\nThe knowledge is stored as RDF Turtle datasets in StaticData, modelled using the Metarelate terminology mapping model.\n\nTo contribute to the project, the static data should be used to populate a local triple store which an instance of the metarelate management software may access. \n\nDependencies\n------------\n* metarelate - https://github.com/metarelate/metarelate\n\nTo use this repository with metarelate, set your local environment variables:\n\n* METARELATE_STATIC_DIR='/path/to/your/working/copy/of/metOcean/staticData/'\n* METARELATE_TDB_DIR='/a/path/to/a/writeable/folder/for/a/local/triplestore'\n* METARELATE_DATA_PROJECT='metOcean' \n"
 },
 {
  "repo": "ctroupin/Python-Course-Cadiz",
  "language": "Jupyter Notebook",
  "readme_contents": "# Introduction to Scientific Python\n\nThe main objective of the course is to make users familiar with the Python programming language.     \nThe course is mostly targeted to users without a strong previous experience with Python, but with some notions of programming (MATLAB, Octave, ...). The different applications presented are oriented to Oceanography in general.\n\n## Content\n\nThe content can be divided into 4 parts:\n1. General description of the language.\n2. Reading/writing information from/to files.\n3. Data processing and analysis.\n4. Data visualisation.\n\nThe material is organised into sub-folders:\n- in [notebooks](./notebooks/) you find the `jupyter-notebook` describing the different programs.\n- [exercises](./exercises/) contains some exercises that will be explained and solved during the course.\n\n## How to start?\n\nDuring the course, participants will use virtual machines (VM) on which all the necessary tools are installed, in order to avoid the sometimes tedious installation steps, often depending on the operating system.\n"
 },
 {
  "repo": "MikkoVihtakari/ggOceanMaps",
  "language": "R",
  "readme_contents": "---\noutput: github_document\n---\n\n```{r, echo = FALSE}\nknitr::opts_chunk$set(\n  collapse = TRUE,\n  message = FALSE, \n  warning = FALSE,\n  comment = \"#>\",\n  fig.path = \"man/figures/README-\"\n)\n```\n\n```{r echo=FALSE, message=FALSE, warning=FALSE}\n#library(knitr)\n#knitr::opts_chunk$set(message = FALSE, warning = FALSE, comment = \"#>\")\n#knit_hooks$set(optipng = hook_optipng)\n#knit_hooks$set(pngquant = hook_pngquant)\n```\n\n\n# ggOceanMaps\n**Plot data on oceanographic maps using ggplot2. R package version `r packageVersion(\"ggOceanMaps\")`**\n\n<!-- badges: start -->\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.4554714.svg)](https://doi.org/10.5281/zenodo.4554714)\n[![R-CMD-check](https://github.com/MikkoVihtakari/ggOceanMaps/workflows/R-CMD-check/badge.svg)](https://github.com/MikkoVihtakari/ggOceanMaps/actions)\n[![CRAN_Status_Badge](https://www.r-pkg.org/badges/version/ggOceanMaps)](https://CRAN.R-project.org/package=ggOceanMaps)\n<!-- badges: end -->\n\n<!-- [![R-CMD-check](https://github.com/MikkoVihtakari/ggOceanMaps/workflows/R-CMD-check/badge.svg)](https://github.com/MikkoVihtakari/ggOceanMaps/actions/workflows/R-CMD-check.yaml) -->\n\n## Overview\n\nThe ggOceanMaps package for [R](https://www.r-project.org/) allows plotting data on bathymetric maps using [ggplot2](https://ggplot2.tidyverse.org/reference). The package is designed for ocean sciences and greatly simplifies bathymetric map plotting anywhere around the globe. ggOceanMaps uses openly available geographic data. Citing the particular data sources is advised by the CC-BY licenses whenever maps from the package are published (see the [*Citations and data sources*](#citations-and-data-sources) section). \n\nThe ggOceanMaps package has been developed by the [Institute of Marine Research](https://www.hi.no/en). Note that the package comes with absolutely no warranty and that maps generated by the package are meant for plotting scientific data only. The maps are coarse generalizations of third-party data and therefore inaccurate. Any [bug reports and code fixes](https://github.com/MikkoVihtakari/ggOceanMaps/issues) are warmly welcomed. See [*Contributions*](#contributions) for further details.\n\n## Installation\n\nThe package is available on [CRAN](https://CRAN.R-project.org/package=ggOceanMaps) and as a [GitHub version](https://github.com/MikkoVihtakari/ggOceanMaps), which is updated more frequently than the CRAN version. \n\nInstallation of the CRAN version:\n\n```{r eval = FALSE}\ninstall.packages(\"ggOceanMaps\")\n```\n\nDue to the package size limitations, ggOceanMaps requires the [ggOceanMapsData](https://github.com/MikkoVihtakari/ggOceanMapsData) package which stores shapefiles used in low-resolution maps.\n\nThe GitHub version of ggOceanMaps can be installed using the [**devtools**](https://cran.r-project.org/web/packages/devtools/index.html) package. \n\n```{r eval = FALSE}\ndevtools::install_github(\"MikkoVihtakari/ggOceanMapsData\") # required by ggOceanMaps\ndevtools::install_github(\"MikkoVihtakari/ggOceanMaps\")\n```\n\n## Usage\n\n**ggOceanMaps** extends on [**ggplot2**](http://ggplot2.tidyverse.org/reference/). The package uses spatial shapefiles, [GIS packages for R](https://cran.r-project.org/web/views/Spatial.html) to manipulate, and the [**ggspatial**](https://cran.r-project.org/web/packages/ggspatial/index.html) package to help to plot these shapefiles. The shapefile plotting is conducted internally in the `basemap` function and uses [ggplot's sf object plotting capabilities](https://ggplot2.tidyverse.org/reference/ggsf.html). Maps are plotted using the `basemap()` or `qmap()` functions that work almost similarly to [`ggplot()` as a base](https://ggplot2.tidyverse.org/reference/index.html) for adding further layers to the plot using the `+` operator. The maps generated this way already contain multiple ggplot layers. Consequently, the [`data` argument](https://ggplot2.tidyverse.org/reference/ggplot.html) needs to be explicitly specified inside `geom_*` functions when adding `ggplot2` layers. Depending on the location of the map, the underlying coordinates may be projected. Decimal degree coordinates need to be transformed to the projected coordinates using the `transform_coord`, [ggspatial](https://paleolimbot.github.io/ggspatial/), or [`geom_sf` functions.](https://ggplot2.tidyverse.org/reference/ggsf.html)\n\n```{r}\nlibrary(ggOceanMaps)\n\ndt <- data.frame(lon = c(-30, -30, 30, 30), lat = c(50, 80, 80, 50))\n\nbasemap(data = dt, bathymetry = TRUE) + \n  geom_polygon(data = transform_coord(dt), aes(x = lon, y = lat), color = \"red\", fill = NA)\n```\n\nSee the [ggOceanMaps website](https://mikkovihtakari.github.io/ggOceanMaps/index.html), [function reference](https://mikkovihtakari.github.io/ggOceanMaps/reference/index.html), and the [user manual](https://mikkovihtakari.github.io/ggOceanMaps/articles/ggOceanMaps.html) for how to use and modify the maps plotted by the package. You may also find [these slides about the package](https://aen-r-workshop.github.io/4-ggOceanMaps/ggOceanMaps_workshop.html#1) useful. \n\n## Data path\n\nWhile ggOceanMaps allows plotting any custom-made shapefiles, the package contains a shortcut to plot higher resolution maps for [certain areas needed by the author](https://github.com/MikkoVihtakari/ggOceanMapsLargeData/tree/master/data) without the need of generating the shapefiles manually. These high-resolution shapefiles are downloaded from the [ggOceanMapsLargeData](https://github.com/MikkoVihtakari/ggOceanMapsLargeData) repository. As a default, the shapefiles are downloaded into a temporary directory meaning that the user would need to download the large shapefiles every time they restart R. This limitation is set by [CRAN policies](https://cran.r-project.org/web/packages/policies.html). You can define a custom folder for high-resolution shapefiles on your computer by modifying your .Rprofile file (e.g. `usethis::edit_r_profile()`). Add the following lines to the file: \n\n```{r eval = FALSE}\n.ggOceanMapsenv <- new.env()\n.ggOceanMapsenv$datapath <- 'YourCustomPath'\n```\n\nIt is smart to use a directory R has writing access to. For example `\"~/Documents/ggOceanMapsLargeData\"` would work for most operating systems.\n\nYou will need to set up the data path to your .Rprofile file only once and ggOceanMaps will find the path even though you updated your R or packages. ggOceanMaps will inform you about your data path when you load the package. \n\n## Citations and data sources\n\nThe data used by the package are not the property of the Institute of Marine Research nor the author of the package. It is, therefore, important that you cite the data sources used in a map you generate with the package. The spatial data used by this package have been acquired from the following sources:\n\n- **ggOceanMapsData land polygons.** [Natural Earth Data](https://www.naturalearthdata.com/downloads/10m-physical-vectors/) 1:10m Physical Vectors with the Land and Minor Island datasets combined. Distributed under the [CC Public Domain license](https://creativecommons.org/publicdomain/) ([terms of use](https://www.naturalearthdata.com/about/terms-of-use/)).\n- **ggOceanMapsData glacier polygons.** [Natural Earth Data](https://www.naturalearthdata.com/downloads/10m-physical-vectors/) 1:10m Physical Vectors with the Glaciated Areas and Antarctic Ice Shelves datasets combined. Distributed under the [CC Public Domain license](https://creativecommons.org/publicdomain/) ([terms of use](https://www.naturalearthdata.com/about/terms-of-use/)).\n- **ggOceanMapsData bathymetry.** [Amante, C. and B.W. Eakins, 2009. ETOPO1 1 Arc-Minute Global Relief Model: Procedures, Data Sources and Analysis. NOAA Technical Memorandum NESDIS NGDC-24. National Geophysical Data Center, NOAA](https://www.ngdc.noaa.gov/mgg/global/relief/ETOPO1/docs/ETOPO1.pdf). Distributed under the [U.S. Government Work license](https://www.usa.gov/government-works).\n- **Detailed shapefiles of Svalbard and Norwegian coast in [ggOceanMapsLargeData](https://github.com/MikkoVihtakari/ggOceanMapsLargeData)** are from [Geonorge.no](https://www.geonorge.no/). Distributed under the [CC BY 4.0 license](https://creativecommons.org/licenses/by/4.0/).\n- **Detailed bathymetry of the Barents Sea in [ggOceanMapsLargeData](https://github.com/MikkoVihtakari/ggOceanMapsLargeData)** is vectorized from the [General Bathymetric Chart of the Oceans](https://www.gebco.net/data_and_products/gridded_bathymetry_data/) 15-arcsecond 2020 grid. [Terms of use](https://www.gebco.net/data_and_products/gridded_bathymetry_data/gebco_2019/grid_terms_of_use.html)\n\nFurther, please cite the package whenever maps generated by the package are published. For up-to-date citation information, please use:\n\n```{r}\ncitation(\"ggOceanMaps\")\n```\n\n## Getting help\n\nIf your problem does not involve bugs in ggOceanMaps, the quickest way of getting help is posting your problem to [Stack Overflow](https://stackoverflow.com/questions/tagged/r). Please remember to include a reproducible example that illustrates your problem.\n\n## Contributions\n\nAny contributions to the package are more than welcome. Please contact the package maintainer Mikko Vihtakari (<mikko.vihtakari@hi.no>) to discuss your ideas on improving the package. Bug reports and corrections should be submitted directly to [the GitHub site](https://github.com/MikkoVihtakari/ggOceanMaps/issues). Please include a [minimal reproducible example](https://en.wikipedia.org/wiki/Minimal_working_example). Considerable contributions to the package development will be credited with authorship. \n\n## Debugging installation\n\nAfter a successful installation, the following code should return a plot shown under\n\n```{r}\nlibrary(ggOceanMaps)\nbasemap(60)\n```\n\nIf the `basemap()` function complains about ggOceanMapsData package not being available, the drat repository may have issues (assuming you followed the installation instructions above). Try installing the ggOceanMapsData package using the devtools/remotes package. The data package does not contain any C++ code and should compile easily. \n\nIf you encounter problems during the devtools installation, you may set the `upgrade` argument to `\"never\"` and try the following steps: \n\n1. Manually update all R packages you have installed (Packages -> Update -> Select all -> Install updates in R Studio). If an update of a package fails, try installing that package again using the `install.packages` function or the R Studio menu. \n1. Run `devtools::install_github(\"MikkoVihtakari/ggOceanMaps\", upgrade = \"never\")`.\n1. If the installation of a dependency fails, try installing that package manually and repeat step 2.\n1. Since R has lately been updated to 4.0, you may have to update your R to the latest major version for all dependencies to work (`stars`, `rgdal` and `sf` have been reported to cause trouble during the installation).\n"
 },
 {
  "repo": "rowg/hfrprogs",
  "language": "MATLAB",
  "readme_contents": "HFR_Progs: High Frequency Radar Program Suite\n\n$Id: README 351 2007-03-09 01:31:02Z dmk $\n\n*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n\nMore detailed documentation will eventually be available at:\n\nProject webpage: https://github.com/rowg/hfrprogs\n\n*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n\nThis suite covers basic processing and analysis of HF Radar data.  It\nis an updated and expanded version of HFRadarmap (by Mike Cook) or\nHFRC (by David Kaplan).  The principal authors of this suite are David\nKaplan (UCSC), Mike Cook (NPS) and Dan Atwater (UCSC/NPS).\n\nMuch of this toolbox is released under the Gnu General Public License,\nthe text of which is included with the toolbox at\nHFR_Progs/license.gpl.txt.  In a nutshell, this license says that the\ncode may be modified and redistributed freely, provided that all\nmodifications to the source code are also made freely available.\n\n*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n\nInstallation Quickstart:\n------------------------\n\nThese instructions are for suitably configured Unix-like systems\n(e.g., Linux, Mac OSX).\n\n1) Unpack\n\n$ unzip HFR_Progs-VERSION.zip\n\n2) Configure Matlab\n\n$ cd HFR_Progs\n$ make\n$ make all_gshhs\n$ cd matlab\n$ matlab\nMATLAB>> HFRPdemo\n\n3) Configure Perl (if desired)\n\n$ sudo perl -MCPAN -e \"install 'MODULE::NAME'\"\n\nIf using the Perl scripts, at a minimum the following modules are\nnecessary: Date::Calc, List::Compare, Net::SSH, Net::SCP,\nConvert::ASN1\n\n*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n\nInstallation:\n-------------\n\n1) Unpack the suite in a suitable location\n\nThe suite is distributed in a zipped archive.  On unix-like systems,\nthis can be expanded by:\n\n$ unzip HFR_Progs-VERSION.zip\n\nThis will create a directory called HFR_Progs-VERSION with the suite\ninside.\n\n2) Install the external Matlab packages\n\nThe core of this suite is the Matlab functions and scripts.  This code\ndepends on several freely available software packages: m_map, openMA,\nt_tide, arrow, mexnc.  All but m_map are to one degree or another\noptional.  These packages can be downloaded off the web and placed in\nthe HFR_Progs/matlab/external_matlab_packages directory.\n\nTo ease installation of these packages, we have created a Makefile\nthat will automatically download and install the packages on suitably\nconfigured systems.  These systems must have make, wget, tar and unzip\ninstalled.\n\nAssuming that these packages are installed, one can download and\ninstall m_map, openMA, t_tide and arrow with:\n\n$ cd HFR_Progs\n$ make\n\nYou may also want to use the high definition coastlines with m_map:\n\n$ make all_gshhs\n\nIf you want to generate netCDF files for use with Gnome, then you will\nneed to install the netCDF package and the mexnc Matlab interface for\nnetCDF.  Installing netCDF is fairly straightforward and explained at\nhttp://www.unidata.ucar.edu/software/netcdf/.  Once installed,\ninstalling and compiling mexnc may be possible with:\n\n$ make all_mexnc\n$ make compile_mexnc NETCDF=/PATH/TO/NETCDF\n\nThis requires a working mex executable.\n\n3) Install Perl modules\n\nPerl may be useful for moving files around and converting some file\nformats.  It is not required for using the Matlab parts of the\ntoolbox.\n\nIf you wish to use the Perl scripts, then certain Perl modules must be\ninstalled (it is assumed Perl itself is already installed).  The\nappropriate way to install these packages depends on the system, but\nthe following should work on many systems:\n\n$ sudo perl -MCPAN -e \"install 'MODULE::NAME'\"\n\nor perhaps\n\n$ su\n$ perl -MCPAN -e \"install 'MODULE::NAME'\"\n\nAt a minimum, the following modules are necessary: Date::Calc,\nList::Compare, Net::SSH, Net::SCP, Convert::ASN1\n\n4) Running matlab\n\nTo use the Matlab toolbox, the HFR_Progs/matlab directory and most\nsubdirectories must be on the Matlab path.  One way to achieve this is\nby starting Matlab from the HFR_Progs/matlab directory.  If Matlab is\nstarted from another directory, then the following should add the\nappropriate directories (and perform a couple of basic tasks, such as\ninitializing the random number generator):\n\nMATLAB>> cd HFR_Progs/matlab\nMATLAB>> startup\n\nAfter this has been done, you can change to any working directory and\nthe toolbox will be available.  You could also modify\nHFR_Progs/matlab/startup.m to fit your needs.\n\nThere is a demonstration of the toolbox along with some sample data in\nHFR_Progs/matlab/demo:\n\nMATLAB>> HFRPdemo\n\nWe are in the process of writing drivers that automate basic\nprocessing of HF Radar data.  These can be found in\nHFR_Progs/matlab/drivers.\n\nThere are a number of readme files inside the HFR_Progs/matlab\ndirectory and subdirectories that provide additional information about\nspecific aspects of the toolbox.\n\n\n"
 },
 {
  "repo": "lkilcher/dolfyn",
  "language": "Python",
  "readme_contents": "<img src=\"img/logo.png\" width=\"70\"> DOLfYN\n=======================\n![Build](https://github.com/lkilcher/dolfyn/actions/workflows/build.yml/badge.svg)\n[![Coverage Status](https://coveralls.io/repos/github/lkilcher/dolfyn/badge.svg?branch=master)](https://coveralls.io/github/lkilcher/dolfyn?branch=master)\n[![Documentation Status](https://readthedocs.org/projects/dolfyn/badge/?version=latest)](https://dolfyn.readthedocs.io/en/latest/?badge=latest)\n\nBIG NEWS!!!\n------\n\nHello everyone! Just so that you know, we have released\ndolfyn 1.0! This is a MAJOR REFACTOR of the code so that DOLfYN is\nnow built on xarray, rather than the somewhat contrived and\npurpose-built `pyDictH5` package.\n\nThis means that DOLfYN 1.0 is _not_ backwards compatible with\nearlier version. This, in turn, means two things:\n\n1. The data files (`.h5` files) you created with earlier versions\nof DOLfYN will no longer load with DOLfYN 1.0.0.\n2. The syntax of DOLfYN 1.0 is completely different from earlier version.\n\nBecause of this, it's probably easiest to continue using earlier\nversions of DOLfYN for your old data. If you want to bring some data\ninto DOLfYN 1.0, you will need to\n`dolfyn.read(binary_source_file.ext)`, and then refactor your code to\nwork properly with DOLfYN's new syntax. I may be providing some\nupdates to dolfyn 0.12 via the v0.12-backports branch (and associated\nreleases), but I doubt that will last long. If you are\nusing 0.13, we recommend switching to 1.0.\n\nVery sorry that we didn't communicate the plan for this change, but\nthe truth is that we simply don't know who our users are. The good\nnews is that I think in the long run this will make DOLfYN a much more\nrobust, powerful, and compatible tool -- especially because we now\nwrite/load xarray-formatted netcdf4 files, which is becoming a\nstandard.\n\nA **HUGE THANK YOU** to @jmcvey3 who did the vast majority of the work\nto make this happen.\n\n\nSummary\n------\n\nDOLfYN is the Doppler Oceanography Library for pYthoN.\n\nIt is designed to read and work with Acoustic Doppler Velocimeter\n(ADV) and Acoustic Doppler Profiler (ADP/ADCP) data. DOLfYN includes\nlibraries for reading binary Nortek(tm) and Teledyne RDI(tm) data\nfiles.\n* Read in binary data files from acoustic Doppler instruments\n* Clean data\n* Rotate vector data through coordinate systems (i.e. beam - instrument - Earth frames of reference)\n* Motion correction for buoy-mounted ADV velocity measurements (via onboard IMU data)\n* Bin/ensemble averaging\n* Calculate turbulence statistics\n\nDocumentation\n-------------\n\nFor details visit the \n[DOLfYN homepage](https://dolfyn.readthedocs.io/en/latest/).  \n\nInstallation\n------------\n\nDOLfYN requires Python 3.7 or later and a number of dependencies. See the \n[install page](https://dolfyn.readthedocs.io/en/latest/install.html)\nfor greater details.\n\nLicense\n-------\n\nDOLfYN is copyright through the National Renewable Energy Laboratory, \nPacific Northwest National Laboratory, and Sandia National Laboratories. \nThe software is distributed under the Revised BSD License.\nSee the [license](LICENSE.txt) for more information.\n\n"
 },
 {
  "repo": "jinbow/popy",
  "language": "Python",
  "readme_contents": "popy\n====\n\nCommonly used routines in Physical Oceanography."
 },
 {
  "repo": "nasa/podaacpy",
  "language": "Python",
  "readme_contents": "podaacpy\n========\n\n|DOI| |license| |PyPI| |documentation| |Travis| |Coveralls| |Requirements Status| |Anaconda-Server Version| |Anaconda-Server Downloads| \n\n|DeepSource|\n\n|image7|\n\nA python utility library for interacting with NASA JPL's\n`PO.DAAC <https://podaac.jpl.nasa.gov>`__\n\n\nSoftware DOI\n------------\n\nIf you are using Podaacpy in your research, please consider citing the software |DOI|. This DOI represents all versions, and will always resolve to the latest one. If you wish to reference actual versions, then please find the appropriate DOI's over at Zenodo.\n\n\nWhat is PO.DAAC?\n----------------\n\n| The Physical Oceanography Distributed Active Archive Center (PO.DAAC)\n  is an element of the\n| Earth Observing System Data and Information System\n  (`EOSDIS <https://earthdata.nasa.gov/>`__).\n| The EOSDIS provides science data to a wide community of users for\n  NASA's Science Mission Directorate.\n\nWhat does podaacpy offer?\n-------------------------\n\nThe library provides a Python toolkit for interacting with all\n`PO.DAAC Web Services v3.2.2 APIs <https://podaac.jpl.nasa.gov/ws>`__, namely\n\n-  `PO.DAAC Web Services <https://podaac.jpl.nasa.gov/ws/>`__: services\n   include\n-  `Dataset\n   Metadata <https://podaac.jpl.nasa.gov/ws/search/dataset/index.html>`__\n   - retrieves the metadata of a dataset\n-  `Granule\n   Metadata <https://podaac.jpl.nasa.gov/ws/metadata/granule/index.html>`__\n   - retrieves the metadata of a granule\n-  `Search\n   Dataset <https://podaac.jpl.nasa.gov/ws/search/dataset/index.html>`__\n   - searches PO.DAAC's dataset catalog, over Level 2, Level 3, and\n   Level 4 datasets\n-  `Search\n   Granule <https://podaac.jpl.nasa.gov/ws/search/granule/index.html>`__\n   - does granule searching on PO.DAAC level 2 swath datasets\n   (individual orbits of a satellite), and level 3 & 4 gridded datasets\n   (time averaged to span the globe)\n-  `Image\n   Granule <https://podaac.jpl.nasa.gov/ws/image/granule/index.html>`__ -\n   renders granules in the PO.DAAC's catalog to images such as jpeg\n   and/or png\n-  `Extract\n   Granule <https://podaac.jpl.nasa.gov/ws/extract/granule/index.html>`__\n   - subsets a granule in PO.DAAC catalog and produces either netcdf3 or\n   hdf4 files\n\n-  | `Metadata Compliance\n     Checker <https://podaac-uat.jpl.nasa.gov/mcc>`__: an online tool and\n     web\n   | service designed to check and validate the contents of netCDF and\n     HDF granules for the\n   | Climate and Forecast (CF) and Attribute Convention for Dataset\n     Discovery (ACDD) metadata conventions.\n\n-  | `Level 2 Subsetting \n      <https://podaac-tools.jpl.nasa.gov/hitide/>`__: allows users to subset \n      and download popular PO.DAAC level 2 (swath) datasets.\n\n-  | `PO.DAAC Drive <https://podaac-tools.jpl.nasa.gov/drive/>`__: an HTTP based \n      data access service. PO.DAAC Drive replicates much of the functionality \n      of FTP while addressing many of its issues.\n\nAdditionally, Podaacpy provides the following ocean-related data services \n\n- `NASA OceanColor Web <https://oceancolor.gsfc.nasa.gov>`_:\n\n- `File Search <https://oceandata.sci.gsfc.nasa.gov/api/file_search>`_ -  locate publically available files within the NASA Ocean Data Processing System (ODPS)\n- `Bulk data downloads via HTTP <https://oceancolor.gsfc.nasa.gov/forum/oceancolor/topic_show.pl?pid=12520>`_ - mimic FTP bulk data downloads using the `HTTP-based data distribution server <https://oceandata.sci.gsfc.nasa.gov>`_.\n\nInstallation\n------------\n\nFrom the cheeseshop\n\n::\n\n    pip3 install podaacpy\n    \nor from conda\n\n::\n\n    conda install -c conda-forge podaacpy    \n\nor from source\n\n::\n\n    git clone https://github.com/nasa/podaacpy.git && cd podaacpy\n    python3 setup.py install\n\nQuickstart\n----------\nCheck out the **examples** directory for our Jupyter notebook examples.\n\nTests\n-----\n\n| podaacpy uses the popular\n  `nose <http://nose.readthedocs.org/en/latest/>`__ testing suite for\n  unit tests.\n| You can run the podaacpy tests simply by running\n\n::\n\n    nosetests\n\nAdditonally, click on the build sticker at the top of this readme to be\ndirected to the most recent build on\n`travis-ci <https://travis-ci.org/nasa/podaacpy>`__.\n\nDocumentation\n-------------\n\nYou can view the documentation online at\n\nhttp://podaacpy.readthedocs.org/en/latest/\n\nAlternatively, you can build the documentation manually as follows\n\n::\n\n    cd docs && make html\n\nDocumentation is then available in docs/build/html/\n\nCommunity, Support and Development\n----------------------------------\n\n| Please open a ticket in the `issue\n  tracker <https://github.com/nasa/podaacpy/issues>`__.\n| Please use\n  `labels <https://help.github.com/articles/applying-labels-to-issues-and-pull-requests/>`__\n  to\n| classify your issue.\n\nLicense\n-------\n\n| podaacpy is licensed permissively under the `Apache License\n  v2.0 <http://www.apache.org/licenses/LICENSE-2.0>`__.\n| A copy of that license is distributed with this software.\n\nCopyright and Export Classification\n-----------------------------------\n\n::\n\n    Copyright 2016-2019, by the California Institute of Technology. ALL RIGHTS RESERVED. \n    United States Government Sponsorship acknowledged. Any commercial use must be \n    negotiated with the Office of Technology Transfer at the California Institute \n    of Technology.\n    This software may be subject to U.S. export control laws. By accepting this software, \n    the user agrees to comply with all applicable U.S. export laws and regulations. \n    User has the responsibility to obtain export licenses, or other export authority \n    as may be required before exporting such information to foreign countries or \n    providing access to foreign persons.\n\n.. |DOI| image:: https://zenodo.org/badge/DOI/10.5281/zenodo.1751972.svg\n   :target: https://doi.org/10.5281/zenodo.1751972\n.. |license| image:: https://img.shields.io/github/license/nasa/podaacpy.svg?maxAge=2592000\n   :target: http://www.apache.org/licenses/LICENSE-2.0\n.. |PyPI| image:: https://img.shields.io/pypi/v/podaacpy.svg?maxAge=2592000?style=plastic\n   :target: https://pypi.python.org/pypi/podaacpy\n.. |documentation| image:: https://readthedocs.org/projects/podaacpy/badge/?version=latest\n   :target: http://podaacpy.readthedocs.org/en/latest/\n.. |Travis| image:: https://img.shields.io/travis/nasa/podaacpy.svg?maxAge=2592000?style=plastic\n   :target: https://travis-ci.org/nasa/podaacpy\n.. |Coveralls| image:: https://coveralls.io/repos/github/nasa/podaacpy/badge.svg?branch=master\n   :target: https://coveralls.io/github/nasa/podaacpy?branch=master\n.. |Requirements Status| image:: https://requires.io/github/nasa/podaacpy/requirements.svg?branch=master\n   :target: https://requires.io/github/nasa/podaacpy/requirements/?branch=master\n.. |Anaconda-Server Version| image:: https://anaconda.org/conda-forge/podaacpy/badges/version.svg\n   :target: https://anaconda.org/conda-forge/podaacpy\n.. |Anaconda-Server Downloads| image:: https://anaconda.org/conda-forge/podaacpy/badges/downloads.svg\n   :target: https://anaconda.org/conda-forge/podaacpy\n.. |image7| image:: https://podaac.jpl.nasa.gov/sites/default/files/image/custom_thumbs/podaac_logo.png\n.. |DeepSource| image:: https://static.deepsource.io/deepsource-badge-light.svg\n    :target: https://deepsource.io/gh/nasa/podaacpy/?ref=repository-badge\n\n"
 },
 {
  "repo": "ProjectPythiaCookbooks/physical-oceanography-cookbook",
  "language": "Jupyter Notebook",
  "readme_contents": "<img src=\"thumbnail.png\" alt=\"thumbnail\" width=\"300\"/>\n\n# Physical Oceanography Cookbook\n\n[![nightly-build](https://github.com/ProjectPythiaCookbooks/physical-oceanography-cookbook/actions/workflows/nightly-build.yaml/badge.svg)](https://github.com/ProjectPythiaCookbooks/physical-oceanography-cookbook/actions/workflows/nightly-build.yaml)\n[![Binder](https://binder-staging.2i2c.cloud/badge_logo.svg)](https://binder-staging.2i2c.cloud/v2/gh/ProjectPythiaCookbooks/physical-oceanography-cookbook/main?labpath=notebooks)\n\nThis Project Pythia Cookbook covers ... (replace `...` with the main subject of your cookbook ... e.g., *working with radar data in Python*)\n\n## Motivation\n\n(Add a few sentences stating why this cookbook will be useful. What skills will you, \"the chef\", gain once you have reached the end of the cookbook?)\n\n## Authors\n\n[First Author](@first-author), [Second Author](@second-author), etc. *Acknowledge primary content authors here*\n\n### Contributors\n\n<a href=\"https://github.com/ProjectPythiaCookbooks/physical-oceanography-cookbook/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=ProjectPythiaCookbooks/physical-oceanography-cookbook\" />\n</a>\n\n## Structure\n(State one or more sections that will comprise the notebook. E.g., *This cookbook is broken up into two main sections - \"Foundations\" and \"Example Workflows.\"* Then, describe each section below.)\n\n### Section 1 ( Replace with the title of this section, e.g. \"Foundations\" )\n(Add content for this section, e.g., \"The foundational content includes ... \")\n\n### Section 2 ( Replace with the title of this section, e.g. \"Example workflows\" )\n(Add content for this section, e.g., \"Example workflows include ... \")\n\n## Running the Notebooks\nYou can either run the notebook using [Binder](https://mybinder.org/) or on your local machine.\n\n### Running on Binder\n\nThe simplest way to interact with a Jupyter Notebook is through\n[Binder](https://mybinder.org/), which enables the execution of a\n[Jupyter Book](https://jupyterbook.org) in the cloud. The details of how this works are not\nimportant for now. All you need to know is how to launch a Pythia\nCookbooks chapter via Binder. Simply navigate your mouse to\nthe top right corner of the book chapter you are viewing and click\non the rocket ship icon, (see figure below), and be sure to select\n\u201claunch Binder\u201d. After a moment you should be presented with a\nnotebook that you can interact with. I.e. you\u2019ll be able to execute\nand even change the example programs. You\u2019ll see that the code cells\nhave no output at first, until you execute them by pressing\n{kbd}`Shift`\\+{kbd}`Enter`. Complete details on how to interact with\na live Jupyter notebook are described in [Getting Started with\nJupyter](https://foundations.projectpythia.org/foundations/getting-started-jupyter.html).\n\n### Running on Your Own Machine\nIf you are interested in running this material locally on your computer, you will need to follow this workflow:\n\n(Replace \"cookbook-example\" with the title of your cookbooks)   \n\n1. Clone the `https://github.com/ProjectPythiaCookbooks/cookbook-example` repository:\n\n   ```bash\n    git clone https://github.com/ProjectPythiaCookbooks/cookbook-example.git\n    ```  \n1. Move into the `cookbook-example` directory\n    ```bash\n    cd cookbook-example\n    ```  \n1. Create and activate your conda environment from the `environment.yml` file\n    ```bash\n    conda env create -f environment.yml\n    conda activate cookbook-example\n    ```  \n1.  Move into the `notebooks` directory and start up Jupyterlab\n    ```bash\n    cd notebooks/\n    jupyter lab\n    ```\n"
 },
 {
  "repo": "IHCantabria/THREDDSExplorer",
  "language": "Python",
  "readme_contents": "## Synopsis\n\nTHREDDS Explorer is a QGIS-based plug-in designed to make it easy for users to access georeferenced data accessible through any THREDDS based server.  \n\nData catalogues and maps are exposed to the user through a simple user interface, which allows him to choose any map or explore all contents of the server without resorting to exploring the default web based THREDDS interface.\n\nThis plug-in should work with most THREDDS servers, and will be able to retrieve any layer provided through WMS, WMS-T and/or WCS services published by the server.\n\n[![THREDDS Explorer](https://raw.githubusercontent.com/IHCantabria/THREDDSExplorer/master/doc/video.jpg)](https://vimeo.com/167414368)\n\n## Installation\n\n**Dependencies:** THREDDSExplorer requires the *processing* plug-in by V. Olaya, available within the standard QGIS Installation (also at https://plugins.qgis.org/plugins/processing/).\n\nInstalling the plug-in basically involves copying the code in a QGIS \"plugins\" directory, as detailed below.\n\nPlease note that it is mandatory that the folder where the code resides is called \"THREDDSExplorer\". If you download the code in ZIP format the default name (\"THREDDSExplorer-$VERSION\") must be changed to \"THREDDSExplorer\".\n\n### For Windows\n\nThe folder where the code should be copied is the following, substituting \"`%USERNAME%`\" with your user name:\n\n\tC:\\Users\\%USERNAME%\\AppData\\Roaming\\QGIS\\QGIS3\\profiles\\default\\python\\plugins\n\n\n### For Linux\n\nNot tested yet. \n\n### For Mac OS X\n\nNot tested yet.\n\n## User Manual\n\nFor more information you can find a PDF manual under the \"doc\" folder.\n"
 },
 {
  "repo": "chadagreene/CDT",
  "language": "HTML",
  "readme_contents": "[![DOI](https://zenodo.org/badge/171331090.svg)](https://zenodo.org/badge/latestdoi/171331090) [![View Climate Data Toolbox for MATLAB on File Exchange](https://www.mathworks.com/matlabcentral/images/matlab-file-exchange.svg)](https://www.mathworks.com/matlabcentral/fileexchange/70338-climate-data-toolbox-for-matlab)\n\n![Climate Data Toolbox for Matlab](CDT_reduced.jpg)\n\n# CDT Contents and Documentation\n[**Click here**](https://www.chadagreene.com/CDT/CDT_Contents.html) to view the CDT documentation online.\n\nA version of the documentation in Chinese can be found [here](https://github.com/chadagreene/CDT/files/6985814/CDT.pdf), translated by Shi Weiheng. \u4e2d\u6587\u6587\u6863\u7531Shi Weiheng\u7ffb\u8bd1\u3002[CDT \u4e2d\u6587\u7ffb\u8bd1.pdf](https://github.com/chadagreene/CDT/files/6985814/CDT.pdf)\n\n# Video Tutorials\n### Analyzing Trends and Global Warming:\n[![How to analyze trends in sea surface temperature using CDT](https://img.youtube.com/vi/t46dTVp7NHY/0.jpg)](https://www.youtube.com/watch?v=t46dTVp7NHY \"The Climate Data Toolbox for MATLAB - Analyzing Trends & Global Warming\")\n\n### El Ni\u00f1o and Empirical Orthogonal Functions:\n[![How to apply EOFs to identify ENSO](https://img.youtube.com/vi/A5UjLO-67GQ/0.jpg)](https://www.youtube.com/watch?v=A5UjLO-67GQ \"The Climate Data Toolbox for MATLAB - El Ni\u00f1o and Empirical Orthogonal Functions\")\n\n# Installing the toolbox:\nThere are a few different ways to install this toolbox. Pick your favorite among the following:\n\n### ...from the Add-On Explorer in MATLAB: \nIn the Home menu of MATLAB, click Add-Ons, and search for Climate Data Toolbox. Click \"Add from GitHub\" and that's all you need to do. Installing this way is easy and will provide the most up-to-date version available. \n\n### ...or as individual files and folders:\nThe files in this GitHub repository will always be the most up to date. So if you want to be on the bleeding edge of innovation, get the cdt folder, put it somewhere Matlab can find it, and then right-click on it from within Matlab and select \"Add to Path--Selected folder and subfolders.\"\n\n### ...as a .mltbx toolbox: \nInstalling as an .mltbx is perhaps the easiest option, but I don't update the .mltbx very often, so you might not get the latest features and bug fixes. \n\nFirst, download the ~100 MB .mltbx file [here](https://chadagreene.com/ClimateDataToolbox.mltbx). After downloading the .mltbx file, installation should be as easy as double clicking on the zip file and clicking \"install\". Or you can navigate to it in the Matlab file explorer, right click on the .mltbx, and click \"Install.\" \n\nThe installation process puts the files in a folder called something like:\n\n```~MATLAB/Add-Ons/Toolboxes/Climate Data Toolbox/```\n\nIf that's not correct, find the CDT folder by typing this into the Matlab Command Window: \n\n```which cdt -all```\n\nIf the which hunt still turns up nothing, that suggests the toolbox hasn't been properly installed. \n\n\n# After installation:\nType \n\n```cdt```\n\ninto the command line to check out the documentation.\n\n# Citing CDT: \nPlease cite our paper! \n\nChad A. Greene, Kaustubh Thirumalai, Kelly A. Kearney, Jos\u00e9 Miguel Delgado, Wolfgang Schwanghart, Natalie S. Wolfenbarger, Kristen M. Thyng, David E. Gwyther, Alex S. Gardner, and Donald D. Blankenship (2019). The Climate Data Toolbox for MATLAB. _Geochemistry, Geophysics, Geosystems,_ 20, 3774-3781. [doi:10.1029/2019GC008392](https://doi.org/10.1029/2019GC008392)\n\nThe Climate Data Toolbox is also mirrored on the MathWorks File Exchange site [here](https://www.mathworks.com/matlabcentral/fileexchange/70338).\n"
 },
 {
  "repo": "JuliaOcean/JuliaOceanSciencesMeeting2020",
  "language": null,
  "readme_contents": "# Julia Ocean Sciences Meeting 2020 workshop\n\n**Post OSM20 update :**\n\n- **video recording of the workshop session** is [available here](https://youtu.be/xOeB-uwoMI0)\n- [workshop](https://agu.confex.com/agu/osm20/meetingapp.cgi/Session/92105) title : Julia (language) users and tools for oceanography\n- time : **took place on** Tuesday, 18 February 2020 @ 12:45 - 13:45\n- abstract:\n\nThere has been a visible uptick in oceanography and climate applications of the Julia language since it reached the v1.0 milestone last year. The growth and appeal for this language were recently highlighted by Nature magazine. It seems very timely to offer this rapidly growing community of open source developers and users an opportunity to meet in person, advertise their recent efforts, and engage with the oceanographic community at large. A tentative agenda for this workshop would include: a brief general presentation of Julia, a survey of existing efforts, a hackathon-type session with both experienced and new users interacting, and open-ended discussion time.\n\n## Workshop outline\n\n_below is the current, still evolving, workshop outline_\n\n### intro (G.F. 15')\n\n- workshop outline and motivation\n- why should you want to use Julia?\n- how can you get started with Julia?\n\t- [julialan.org](https://julialang.org)\n- what are Julia's salient features?\n- state of the ocean & climate stack?\n\n### state of the Julia ocean stack (10 x 2' contributions)\n\n\nAs for almost all open source efforts, listed packages should be regarded as collaborative & ongoing development projects. They are under version control, documented as much as possible, and can generally be installed via `Julia`'s awesome [package manager](https://julialang.github.io/Pkg.jl/v1/).\n\n_The list is a work in progress. Also categories 1. 2. and 3. below are approximate -- some packages arguably belong in 1. 2. and 3._\n\n1. plotting, I/O, and data analysis packages\n\t- [Plots.jl](http://docs.juliaplots.org/latest/), [PyPlot.jl](https://github.com/JuliaPy/PyPlot.jl), [Makie.jl](http://makie.juliaplots.org/stable), ... (volunteer ?)\n\t- [NetCDF.jl](https://juliageo.org/NetCDF.jl/dev/), [NCTiles.jl](https://github.com/gaelforget/NCTiles.jl), [Zarr.jl](https://meggart.github.io/Zarr.jl/latest/), [CSV.jl](https://juliadata.github.io/CSV.jl/stable/), ... (volunteer ?)\n\t- [DataFrames.jl](http://juliadata.github.io/DataFrames.jl/stable/), [ClimateTools.jl](https://juliaclimate.github.io/ClimateTools.jl/stable/), [ESDL.jl](https://github.com/esa-esdl/ESDL.jl), ... (volunteer ?)\n\n2. common ocean models, analysis tools, and data sets\n\t- [GlobalOceanNotebooks](https://github.com/JuliaClimate/GlobalOceanNotebooks), [MeshArrays.jl](https://juliaclimate.github.io/MeshArrays.jl/dev/), [IndividualDisplacements.jl](https://juliaclimate.github.io/IndividualDisplacements.jl/dev/), [ArgoData.jl](https://gaelforget.github.io/ArgoData.jl/dev/), [ClimateTasks.jl](https://gaelforget.github.io/ClimateTasks.jl/dev/), ... (@gaelforget)\n\t- [AIBECS.jl](https://briochemc.github.io/AIBECS.jl/stable/), [WorldOceanAtlasTools.jl](https://github.com/briochemc/WorldOceanAtlasTools.jl), [OceanographyCruises.jl](https://github.com/briochemc/OceanographyCruises.jl), ... (@briochemc)\n\t- [NCDatasets.jl](https://alexander-barth.github.io/NCDatasets.jl/dev/), [DIVAnd.jl](https://gher-ulg.github.io/DIVAnd.jl/latest/), [PhysOcean.jl](https://github.com/gher-ulg/PhysOcean.jl) (@Alexander-Barth, to be confirmed)\n\n3. models written in Julia\n\n\t- [ShallowWaters.jl](https://github.com/milankl/ShallowWaters.jl) (@milankl)\n\t- [Oceananigans.jl](https://github.com/climate-machine/Oceananigans.jl) (@ali-ramadan or @glwagner)\n\t- [geophysicalflows.jl](https://github.com/FourierFlows/GeophysicalFlows.jl) (@navidcy or @glwagner)\n\n_New to github etc? See e.g. [these guides](https://guides.github.com) and `Lecture03` in [these tutorials](https://github.com/PraCTES/MIT-PraCTES); it's all user-friendly._\n\n### Q & A (20')\n\nPlease feel free to add questions ahead of, during, and after the workshop in [this thread](https://github.com/gaelforget/JuliaOceanSciencesMeeting2020/issues/4) for example.\n\n### looking forward (G.F. 5')\n\n- this repo; during & after OSM020\n- github organizations & contributors\n- JuliaClimate and JuliaOcean organizations\n"
 },
 {
  "repo": "Alexander-Barth/NCDatasets.jl",
  "language": "Julia",
  "readme_contents": "# NCDatasets\n\n[![Build Status](https://github.com/Alexander-Barth/NCDatasets.jl/workflows/CI/badge.svg)](https://github.com/Alexander-Barth/NCDatasets.jl/actions)\n[![codecov.io](http://codecov.io/github/Alexander-Barth/NCDatasets.jl/coverage.svg?branch=master)](http://codecov.io/github/Alexander-Barth/NCDatasets.jl?branch=master)\n[![documentation stable](https://img.shields.io/badge/docs-stable-blue.svg)](https://alexander-barth.github.io/NCDatasets.jl/stable/)\n[![documentation dev](https://img.shields.io/badge/docs-dev-blue.svg)](https://alexander-barth.github.io/NCDatasets.jl/dev/)\n\n\n`NCDatasets` allows one to read and create netCDF files.\nNetCDF data set and attribute list behave like Julia dictionaries and variables like Julia arrays.\n\n\nThe module `NCDatasets` provides support for the following [netCDF CF conventions](http://cfconventions.org/):\n* `_FillValue` will be returned as `missing` ([more information](https://docs.julialang.org/en/v1/manual/missing/))\n* `scale_factor` and `add_offset` are applied if present\n* time variables (recognized by the `units` attribute) are returned as `DateTime` objects.\n* Support of the [CF calendars](http://cfconventions.org/Data/cf-conventions/cf-conventions-1.7/cf-conventions.html#calendar) (standard, gregorian, proleptic gregorian, julian, all leap, no leap, 360 day)\n* The raw data can also be accessed (without the transformations above).\n* [Contiguous ragged array representation](http://cfconventions.org/Data/cf-conventions/cf-conventions-1.7/cf-conventions.html#_contiguous_ragged_array_representation)\n\nOther features include:\n* Support for NetCDF 4 compression and variable-length arrays (i.e. arrays of vectors where each vector can have potentailly a different length)\n* The module also includes an utility function [`ncgen`](https://alexander-barth.github.io/NCDatasets.jl/stable/dataset/#NCDatasets.ncgen) which generates the Julia code that would produce a netCDF file with the same metadata as a template netCDF file.\n\n## Installation\n\nInside the Julia shell, you can download and install the package by issuing:\n\n```julia\nusing Pkg\nPkg.add(\"NCDatasets\")\n```\n\nWindows users are required to pin the version of NetCDF_jll until this [issue](https://github.com/JuliaPackaging/Yggdrasil/issues/4511) is resolved (help is more than welcome).\n\n```julia\nusing Pkg\nPkg.add(\"NetCDF_jll\")\nPkg.pin(name=\"NetCDF_jll\", version=\"400.702.400\")\n```\n\n\n# Manual\n\nThis Manual is a quick introduction in using NCDatasets.jl. For more details you can read the [stable](https://alexander-barth.github.io/NCDatasets.jl/stable/) or [latest](https://alexander-barth.github.io/NCDatasets.jl/latest/) documentation.\n\n* [Explore the content of a netCDF file](#explore-the-content-of-a-netcdf-file)\n* [Load a netCDF file](#load-a-netcdf-file)\n* [Create a netCDF file](#create-a-netcdf-file)\n* [Edit an existing netCDF file](#edit-an-existing-netcdf-file)\n\n## Explore the content of a netCDF file\n\nBefore reading the data from a netCDF file, it is often useful to explore the list of variables and attributes defined in it.\n\nFor interactive use, the following commands (without ending semicolon) display the content of the file similarly to `ncdump -h file.nc`:\n\n```julia\nusing NCDatasets\nds = Dataset(\"file.nc\")\n```\n\nThis creates the central structure of NCDatasets.jl, `Dataset`, which represents the contents of the netCDF file (without immediatelly loading everything in memory). `NCDataset` is an alias for `Dataset`.\n\nThe following displays the information just for the variable `varname`:\n\n```julia\nds[\"varname\"]\n```\n\nwhile to get the global attributes you can do:\n```julia\nds.attrib\n```\nwhich produces a listing like:\n\n```\nDataset: file.nc\nGroup: /\n\nDimensions\n   time = 115\n\nVariables\n  time   (115)\n    Datatype:    Float64\n    Dimensions:  time\n    Attributes:\n     calendar             = gregorian\n     standard_name        = time\n     units                = days since 1950-01-01 00:00:00\n[...]\n```\n\n## Load a netCDF file\n\nLoading a variable with known structure can be achieved by accessing the variables and attributes directly by their name.\n\n```julia\n# The mode \"r\" stands for read-only. The mode \"r\" is the default mode and the parameter can be omitted.\nds = Dataset(\"/tmp/test.nc\",\"r\")\nv = ds[\"temperature\"]\n\n# load a subset\nsubdata = v[10:30,30:5:end]\n\n# load all data\ndata = v[:,:]\n\n# load all data ignoring attributes like scale_factor, add_offset, _FillValue and time units\ndata2 = v.var[:,:]\n\n\n# load an attribute\nunit = v.attrib[\"units\"]\nclose(ds)\n```\n\nIn the example above, the subset can also be loaded with:\n\n```julia\nsubdata = Dataset(\"/tmp/test.nc\")[\"temperature\"][10:30,30:5:end]\n```\n\nThis might be useful in an interactive session. However, the file `test.nc` is not directly closed (closing the file will be triggered by Julia's garbage collector), which can be a problem if you open many files. On Linux the number of opened files is often limited to 1024 (soft limit). If you write to a file, you should also always close the file to make sure that the data is properly written to the disk.\n\nAn alternative way to ensure the file has been closed is to use a `do` block: the file will be closed automatically when leaving the block.\n\n```julia\ndata =\nDataset(filename,\"r\") do ds\n    ds[\"temperature\"][:,:]\nend # ds is closed\n```\n\n## Create a netCDF file\n\nThe following gives an example of how to create a netCDF file by defining dimensions, variables and attributes.\n\n```julia\nusing NCDatasets\nusing DataStructures\n# This creates a new NetCDF file /tmp/test.nc.\n# The mode \"c\" stands for creating a new file (clobber)\nds = Dataset(\"/tmp/test.nc\",\"c\")\n\n# Define the dimension \"lon\" and \"lat\" with the size 100 and 110 resp.\ndefDim(ds,\"lon\",100)\ndefDim(ds,\"lat\",110)\n\n# Define a global attribute\nds.attrib[\"title\"] = \"this is a test file\"\n\n# Define the variables temperature with the attribute units\nv = defVar(ds,\"temperature\",Float32,(\"lon\",\"lat\"), attrib = OrderedDict(\n    \"units\" => \"degree Celsius\"))\n\n# add additional attributes\nv.attrib[\"comments\"] = \"this is a string attribute with Unicode \u03a9 \u2208 \u2211 \u222b f(x) dx\"\n\n# Generate some example data\ndata = [Float32(i+j) for i = 1:100, j = 1:110]\n\n# write a single column\nv[:,1] = data[:,1]\n\n# write a the complete data set\nv[:,:] = data\n\nclose(ds)\n```\n\n\n## Edit an existing netCDF file\n\nWhen you need to modify variables or attributes in a netCDF file, you have\nto open it with the `\"a\"` option. Here, for example, we add a global attribute *creator* to the\nfile created in the previous step.\n\n```julia\nds = Dataset(\"/tmp/test.nc\",\"a\")\nds.attrib[\"creator\"] = \"your name\"\nclose(ds);\n```\n\n\n# Benchmark\n\nThe benchmark loads a variable of the size 1000x500x100 in slices of 1000x500\n(applying the scaling of the CF conventions)\nand computes the maximum of each slice and the average of each maximum over all slices.\nThis operation is repeated 100 times.\nThe code is available at https://github.com/Alexander-Barth/NCDatasets.jl/tree/master/test/perf .\n\n\n| Module           | median | minimum |  mean | std. dev. |\n|:---------------- | ------:| -------:| -----:| ---------:|\n| R-ncdf4          |  0.572 |   0.550 | 0.575 |     0.023 |\n| python-netCDF4   |  0.504 |   0.498 | 0.505 |     0.003 |\n| julia-NCDatasets |  0.228 |   0.212 | 0.226 |     0.005 |\n\nAll runtimes are in seconds.\nJulia 1.6.0 (with NCDatasets b953bf5), R 3.4.4 (with ncdf4 1.17) and Python 3.6.9 (with netCDF4 1.5.4).\nThis CPU is a i7-7700.\n\n\n# Filing an issue\n\nWhen you file an issue, please include sufficient information that would _allow somebody else to reproduce the issue_, in particular:\n1. Provide the code that generates the issue.\n2. If necessary to run your code, provide the used netCDF file(s).\n3. Make your code and netCDF file(s) as simple as possible (while still showing the error and being runnable). A big thank you for the 5-star-premium-gold users who do not forget this point! \ud83d\udc4d\ud83c\udfc5\ud83c\udfc6\n4. The full error message that you are seeing (in particular file names and line numbers of the stack-trace).\n5. Which version of Julia and `NCDatasets` are you using? Please include the output of:\n```\nversioninfo()\nusing Pkg\nPkg.installed()[\"NCDatasets\"]\n```\n6. Does `NCDatasets` pass its test suite? Please include the output of:\n\n```julia\nusing Pkg\nPkg.test(\"NCDatasets\")\n```\n\n# Alternative\n\nThe package [NetCDF.jl](https://github.com/JuliaGeo/NetCDF.jl) from Fabian Gans and contributors is an alternative to this package which supports a more Matlab/Octave-like interface for reading and writing NetCDF files.\n\n# Credits\n\n`netcdf_c.jl`, `build.jl` and the error handling code of the NetCDF C API are from NetCDF.jl by Fabian Gans (Max-Planck-Institut f\u00fcr Biogeochemie, Jena, Germany) released under the MIT license.\n\n<!--  LocalWords:  NCDatasets codecov io NetCDF FillValue DataArrays\n -->\n<!--  LocalWords:  DateTime ncdump nc julia ds Dataset varname attrib\n -->\n<!--  LocalWords:  lon defDim defVar dx subdata println attname jl\n -->\n<!--  LocalWords:  attval filename netcdf API Gans Institut f\u00fcr Jena\n -->\n<!--  LocalWords:  Biogeochemie macOS haskey runnable versioninfo\n -->\n"
 },
 {
  "repo": "LijoDXL/OceanographyWithPython",
  "language": "Jupyter Notebook",
  "readme_contents": "# Python for data analysis in Oceanography\n\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/LijoDXL/OceanographyWithPython/master)\n[![License](https://img.shields.io/badge/License-MIT-blue.svg)](https://github.com/LijoDXL/OceanographyWithPython/blob/master/LICENSE)\n\n[![Twitter Follow](https://img.shields.io/twitter/follow/lijodxl?style=social)](https://twitter.com/LIJODXL)\n\n## Getting started\n\nYou can run this tutorial online without installing anything on your local machine just by clicking\n`launch Binder` icon from the top right corner of this page.\n\nIf you wish to have a local copy instead, follow the steps below:\n* Download [miniconda](https://docs.conda.io/en/latest/miniconda.html) (Python 3.x 64-bit version recommended) and install it using the instructions provided [here](https://conda.io/projects/conda/en/latest/user-guide/install/index.html). Skip this step if you already have miniconda/Anaconda installed.\n* Copy and paste the following command in your terminal to clone this repository:\n```bash\ngit clone https://github.com/LijoDXL/OceanographyWithPython.git\n```\n* Change to the cloned directory and create a new `conda` environment:\n```bash\ncd OceanographyWithPython\nconda env create -f PHYoceanENV.yml\n```\n* You can now activate the new environment by typing:\n```bash\nconda activate PHY_OCEAN\n# to deactivate env:\nconda deactivate\n```\n* Your terminal prompt will change to indicate the currently active environment. That's it, now try `JupyterLab` by typing in your termimal:\n```bash\njupyter lab\n```\n\n## Adding packages in your environment\n\nSuppose you want to install a new package to your `PHY_OCEAN` environment. Here is what you should do:\n* Check the package's documentation page for a section on installation. If available, follow the instructions there. It will be usually of the form `conda install -c <channel-name> <package-name>`.\n* If not, head over to [anaconda.org](https://anaconda.org/) and search with the package name there.\n* From the search results, click the one with most number of downloads (`conda-forge` channel preferred) to find the installation step.\n* Before you install, **do not forget** to activate your environment by typing:\n```bash\nconda activate PHY_OCEAN\n```\n\n```{note}\nMore details on managing environment with conda can be found [here](https://docs.conda.io/projects/conda/en/latest/user-guide/getting-started.html).\n```\n\n## Other tools worth learning\n\n* [Tmux](https://github.com/tmux/tmux/wiki): Easily organize your various terminal sessions. Highly useful if you work with remote sessions a lot.\n* [Git](https://git-scm.com): Version control your workflow. Never have the hassle of dealing with files like draft.doc,draft_edited.doc,draft_final_correction.doc,final_v1.doc,final_v2.doc,absolutely_final.doc.\n\n````{margin}\n```{note}\n* Learn more about git [[1]](http://swcarpentry.github.io/git-novice/)\n[[2]](https://barbagroup.github.io/essential_skills_RRC/git/git/)\n[[3]](https://www.atlassian.com/git/tutorials/comparing-workflows).\n\n* Learn more about Tmux [[1]](https://thoughtbot.com/blog/a-tmux-crash-course)\n[[2]](https://www.hamvocke.com/blog/a-quick-and-easy-guide-to-tmux/)\n[[3]](https://tmuxp.git-pull.com/en/latest/about_tmux.html).\n\n```\n````\n\n```{image} /assets/images/phdComic.jpg\n:align: \"center\"\n:scale: 50%\n:name: PhdComic\n```\n\n"
 },
 {
  "repo": "c-proof/pyglider",
  "language": "Roff",
  "readme_contents": "![](docs/_static/PyGliderHorizontal.svg)\n\nPython tools for interacting with ocean glider data.   PyGlider takes data from\nTeledyne/Webb Slocum gliders and Alseamar SeaExplorers and creates CF-compliant\nNetCDF files.\n\nFor documentation, please see <http://pyglider.readthedocs.io>\n\n### Contact\n\nGet in touch with us using Discussion above or by opening an issue.\n"
 },
 {
  "repo": "pangeo-gallery/physical-oceanography",
  "language": "Jupyter Notebook",
  "readme_contents": "# Pangeo Gallery Template\n\nThis repository stores an example gallery repo for the Pangeo Gallery.\nIt is configured to automatically build itself using\n[binderbot](https://github.com/pangeo-gallery/binderbot).\nIt is linked, via a git submodule, the the\n[gallery website repo](https://github.com/pangeo-gallery/pangeo-gallery).\nWhenever the notebooks are updated in this, repository\ndispatch is used to trigger a gallery rebuild. This keeps\n[gallery.pangeo.io](http://gallery.pangeo.io) always in sync with this repo.\n\nThe repo contains the following elements:\n\n- A set of jupyter notebooks, numbered in the order that we want them to\n  appear on the gallery website.\n- A configuration file, `binder-gallery.yaml`, which provides important\n  configuration parameters (see [pangeo gallery documentation](http://gallery.pangeo.io)).\n- Github workflows, which make the magic happen! (Don't touch these.)\n"
 },
 {
  "repo": "DamienIrving/capstone-oceanography",
  "language": "Jupyter Notebook",
  "readme_contents": "capstone-oceanography\n==========================\n\nIntroduction to using Python in the ocean, weather and climate sciences.\n\n> Please see [https://github.com/swcarpentry/lesson-example](https://github.com/swcarpentry/lesson-example)\n> for instructions on formatting, building, and submitting lessons,\n> or run `make` in this directory for a list of helpful commands."
 },
 {
  "repo": "physoce/physoce-py",
  "language": "Python",
  "readme_contents": "# physoce\n\nPython tools for physical oceanography. Includes functions for plotting, time series analysis, statistics and surface wave dynamics.\n\nDocumentation can be found at https://physoce.github.io/physoce-py\n\nTo install the latest version using pip, run the following the terminal/command prompt:\n\n`pip install git+https://github.com/physoce/physoce-py`\n\nThis package can also be installed from the Python Package Index (PyPI), which is updated less frequently:\n\n`pip install physoce`\n\nFeedback and ideas for future work are welcome. Please feel free to submit an issue.\n"
 },
 {
  "repo": "iuryt/Panthalassan",
  "language": "Jupyter Notebook",
  "readme_contents": "# Panthalassan\n\n|[Portugu\u00eas](https://github.com/iuryt/Panthalassan/blob/main/README_pt.md)|\n\n**Disclosure: This is a work in progress!!**\n\nThe idea is to develop a material for a course in Oceanographic Data Analysis and Modelling.\n\n## Justification\n\nProgramming is becoming an essential tool for most of the research in oceanography. With global numerical simulations down to submesoscale and higher-resolution observations, processing and analyzing big data in oceanography is only possible by coding the analyses by yourself and making use of top-level tools and parallel processing in collaborative platforms. Many under/graduate courses have tried to overcome this gap in their programs by integrating computational science into their main curriculum. The lack of resources in developing country institutions unbalance the opportunities in this area, but there are many open-source and cloud tools that could be used to introduce students to this new world. The language barrier is another problem. Most of the programming tutorials are in English, with pratically no online course for oceanographic data analysis in Portuguese and other languages. How can we overcome the language and technological barriers and give developing-country students the opportunity to learn programming data analysis? There are multiple answers to this question, but collaborative learning is usually the way to develop the independence of the students in their own learning process.\n\n## The project\n\nThe subject covers acquiring, presenting, and analyzing the most common types ofobservational data and model output, such as Argo floats, satellite-derived sea-surface temperature, altimetry, moored data, CTD casts, and general circulation models. This course is designed to provide enough experience for the students to learn how to read, understand and plot time series, profiles, cross-sections, maps and do analyses using the top-level module formulti-dimensional data, the xarray project. The main objective is to give enough experience with the main tools and develop the self-awareness and independence of the students in their own learning process.\n\n## The currently available tutorials\n\n| Tutorial    | Badge       |\n| ----------- | ----------- |\n| PIRATA      | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/iuryt/Panthalassan/blob/main/notebooks/01-PIRATA.ipynb)      |\n| BESM      | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/iuryt/Panthalassan/blob/main/notebooks/02-BESM.ipynb)      |\n| CTD (csv)      | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/iuryt/Panthalassan/blob/main/notebooks/03-CTD.ipynb)      |\n| NEMO      | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/iuryt/Panthalassan/blob/main/notebooks/04-NEMO.ipynb)      |\n| MEOP      | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/iuryt/Panthalassan/blob/main/notebooks/05-MEOP.ipynb)      |\n"
 },
 {
  "repo": "pyoceans/pocean-core",
  "language": "Python",
  "readme_contents": "# \ud83c\udf10 pocean-core\n\n[![Build Status](https://travis-ci.org/pyoceans/pocean-core.svg?branch=master)](https://travis-ci.org/pyoceans/pocean-core)\n[![Build status](https://ci.appveyor.com/api/projects/status/gds2iavceg5unj0a?svg=true)](https://ci.appveyor.com/project/ocefpaf/pocean-core)\n[![license](https://img.shields.io/github/license/pyoceans/pocean-core.svg)](https://github.com/pyoceans/pocean-core/blob/master/LICENSE.txt)\n[![GitHub release](https://img.shields.io/github/release/pyoceans/pocean-core/all.svg)\n\n\n\ud83d\udc0d + \ud83c\udf0a\n\nA python framework for working with met-ocean data\n\n## Resources\n+ **Documentation:** <https://pyoceans.github.io/pocean-core/docs/>\n+ **API:** <https://pyoceans.github.io/pocean-core/docs/api/pocean.html>\n+ **Source Code:** <https://github.com/pyoceans/pocean-core/>\n+ **Git clone URL:** <https://github.com/pyoceans/pocean-core.git>\n"
 },
 {
  "repo": "TEOS-10/GibbsSeaWater.jl",
  "language": "Julia",
  "readme_contents": "[![Build Status](https://github.com/TEOS-10/GibbsSeaWater.jl/workflows/CI/badge.svg)](https://github.com/TEOS-10/GibbsSeaWater.jl/actions)\n[![Build status Windows](https://ci.appveyor.com/api/projects/status/77kj4lug424x20y9/branch/master?svg=true)](https://ci.appveyor.com/project/Alexander-Barth/gibbsseawater-jl-ojx2d/branch/master)\n[![codecov.io](http://codecov.io/github/TEOS-10/GibbsSeaWater.jl/coverage.svg?branch=master)](http://codecov.io/github/TEOS-10/GibbsSeaWater.jl?branch=master)\n\n\n# GibbsSeaWater.jl\n\nGibbsSeaWater.jl is a Julia wrapper for [GSW-C#master](https://github.com/TEOS-10/GSW-C/), which is the C implementation of the Thermodynamic Equation of Seawater 2010 (TEOS-10).\n\n## Installation\n\nStart Julia and issue the following commands:\n\n```julia\nusing Pkg\nPkg.add(\"GibbsSeaWater\")\n```\n\n## Example\n\nFor arrays, one should use the [vectorized \"dot\" operator](https://docs.julialang.org/en/v1/manual/mathematical-operations/#man-dot-operators-1):\n\n```julia\nC = [45.8;34.7]\nT = [28.9;22.8]\nP = [10.0;50.0]\nSP = gsw_sp_from_c.(C,T,P)\n```\n\n## About TEOS-10\n\nPlease check the [official site](http://www.teos-10.org) and [official repository](https://github.com/TEOS-10), which provide the official implementations (C/Fortran/Matlab/PHP) and the wrappers.\n"
 },
 {
  "repo": "gmanuch/DataScienceOceanography",
  "language": "Jupyter Notebook",
  "readme_contents": "# DataScienceOceanography\n\ntesting updates with github\n"
 },
 {
  "repo": "omuse-geoscience/omuse",
  "language": "Fortran",
  "readme_contents": "# OMUSE #\n\nOMUSE stands for Oceanographic MUltipurpose Software Environment. It is a \npackage to conduct numerical experiments in oceanography and other Earth \nsciences. Example OMUSE applications can be found in the examples \n[repository](https://github.com/omuse-geoscience/omuse-examples).\n\n### For whom is OMUSE? ###\n\nOMUSE aims to be useable by any researcher or student with a basic knowledge of \nthe Python programming language.\n\n### What is this repository for? ###\n\nThis repository contains the source tree for OMUSE, including OMUSE specific framework\ncomponents and community codes.\n\n### How do I get set up? ###\n\nWhile there are some packages available on [pipy](www.pypi.org), we recommend at the moment \nto do a pip developer install:\n\n- setup a python environment, e.g. using virtualenv, and activate it.\n- in a suitable working directory clone the [AMUSE](https://github.com/amusecode/amuse) repository: `git clone https://github.com/amusecode/amuse`\n- go into the created directory: `cd amuse`\n- do the developer install from here: `pip install -e . [MPI]` The MPI is optional. \n- Going back to the working directory (`cd ..`) also clone the OMUSE repository: `git clone https://github.com/omuse-geoscience/omuse`,\n- go into the source directory `cd omuse` and set the environment variable `DOWNLOAD_CODES`, e.g. `export DOWNLOAD_CODES=latest`.\n- now, do `pip install -e .` from the root of the package\n- type `python setup.py build_codes --inplace` to build the codes. \n- the file `build.log` will report any errors in the build process.\n\nThis installs amuse-devel and omuse-devel. The community codes of OMUSE can \nbe build manually by going into each directory:\n\n + src/omuse/community/adcirc\n + src/omuse/community/swan\n + etc\n\nand typing: first `make download` (for some) and then `make`\n\nOMUSE has been tested on OSX and linux machines, with ifort and gfortran \ncompilers, on desktop machines and on the Carthesius supercomputer.\n\nIn addition to the AMUSE dependencies, OMUSE needs/ can use the following \npackages:\n\n + matplotlib basemap\n + netCDF and netCDF for fortran and the python bindings\n + GRIB_API\n\n### Documentation ###\n\nDocumentation can be found [here](https://omuse.readthedocs.io). In addition the base  [AMUSE documentation](https://amuse.readthedocs.io) can be consulted.\n\n### Reporting issues ###\n\nIssues can be reported at the OMUSE issue tracker; for framework issues, \nreport them at the AMUSE [repository](https://github.com/amusecode/amuse).\n\n### Contribution guidelines ###\n\nContributions are welcome. Note that most framework development happens at \nthe AMUSE [repository](https://github.com/amusecode/amuse) A primer for \nwriting code interfaces and other documentation can be found on the amuse \n[website](www.amusecode.org).\n"
 },
 {
  "repo": "team-ocean/veros",
  "language": "Python",
  "readme_contents": "<p align=\"center\">\n<img src=\"doc/_images/veros-logo-400px.png?raw=true\">\n</p>\n\n<p align=\"center\">\n<i>Versatile Ocean Simulation in Pure Python</i>\n</p>\n\n<p align=\"center\">\n  <a href=\"http://veros.readthedocs.io/?badge=latest\">\n    <img src=\"https://readthedocs.org/projects/veros/badge/?version=latest\" alt=\"Documentation status\">\n  </a>\n  <a href=\"https://github.com/team-ocean/veros/actions/workflows/test-all.yml\">\n    <img src=\"https://github.com/team-ocean/veros/actions/workflows/test-all.yml/badge.svg\" alt=\"Test status\">\n  </a>\n  <a href=\"https://codecov.io/gh/team-ocean/veros\">\n    <img src=\"https://codecov.io/gh/team-ocean/veros/branch/master/graph/badge.svg\" alt=\"Code Coverage\">\n  </a>\n  <a href=\"https://zenodo.org/badge/latestdoi/87419383\">\n    <img src=\"https://zenodo.org/badge/87419383.svg\" alt=\"DOI\">\n  </a>\n</p>\n\nVeros, *the versatile ocean simulator*, aims to be the swiss army knife of ocean modeling. It is a full-fledged primitive equation ocean model that supports anything between idealized toy models and [realistic, high-resolution, global ocean simulations](https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2021MS002717). And because Veros is written in pure Python, the days of struggling with complicated model setup workflows, ancient programming environments, and obscure legacy code are finally over.\n\n*In a nutshell, we want to enable high-performance ocean modelling with a clear focus on flexibility and usability.*\n\nVeros supports a NumPy backend for small-scale problems, and a\nhigh-performance [JAX](https://github.com/google/jax) backend\nwith CPU and GPU support. It is fully parallelized via MPI and supports\ndistributed execution on any number of nodes, including multi-GPU architectures (see also [our benchmarks](https://veros.readthedocs.io/en/latest/more/benchmarks.html)).\n\nThe dynamical core of Veros is based on [pyOM2](https://wiki.cen.uni-hamburg.de/ifm/TO/pyOM2), an ocean model with a Fortran backend and Fortran and Python frontends.\n\nTo learn more about Veros, make sure to [visit our documentation](https://veros.readthedocs.io/en/latest/).\n\n#### How about a demonstration?\n\n<p align=\"center\">\n  <a href=\"https://vimeo.com/391237951\">\n      <img src=\"doc/_images/veros-preview.gif?raw=true\" alt=\"0.25\u00d70.25\u00b0 high-resolution model spin-up\">\n  </a>\n</p>\n\n<p align=\"center\">\n(0.25\u00d70.25\u00b0 high-resolution model spin-up, click for better\nquality)\n</p>\n\n## Features\n\nVeros provides\n\n-   a fully staggered **3-D grid geometry** (*C-grid*)\n-   support for both **idealized and realistic configurations** in\n    Cartesian or pseudo-spherical coordinates\n-   several **friction and advection schemes**\n-   isoneutral mixing, eddy-kinetic energy, turbulent kinetic energy,\n    and internal wave energy **parameterizations**\n-   several **pre-implemented diagnostics** such as energy fluxes,\n    variable time averages, and a vertical overturning stream function\n    (written to netCDF4 output)\n-   **pre-configured idealized and realistic set-ups** that are ready to\n    run and easy to adapt\n-   **accessibility and extensibility** - thanks to the\n    power of Python!\n\n## Veros for the impatient\n\nA minimal example to install and run Veros:\n\n```bash\n$ pip install veros\n$ veros copy-setup acc --to /tmp/acc\n$ veros run /tmp/acc/acc.py\n```\n\nFor more detailed installation instructions, have a look at [our\ndocumentation](https://veros.readthedocs.io).\n\n## Basic usage\n\nTo run Veros, you need to set up a model --- i.e., specify which settings\nand model domain you want to use. This is done by subclassing the\n`VerosSetup` base class in a *setup script* that is written in Python. You\nshould use the `veros copy-setup` command to copy one into your current\nfolder. A good place to start is the\n[ACC model](https://github.com/team-ocean/veros/blob/master/veros/setups/acc/acc.py):\n\n```bash\n$ veros copy-setup acc\n```\n\nAfter setting up your model, all you need to do is call the `setup` and\n`run` methods on your setup class. The pre-implemented setups can all be\nexecuted via `veros run`:\n\n```bash\n$ veros run acc.py\n```\n\nFor more information on using Veros, have a look at [our\ndocumentation](http://veros.readthedocs.io).\n\n## Contributing\n\nContributions to Veros are always welcome, no matter if you spotted an\ninaccuracy in [the documentation](https://veros.readthedocs.io), wrote a\nnew setup, fixed a bug, or even extended Veros\\' core mechanics. There\nare 2 ways to contribute:\n\n1.  If you want to report a bug or request a missing feature, please\n    [open an issue](https://github.com/team-ocean/veros/issues). If you\n    are reporting a bug, make sure to include all relevant information\n    for reproducing it (ideally through a *minimal* code sample).\n2.  If you want to fix the issue yourself, or wrote an extension for\n    Veros - great! You are welcome to submit your code for review by\n    committing it to a repository and opening a [pull\n    request](https://github.com/team-ocean/veros/pulls). However,\n    before you do so, please check [the contribution\n    guide](http://veros.readthedocs.io/quickstart/get-started.html#enhancing-veros)\n    for some tips on testing and benchmarking, and to make sure that\n    your modifications adhere with our style policies. Most importantly,\n    please ensure that you follow the [PEP8\n    guidelines](https://www.python.org/dev/peps/pep-0008/), use\n    *meaningful* variable names, and document your code using\n    [Google-style\n    docstrings](http://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html).\n\n## How to cite\n\nIf you use Veros in scientific work, please consider citing [the following publication](https://gmd.copernicus.org/articles/11/3299/2018/):\n\n```bibtex\n@article{hafner_veros_2018,\n\ttitle = {Veros v0.1 \u2013 a fast and versatile ocean simulator in pure {Python}},\n\tvolume = {11},\n\tissn = {1991-959X},\n\turl = {https://gmd.copernicus.org/articles/11/3299/2018/},\n\tdoi = {10.5194/gmd-11-3299-2018},\n\tnumber = {8},\n\tjournal = {Geoscientific Model Development},\n\tauthor = {H\u00e4fner, Dion and Jacobsen, Ren\u00e9 L\u00f8we and Eden, Carsten and Kristensen, Mads R. B. and Jochum, Markus and Nuterman, Roman and Vinter, Brian},\n\tmonth = aug,\n\tyear = {2018},\n\tpages = {3299--3312},\n}\n```\n\nOr have a look at [our documentation](https://veros.readthedocs.io/en/latest/more/publications.html)\nfor more publications involving Veros.\n"
 },
 {
  "repo": "uwdb/istc_oceanography",
  "language": "Jupyter Notebook",
  "readme_contents": "# istc_oceanography\n\nPlaceholder for datasets, todo items for the ISTC hackathon and demo.\n\nAccess the TX-E1 cluster via `ssh USERNAME@txe1-login.mit.edu`.\nMake sure your user account is in the `istcdata` group.\n\nMain folder is `/home/gridsan/groups/istcdata/datasets/ocean_metagenome`\n\nWe need to find a link between the metadata and the genomics data that has been processed in the `overlapped_trimmed_data` folder.\n\n\n\n\n## README file on TX-E1\n\nSimons metagenomics samples\n\nDirectories and sample processing steps:\n\n1) `raw_data`: directory containing the raw Illumina NextSeq data from BioMicro. Each subfolder (150122Chi, etc) represents the BioMicro project name.\n2) `renamed_raw_data`: directory with symbolic links to the original raw file. Links are renamed in the format S0010_1_sequence.fastq etc, where \"S0010\" would be sample #10 from Maddie's master extraction spreadsheet. Done to simplify naming and make bulk preprocessing simple. Renaming is based on the `simons_mg_samplekey.txt` file, using the script `renamefiles.sh`\n3) `overlapped_trimmed_data`: Output of the \"metagenome_preprocessing.sh\" script. Steps are:\n   a: run `cutadapt_1.8.1` to remove any adapter sequences in the raw sequence data\n   b: run `clc_overlap_reads` to overlap any paired-end data (min 75 bp output)\n   c: run `clc_quality_trim` on nonoverlapping reads to remove low-quality reads/sections of reads (min 75 bp output)\n\n\n"
 },
 {
  "repo": "mjharriso/MIDAS",
  "language": "Python",
  "readme_contents": "[![Build Status](https://travis-ci.org/mjharriso/MIDAS.svg?branch=master)](https://travis-ci.org/mjharriso/MIDAS)\n\n# DESCRIPTION\n\n MIDAS (MIDAS Midas is Data Analysis Software)\n is a Python package primarily for processing\n gridded data stored in CF-compliant NetCDF/HDF5 format\n (http://cfconventions.org).\n\n * spatial interpolation between quadrilateral meshes\n * temporal interpolation between calendar dates (datetime)\n * conservative re-mapping in the vertical dimension (MOM6/ALE)\n * spatial integration/averaging with generalized horizontal/vertical coordinates\n * temporal averaging (Datetime).\n\n MIDAS was first developed by Matt Harrison as an employee of NOAA/GFDL\n and has been used for the generation of realistic global model configurations\n and post-run analysis scripts (https://github.com/NOAA-GFDL/MOM6-examples.git)\n\n\n This work is licensed under the Creative Commons\n Attribution-NonCommercial-ShareAlike 3.0 Unported License.\n To view a copy of this license, visit\n http://creativecommons.org/licenses/by-nc-sa/3.0/\n or send a letter to Creative Commons, 444 Castro Street,\n Suite 900, Mountain View, California, 94041, USA.\n\n# SIMPLE INSTALL (reduced functionality)\n\nIf you just want the basics, and are not planning to use the remapping features, then simply type (bash)\n\n```\nINSTALL_PATH='/your/python/module/path'\npython setup_nofort.py install --prefix=$INSTALL_PATH\n\n```\n\nIf you have root access and want to use default installation paths, then omit the prefix option.  If you are using conda, then the default location is specific to your current environment.\n\n\n# CONDA INSTALLATION\n\n1. Download and install Anaconda\n\n```\n(wget https://repo.anaconda.com/archive/Anaconda3-5.1.0-Linux-x86_64.sh;./Anaconda3-5.1.0-Linux-x86_64.sh)\n```\n\n2. Update your existing shell to add conda to your path\n\n```\nsource ~/.bashrc\n```\n\n3. Now update conda\n\n```\nconda update conda\nconda install conda-build\n```\n\n4. Install gfortran development libraries (if needed)\n\n```\nsudo apt-get install libgfortran-6-dev\n```\n\noptionally install mpich2 and associated libraries\n\n```\n(sudo apt-get install libmpich2-dev)\n```\n\nOr else if you do not have root privileges\n\n```\n(. activate;wget http://www.mpich.org/static/downloads/3.2.1/mpich-3.2.1.tar.gz;cd Downloads;tar xvf mpich-3.2.1.tar.gz;cd mpich-?3.2.1;./configure --enable-sha\\\nred --prefix=$CONDA_PREFIX;make; make install)\n```\n\n\n# MIDAS INSTALLATION\n\nOn linux platforms, simply type\n\n```\nmake\n```\n\nThis will take some time, since the Makefile will be downloading and compiling several large packages, like hdf5 and netcdf. Why are we compiling everything when there are pre-compiled binaries avaialble from the Anaconda cloud? Problems arise when linking c and c++ libraries to fortran APIs. There are often strict requirements for matching versions and glibc compatability which make using pre-compiled code unfeasible.\n\nFor platforms with multiple users, it is recommended that the compiled libraries and Python packages be made available through a local channel.\n\n# MIDAS STEP-BY-STEP INSTALLATION\n\nFor best results, build the following libraries yourself - conda does not handle dependencies for linking c and c++ libraries to fortran APIs - consider yourself lucky if you can work with pre-compiled packages and associated libraries\n\n```\n(conda create --name MIDAS)\ngit clone git@github.com:MJHarrison-GFDL/conda-recipes.git\n(. activate MIDAS; cd conda-recipes/zlib;conda build .;conda install --use-local zlib)\n(. activate MIDAS; cd conda-recipes/hdf5;conda build .;conda install --use-local hdf5)\n(. activate MIDAS; cd conda-recipes/libnetcdf;conda build .;conda install --use-local libnetcdf)\n(. activate MIDAS; cd conda-recipes/libnetcdff;conda build .;conda install --use-local libnetcdff)\n```\n\nInstall the netCDF4 python API\n\n```\n(. activate MIDAS;pip install netCDF4)\n```\n\nInstall MIDAS\n\n```\ngit clone git@github.com:mjharriso/MIDAS.git\n(. activate MIDAS; cd MIDAS;git checkout master;. build.sh)\n```\n\n**TROUBLESHOOTING**\n\nif you have a problem with libmkl missing:\n\n```\n(. activate MIDAS; conda install nomkl numpy scipy scikit-learn numexpr)\n(. activate MIDAS; conda remove mkl mkl-service)\n```\n\nmissing libcomm_err.so.3 at runtime?\n\n```\n(ln -s $CONDA_PREFIX/pkgs/krb5-1.14.6-0/lib/libcom_err.so.3 $CONDA_PREFIX/lib/.)\n```\n\n\n**EXAMPLES**\n\n```\ncd examples\nsource activate MIDAS\npython contour_example.py # Fetches OpenDAP URL from NODC and plots with Matplotlib\npython hinterp_example.py # fms/horiz_interp does bi-linear interpolation from the original\n                 \t  # 1-deg to a 5-deg grid with masking\npython hist.py            # volume-weighted histogram of salinity in the Indian Ocean\npython subtile.py         # use subtiling algorithm to calculate un-weighted\n \t       \t\t  # cell average bathymetry and roughness. Example along the Eastern US\nsource deactivate\n```\n\n**HOW TO OBTAIN DOCUMENTATION**\n\n```\nipython\nimport midas.rectgrid as rectgrid\nrectgrid.[Tab]   # complete listing of methods\nrectgrid.quadmesh       # a generic rectangular grid description\n\t\t\t\t    # Can be read from a file or provided as\n\t\t\t\t    # 2-d lat/lon position arrays.\nrectgrid.state?  # Description of state instance\nrectgrid.state.state.[Tab] # available methods for state objects\nrectgrid.state.volume_integral?  # integrate scalars over the domain\n\t\t\t\t     # in 'X','Y','Z','XY' or 'XYZ'\nrectgrid.state?? # View the code\n```\n\n**MORE INFORMATION**\n\ntype\n\n```\n(. activate MIDAS;conda list)\n```\n\nthis returns your current environment which should look something like this:\n\n```\n# Name                    Version                   Build  Channel\nca-certificates           2018.4.16                     0    conda-forge\ncurl                      7.60.0                        0    conda-forge\nhdf5                      1.8.20                        0    local\nkrb5                      1.14.6                        0    conda-forge\nlibnetcdf                 4.4.1                         0    local\nlibnetcdff                4.4.4                         0    local\nlibssh2                   1.8.0                         2    conda-forge\nopenssl                   1.0.2o                        0    conda-forge\nzlib                      1.2.11                        1    local\n\n```"
 },
 {
  "repo": "brianemery/hfr_cs_processing",
  "language": "MATLAB",
  "readme_contents": "## HFR CS PROCESSING TOOLBOX FOR MATLAB ##\n\nv2.0\n\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.5598294.svg)](https://doi.org/10.5281/zenodo.5598294)\n\nTools for processing oceanographic HF radar cross spectra with direction\nfinding methods. \n\n- Provides research quality software for processing HFR data following the\n  methods of Lipa et al. (2006).\n- Employs a home made ship removal algorithm that follows what is known \n  about how it's really done. (Presently disabled) \n- Generalizes the cross spectra data structure for arbitrary arrays.\n- Allows the use of the imageFOL toolbox by Anthony Kirincich \n  (https://github.com/akirincich/imageFOLs).\n- Includes several DF methods used in Emery (2018), and an estimate of \n  MUSIC error from Stoica and Nehorai (1989) used in Emery and Washburn (2018).\n- Uses my version of CODAR's method for single vs dual determination. Can use \n  the Likelihood Ratio (See Emery et. al 2022) for larger arrays. \n- Will work with data from LERA and WERA. It has been used to \n  process LERA data.\n\n\nHOW TO USE IT\n- download it and cd to the unzipped directory\n- run install_hfr_cs_proc.m to modify the MATLAB path\n- run the demo to make sure it works (run_cs_processing_demo.m)\n- edit run_cs_processing.m for more advanced applications \n- edit doa_on_cs.m (~line 80) to include the use of parfor \n\n\nTO DO\n- Could build some edge case tests using simiulations (eg 0-360 \n  transition, etc)\n- This commit includes many more mfiles than are actually needed due to MATLAB's \n  dependency tool - need a better way to isolate tools. \n\n\nACKNOWLEDGMENT\n\nThe release is self contained but includes code from the following people\nand or toolboxes. HFRProgs [1] by David Kaplan set the standard, and data\nstructures here follow the same basic formatting. I've include a few mfiles\nfrom HFRprogs as dependencies. As mentioned above, this toolbox can use \nimageFOL by Anthony Kirincich [2]. The toolbox includes code obtained\nfrom CODAR Ocean Sensors for reading cross spectra files. Version 1.5 \nincludes data from BML1 provided by William Speiser and John Largier. \n\n[1] https://github.com/rowg/hfrprogs  \n[2] https://github.com/akirincich/imageFOLs   \n\n\nREFERENCES\n\nLipa, B., B. Nyden, D. S. Ullman, and E. Terrill, (2006). SeaSonde radial velocities: Derivation and internal consistency. IEEE Journal of Oceanic Engineering, 31 (4) 850?861.  \n\nStoica, P., & Nehorai, A. (1989). MUSIC, maximum likelihood, and the Cramer-Rao bound. IEEE Transactions on Acoustics, Speech, and Signal Processing, 37(5), 720-741.  \n\nEmery, B. and Washburn, L. (2019). Uncertainty Estimates for SeaSonde HF Radar Ocean Current Observations. Journal of Atmospheric and Oceanic Technology 36.2: 231-247.\n\nEmery, B. (2019). Evaluation of Alternative Direction of Arrival Methods for Oceanographic HF Radars. IEEE Journal of Oceanic Engineering, doi: 10.1109/JOE.2019.2914537.  \n\nEmery, B., Kirincich, A., and Washburn, L. (2022). Direction Finding and Likelihood Ratio Detection for \nOceanographic HF Radars. Journal of Atmospheric and Oceanic Technology, 39(2), 223-235. https://journals.ametsoc.org/view/journals/atot/39/2/JTECH-D-21-0110.1.xml  \n\nARCHITECTURE GOALS\n\n- arbitrary array geometry, fft length, etc\n- arbitrary doa method \n\n\nNOTES\n\n- .m files use functions as blocks of code - code folding (cmd =) makes it easy to move among these.\n- Data structures contain variables of similar origin following the HFRProgs\n  convention, rows = locations, cols = time\n- Data structures are initialized with appropriately named function \n  (e.g. doa_struct.m) to enable standardization\n- End product is presently a structure that I'm calling a DOA structure, which is roughly\n  equivalent to a SeaSonde short-time radial (pre-merge), plus a lot more meta data.\n\n\nCODING PRINCIPLES\n\n- Minimize repetition (dont repeat yourself)\n- Make code re-usable and recyclable. Make general functions. \n- Code should be pretty and readable sentences and paragraphs, aid the reader when\n  possible\n- balance between future usages (flexibility) and getting the current job done (purpose built)\n- Good design is simple\n- Write computer programs to make them easy for people to read.\n- Have  a clear division between code that is custom for a particular application, \n  and the general/easily repurposed code\n  \n  \nHOW TO CITE\n\n(See the Zenodo site to cite specific older versions): \n\nEmery, B., 2021: HFR CS Processing Toolbox for MATLAB, software release version 2.0, https://doi.org/10.5281/zenodo.5598294. URL: https://doi.org/10.5281/zenodo.5598294, doi:10.4445281/zenodo.5598294.\n\nUsing Bibtex:\n \n```\n@misc{Emery2021code,\n  author       = {Brian Emery},\n  title        = {{HFR CS Processing Toolbox for MATLAB}, Software Release Version 2.0, https://doi.org/10.5281/zenodo.5598294},\n  version      = {2.0},\n  month        = oct,\n  year         = 2021,\n  doi          = {10.5281/zenodo.5598294},\n  url          = {https://doi.org/10.5281/zenodo.5598294}\n}\n```\n\nVERSION NOTES\n\n1.0 (18 July 2018)\nOriginal version can be found here: [![DOI](https://zenodo.org/badge/84593561.svg)](https://zenodo.org/badge/latestdoi/84593561)\n\n1.5 (16 June 2020)\nUpdates include an install script (install_hfr_cs_proc.m) and demonstration code (run_cs_processing_demo.m) that \nuses test data from BML1. This update also includes the likelihood ratio detection method from an in-prep\nmanuscript (suitable for use with MLE direction finding - more about this at a later time). \n\n2.0 (21 Oct 2021) \nUpdates, improvements and new features related to Emery et al. 2022.\n\n2.1 (1 June 2022)\nUpdates including radial metrics, many code fixes and likely bug introductions. \n"
 },
 {
  "repo": "gher-ulg/DIVA",
  "language": "Fortran",
  "readme_contents": "[![Project Status: Inactive \u2013 The project has reached a stable, usable state but is no longer being actively developed; support/maintenance will be provided as time allows.](https://www.repostatus.org/badges/latest/inactive.svg)](https://www.repostatus.org/#inactive)\n[![Build Status](https://travis-ci.org/gher-ulg/DIVA.svg?branch=master)](https://travis-ci.org/gher-ulg/DIVA)\n[![DOI](https://zenodo.org/badge/80114691.svg)](https://zenodo.org/badge/latestdoi/80114691)\n\n![made-with-bash](https://img.shields.io/badge/Made%20with-Bash-1f425f.svg) \n\n\nIMPORTANT: this original `DIVA` tool will remain available, but will not be further developped. For new features, users are invited to switch to the generalization in N-dimensions [`DIVAnd`](https://github.com/gher-ulg/DIVAnd.jl) using a very modern progamming language: [`Julia`](julialang.org/).\n\n\n# DIVA (Data-Interpolating Variational Analysis)\n\nDIVA allows the spatial interpolation of data (*analysis*) in an optimal way, comparable to *optimal interpolation* (OI). In comparison to OI, it takes into account coastlines, sub-basins and advection. Calculations are highly optimized and rely on a [*finite element*](https://en.wikipedia.org/wiki/Finite_element_method) resolution. \n\nTools to generate the finite element mesh are provided as well as tools to optimize the parameters of the analysis. Quality control of data can be performed and error fields can be calculated. In addition, *detrending* of data is possible. Finally 3D and 4D extensions are included with emphasis on direct computations of climatologies from [Ocean Data View](https://odv.awi.de/) (ODV) spreadsheet files.\n\n![Diva logo](https://cloud.githubusercontent.com/assets/11868914/24106959/c6d8fb44-0d89-11e7-921b-a36fcccf5a21.png)\n\n## Getting started\n\n### Prerequisites\n\n* A fortran compiler: [gfortran](https://gcc.gnu.org/wiki/GFortran), ifort, pgf, ...\n* The [NetCDF](https://www.unidata.ucar.edu/software/netcdf/netcdf-4/newdocs/netcdf-f90.html) library for Fortran for the output writing.\n* [gnuplot](http://www.gnuplot.info/) for the creation of graphics [optional].\n\nFor Debia architectures, you can run:\n```bash\nsudo apt-get install -y software-properties-common\t# needed to use add-apt-repository\nsudo add-apt-repository universe  \t                # needed to get netCDF\nsudo apt-get install -y git make                   # needed for the compilation\nsudo apt-get install -y gfortran netcdf-bin libnetcdf-dev libnetcdff-dev\n```\n\n### Installing\n\n1. Download the latest stable [release](https://github.com/gher-ulg/DIVA/releases) and extract the archive:\n```bash\ntar xvf DIVA-4.7.2.tar.gz\n```\nor clone the project and checkout the last version.\n2. Go in the source directory\n```bash\ncd DIVA-4.7.2/DIVA3D/src/Fortran/\n```\n3. Run the compilation script:\n```bash\nmake\n```\nNotes: \n- the compiler (by default `gfortran`) and its flags can be modified by editing `Makefile`\n- the netCDF _library_ and _include_ flags are deduced from `nf-config` command, which provides the options with which netCDF was build.     \nThe values can be specified differently (if for example you use a non-standard path) by editing the lines\n```bash\nexport nclib=$(shell nf-config --flibs)\nexport ncinc=$(shell nf-config --fflags)\n```\n\n### Testing\n\nGo in the main execution directory (*divastripped*) and run the tests:\n```bash\ncd ../../divastripped/\ndivatest\ndivatest0\n...\n```\n## How does it work?\n\nDIVA is a software tool developed for gridding in situ data.\nIt uses a finite-element method to solve a variational principle which takes into account:\n 1. the distance between analysis and data (observation constraint),\n 2. the regularity of the analysis (*smoothness* constraint),\n 3. physical laws (behaviour constraint). \n \n ![800px-diva_gridding_canary](https://cloud.githubusercontent.com/assets/11868914/24946939/09c918fc-1f65-11e7-9974-06264c70ec1e.png)\n\nThe advantage of the method over classic interpolation methods is multiple:\n* the coastline are taken into account during the analysis, since the variational principle is solved only in the region covered by the sea. This prevents the information from traveling across boundaries (e.g., peninsula, islands, etc) and then produce artificial mixing between water masses.\n* the numerical cost is not dependent on the number of data, but on the number of degrees of freedom, itself related to the size of the finite-element mesh. \n\n##  How to try DIVA without installing?\n\nIf you are familiar with Ocean Data View [ODV](http://odv.awi.de/) software tool, you can perfom DIVA gridding when plotting vertical or horizontal sections, as the other 25000 scientists using ODV.\n\nYou can also use basic DIVA features in a web application [Diva on Web](https://ec.oceanbrowser.net/emodnet/diva.html) if you have your data ready in a simple three-column ascii file or ODV ascii spreadsheet format. \n\n![divaonweb](https://cloud.githubusercontent.com/assets/11868914/24947093/a980dd26-1f65-11e7-8715-f1e50bd69a83.png)\n\n## Related tools \n\n* [DIVAnd.jl](https://github.com/gher-ulg/divand.jl) performs n-dimensional variational analysis of arbitrarily located observations (written in Julia language).\n* [divand.py](https://github.com/gher-ulg/divand.py) is the Python interface to [DIVAnd.jl](https://github.com/gher-ulg/divand.jl).\n* [DivaPythonTools](https://github.com/gher-ulg/DivaPythonTools) is a set of utilies to read, write and plot the content of input or output files used in Diva.\n\n## Publications & documents \n\nCheck the [GHER publications]([http://orbi.ulg.ac.be/ulg-report?query=%28%28affil%3A%22GeoHydrodynamics+and+Environment+Research%22%29+OR+%28affil%3A%22Oc%C3%A9anographie+physique%22%29%29&model=a&format=apa&sort_by0=1&order0=DESC&sort_by1=3&order1=ASC&sort_by2=2&order2=ASC&output=html&language=en&title=GHER+publications]) for the most recent updates.\n\n### Articles\n\n#### Theory\n\nBarth, A., Beckers, J.-M., Troupin, C., Alvera-Azc\u00e1rate, A., and Vandenbulcke, L.: divand-1.0: n-dimensional variational data analysis for ocean observations, Geosci. Model Dev., 7, 225-241, [doi:10.5194/gmd-7-225-2014](https://doi.org/10.5194/gmd-7-225-2014), 2014.\n\nTroupin, C.; Sirjacobs, D.; Rixen, M.; Brasseur, P.; Brankart, J.-M.; Barth, A.; Alvera-Azc\u00e1rate, A.; Capet, A.; Ouberdous, M.; Lenartz, F.; Toussaint, M.-E. & Beckers, J.-M. (2012) Generation of analysis and consistent error fields using the Data Interpolating Variational Analysis (Diva). *Ocean Modelling*, **52-53**: 90-101. doi:[10.1016/j.ocemod.2012.05.002](https://doi.org/10.1016/j.ocemod.2012.05.002)\n\nBeckers, J.-M.; Barth, A.; Troupin, C. & Alvera-Azc\u00e1rate, A. Some approximate and efficient methods to assess error fields in spatial gridding with DIVA (Data Interpolating Variational Analysis) (2014). *Journal of Atmospheric and Oceanic Technology*,  **31**: 515-530. doi:[10.1175/JTECH-D-13-00130.1](https://doi.org/10.1175/JTECH-D-13-00130.1)\n\n#### Applications \n\nCapet, A.; Troupin, C.; Carstensen, J.; Gr\u00e9goire, M. & Beckers, J.-M. Untangling spatial and temporal trends in the variability of the Black Sea Cold Intermediate Layer and mixed Layer Depth using the DIVA detrending procedure (2014). *Ocean Dynamics*, **64**: 315-324. doi:[10.1007/s10236-013-0683-4](https://doi.org/10.1007/s10236-013-0683-4)\n\nTroupin, C.; Mach\u00edn, F.; Ouberdous, M.; Sirjacobs, D.; Barth, A. & Beckers, J.-M. High-resolution Climatology of the North-East Atlantic using Data-Interpolating Variational Analysis (Diva) (2010). *Journal of Geophysical Research*, **115**: C08005. doi:[10.1029/2009JC005512](https://doi.org/10.1029/2009JC005512)\n\n### User guide\n\nThe most recent version is available in [PDF](https://github.com/gher-ulg/Diva-User-Guide/raw/master/DivaUserGuide.pdf).\n\n### Posters and presentations\n\nCheck the complete list of documents hosted through the [ULiege Orbi catalogue](http://orbi.ulg.ac.be/orbi-report?query=%28%28affil%3A%22GeoHydrodynamics+and+Environment+Research%22%29+OR+%28affil%3A%22Oc%C3%A9anographie+physique%22%29%29&model=a&format=apa&sort_by0=1&order0=DESC&sort_by1=3&order1=ASC&sort_by2=2&order2=ASC&output=html&language=en&title=GHER+publications).\n\n## Acknowledgments\n\nThe DIVA development has received funding from:\n- the European Union Sixth Framework Programme (FP6/2002-2006) under grant agreement n\u00b0 026212, [SeaDataNet](http://www.seadatanet.org/), \n- the Seventh Framework Programme (FP7/2007-2013) under grant agreement n\u00b0 283607, SeaDataNet II, \n- SeaDataCloud and \n- [EMODNet](http://www.emodnet.eu/) (MARE/2008/03 - Lot 3 Chemistry - SI2.531432) from the [Directorate-General for Maritime Affairs and Fisheries](http://ec.europa.eu/dgs/maritimeaffairs_fisheries/index_en.htm).\n"
 },
 {
  "repo": "wavebitscientific/ndbc",
  "language": "Python",
  "readme_contents": "# ndbc\n\n[![Build Status](https://travis-ci.org/wavebitscientific/ndbc.svg?branch=master)](https://travis-ci.org/wavebitscientific/ndbc)\n[![GitHub issues](https://img.shields.io/github/issues/wavebitscientific/ndbc.svg)](https://github.com/wavebitscientific/ndbc/issues)\n\nA Python interface to National Data Buoy Center data.\n\n## Getting started\n\n```\npip install git+https://github.com/wavebitscientific/ndbc\n```\n\n## Usage\n\n```python\nfrom ndbc import Station\nfrom datetime import datetime\n\n# initialize without getting the data\nstation = Station(42001)\n\nstation.name\n# 'MID GULF - 180 nm South of Southwest Pass, LA'\n\nstation.lon\n# -89.668\n\nstation.lat\n# 25.897\n\n# initialize and get the data\nstation = Station(42001, datetime(2017,10,1), datetime(2017,11,1))\n\n# get a different time window\nstation.get_stdmet(datetime(2015,1,1), datetime(2017,1,1))\n\n```\n\n## Features\n\n* [x] Standard meteorological data: wind speed and direction, air pressure, air and water temperature, dew-point temperature, wave height, period, and direction.\n* [ ] Omnidirectional (1-d) wave spectrum data\n* [ ] Directional (2-d) wave spectrum data\n* [ ] Derived diagnostics\n* [ ] [What else?](https://github.com/wavebitscientific/ndbc/issues/new)\n"
 },
 {
  "repo": "VACUMM/vacumm",
  "language": "Python",
  "readme_contents": "VACUMM\n======\n\n.. image:: https://zenodo.org/badge/22859/VACUMM/vacumm.svg\n   :target: https://zenodo.org/badge/latestdoi/22859/VACUMM/vacumm\n.. image:: https://travis-ci.org/VACUMM/vacumm.svg?branch=master\n    :target: https://travis-ci.org/VACUMM/vacumm\n\nVACUMM provides generic and specialized tools for the validation of ocean models,\nand more especially the MARS model from `IFREMER <http://www.ifremer.fr>`_.\nThe heart of VACUMM is a\n`library <http://www.ifremer.fr/vacumm/library/index.html>`_  written mainly\nin the `Python <http://www.python.org>`_ language,\nwhose `core <http://www.ifremer.fr/vacumm/library/misc.html>`_\ncan be used for the **preprocessing** and the\n**postprocessing** of oceanic and atmospheric data coming from models or observations.\nThe library for instance also has specialized modules for managing outputs from\n`models <http://www.ifremer.fr/vacumm/library/data/model.html>`_ and making advanced\n`diagnostics <http://www.ifremer.fr/vacumm/library/diag.html>`_.\n\n.. code-block:: python\n\n    >>> from vcmq import *\n    >>> sst = DS(data_sample('mars3d.xy.nc'), 'mars').get_sst()\n    >>> map2(sst)\n\n\nFeatures\n--------\n\n- A huge documentation with a gallery, a lot of examples and the complete API:\n  http://www.ifremer.fr/vacumm\n- Full UV-CDAT support and extensions.\n- Matplotlib/basemap graphics with advanced plotting objects like geographical mapping tools.\n- Numerous utilities for manipulating and converting time data.\n- Regridding and interpolation of random or gridded data, in 1D or 2D, with curvilinear grid support.\n- Helper routines for inspecting and reading NetCDF objects in single or multiple file datasets.\n- Generic and specialized 1D and 2D filters working on masked variables.\n- Specialized physical and numerical diagnostics, like dynamics, thermodynamics, spectral analyses, tides, etc.\n- Support and extension of CF conventions for searching or formatting variables.\n- Miscellaneous location utilities such as readers of sigma coordinates for ocean models, or Arakawa grid converters.\n- High level generic interface for reading and post-processing NetCDF data from standard or known dataset, such as model outputs or satellite products.\n- Statistical accumulator for large datasets.\n- Interfaces for working with random and gridded bathymetries, and with shorelines.\n- Utilities for working with masks and selection of spatial data.\n- Utilities for working with input and output remote files.\n- Advanced logging classes.\n- Extensions to sphinx for Fortran and Python.\n- A collection of scripts for some diagnostics.\n\n\nDependencies\n------------\n\nMandatory:\n`CDAT <http://uvcdat.llnl.gov>`_ (or more specifically\n`cdms2 <http://uvcdat.llnl.gov>`_,\n`cdutil <http://uvcdat.llnl.gov>`_,\n`genutil <http://uvcdat.llnl.gov>`_ from CDAT, and\n`matplotlib <https://matplotlib.org>`_,\n`basemap <https://matplotlib.org/basemap>`_),\n`configobj <http://www.voidspace.org.uk/python/configobj.html>`_.\n\nOptional:\n`seawater <https://pypi.python.org/pypi/seawater>`_,\n`PIL <https://pypi.python.org/pypi/PIL>`_,\n`pytz <http://pytz.sourceforge.net>`_,\n`paramiko <http://www.paramiko.org>`_,\n`xlwt <https://pypi.python.org/pypi/xlwt>`_,\n`sphinx-fortran <https://pypi.python.org/pypi/sphinx-fortran>`_,\n`cmocean <https://pypi.python.org/pypi/cmocean>`_.\n\n\nDownload\n--------\n\nTo download VACUMM sources, please go to this page:\nhttp://www.ifremer.fr/vacumm/user.install.download.html\n\n\nInstallation\n------------\n\nFrom sources::\n\n    $ python setup.py install\n\nUsing `conda <http://conda.pydata.org/docs/index.html>`_::\n\n    $ conda install -c vacumm -c conda-forge -c cdat  vacumm\n\nFor more information, please go to this:\nhttp://www.ifremer.fr/vacumm/user.install.installations.html\n\nRelease notes\n-------------\n\nRelease notes for each version are available here:\nhttp://www.ifremer.fr/vacumm/appendix.release.html\n\n\nDocumentation\n-------------\n\nThe documentation is available here:\nhttp://www.ifremer.fr/vacumm\n\n\nLicense\n-------\n\nVACUMM is under the :ref:`CeCiLL <appendix.license>` license,\nwhich is compatible with well knwon GPL license.\n\n\nSupport\n-------\n\nYou can submit `issues <https://github.com/VACUMM/vacumm/issues>`_\nand `pull requests <https://github.com/VACUMM/vacumm/issues>`_\nfrom the GitHub site.\n\nStephane Raynaud (raynaud (at) gmail.com),\nGuillaume Charria (Guillaume.Charria (at) ifremer.fr).\n\nSee the contact page:\nhttp://www.ifremer.fr/vacumm/contact.html\n\n\n"
 },
 {
  "repo": "SHYFEM-model/shyfem",
  "language": "Fortran",
  "readme_contents": "\n#------------------------------------------------------------------------\n#\n#    Copyright (C) 1985-2020  Georg Umgiesser\n#\n#    This file is part of SHYFEM.\n#\n#    SHYFEM is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU General Public License as published by\n#    the Free Software Foundation, either version 3 of the License, or\n#    (at your option) any later version.\n#\n#    SHYFEM is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n#    GNU General Public License for more details.\n#\n#    You should have received a copy of the GNU General Public License\n#    along with SHYFEM. Please see the file COPYING in the main directory.\n#    If not, see <http://www.gnu.org/licenses/>.\n#\n#------------------------------------------------------------------------\n\n    This is the README file for the SHYFEM model\n\n#------------------------------------------------------------------------\n\n    Contact:\n\n    Georg Umgiesser\n    Oceanography, ISMAR-CNR\n    Arsenale Tesa 104, Castello 2737/F\n    30122 Venezia\n    Italy\n\n    Tel.   : +39-041-2407943\n    Fax    : +39-041-2407940\n    E-Mail : georg.umgiesser@ismar.cnr.it\n\n#------------------------------------------------------------------------\n\n    Availability:\n\n    https://github.com/SHYFEM-model/\n\n    Comments and issues should be posted in the Issues section \n    on the abovementioned GitHub repository\n\n#------------------------------------------------------------------------\n\n    Documentation:\n\n    The manual can be found in femdoc/final/shyfem.pdf\n\n    Example applications can be found in examples\n\n#------------------------------------------------------------------------\n\n    The software is also supported by various institutions:\n\n    CNR - ISMAR (Istituto di Scienze Marine)\n    ---------\n\tGeorg Umgiesser, system engineering\n\tChristian Ferrarin, renewal times, sediment transport\n\tMarco Bajo, system solvers, assimilation\n\tDebora Bellafiore, non-hydrostatic modeling, fluid mud\n\tMichol Ghezzo, ecological modeling, lagrangian model\n\tFrancesca De Pascalis, lagrangian model\n\tWilliam McKiver, non-hydrostatic modeling\n\n    CNR - IAS (Istituto di Ambiente Marino Costiero)\n    ---------\n\tAndrea Cucco, lagrangian model, renewal times\n\n    INOGS (Istituto Nazionale di Oceanografia e di Geofisica Sperimentale)\n    ---------\n\tCosimo Solidoro, ecological modeling, BFM\n\tDonata Melaku Canu, ecological modeling, EUTRO\n\tEric Pascolo, OMP parallelization\n \n    MRI (Marine Research Institute, Klaipeda University, Lithuania)\n    ---------\n\tPetras Zemlys, ecological modeling, AquaBC\n\t\n    Istanbul University\n    ---------\n\tAli Erturk, ecological modeling, AquaBC\n \n    Fondazione CMCC (Centro Mediterraneno per i cambiamenti climatici)\n    ---------\n\tSilvia Mocavero, MPI parallization\n\tGiorgio Micaletto, MPI parallization\n\t\n#------------------------------------------------------------------------\n\nDirectory structure\n===================\n\nexamples\texample applications\nfem3d\t\tFEM model and utility routines\nfemadj\t\tRoutines for the adjustment of the grid after the\n\t\tautomatic mesh generation. The program to run is shyadj.\nfemanim\t\tRoutines to generate animations from postscript files\nfembin\t\tBinaries used by the model\nfemcheck\tRoutines to set up and check the installation\nfemdoc\t\tDocumentation, manual, etc.\nfemdummy\tDummy directory\nfemersem\tRoutines for BFM (ERSEM) ecological model\nfemgotm\t\tRoutines for GOTM turbulence clousre model\nfemlib\t\tLibrary routines (populated during compilation)\nfemplot\t\tPlotting routines for post processing. The routine\n\t\tto run is shyplot.\nfemregres\tRoutines to do regression testing. Developers only.\nfemutil\t\tVarious utility programs\ngrid\t\tVisualization routine for GRD files\nhcbs\t\tOutdated files for plotting to monitor\nmesh\t\tAutomatic mesh generator\npost\t\tLibrary for plotting to Postscript files.\ntmp\t\tTemporary directory\n\nCompiling the model\n===================\n\nYou can compile everything from the SHYFEM directory by running the\ncommand \"make fem\". Other commands are:\n\nmake help\tgives help on available make targets\nmake fem\tcompiles everything\nmake doc\tmakes documentation in femdoc (manual)\nmake all\tcompiles (shyfem) and makes documents (doc)\nmake clean\tdeletes objetc and executable files in all subdirectories\nmake cleanall\tas clean, but cleans also libraries\n\n#------------------------------------------------------------------------\n\n"
 },
 {
  "repo": "powellb/seapy",
  "language": "Python",
  "readme_contents": "# State Estimation and Analysis in PYthon (SEAPY)\n\nTools for working with ocean models and data.\n\nSEAPY requires: basemap, h5py, joblib, netcdf4, numpy, numpy_groupies, rich and scipy.\n\n\n## Installation\n\n### Install from Conda-Forge\n\nInstall from [conda-forge](https://conda-forge.org/) with the Conda package manager:\n```\n$ conda install -c conda-forge seapy\n```\n\nYou should also consider making conda-forge your default channel. See the [conda-forge tips and tricks page](https://conda-forge.org/docs/user/tipsandtricks.html).\n\nThe Conda-Forge [SEAPY feedstock](https://github.com/conda-forge/seapy-feedstock) is maintained by Filipe Fernandes, [ocefpaf](https://github.com/ocefpaf/). As of February 2021 there are binary packages on all the platforms that Conda-Forge supports: Python 3.6 through 3.9 on Linux, Windows and Mac OSX (all 64-bit).\n\nTo remove seapy:\n```\n$ conda remove seapy\n```\n\n## Install from PyPI\n\nInstall from [PyPI](https://pypi.org/) with PIP:\n```\n$ pip install seapy-ocean\n```\n\nNote that on PyPI (but nowhere else) the package name is seapy-ocean to avoid a name clash with another package. The module name is still seapy.\n\nSEAPY packages on PyPI have been built and uploaded by Mark Hadfield, [hadfieldnz](https://pypi.org/user/hadfieldnz/). There is a source distribution that should build with no problems on Linux (and Mac OSX, but we haven't tested it). In the pst there have been binary distributions for Windows (64-bit), but these have now been deleted as binary builds with PIP are no longer supported.\n\nIn a Conda environment, it is quite possible to install with PIP, but dependency handling and updating will be cleaner if you use the Conda package.\n\nTo remove seapy-ocean\n```\n$ pip uninstall seapy-ocean\n```\n\n## Install from source code on GitHub.com\n\nThe SEAPY source code is maintained by Brian Powell, (powellb)[https://github.com/powellb]. Releases are made on the [master branch](https://github.com/powellb/seapy/tree/master)\n\nInstall from [GitHub.com](https://github.com/) with PIP:\n```\n$ pip install git+git://github.com/powellb/seapy@master\n```\n\nOR clone a copy of the source and install in editable mode, eg:\n```\n$ git clone https://github.com/powellb/seapy.git\n$ pip install -e seapy\n```\n\nWith an [editable-mode](https://pip.pypa.io/en/stable/cli/pip_install/#editable-installs) installation, changes you make to your copy of the source code will take effect when you import the module.\n\nIn principle it is possible to build from source on Windows--and success with this has been achieved in the past--but the process tends to break with changes in the environment or Python version, so we don't recommend it or support it.\n\n## Contributing\n\n\nIf you've installed from source in editable mode, then you should definitely consider forking your own copy of the repository. This allows you to keep your changes under revision control on GitHub.com and potentially contribute them to the main project. You should follow the procedures described in this [Git Workflow](https://www.asmeurer.com/git-workflow/) document.\n\n[Forking on GitHub.com](https://help.github.com/en/articles/fork-a-repo) is a lightweight process that won't complicate your workflow and keeps the relationship between your work and the original project clear, so it is strongly advised to do it early. However the immutable and unique nature of Git commits means that you can create and populate a fork later if you want to, as long as you have saved your work somewhere in Git format. To create a fork you will need a [GitHub.com user account](https://help.github.com/en/articles/signing-up-for-a-new-github-account).\n\n\nAll your changes should be committed to a branch other than \"master\", which is reserved for the master branch in Brian Powell's repository (or copies thereof). A common practice in the [existing SEAPY forks](https://github.com/powellb/seapy/network/members) is to use a branch name matching your user name for your own work. However if you are developing a specific feature or bug fix to be pulled into master, it may be sensible to name the branch after that feature or bug fix.\n\n\n## Examples\n\nMany of the time-saving features are in generating fields for running the ROMS model.\n\n1. To load the meta information about a model (ROMS, HYCOM, MITgcm, POM, SODA), load an output file (history, average, climatology, grid, etc.) via:\n\n        >> mygrid = seapy.model.asgrid(filename)\n\n        >> mygrid\n        C-Grid: 32x194x294\n\n        >> print(mygrid)\n        filename\n        32x194x294: C-Grid with S-level\n        Available: I,J,_isroms,_nc,angle,cgrid,cs_r,depth_rho,depth_u,depth_v,dm,dn,eta_rho,eta_u,eta_v,f,filename,h,hc,lat_rho,lat_u,lat_v,lm,ln,lon_rho,lon_u,lon_v,mask_rho,mask_u,mask_v,n,name,pm,pn,s_rho,shape,spatial_dims,tcline,theta_b,theta_s,thick_rho,thick_u,thick_v,vstretching,vtransform,xi_rho,xi_u,xi_v\n\n\n2. Most methods available in SEAPY require a grid, which can be specified as a \"filename\" or as a grid object.\n\n3. Find out how to download global HYCOM data that will span my grid from 1/1/2015 through 5/1/2015:\n\n\n        >> seapy.model.hycom.load_history(\"hycom_file.nc\", start_time=datetime(2015,1,1),\n                                         end_time=datetime(2015,5,1),\n                                         grid=mygrid, load_data=False)\n        ncks -v water_temp,salinity,surf_el,water_v,water_u -d time,352,352 -d lat,1204,1309 -d lon,2438,2603 http://tds.hycom.org/thredds/dodsC/GLBu0.08/expt_91.1 hycom_file.nc\n\nThis will display the 'ncks' command necessary to download the data. If you want to have SEAPY download it (not recommended due to server-speed), use `'load_data=True'`.\n\n4. Once you have HYCOM data, interpolate it to your grid\n\n        >> seapy.roms.interp.to_clim(\"hycom_file.nc\", \"my_clim.nc\",\n                          dest_grid=mygrid, nx=1/6, ny=1/6,\n                          vmap={\"surf_el\":\"zeta\", \"water_temp\":\"temp\",\n                          \"water_u\":\"u\", \"water_v\":\"v\", \"salinity\":\"salt\"})\n\n5. Generate boundary conditions from the climatology\n\n        >> seapy.roms.boundary.from_roms(\"my_clim.nc\", \"my_bry.nc\")\n\n6. Generate initial conditions from the climatology\n\n        >> seapy.roms.initial.from_roms(\"my_clim.nc\", \"my_ini.nc\")\n\n7. You now have what you need to run your model\n\n8. To set up data assimilation, download the raw observations (e.g., `aviso_map_day1.nc`, `aviso_map_day2.nc`, `argo_day1.nc` ). You can then process the data:\n\n        >> dt = 400/86400       # time-step of the model in days\n        >> aviso_gen = seapy.roms.obsgen.aviso_sla_map(mygrid, dt)\n        >> aviso_gen.batch_files(seapy.list_files('.','aviso.*nc'), 'aviso_roms_#.nc')\n        >> argo_gen = seapy.roms.obsgen.argo_ctd(mygrid, dt)\n        >> obs = argo_gen.convert_file(\"argo_day1.nc\")\n        >> obs.to_netcdf(\"argo_roms_1.nc\")\n\n9. Put all of the processed observations files together into a file for a given assimilation window\n\n        >> seapy.roms.obs.merge_files(seapy.list_files('.*roms_[0-9]+.nc'), 'roms_obs_#.nc', np.arange([0, 10.1, 5]))\n\nThere are many more things that can be done, but these show some of the power available via simple commands.\n"
 },
 {
  "repo": "aist-oceanworks/mudrod",
  "language": "Java",
  "readme_contents": "# MUDROD\n## Mining and Utilizing Dataset\u00a0Relevancy from Oceanographic Datasets to Improve Data Discovery and Access\n\nMUDROD now lives at the Apache Software Foundation under the Apache Science Daat Analytics Platform (SDAP) Incubating project.\n\nYou can access the SDAP Website at https://sdap.apache.org\n\nThe SDAP Source Code lives at https://github.com/apache/incubator-sdap-mudrod\n \n# License\nThis source code is licensed under the [Apache License v2.0](http://www.apache.org/licenses/LICENSE-2.0), a\ncopy of which is shipped with this project. \n"
 },
 {
  "repo": "oscarbranson/cbsyst",
  "language": "Python",
  "readme_contents": "<div align=\"right\">\n  <a href=\"https://github.com/oscarbranson/cbsyst/actions\"><img src=\"https://github.com/oscarbranson/cbsyst/actions/workflows/tests.yml/badge.svg\" alt=\"GHA\" height=\"18\"></a>\n  <a href=\"https://badge.fury.io/py/cbsyst\"><img src=\"https://badge.fury.io/py/cbsyst.svg\" alt=\"PyPI version\" height=\"18\"></a>\n  <a href=\"https://anaconda.org/conda-forge/cbsyst\"> <img src=\"https://anaconda.org/conda-forge/cbsyst/badges/version.svg\" alt=\"conda-forge version\" height=\"18\"/></a>\n  <a href=\"https://doi.org/10.5281/zenodo.1402261\"> <img src=\"https://zenodo.org/badge/DOI/10.5281/zenodo.1402261.svg\" alt=\"DOI\" height=\"18\"></a>\n</div>\n\n<div align=\"left\">\n  <img src=\"https://raw.githubusercontent.com/oscarbranson/cbsyst/master/logo/cbsyst.png\" alt=\"CBsyst\" height=\"96\">\n</div>\n\n**A Python module for calculating seawater carbon and boron chemistry.** \n\nThis will be particularly useful for anyone thinking about oceans in the distant past, when Mg and Ca concentrations were different. I use [Mathis Hain's MyAMI model](http://www.mathis-hain.net/resources/Hain_et_al_2015_GBC.pdf) to adjust speciation constants for Mg and Ca concentration.\n\n***Tested** in the modern ocean against GLODAPv2 data (see below). Performs as well as Matlab CO2SYS.*\n\n## Work in Progress:\n- [ ] [Compare to CO2SYS](https://github.com/oscarbranson/cbsyst/issues/6), a la [Orr et al (2015)](http://www.biogeosciences.net/12/1483/2015/bg-12-1483-2015.pdf)?\n\nIf anyone wants to help with any of this, please do contribute!\nA full list of bite-sized tasks that need doing is available on the [Issues](https://github.com/oscarbranson/cbsyst/issues) page.\n\n## Acknowledgement\nThe development of `cbsyst` has been greatly aided by [CO2SYS](http://cdiac.ornl.gov/oceans/co2rprt.html), and the [Matlab conversion of CO2SYS](http://cdiac.ornl.gov/ftp/oceans/co2sys/).\nIn particular, these programs represent a gargantuan effort to find the most appropriate coefficient formulations and parameterisations from typo-prone literature.\nCO2SYS has also provided an invaluable benchmark throughout development.\n\n## Data Comparison\nI have used the [GLODAPv2 data set](cbsyst/test_data/GLODAP_data/Olsen_et_al-2016_GLODAPv2.pdf) to test how well `cbsyst` works with modern seawater.\n\n### Method:\nImport the entire GLODAPv2 data set, remove all data where `flag != 2` (2 = good data), and exclude all rows that don't have all of (salinity, temperature, pressure, tco2, talk, phosphate, silicate and phtsinsitutp) - i.e. salinity, temperature, pressure, nutrients and all three measured carbonate parameters.\nThe resulting dataset contains 79,896 bottle samples. \nThe code used to process the raw GLODAPv2 data is available [here](cbsyst/test_data/GLODAP_data/get_GLODAP_data.py).\n\nNext, calculate the carbonate system from sets of two of the measured carbonate parameters, and compare the calculated third parameter to the measured third parameter (i.e. calculate Alkalinity from pH and DIC, then compared calculated vs. measured Alkalinities). The code for making these comparison plots is [here](cbsyst/test_data/GLODAP_data/plot_GLODAPv2_comparison.py).\n\n### Results:\n**Calculated pH** (from DIC and Alkalinity) is offset from measured values by -0.0011 (-0.030/+0.027).\n![Calculated vs Measured pH](cbsyst/test_data/GLODAP_data/Figures/pH_comparison.png)\n\n**Calculated Alkalinity** (from pH and DIC) is offset from measured values by 0.39 (-11/+12) umol/kg.\n![Calculated vs Measured TA](cbsyst/test_data/GLODAP_data/Figures/TA_comparison.png)\n\n**Calculated DIC** (from pH and Alkalinity) is offset from measured values by -0.38 (-11/+10) umol/kg.\n![Calculated vs Measured DIC](cbsyst/test_data/GLODAP_data/Figures/DIC_comparison.png)\n\nReported statistics are median \u00b195% confidence intervals extracted from the residuals (n = 79,896).\n\nData are idential to within rouding errors as values calculated by Matlab CO2SYS (v1.1).\n\n### Conclusions:\n`cbsyst` does a good job of fitting the GLODAPv2 dataset!\n\n## Technical Details\n### Constants\nConstants calculated by an adaptation of [Mathis Hain's MyAMI model](http://www.mathis-hain.net/resources/Hain_et_al_2015_GBC.pdf). \nThe [original MyAMI code](https://github.com/MathisHain/MyAMI) is available on GitHub.\nA stripped-down version of MyAMI is [packaged with cbsyst](cbsyst/MyAMI_V2.py).\nIt has been modified to make it faster (by vectorising) and more 'Pythonic'.\nAll the Matlab interface code has been removed.\n\nConstants not provided by MyAMI (KP1, KP2, KP3, KSi, KF) are formulated following [Dickson, Sabine & Christian's (2007) 'Guide to best practices for ocean CO<sub>2</sub> measurements.'](http://cdiac.ornl.gov/oceans/Handbook_2007.html).\n\nPressure corrections are applied to the calculated constants following Eqns. 38-40 of [Millero et al (2007)](cbsyst/docs/Millero_2007_ChemicalReview.pdf), using (typo-corrected) constants in their Table 5.\nAll constants are on the pH Total scale.\n\n### Calculations\nSpeciation calculations follow [Zeebe and Wolf-Gladrow (2001)](https://www.elsevier.com/books/co2-in-seawater-equilibrium-kinetics-isotopes/zeebe/978-0-444-50946-8).\nCarbon speciation calculations are described in Appendix B, except where Alkalinity is involved, in which cases the formulations of [Ernie Lewis' CO2SYS](http://cdiac.ornl.gov/oceans/co2rprt.html) are used.\nBoron speciation calculations in Eqns. 3.4.43 - 3.4.46.\n\nBoron isotopes are calculated in terms of fractional abundances instead of delta values, as outlines [here](cbsyst/docs/B_systematics.pdf).\nDelta values can be provided as an input, and are given as an output.\n\n\n# Installation\n\n**Requires Python 3.5+**. \nDoes *not* work in 2.7. Sorry.\n\n### PyPi\n```bash\npip install cbsyst\n```\n\n### Conda-Forge\n```bash\nconda install cbsyst -c conda-forge\n```\n\n## Example Usage\n\n```python\nimport cbsyst as cb\nimport numpy as np\n\n# Create pH master variable for demo\npH = np.linspace(7,11,100)  # pH on Total scale\n\n# Example Usage\n# -------------\n# The following functions can be used to calculate the\n# speciation of C and B in seawater, and the isotope\n# fractionation of B, given minimal input parameters.\n#\n# See the docstring for each function for info on\n# required minimal parameters.\n\n# Carbon system only\nCsw = cb.Csys(pHtot=pH, DIC=2000.)\n\n# Boron system only\nBsw = cb.Bsys(pHtot=pH, BT=433., dBT=39.5)\n\n# Carbon and Boron systems\nCBsw = cb.CBsys(pHtot=pH, DIC=2000., BT=433., dBT=39.5)\n\n# NOTE:\n# At present, each function call can only be used to\n# calculate a single minimal-parameter combination -\n# i.e. you can't pass it multiple arrays of parameters\n# with different combinations of parameters, as in\n# the Matlab CO2SYS code.\n\n# Example Output\n# --------------\n# The functions return a Bunch (modified dict with '.' \n# attribute access) containing all system parameters\n# and constants.\n#\n# Output for a single input condition shown for clarity:\n\nout = cb.CBsys(pHtot=8.1, DIC=2000., BT=433., dBT=39.5)\nout\n\n>>> {'ABO3': array([ 0.80882931]),\n     'ABO4': array([ 0.80463763]),\n     'ABT': array([ 0.80781778]),\n     'BO3': array([ 328.50895695]),\n     'BO4': array([ 104.49104305]),\n     'BT': array([ 433.]),\n     'CO2': array([ 9.7861814]),\n     'CO3': array([ 238.511253]),\n     'Ca': array([ 0.0102821]),\n     'DIC': array([ 2000.]),\n     'H': array([  7.94328235e-09]),\n     'HCO3': array([ 1751.7025656]),\n     'Ks': {'K0': array([ 0.02839188]),\n      'K1': array([  1.42182814e-06]),\n      'K2': array([  1.08155475e-09]),\n      'KB': array([  2.52657299e-09]),\n      'KSO4': array([ 0.10030207]),\n      'KW': array([  6.06386369e-14]),\n      'KspA': array([  6.48175907e-07]),\n      'KspC': array([  4.27235093e-07])},\n     'Mg': array([ 0.0528171]),\n     'S': array([ 35.]),\n     'T': array([ 25.]),\n     'TA': array([ 2333.21612227]),\n     'alphaB': array([ 1.02725]),\n     'dBO3': array([ 46.30877684]),\n     'dBO4': array([ 18.55320208]),\n     'dBT': array([ 39.5]),\n     'deltas': True,\n     'fCO2': array([ 344.68238018]),\n     'pCO2': array([ 345.78871573]),\n     'pHtot': array([ 8.1]),\n     'pdict': None}\n\n# All of the calculated output arrays will be the same length as the longest\n# input array.\n\n# Access individual parameters by:\nout.CO3\n\n>>> array([ 238.511253])\n\n# Output data for external use:\ndf = cb.data_out(out, 'example_export.csv')\n\n# This returns a pandas.DataFrame object with all C and B parameters.\n# It also saves the data to the specified file. The extension of the\n# file determined the format it is saved in (see data_out docstring).\n\n```\n\n## Technical Note: Whats is a `Bunch`?\n\nFor code readability and convenience, I've used Bunch objects instead of traditional dicts.\nA [Bunch](cbsyst/helpers.py#L6) is a modification of a dict, which allows attribute access via the dot (.) operator.\nApart from that, it works *exactly* like a normal dict (all the usual methods are available transparrently).\n\n**Example:**\n```python\nfrom cbsyst.helpers import Bunch\n\n# Make a bunch\nbun = Bunch({'a': 1,\n             'b': 2})\n\n# Access items of bunch...\n# as a dict:\nbun['a']\n\n>>> 1\n\n# as a Bunch:\nbun.a\n\n>>> 1\n```"
 },
 {
  "repo": "USGS-CMG/stglib",
  "language": "Python",
  "readme_contents": "# stglib - Process data from a variety of oceanographic instrumentation\n\n[![Documentation Status](https://readthedocs.org/projects/stglib/badge/?version=latest)](http://stglib.readthedocs.io/en/latest/?badge=latest)\n![stglib](https://github.com/dnowacki-usgs/stglib/workflows/stglib/badge.svg)\n[![Anaconda-Server Badge](https://anaconda.org/conda-forge/stglib/badges/version.svg)](https://anaconda.org/conda-forge/stglib)\n\nThis package contains code to process data from a variety of oceanographic instrumentation, consistent with the procedures of the USGS [Coastal/Marine Hazards and Resources Program](https://marine.usgs.gov) (formerly Coastal and Marine Geology Program).\n\nCurrently, this package has at least partial support for:\n\n- Nortek Aquadopp profilers, in mean-current and wave-burst modes\n- RBR pressure (including waves) and turbidity sensors\n- YSI EXO2 water-quality sondes\n- SonTek IQ flow monitors\n- WET labs sensors, including ECO NTUSB and ECO PAR\n- Onset HOBO pressure sensors\n- Vaisala Weather Transmitter WXT sensors\n- In-Situ Aqua TROLL sensors\n- RD Instruments ADCPs\n- Moving-boat ADCP data processed using [QRev](https://hydroacoustics.usgs.gov/movingboat/QRev.shtml), for use in index-velocity computation\n\nThis package makes heavy use of [NumPy](http://www.numpy.org), [xarray](http://xarray.pydata.org/en/stable/), and [netCDF4](http://unidata.github.io/netcdf4-python/). It works on Python 3.7+.\n\n[Read the documentation](http://stglib.readthedocs.io/).\n"
 },
 {
  "repo": "nikita-0209/downscale-sst",
  "language": "Jupyter Notebook",
  "readme_contents": "# Downscaling Oceanographic Satellite Data with Convolutional Neural Networks\n\nA widely measured variable in the ocean, Sea SurfaceTemperature  (SST),  is  a  strong  indicator  of  pollution, productivity, global climate change and stress to corals and other species.  It is an estimate of the energy in the sea due to the motion of molecules. High resolution satellite sensors are effectively measure Sea Surface Temperature under clear sky conditions.  However under cloudy conditions, high resolution SST Measurements are not available.With  the  help  of  a  deep  learning  architecture,  the available images with low spatial resolution can beenhanced to produce images of high spatial resolution.\n\n## Getting Started\n\nThese instructions will get you a copy of the project up and running on your local machine.\n\n### Prerequisites\n\n* Python3\n* Tensorflow 2.x\n* NetCDF\n* Basemaps Toolkit\n\n### Installing\n\nAfter ensuring you have the above mentioned versions of Tensorflow and Python. The other two prerequisties can be installed in a Colab notebook as:\n\nNetCDF: \n```\n!pip install netCDF4\n```\n\nAnd \n\n```\n!apt-get install libgeos-3.5.0\n!apt-get install libgeos-dev\n!pip install https://github.com/matplotlib/basemap/archive/master.zip\n```\n\n## The Data\n\nGroup of High Resolution Sea Surface Temperature (GHRSST) data engulf SST observations from all kinds of available sources. The  GHRSST was established to foster an international focus and coordination for the development of a new generation of global, multi-sensor, high-resolution near real time SST datasets. Major contribution in this dataset comes from the space-borne satellite radiometers. The Level-4 (L4) product is generated using various objective analysis techniques to produce gap-free SST maps over the global oceans. In this study, we have used this L4 GHRSST products with a regular spatial resolution of ~ 1 km. The data was downloaded from [Physical Oceanography Distributed Active Archive Center,](https://podaac.jpl.nasa.gov/GHRSST)\n\n## Methodology\n\n### Create Data\n\nSea Surface Temperature Data is stored in NetCDF Format. Along with recording the sea surface temperature, these data files denote land values by a particular constant. This constant, known as the fix value differs from one data file to another. To maintain uniformity, all NetCDF files were rewritten to assign a single fix value, (in this case 0) to denote the land values. For purposes of training, each of the given SST fields were divided into overlapping patches. \nThis can be done by [Create Dataset.ipynb](https://github.com/nikita-0209/downsample-sst/blob/master/Create_Dataset.ipynb). Remember to change the paths to where your NetCDF Files are stored. \nAs required by the architecture, the images were normalized to range [0,1] by dividing each pixel with the maximum of pixel values in both the data files.\n\n### Models\n\nSince Super Resolution Convolutional Neural Network had already been tried and tested on bicubic interpolated SST Fields by Aurelien Ducournau and Ronan Fablet in their paper Deep Learning for Ocean Remote Sensing: An Application of Convolutional Neural Networks for Super-Resolution on Satellite-Derived SST Data, initial experiments were carried out on this architecture. Since the results weren't satisfactory, a deeper architecture, namely Very Deep Super Resolution CNN was experimented with. For both the variants, the activation function used is ReLu. Each model is optimised by adaptive moment estimation. A batch size of 64 was chosen. \n\nTo run the SRCNN Architecture:\n```\npython srcnn_server.py --file_name_low <path to hdf5 array of low resolution patches>  --file_name_high  <path to hdf5 array of high resolution patches>\n```\n\nTo run the VDSR Architecture:\n```\npython vdsr_server.py --file_name_low <path to hdf5 array of low resolution patches>  --file_name_high  <path to hdf5 array of high resolution patches>\n```\n\nThe checkpoints of the best models will be saved in ckpts directory. Currently the number of epochs is 100.\n\n## Evaluate\n\nEach model was trained to minimise the mean square error between the predicted and the expected patch. Along with that, a popular metric used for comparing quality of images, Peak Signal Noise Ratio (PSNR) was calculated. The smaller the MSE, the greater is the PSNR and the better is the image quality.\n[Evaluate.ipynb](https://github.com/nikita-0209/downsample-sst/blob/master/Evaluate.ipynb) initialises the model, loads the weights and calcuulates PSNR of the predicted patches.\n\n## Predictions\n\nTo reconstruct the entire SST Field from the predicted patches, run [Prediction.ipynb](https://github.com/nikita-0209/downsample-sst/blob/master/Prediction.ipynb). This model assumes that VDSR was trained. Feel free to replace the architecture if needed.\nThe model is initialised, the weights are loaded, each patch is predicted and appropriately arranged to form the final complete SST Field.\n\nRemember to change paths to the saved hdf5 files of patches and model weights. \nModify the path while writing the created NetCDF File of Predicted SST Fields.\n\n##  Evaluation Results\n\nIn order to gain a better insight into the performance of the model, several fields like Mean of Predicted SST Fields, Domain Averaged Root Mean Square Error (RMSE) and Domain Averaged Bias have been plotted in [Plot Evaluation.ipynb](https://github.com/nikita-0209/downsample-sst/blob/master/Plot_Evaluation.ipynb). The terms Mean SST and Domain Averaged RMSE are self-explanatory with mean SST referring to the mathematical mean of the predicted SST Fields, and the latter referring to the averaged root mean squared error between considered fields. \nThe Domain Averaged Bias is defined as the mean of the differences between the considered SST Fields, say the input and the predicted.\n\n## Detailed Report\n\nA detailed report of this project is available: [here](https://drive.google.com/file/d/1ssvq1EZvxojmPIaApwvClPOaoqPUlmu8/view?usp=sharing).\n\n## Authors\n\n* **Nikita Saxena** \n\n## Acknowledgments\nI express my sincere thanks to Dr. Rashmi Sharma, who provided me with the opportunity to work on this project. I pay my deep sense of gratitude to Dr. Neeraj Agarwal and Dr. Jishad M, without whose valuable guidance and supervision the project couldn't have been completed.\n\n\n"
 },
 {
  "repo": "gher-ulg/PhysOcean.jl",
  "language": "Julia",
  "readme_contents": "# PhysOcean\n\n[![Build Status](https://github.com/gher-ulg/PhysOcean.jl/workflows/CI/badge.svg)](https://github.com/gher-ulg/PhysOcean.jl/actions)\n[![Build Status Windows](https://ci.appveyor.com/api/projects/status/github/gher-ulg/PhysOcean.jl?branch=master&svg=true)](https://ci.appveyor.com/project/Alexander-Barth/physocean-jl)\n[![codecov.io](http://codecov.io/github/gher-ulg/PhysOcean.jl/coverage.svg?branch=master)](http://codecov.io/github/gher-ulg/PhysOcean.jl?branch=master)\n\n[![documentation stable](https://img.shields.io/badge/docs-stable-blue.svg)](https://gher-ulg.github.io/PhysOcean.jl/stable/)\n[![documentation latest](https://img.shields.io/badge/docs-latest-blue.svg)](https://gher-ulg.github.io/PhysOcean.jl/latest/)\n\n# Content\n\nYou will find here some general tools for Physical Oceanography (state equations, bulk formulas, geostrophic velocity calculations ...). \n\n# Installing\n\nYour need [Julia](http://julialang.org) to use `PhysOcean`. The command line version is sufficient for `PhysOcean`.\nInside Julia, you can download and install the package by issuing:\n\n```julia\nusing Pkg\nPkg.add(\"PhysOcean\")\n```\n\nOr if you want to use the latest version, you can use the following command:\n\n```julia\nusing Pkg\nPkg.add(PackageSpec(name=\"PhysOcean\", rev=\"master\"))\n```\n\n# Testing\n\nA test script is included to verify the correct functioning of the toolbox.\nThe script should be run in a Julia session.\n\n```julia\nusing Pkg\nPkg.test(\"PhysOcean\")\n```\n\n"
 },
 {
  "repo": "boshek/rsoi",
  "language": "R",
  "readme_contents": "---\noutput: github_document\n---\n\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n```{r, echo = FALSE}\nknitr::opts_chunk$set(\n  collapse = TRUE,\n  comment = \"#>\",\n  fig.path = \"man/figures/\", \n  warning = FALSE\n)\n```\n\n\n\n[![License: GPL v3](https://img.shields.io/badge/License-GPL%20v3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0) \n[![R-CMD-check](https://github.com/boshek/rsoi/workflows/R-CMD-check/badge.svg)](https://github.com/boshek/rsoi/actions)\n\n[![CRAN\\_Status\\_Badge](https://www.r-pkg.org/badges/version/rsoi)](https://cran.r-project.org/package=rsoi) [![CRAN Downloads](https://cranlogs.r-pkg.org/badges/rsoi?color=brightgreen)](https://CRAN.R-project.org/package=rsoi) [![cran checks](https://cranchecks.info/badges/worst/rsoi)](https://cran.r-project.org/web/checks/check_results_rsoi.html)\n\n\n# rsoi\nAn R package to download the most up to date of these climate indices:\n\n- Southern Oscillation Index\n- Oceanic Nino Index \n- North Pacific Gyre Oscillation\n- North Atlantic Oscillation\n- Arctic Oscillation\n- Antarctic Oscillation\n- Multivariate ENSO Index Version 2\n- Pacific Decadal Oscillation\n- Dipole Mode Index\n\n## Installation\nFor the development version \n\n```{r, eval = FALSE, echo = TRUE}\ninstall.packages(\"rsoi\")\n\nlibrary(rsoi)\nlibrary(tibble)\n```\n\n```{r, eval = TRUE, echo = FALSE}\nlibrary(rsoi)\nlibrary(tibble)\n```\n\n## Usage\nDownload Oceanic Nino Index data\n```{r, eval = TRUE, echo = TRUE, warning=FALSE, message=FALSE}\noni <- download_oni()\nhead(oni)\n```\n\nAnd a quick plot to illustrate the data:\n```{r plot, eval = TRUE, echo = TRUE, warning=FALSE, message=FALSE}\nbarcols <- c('#edf8b1','#7fcdbb','#2c7fb8')\n\nbarplot(oni$ONI, names.arg = oni$Date, ylab = \"Oceanic Nino Index\" , \n    col = barcols[oni$phase], border = NA, space = 0,\n    xaxt = \"n\")\n```\n\n## Inspired by\nThe idea for this package borrows heavily from the `rpdo` package. `rsoi` now supercedes `rpdo` as a source of data in R for Pacific Decadal Oscillation. \n\n\n"
 },
 {
  "repo": "ocean-data-challenges/2020a_SSH_mapping_NATL60",
  "language": "Jupyter Notebook",
  "readme_contents": "[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.4045400.svg)](https://doi.org/10.5281/zenodo.4045400)\n\n# SSH Mapping Data Challenge 2020a\n\nThis repository contains codes and sample notebooks for downloading and processing the SSH mapping data challenge.\n\nThe quickstart can be run online by clicking here:\n[![Binder](https://binder.pangeo.io/badge_logo.svg)](https://binder.pangeo.io/v2/gh/ocean-data-challenges/2020a_SSH_mapping_NATL60/master?filepath=quickstart.ipynb)\n\n## Motivation\n\nThe goal is to investigate how to best reconstruct sequences of Sea Surface Height (SSH) maps from partial satellite altimetry observations. This data challenge follows an _Observation System Simulation Experiment_ framework: \"Real\" full SSH are from a numerical simulation with a realistic, high-resolution ocean circulation model: the reference simulation. Satellite observations are simulated by sampling the reference simulation based on realistic orbits of past, existing or future altimetry satellites. A baseline reconstruction method is provided (see below) and the practical goal of the challenge is to beat this baseline according to scores also described below and in Jupyter notebooks.\n\n### Reference simulation\nThe reference simulation is the NATL60 simulation based on the NEMO model (Ajayi et al. 2020 doi:[10.1029/2019JC015827](https://doi.org/10.1029/2019JC015827)). The simulation is run without tidal forcing. \n\n### Observations\nThe SSH observations include simulations of Topex-Poseidon, Jason 1, Geosat Follow-On, Envisat, and SWOT altimeter data. This nadir altimeters constellation was operating during the 2003-2005 period and is still considered as a historical optimal constellation in terms of spatio-temporal coverage. The data challenge simulates the addition of SWOT to this reference constellation. No observation error is considered in this challenge.\n\n### Data sequence and use\n \nThe SSH reconstructions are assessed over the period from 2012-10-22 to 2012-12-02: 42 days, which is equivalent to two SWOT cycles in the SWOT science phase orbit.\nFor reconstruction methods that need a spin-up, the **observations** can be used from 2012-10-01 until the beginning of the evaluation period (21 days). This spin-up period is not included in the evaluation. For reconstruction methods that need learning from full fields, the **reference data** can be used from 2013-01-02 to 2013-09-30. The reference data between 2012-12-02 and 2013-01-02 should never be used so that any learning period or other method-related-training period can be considered uncorrelated to the evaluation period.\n\n![Data Sequence](figures/DC-data_availability.png)\n\n## Leaderboard\n\n| Method     |   \u00b5(RMSE) |   \u03c3(RMSE) |   \u03bbx (degree) |   \u03bbt (days) | Notes                     | Reference        |\n|:-----------|------------------------:|---------------------:|-------------------------:|-----------------------:|:--------------------------|:-----------------|\n| baseline OI 1 nadir |                    0.69 |                 0.03 |                     3.31 |                  33.32 | Covariances not optimized | quickstart.ipynb |\n| baseline OI 4 nadirs |                    0.83 |                 0.04 |                     2.25 |                  15.67 | Covariances not optimized | quickstart.ipynb |\n| baseline OI 1 swot |                    0.85 |                 0.05 |                     1.22 |                  12.38 | Covariances not optimized | quickstart.ipynb |\n| | | | | | | |\n| duacs 4 nadirs |       0.92 |      0.01 |          1.42 |       12.0 | Covariances DUACS | eval_duacs.ipynb |\n| bfn 4 nadirs  |       0.92 |      0.02 |          1.23 |       10.6 | QG Nudging | eval_bfn.ipynb |\n| dymost 4 nadirs |       0.91 |      0.01 |          1.36 |       11.79 | Dynamic mapping | eval_dymost.ipynb |\n| miost 4 nadirs |       0.93 |      0.01 |          1.35 |       10.19 | Multiscale mapping | eval_miost.ipynb |\n| 4DVarNet 4 nadirs :trophy: |       **0.94** |      **0.01** |          **0.83** |       **8.01** | 4DVarNet mapping | eval_4dvarnet.ipynb |\n| | | | | | | |\n| duacs 1 swot + 4 nadirs |       0.92 |      0.02 |          1.22 |       11.15 | Covariances DUACS | eval_duacs.ipynb |\n| bfn 1 swot + 4 nadirs  |       0.93 |      0.02 |           0.8 |        10.09 | QG Nudging | eval_bfn.ipynb |\n| dymost 1 swot + 4 nadirs |       0.93 |      0.02 |           1.2 |        10.07 | Dynamic mapping | eval_dymost.ipynb |\n| miost 1 swot + 4 nadirs |       0.94 |      **0.01** |          1.18 |       10.14 | Multiscale mapping | eval_miost.ipynb |\n| 4DVarNet 1 swot + 4 nadirs :trophy: |       **0.95** |      **0.01** |          **0.62** |        **5.29** | 4DVarNet mapping | eval_4dvarnet.ipynb |\n\n**\u00b5(RMSE)**: average RMSE score.  \n**\u03c3(RMSE)**: standard deviation of the RMSE score.  \n**\u03bbx**: minimum spatial scale resolved.  \n**\u03bbt**: minimum time scale resolved. \n \n## Quick start\nYou can follow the quickstart guide in [this notebook](https://github.com/ocean-data-challenges/2020a_SSH_mapping_NATL60/blob/master/quickstart.ipynb) or launch it directly from <a href=\"https://binder.pangeo.io/v2/gh/ocean-data-challenges/2020a_SSH_mapping_NATL60/master?filepath=quickstart.ipynb\" target=\"_blank\">binder</a>.\n\n## Download the data\nThe data are hosted on the [AVISO+ website](https://www.aviso.altimetry.fr/en/data/products/ocean-data-challenges/2020a-ssh-mapping-natl60.html) and tagged with DOI: 10.24400/527896/a01-2020.002. The website also provides a data handbook. This is the recommended access. This [wiki](https://github.com/ocean-data-challenges/2020a_SSH_mapping_NATL60/wiki/AVISO---account-creation) can help you create an AVISO account to access the data. The data are also temporarily available [here](https://ige-meom-opendap.univ-grenoble-alpes.fr/thredds/catalog/meomopendap/extract/ocean-data-challenges/dc_data1/catalog.html). They are presented with the following directory structure:\n\n```\n. \n|-- dc_obs\n|   |-- 2020a_SSH_mapping_NATL60_topex-poseidon_interleaved.nc\n|   |-- 2020a_SSH_mapping_NATL60_nadir_swot.nc \n|   |-- 2020a_SSH_mapping_NATL60_karin_swot.nc\n|   |-- 2020a_SSH_mapping_NATL60_jason1.nc\n|   |-- 2020a_SSH_mapping_NATL60_geosat2.nc\n|   |-- 2020a_SSH_mapping_NATL60_envisat.nc\n\n|-- dc_ref\n|   |-- NATL60-CJM165_GULFSTREAM_y****m**d**.1h_SSH.nc\n\n```\n\nTo start out download the *observation* dataset (dc_obs, 285M) from the temporary data server, use: \n```shell\nwget https://ige-meom-opendap.univ-grenoble-alpes.fr/thredds/fileServer/meomopendap/extract/ocean-data-challenges/dc_data1/dc_obs.tar.gz\n```\n\nand the *reference* dataset (dc_ref, 11G) using (*this step may take several minutes*): \n\n```shell\nwget https://ige-meom-opendap.univ-grenoble-alpes.fr/thredds/fileServer/meomopendap/extract/ocean-data-challenges/dc_data1/dc_ref.tar.gz\n```\nand then uncompress the files using `tar -xvf <file>.tar.gz`. You may also use `ftp`, `rsync` or `curl`to donwload the data.  \n\n\n## Baseline and evaluation\n\n### Baseline\nThe baseline mapping method is optimal interpolation (OI), in the spirit of the present-day standard for DUACS products provided by AVISO. OI is implemented in the [`baseline_oi`](https://github.com/ocean-data-challenges/2020a_SSH_mapping_NATL60/blob/master/notebooks/baseline_oi.ipynb) Jupyter notebook. The SSH reconstructions are saved as a NetCDF file in the `results` directory. The content of this directory is git-ignored.\n   \n### Evaluation\n\nThe evaluation of the mapping methods is based on the comparison of the SSH reconstructions with the *reference* dataset. It includes two scores, one based on the Root-Mean-Square Error (RMSE), the other based on Fourier wavenumber spectra. The evaluation notebook [`example_data_eval`](https://github.com/ocean-data-challenges/2020a_SSH_mapping_NATL60/blob/master/notebooks/example_data_eval.ipynb) implements the computation of these two scores as they could appear in the leaderboard. The notebook also provides additional, graphical diagnostics based on RMSE and spectra.\n\n## Data processing\n\nCross-functional modules are gathered in the `src` directory. They include tools for regridding, plots, evaluation, writing and reading NetCDF files. The directory also contains a module that implements the baseline method.  \n\n## Acknowledgement\n\nThe structure of this data challenge was to a large extent inspired by [WeatherBench](https://github.com/pangeo-data/WeatherBench).\n"
 },
 {
  "repo": "jessecusack/ocean_tools",
  "language": "Python",
  "readme_contents": "Ocean Tools\n=============\nThis repository contains oceanographic data analysis tools.\n\nModules\n-------\n\n* GM: functions for applying the Garrett and Munk internal wave spectra.\n* TKED: functions for estimating turbulent kinetic energy dissipation from finescale observations. Including the finescale parameterisation, Thorpe scale estimates and the large eddy method.\n* gravity_waves: linear inertia-gravity wave dynamics.\n* sandwell: read data from the Smith and Sandwell bathymetric binary file into a numpy array efficiently.\n* utils: miscellaneous functions.\n* window: functions for splitting data into chunks.\n\nAll very much a work in progress.\n\nInstallation\n------------\n\nFirst clone or download the repository. Then install using pip:\n\n``cd ocean_tools``\n\n``pip install .``\n\nOptionally, use the the -e flag, to make it editable.\n\n``pip install -e .``\n"
 },
 {
  "repo": "jklymak/Eos314Text",
  "language": "TeX",
  "readme_contents": "# EOS 314 course notes\n\nSee the pdf for the notes.  Written using https://github.com/Tufte-LaTeX/tufte-latex\n\nCompiled versions can be found at http://ocean-physics.seos.uvic.ca/~jklymak/Eos314Text/\n"
 },
 {
  "repo": "dankelley/dal-oce-thesis",
  "language": "TeX",
  "readme_contents": "## Overview\n\nThis directory contains materials that can help with preparing theses at\nDalhousie University, using latex. It is provided \"as is\" by a professor who is\ntrying to help his students. It is not an official product of the university,\nand its results are not claimed to meet the formatting requirements.  \n\nSee http://dankelley.github.io/dal-oce-thesis/index.html for documentation.\n\n## Installation\n\nIf you want to use the ocethesis package like any regular LaTeX package without\ncopying the provided style sheets into the directory of your document, you may\ninstall the package in a root path of your TeX distribution. The procedure\ndepends on the operating system, and so some steps listed below are divided\ninto Linux and MacOS (formerly OSX) variants; a procedure on Windows may be\nadded if a user explains it to the developers.\n\n### Step 1: set up directories\n\nOpen a terminal and type\n```\nkpsewhich -var-value=TEXMFHOME\n```\n\nto find the root path of your TeX distribution. On linux and unix-like systems,\nthis is typically `/home/<username>/texmf`, where `<username>` is the name of\nyour user account on your computer. On MacOS (or the previous OSX) this will be\n`/Users/<username>/Library/texmf`.  It is possible that this directory (and\nnecessary subdirectories) does not exist. In this case use the terminal and\ncreate the requisite subdirectories, with\n```\nmkdir -p ~/texmf/tex/latex\nmkdir -p ~/texmf/bibtex/bst\n```\non Linux or\n```\nmkdir -p ~/Library/texmf/tex/latex\nmkdir -p ~/Library/texmf/bibtex/bst\n```\non MacOS.\n\n\n### Step 2: clone the Dalhousie thesis repository\n\nAt this stage, you are set up for doing work with Latex, and you will not need\nto repeat these steps for any later updates to the Dalhousie thesis style sheet.\n\nThe next step is to clone the dal-oce-thesis repository. In the terminal, type\n```\ncd ~/texmf/tex/latex\ngit clone https://github.com/dankelley/dal-oce-thesis.git ocethesis\n```\non Linux, or \n```\ncd ~/Library/texmf/tex/latex\ngit clone https://github.com/dankelley/dal-oce-thesis.git ocethesis\n```\non MacOS.\n\nFinally, set up the bibliography style sheet, with\n```\ncd ~/texmf/bibtex/bst\nln -s ../../tex/latex/ocethesis/ocethesis.bst .\n```\non Linux or\n```\ncd ~/Library/texmf/bibtex/bst\nln -s ../../tex/latex/ocethesis/ocethesis.bst .\n```\non MacOS.\n\n\n### Step 3. testing the setup\n\nYou can test if your TeX distribution is able to find the class file and style\nsheets by typing in the terminal:\n```\nkpsewhich ocethesis.cls\nkpsewhich ocethesis.bst\n```\nand verifying that both commands report the full path to the respective files,\nas you've set them up.\n\n## Updating the installation\n\nIf `dal-oce-thesis` gets updated, you may update your installation by doing\n```\ncd ~/texmf/tex/latex/ocethesis\n```\non Linux or\n```\ncd ~/Library/texmf/tex/latex/ocethesis\n```\non MacOS, and then typing\n```\ngit pull\n```\n\n\n"
 },
 {
  "repo": "obidam/pcm",
  "language": "Jupyter Notebook",
  "readme_contents": "Profile Classification Modelling (PCM)\n======================================\n[![DOI](https://img.shields.io/badge/DOI--Article-10.1016%2Fj.pocean.2016.12.008-orange.svg)](http://dx.doi.org/10.1016/j.pocean.2016.12.008)  \n\n[![license](https://img.shields.io/github/license/mashape/apistatus.svg)]()\n\n**Profile Classification Modelling** is a scientific analysis approach based on vertical profiles classification that can be used in a variety of oceanographic problems (front detection, water mass identification, natural region contouring, reference profile selection for validation, etc ...).  \nIt is being developed at Ifremer/LOPS in collaboration with IMT Atlantique since 2015, and has become mature enough (with publication and communications) to be distributed and made publicly available for continuous improvements with a community development.\n\n**Ocean dynamics** and its 3-dimensional structure and variability is so complex that it is very difficult to develop objective and efficient diagnostics of horizontally and vertically coherent oceanic patterns. However, identifying such **patterns** is crucial to the understanding of interior mechanisms as, for instance, the integrand giving rise to Global Ocean Indicators (e.g. heat content and sea level rise). We believe that, by using state of the art **machine learning** algorithms and by building on the increasing availability of ever-larger **in situ and numerical model datasets**, we can address this challenge in a way that was simply not possible a few years ago. Following this approach, **Profile Classification Modelling** focuses on the smart identification of vertically coherent patterns and their spatial distribution.\n\n*References*: \n\n- Maze, G., et al. Coherent heat patterns revealed by unsupervised classification of Argo temperature profiles in the North Atlantic Ocean. *Progress in Oceanography*, 151, 275-292 (2017)  \n    [http://dx.doi.org/10.1016/j.pocean.2016.12.008](http://dx.doi.org/10.1016/j.pocean.2016.12.008)\n- Maze, G., et al. Profile Classification Models. *Mercator Ocean Journal*, 55, 48-56 (2017).   \n    [http://archimer.ifremer.fr/doc/00387/49816](http://archimer.ifremer.fr/doc/00387/49816)\n- Maze, G. A Profile Classification Model from North-Atlantic Argo temperature data. *SEANOE Sea scientific open data edition*.  \n    [http://doi.org/10.17882/47106](http://doi.org/10.17882/47106)\n\n## Python package\n[![Python 2.7](https://img.shields.io/badge/python-2.7-blue.svg)](https://www.python.org/downloads/release/python-270/) [![](https://img.shields.io/badge/xarray-0.10.0-blue.svg)](http://xarray.pydata.org/en/stable/) \n\nWe are currently developing a Python package to work with PCM easily.  \n**You can check out the first release on the [pyXpcm](https://github.com/obidam/pyxpcm) homepage.**  \nOtherwise you can still [look at a\nclassic PCM workflow on this notebook](https://github.com/obidam/pcm/blob/master/python/PCM-workflow-classic-demo.ipynb).\n\n## Matlab toolbox\n[![DOI](https://img.shields.io/badge/DOI--Matlab-10.5281%2Fzenodo.400018-orange.svg)](http://dx.doi.org/10.5281/zenodo.400018) [![](https://img.shields.io/badge/matlab->R2016b-blue.svg)]()  \n\nThis is the original code for PCM used in [Maze et al (2017)](http://dx.doi.org/10.1016/j.pocean.2016.12.008).  \n\n[You can get started with the Matlab toolobx following this wiki page](https://github.com/obidam/pcm/blob/master/matlab/README.md)\n\nNote that @gmaze will provide help to use it, but won't maintain the code. Contact us if you want to contribute !\n\n"
 },
 {
  "repo": "BCODMO/Ocean-Data-Ontology",
  "language": null,
  "readme_contents": "# Ocean Data Ontology #\n\n![documentation v0.9.0](https://img.shields.io/badge/documentation-v0.9.0-blue.svg)\n\nApplication-level ontology for describing oceanographic datasets\n\n## PREFIX ##\n\n<code>PREFIX odo: <http://ocean-data.org/schema></code>\n\nhttp://prefix.cc/odo\n\n\n## FUNDING ##\n\nThis work is funded by the National Science Foundation under the following awards:\n\nA Data Management System and Portal for Access to Ecological and Biogeochemical Ocean Data - BCO-DMO  \nAward #1031253  \nURL: http://www.nsf.gov/awardsearch/showAward?AWD_ID=1031253\n\nBiological and Chemical Oceanography Data Management Office (BCO-DMO): A System for Access to Ecological and Biogeochemical Ocean Data  \nAward #1435578  \nURL: http://www.nsf.gov/awardsearch/showAward?AWD_ID=1435578\n\n### NOTES ###\n\nRDF/XML: https://ocean-data.org/schema/ontology.xml\nJSON-LD: https://ocean-data.org/schema/ontology.json\nN-Triples: https://ocean-data.org/schema/ontology.nt\nTurtle: https://ocean-data.org/schema/ontology.ttl\n"
 },
 {
  "repo": "OceanOptics/Inlinino",
  "language": "Python",
  "readme_contents": "Inlinino\n========\n[![License: GPL v3](https://img.shields.io/badge/License-GPLv3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0)\n[![Python 3.8](https://img.shields.io/badge/Python-3.8-blue.svg)](https://www.python.org/downloads/)\n[![Documentation Status](https://readthedocs.org/projects/inlinino/badge/?version=latest)](https://inlinino.readthedocs.io/en/latest/?badge=latest)\n\nInlinino is an open-source software data logger for oceanographers. It primarily log measurements from optical instruments deployed on research vessels during month long campaigns. Secondarily, it  provides real-time visualization, which helps users troubleshoot instruments in the field and ensure collection of quality data. Inlinino is designed to interface with either serial (RS-232) or analog instruments. The data received is logged in a timestamped raw format (as communicated by the instrument) or in a comma separated file (csv) for easy importation in data analysis software. Typically, a new log file is created every hour for simplicity of post-processing and easy backups. Instruments supported are: SeaBird TSG, Satlantic PAR, WET Labs ECO sensors (e.g. ECO-BB3, ECO-FLBBCD, ECO-BBFL2, ECO-3X1M, ECO-BB9, ECO-BBRT), WET Labs ACS, Sequoia LISST, and analog sensors through a data acquisition system (DataQ DI-1100 ). Other instruments can be added via the user interface if they output simple ascii data frame, otherwise the code is intended to be modular to support new instruments. \n     \nThe documentation of the project is available at [http://inlinino.readthedocs.io](http://inlinino.readthedocs.io/en/latest/).\n\nAppropriate citation is:\nHaentjens, N. and Boss, E., 2020. Inlinino: A Modular Software Data Logger for Oceanography. DIY Oceanography. DOI: [10.5670/oceanog.2020.112](https://doi.org/10.5670/oceanog.2020.112)\n\n\n### Installation\nInlinino was bundled into a Windows executable and a macOS application. Both are available for download with a quick start guide in the [documentation](https://inlinino.readthedocs.io/en/latest/quick_start.html). Otherwise Inlinino can be installed from source using the setup.py file available on this repository, following the instructions below.\n\nDownload Inlinino code.\n \n    wget https://github.com/OceanOptics/Inlinino/archive/master.zip\n    unzip master.zip\n    cd Inlinino-master\n \nTo install Inlinino (tested with python 3.8 only, should work with newer python versions):\n\n    pip install -r requirements.txt\n\nInlinino can then be started from the folder containing inlinino's source code with.\n\n    python -m inlinino\n\n### Inlinino Software\nThe application is written in Python\u00a03, on top of pySerial, numpy, and PyQt5. The current version works with a \"classic\" Graphical User Interface. A web interface started to be implemented and can be found in the branch `tb-app` of this repository. A command line interface used to be available but was not ported to version >2.0.\n\nThe code is organized in:\n  + `docs`: User Documentation ([ReadTheDocs](https://inlinino.readthedocs.io/))\n  + `inlinino`: Inlinino source code\n    - `instruments/`:  Instrument interfaces, more instrument types can be added there.\n    - `ressources/`: User Interface Layout and Logo.\n    - `*.py`: Core code of Inlinino.\n    - `inlinino_cfg.json`: Applications parameters are saved in this file ([ReadTheDocs](https://inlinino.readthedocs.io/en/latest/cfg.html))\n  + `mcu_firmwares`: Firmwares to upload on a microcontroller for the previous DAQ module (deprecated)\n    - `PASC.cpp`: Precision analog to serial converter (PASC) firmware\n    - `Simulino.cpp `: Instrument simulator to test Inlinino with microcontrollers simulating the behavior of scientific instruments.\n  + `make.py`: Bundles Inlinino application into a .app or .exe depending on platform. pyInstaller must be installed.\n  + `setup.py`: Python environment setup file.\n\nWhen Inlinino is started an engineering log file is created in `logs/inlinino_<YYYYMMDD>_<hhmmss>.log` and keep track of most tasks executed (e.g. user interaction, creation of data log files, warnings, and potential errors).\n\n### Questions and issues\nFor any questions or issues regarding Inlinino please contact [me](mailto:nils.haentjens+inlinino@maine.edu).\n"
 },
 {
  "repo": "TEOS-10/GSW-R",
  "language": "R",
  "readme_contents": "# gsw\n\n[![R-CMD-check](https://github.com/TEOS-10/GSW-R/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/TEOS-10/GSW-R/actions/workflows/R-CMD-check.yaml)\n![RStudio CRAN mirror downloads](http://cranlogs.r-pkg.org/badges/last-month/gsw)\n![RStudio CRAN mirror downloads](http://cranlogs.r-pkg.org/badges/last-week/gsw)\n![RStudio CRAN mirror downloads](http://cranlogs.r-pkg.org/badges/last-day/gsw)\n\ngsw is an R package that provides a connection to the thermodynamic equation of\nseawater as defined at [teos-10.org](http://www.teos-10.org).  An earlier version\nof gsw was referenced to TEOS-10 version 3.03, but the present one is\nreferenced to version 3.05-4, which is current as of Aug 7, 2017. See the\n[CRAN](https://cran.r-project.org/package=gsw) website for check statistics,\netc.\n\nAll the gsw functions reproduce [teos-10.org](http://www.teos-10.org) test\nvalues to a tolerance of 1.5e-8, the default for numerical comparison in R\nworking on a 64-bit machine. (The tests are part of the package-building\nprocess, as is usual in R.)\n\nFunction names, as well as most argument names, match those used in the TEOS-10\n[documentation](http://www.teos-10.org/pubs/gsw/html/gsw_contents.html).\n\n"
 },
 {
  "repo": "SBFRF/pyDIWASP",
  "language": "Python",
  "readme_contents": "# pyDiwasp\nconversion of diwasp package (DIWASP: DIrectional WAve SPectrum analysis Version 1.4) for python\nconverted from https://github.com/metocean/diwasp\n\nI would lOVE help making this into a more pythonic representation of the original diwasp tool.  check issues for needed functionality adds.  \n\n## Toolbox contents:\n### Main functions:\n- dirspec.m           Main function for directional wave analysis\n- readspec.m          Reads in DIWASP format spectrum files\n- writespec.m         Writes DIWASP format spectrum files\n- plotspec.m          Plots DIWASP spectrums\n- testspec.m          Testing function for the estimation methods\n- makespec.m          Makes a fake spectrum and generates fake data for testing dirspec.m\n- infospec.m          Returns information about a directional spectrum\n- data_structures.m   is a help file describing the new Version 1.1 data structures\n\n## Private functions (some can be used as stand alone functions):\n### The transfer functions\n- /private/elev.m\n- /private/pres.m\n- /private/velx.m\n- /private/vely.m\n- /private/velz.m\n- /private/slpx.m\n- /private/slpy.m\n- /private/vels.m\n- /private/accs.m\n\n### The estimation functions\n- /private/DFTM.m\n- /private/EMLM.m\n- /private/IMLM.m\n- /private/EMEP.m\n- /private/BDM.m\n\n### Miscellaneous functions\n- /private/smoothspec.m\n- /private/wavenumber.m\n- /private/makerandomsea.m\n- /private/makewavedata.m\n- /private/Hsig.m\n- /private/gsamp.m\n- /private/check_data.m\n  \n\ncarying original license agreement and copyright\n\n## License agreement\nDIWASP, is free software; you can redistribute it and/or modify it under the terms of the \nGNU General Public License as published by the Free Software Foundation. \nHowever, the DIWASP license includes the following addendum concerning its usage:\nThis software and any derivatives of it shall only be used for educational purposes or \nscientific research without the intention of any financial gain. \nUse of this software or derivatives for any purpose that results in financial gain \nfor a person or organization without written consent from the author is a breach of the license agreement.\nThis software is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; \nwithout even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. \nIn addition the author is not liable in any way for consequences arising from the application of \nsoftware output for any design or decision-making process.\nThe GNU General Public License forms the main part of the license agreement included in the package. \n\nCopyright (C) 2002 David Johnson   Coastal Oceanography Group, CWR, UWA, Perth\n\n"
 },
 {
  "repo": "GenSci/NDBC",
  "language": "Python",
  "readme_contents": "# NDBC\n\n![alt text](http://www.ndbc.noaa.gov/images/nws/noaaleft.jpg \"NOAA\") ![alt text](http://www.ndbc.noaa.gov/images/nws/ndbc_title.jpg \"NDBC\")\n\nThis repository represents my attempts to build out Python class(es)\nto facilitate the acquisition, analysis, and visualization of National\nData Buoy Center (NDBC) data. The goal is to develop a set of APIs to\nfacilitate rapid discovery of data resources, exploratory data analysis,\nand allow integration into automated data workflows.\n\n## NDBC.py\n\nThis file defines the DataBuoy class. The purpose of this class is to\nallow a user to define a specific data buoy they wish to gather data\nfrom and provide the user with methods to collect and analyze this data.\n\nDependencies are listed in `requirements.txt`\n\n## Usage\n\n#### Installation\n\nInstall using pip from PyPI\n\n```\npip install NDBC\n```\n\nThen you are ready to start using this module in exploratory data analyses and scripted workflows.\n\n#### Methods of DataBuoy Class\n\n`.set_station_id`\n\nIf a DataBuoy class has been instantiated without any `station_id` argument, this method allows for setting a station id\n\n```\nfrom NDBC.NDBC import DataBuoy\nDB = DataBuoy()\nDB.set_station_id('46042') # <- Either strings or numbers are acceptable\n```\n\n`.get_station_metadata()`\n\nPerform a scrape of the public webpage for a specified data station and save a dictionary of available metadata to the `.station_info` property. This is only available if a DataBuoy has a valid `station_id` set (either during class instantiation or using\nthe `set_station_id` method).\n\n```\nfrom NDBC.NDBC import DataBuoy\nDB = DataBuoy(46042)\nDB.get_station_metadata()\nDB.station_info\n{   'Air temp height': '4 m above site elevation',\n    'Anemometer height': '5 m above site elevation',\n    'Barometer elevation': 'sea level',\n    'Sea temp depth': '0.6 m below water line',\n    'Site elevation': 'sea level',\n    'Watch circle radius': '1789 yards',\n    'Water depth': '1645.9 m',\n    'lat': '36.785 N',\n    'lon': '122.398 W'}\n```\n\n- `.get_data(datetime_index=False)`\n\nAfter importing, the DataBuoy class is instantiated with the ID of the\nstation from which historical data is sought. Then data may be gathered for\nthe years and months specified. If no time period is specified, the most recent\nfull month available is retrieved.\n\nThe default behavior is to append datetime values built from date part columns (YY, MM, DD, etc.) to a column 'datetime'. If value `True` is passed as the `datetime_index` argument, the datetime values will be used as index values for the returned dataframe. In some cases this is advantageous for time series analyses.\n\n```\nfrom NDBC.NDBC import DataBuoy\n\nn42 = DataBuoy(46042)  # <- String or numeric station ids are valid\n\nn42.get_data(datetime_index=True)  # <- no year, month argumets so latest full month is retrieved. Default data type is 'stdmet'\n\nOct not available.   # <- Where data is missing, messages are returned to the terminal via a logger.warning() call\nSep not available.\n\nn42.data  # <- anticipating additional data collection methods, the .data property returns a dictionary.  Indiviudual\n               data products are returned as pandas DataFrame objects\n\n# Datetime objects are compiled from individual year, month, day, hour, minute columns and used as the index to support\n# slicing data by time frames.\n\n{'stdmet':          WDIR WSPD  GST  WVHT    DPD   APD  MWD    PRES  ATMP  WTMP   DEWP   VIS   TIDE\n2019-07-31 23:50:00  298  3.6  5.2  1.25   7.69  5.37  303  1015.1  13.4  15.2  999.0  99.0  99.00\n2019-08-01 00:50:00  301  5.7  7.2  1.26   7.14  5.42  306  1014.8  13.4  15.3  999.0  99.0  99.00\n2019-08-01 01:50:00  323  6.6  8.3  1.33   7.14  5.47  312  1014.5  13.2  15.1  999.0  99.0  99.00\n2019-08-01 02:50:00  347  5.8  7.7  1.32   7.69  5.15  319  1014.5  12.7  15.1  999.0  99.0  99.00\n2019-08-01 03:50:00  353  5.6  7.2  1.26   7.69  5.31  325  1014.9  12.6  15.0  999.0  99.0  99.00\n...                  ...  ...  ...   ...    ...   ...  ...     ...   ...   ...    ...   ...    ...\n2019-08-31 18:50:00  999  6.2  7.4  0.87  13.79  4.67  186  1014.6  17.0  17.2  999.0  99.0  99.00\n2019-08-31 19:50:00  999  6.8  8.3  0.83  13.79  4.56  178  1014.2  17.2  17.3  999.0  99.0  99.00\n2019-08-31 20:50:00  999  6.5  7.8  0.89  13.79  4.38  195  1013.8  17.5  17.4  999.0  99.0  99.00\n2019-08-31 21:50:00  999  7.5  8.9  0.95  13.79  4.52  190  1013.1  17.5  17.3  999.0  99.0  99.00\n2019-08-31 22:50:00  999  8.0  9.4  0.95  13.79  4.09  171  1012.7  17.7  17.1  999.0  99.0  99.00\n\n[741 rows x 13 columns]}\n```\n\nBy default the `get_data()` function will fetch the most current month's data. However, the function can take lists of years & months ([int]) to specify a timeframe.\n\n```\n>>> n42 = NDBC.DataBuoy('46042')\n>>> n42.get_data(months=[1,2], years=range(2019, 2020), datetime_index=True, data_type='swden)\nYear 2019 not available.\nYear 2020 not available.\n \n>>> n42.data\n{'swden': {'data':                      .0200  .0325  .0375  .0425  .0475  .0525  .0575  .0625  .0675  .0725  .0775  .0825  .0875  ...  .3000  .3100  .3200  .3300  .3400  .3500  .3650  .3850  .4050  .4250  .4450  .4650  .4850\n2021-01-01 00:40:00    0.0    0.0    0.0   0.00   1.17   9.11  24.25  24.95  15.84  20.44  26.48  20.63  12.72  ...   0.28   0.31   0.19   0.20   0.13   0.07   0.06   0.05   0.03   0.01   0.01   0.00    0.0\n2021-01-01 01:40:00    0.0    0.0    0.0   0.00   0.00  13.76  26.55  22.40  24.12  30.09  23.41  15.74  14.95  ...   0.25   0.16   0.12   0.16   0.06   0.16   0.06   0.03   0.05   0.02   0.01   0.00    0.0\n2021-01-01 02:40:00    0.0    0.0    0.0   0.00   0.93   4.40  16.03  33.95  41.48  38.02  31.47  18.88  14.59  ...   0.21   0.15   0.18   0.14   0.14   0.10   0.07   0.05   0.03   0.02   0.01   0.00    0.0\n2021-01-01 03:40:00    0.0    0.0    0.0   0.07   1.14   6.95  27.94  45.68  41.92  30.11  25.03  19.52  10.93  ...   0.22   0.20   0.16   0.09   0.08   0.15   0.09   0.04   0.02   0.01   0.00   0.01    0.0\n2021-01-01 04:40:00    0.0    0.0    0.0   0.00   0.76   3.64  11.23  18.23  29.84  27.19  12.85  11.20   9.77  ...   0.13   0.17   0.14   0.16   0.08   0.08   0.07   0.08   0.05   0.01   0.01   0.00    0.0\n...                    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...  ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...\n2021-02-28 19:40:00    0.0    0.0    0.0   0.00   0.00   0.00   0.06   0.25   1.42   2.50   9.48  11.48   8.46  ...   0.21   0.13   0.11   0.08   0.10   0.04   0.02   0.02   0.03   0.01   0.00   0.00    0.0\n2021-02-28 20:40:00    0.0    0.0    0.0   0.02   0.05   0.08   0.24   1.02   3.97   4.97   4.99   8.31  10.09  ...   0.21   0.07   0.09   0.06   0.05   0.10   0.04   0.03   0.01   0.01   0.00   0.00    0.0\n2021-02-28 21:40:00    0.0    0.0    0.0   0.00   0.00   0.15   0.30   0.36   1.63   4.18   6.85   7.82   7.98  ...   0.12   0.11   0.09   0.08   0.04   0.05   0.06   0.02   0.01   0.01   0.00   0.00    0.0\n2021-02-28 22:40:00    0.0    0.0    0.0   0.00   0.01   0.09   0.10   0.32   2.84   3.82   3.91   4.92   5.17  ...   0.17   0.09   0.13   0.05   0.05   0.08   0.06   0.03   0.01   0.01   0.00   0.00    0.0\n2021-02-28 23:40:00    0.0    0.0    0.0   0.00   0.00   0.00   0.18   0.25   1.78   3.97   5.08   4.98   5.40  ...   0.07   0.10   0.11   0.08   0.08   0.06   0.03   0.02   0.01   0.01   0.00   0.00    0.0\n\n[1413 rows x 47 columns]}}\n\n```\n\nLikely due to my own biases in my research interests, the `get_data()`  function will default to fetching\nstandard meteorological data.  However, users can specify different data packages like so `get_data(data_type='cwind')`.  To view which data packages\nare currently supported examine the `DataBuoy.DATA_PACKAGES` attribute:\n```\n{'cwind': {'name': 'Continous Wind Data', 'url_char': 'c'},\n 'srad': {'name': 'Solar radiation data', 'url_char': 'r'},\n 'stdmet': {'name': 'Standard meteoroligcal data', 'url_char': 'h'},\n 'swden': {'name': 'Spectral Wave Density data', 'url_char': 'w'},\n 'swdir': {'name': 'Spectral wave (alpha1) direction data', 'url_char': 'd'},\n 'swdir2': {'name': 'Spectral wave (alpha2) direction data', 'url_char': 'i'},\n 'swr1': {'name': 'Spectral wave (r1) direction data', 'url_char': 'j'},\n 'swr2': {'name': 'Spectral wave (r2) direction data', 'url_char': 'k'}}\n\n```\n\nUsing the pandas DataFrame to store the returned data provides access to the wide array of methods the pandas package\nprovides.\n\n- `.save(filename(optional))`\n\nSaves an instantiated DataBuoy object as JSON to a file. If `filename` is not specified the file name will follow the\n`databuoy_{station_id}.json` convention.\n\n```\ndb = DataBuoy(46042)\ndb.save('/path/to/file/my_filename.json')\n```\n\n_classmethod_\n\n- `.load(filename)`\n  Instantiate a DataBuoy object from a file, generated by the `.save()` method.\n\n```\ndb = DataBuoy.load('/path/to/file.json')\n```\n"
 },
 {
  "repo": "jonathansharp/CO2-System-Extd",
  "language": "MATLAB",
  "readme_contents": "# CO2-System-Extd\n\n<a href=\"https://zenodo.org/badge/latestdoi/198885961\"><img src=\"https://zenodo.org/badge/198885961.svg\" alt=\"DOI\"></a> [![View CO2SYSv3 for MATLAB on File Exchange](https://www.mathworks.com/matlabcentral/images/matlab-file-exchange.svg)](https://www.mathworks.com/matlabcentral/fileexchange/78378-co2sysv3-for-matlab)\n\n## ABOUT\n\nThis repository includes software compatible with MATLAB and GNU Octave for calculating marine CO2 system variables (CO2SYS.m), computing partial derivatives of calculated CO2 system variables with respect to inputs (derivnum.m), and propagating uncertainties for CO2 system calculations (errors.m). This software performs similarly to previously released versions of CO2SYS.m (v1: https://cdiac.ess-dive.lbl.gov/ftp/co2sys/CO2SYS_calc_MATLAB_v1.1/; v2: https://github.com/jamesorr/CO2SYS-MATLAB), and includes the following extended capabilities, additions, and bug fixes (among other minor changes):\n \n1) Can accept input parameters of [CO3], [HCO3], and [CO2], and propagate their uncertainties\n2) Includes NH3 and HS as alkalinity contributors, and propagates their uncertainties\n3) Uses separate inputs to specify choices for characterizations of K1K2, KSO4, KF, and TB\n4) Does not evaluate input parameters equal to -999 or NaN\n5) Exits pH iteration loops that do not converge and indicates where a problem occurred\n6) Provides exactly identical pH results for a given input line, no matter the other lines of input parameters (this is not necessarily the case for prior versions of CO2SYS.m)\n7) Uses an updated definition of the ideal gas constant (https://physics.nist.gov/cgi-bin/cuu/Value?r)\n8) Fixes bugs in CO2SYS.m Revelle factor calculation and derivnum.m output conditions\n9) Includes K1 and K2 constants defined by Sulpis et al. (2020), K2 constant defined by Schockman and Byrne (2021), KF constant defined by Perez and Fraga (1987), and KSO4 constant of Waters and Millero (2013)\n10) Determines initial pH in iterative solvers using the approach of Munhoven (2013), detailed further in Humphreys et al. (submitted), rather than simply using an initial estimate of 8.0 each time.\n11) Obtains free scale pH properly within iterative pH solvers no matter the input scale, rather than making the simplification that input pH is always on the total scale.\n12) Includes substrate-inhibitor ratio (Bach, 2015) as an output argument from CO2SYS.\n13) Calculates uncertainties at output conditions that are associated with equilibrium constants with respect to equilibrium constants at output conditions, rather than input conditions as previously. This essentially assume pK uncertainty is constant regardless of temperature and pressure.\n14) Calculates derivatives and errors for the Revelle factor.\n15) errors.m includes optional calcium concentration uncertainty input as discussed in Dillon et al. (2020)\n\nAlso included in this repository is a routine to compare CO2SYSv3 to CO2SYSv2 (compare_versions.m), a routine to calculate total concentrations of conservative elements (Na, Mg, Cl, etc.) from CO2SYS output (TOTALS.m), and an example function to run CO2SYSv3 and plot some of the output (example_CO2SYS.m).\n\n## HISTORY\n\nCO2SYS was initially developed by Lewis and Wallace (1998) for MS DOS, later adapted for MS Excel and MATLAB by Pierrot (2006). The code was vectorized, refined, and optimized for computational speed by van Heuven (2011). Options for error propagation were added by Orr et al. (2018). This software builds upon those previous versions.\n\n## INSTALLATION AND USE\n\nDownload the files in this repository and place them in a directory that is in the MATLAB search path. Or add the directory where they are located to the search path (https://www.mathworks.com/help/matlab/matlab_env/add-remove-or-reorder-folders-on-the-search-path.html).\n\nTo perform CO2 system calculations, use CO2SYS.m as directed in the function's help text. All sub-routines that will be called upon are contained within the CO2SYS.m function.\n\nTo propagate uncertainties in CO2 system calculations, use errors.m as directed in the function's help text. The errors.m routine will call upon CO2SYS.m and derivnum.m. As such, this version of errors.m is compatible only with CO2SYSv3.\n\nTo compute partial derivatives of calculated CO2 system variables with respect to input parameters, use derivnum.m as directed in the function's help text. The derivnum.m routine will call upon CO2SYS.m. As such, this version of derivnum.m is compatible only with CO2SYSv3.\n\n## CITATION\n\nThe full citation for CO2SYSv3 (Sharp et al., 2020) is given below. Cite this version if using CO2SYSv3 for CO2 system calculations or propagating errors in CO2 system calculations using the extended errors.m or derivnum.m routines provided here.\n\nIf using any CO2SYS program for CO2 system calculations, cite also the original CO2SYS DOS work of Lewis and Wallace (1998).\n\nIf using the CO2SYS MATLAB program for CO2 system calculations, cite also the work of van Heuven et al. (2011).\n\nIf using the derivnum.m and/or errors.m programs for CO2 system error propagations, cite also the work of Orr et al. (2018).\n\n## REFERENCES\n\nBach, L. T. (2015). Reconsidering the role of carbonate ion concentration in calcification by marine organisms. Biogeosciences 12(16), 4939\u20134951.\n\nDillon, W. D. N., Dillingham, P. W., Currie, K. I., McGraw, C. M., 2020. Inclusion of uncertainty in the calcium-salinity relationship improves estimates of ocean acidification monitoring data quality. Marine Chemistry 226, 103872.\n\nHumphreys, M.P., Lewis, E.R., Sharp, J.D., Pierrot, D. PyCO2SYS: marine carbonate system calculations in Python. Submitted to Geoscientific Model Development.\n\nLewis, E., Wallace, D. W. R., 1998. Program Developed for CO2 System Calculations. ORNL/CDIAC-105. Carbon Dioxide Information Analysis Center, Oak Ridge National Laboratory, Oak Ridge, TN.\n\nMunhoven, G., Mathematics of the total alkalinity\u2013pH equation \u2013 pathway to robust and universal solution algorithms: the SolveSAPHE package v1.0.1. Geoscientific Model Development 6, 1367\u20131388, 2013\n\nOrr, J.C., Epitalon, J.-M., Dickson, A. G., Gattuso, J.-P., 2018. Routine uncertainty propagation for the marine carbon dioxide system. Marine Chemistry 207, 84-107.\n\nSharp, J.D., Pierrot, D., Humphreys, M.P., Epitalon, J.-M., Orr, J.C., Lewis, E.R., Wallace, D.W.R. (2021, May 20). CO2SYSv3 for MATLAB (Version v3.2.0). Zenodo. http://doi.org/10.5281/zenodo.3950562\n\nSulpis, O., Lauvset, S. K., and Hagens, M., 2020. Current estimates of K1* and K2* appear inconsistent with measured CO2 system parameters in cold oceanic regions. Ocean Science Discussions, 1-27.\n\nvan Heuven, S., Pierrot, D., Rae, J.W.B., Lewis, E., Wallace, D.W.R., 2011. MATLAB Program Developed for CO2 System Calculations. ORNL/CDIAC-105b. Carbon Dioxide Information Analysis Center, Oak Ridge National Laboratory, Oak Ridge, TN.\n"
 },
 {
  "repo": "miniufo/xgrads",
  "language": "Jupyter Notebook",
  "readme_contents": "# xgrads\n\n[![DOI](https://zenodo.org/badge/244529165.svg)](https://zenodo.org/badge/latestdoi/244529165)\n![GitHub](https://img.shields.io/github/license/miniufo/xgrads)\n[![Documentation Status](https://readthedocs.org/projects/xgrads/badge/?version=latest)](https://xgrads.readthedocs.io/en/latest/?badge=latest)\n\n\n![3D plot](https://raw.githubusercontent.com/miniufo/xgrads/master/pics/3D.png)\n\n\n## 1. Introduction\nThe Grid Analysis and Display System ([GrADS](http://cola.gmu.edu/grads/) or [OpenGrADS](http://www.opengrads.org/)) is a widely used software for easy access, manipulation, and visualization of earth science data.  It uses a [descriptor (or control) file with a suffix `.ctl`](http://cola.gmu.edu/grads/gadoc/descriptorfile.html) to  describe a raw binary 4D dataset.  The `ctl` file is similar to the header information of a [NetCDF](https://www.unidata.ucar.edu/software/netcdf/docs/file_structure_and_performance.html) file, containing all the information about dimensions, attributes, and variables except for the variable data.\n\nThis python package [`xgrads`](https://github.com/miniufo/xgrads) is designed for parse and read the `.ctl` file commonly used by [GrADS](http://cola.gmu.edu/grads/).  Right now it can parse various kinds of `.ctl` files.  However, only the commonly used raw binary 4D datasets can be read using [`dask`](https://dask.org/) and return as a [`xarray.Dataset`](http://xarray.pydata.org/en/stable/)  Other types of binary data, like `dtype` is `station` or`grib`, may be supported in the future.\n\n---\n## 2. How to install\n**Requirements**\n`xgrads` is developed under the environment with `xarray` (=version 0.15.0), `dask` (=version 2.11.0), `numpy` (=version 1.15.4), `cartopy` (=version 0.17.0), and `pyproj` (=version 1.9.6).  Older versions of these packages are not well tested.\n\n**Install via pip**\n```\npip install xgrads\n```\n\n**Install from github**\n```\ngit clone https://github.com/miniufo/xgrads.git\ncd xgrads\npython setup.py install\n```\n\n\n---\n## 3. Examples\n### 3.1 Parse a `.ctl` file\nParsing a `.ctl` file is pretty simple using the following code:\n```python\nfrom xgrads import CtlDescriptor\n\nctl = CtlDescriptor(file='test.ctl')\n\n# print all the info in ctl file\nprint(ctl)\n```\n\nIf you have already load the ASCII content in the `.ctl` file, you can do it as:\n```python\ncontent = \\\n    \"dset ^binary.dat\\n\" \\\n    \"* this is a comment line\\n\" \\\n    \"title 10-deg resolution model\\n\" \\\n    \"undef -9.99e8\\n\" \\\n    \"xdef 36 linear   0 10\\n\" \\\n    \"ydef 19 linear -90 10\\n\" \\\n    \"zdef  1 linear   0  1\\n\" \\\n    \"tdef  1 linear 00z01Jan2000 1dy\\n\" \\\n    \"vars  1\\n\" \\\n    \"test  1 99 test variable\\n\" \\\n    \"endvars\\n\"\n\nctl = CtlDescriptor(content=content)\n\n# print all the info\nprint(ctl)\n```\n---\n\n### 3.2 Read binary data into a `xarray.Dataset`\nReading a `.ctl` related binary data file is also pretty simple using the following code:\n```python\nfrom xgrads import open_CtlDataset\n\ndset = open_CtlDataset('test.ctl')\n\n# print all the info in ctl file\nprint(dset)\n```\n\nThen you have the `dset` as a `xarray.Dataset`.  This is similar to [`xarray.open_dataset`](http://xarray.pydata.org/en/stable/generated/xarray.open_dataset.html) that use [`dask`](https://dask.org/) to chunk (buffer) parts of the whole dataset in physical memory if the whole dataset is too large to fit in.\n\nIf there are many `.ctl` files in a folder, we can also open all of them in a single call of `open_mfdataset` as:\n```python\nfrom xgrads import open_mfDataset\n\ndset = open_mfDataset('./folder/*.ctl')\n\n# print all the info in ctl file\nprint(dset)\n```\nassuming that every `.ctl` file has similar data structure except the time step is different.  This is similar to [`xarray.open_mfdataset`](http://xarray.pydata.org/en/v0.12.3/generated/xarray.open_mfdataset.html).\n\n---\n\n### 3.3 Convert a GrADS dataset to a NetCDF dataset\nWith the above functionality, it is easy to convert a `.ctl` ([GrADS](http://cola.gmu.edu/grads/)) dataset to a `.nc` ([NetCDF](https://www.unidata.ucar.edu/software/netcdf/docs/file_structure_and_performance.html)) dataset:\n```python\nfrom xgrads import open_CtlDataset\n\nopen_CtlDataset('input.ctl').to_netcdf('output.nc')\n```\n"
 },
 {
  "repo": "UCSD-E4E/Smartfin",
  "language": "Jupyter Notebook",
  "readme_contents": "# Smartfin [Visit Here!](https://smartfin.org)\n  \nSee our work-in-progress: [HERE](https://github.com/hwanggit/Smartfin/wiki)\n\nThe Smartfin Project aims to unite the surfing community and the research community in an effort to fill this gap in our oceanographic data. It began years ago with the collaborative vision of Scientists and Researchers working with the Scripps Institute of Oceanography, who hoped to innovate a new way to model the behaviour of our oceans. \n\n### Click the Image below to play the video\n\n[![image](https://github.com/hwanggit/Smartfin/blob/master/Images/Screen%20Shot%202019-06-13%20at%203.17.25%20PM.png)](https://youtu.be/wi0JMVOShCc)\n\n![text](https://github.com/hwanggit/Smartfin/blob/master/Images/longboard-smartfin-logo_web1920x1335.jpg)\n\n![text](https://github.com/hwanggit/Smartfin/blob/master/Images/Screen%20Shot%202019-05-08%20at%202.01.41%20PM.png)\n\n"
 },
 {
  "repo": "OceanOptics/MISCToolbox",
  "language": "MATLAB",
  "readme_contents": "MISC Lab Toolbox\n================\n\n**Matlab tools for oceanographic analysis focusing on the inherent optical properties (IOPs) of the open ocean.**\n\nFeatures\n--------\nThe functions available are:\n* compute_bbp.m: Compute the particulate backscattering b_bp from the backscattering bb\n* correct_npq.m: Correct for non photochemical quenching using Xing et al. (2012) and/or Sackmann et al. (2008)\n* etimate_mld.m: Estimate the mixed layer depth (MLD) with one of the following method: fixed temperature threshold, fixed density threshold, variable density threshold or fixed density gradient.\n* meshprofile.m: Interpolate data between profile (often used with scatter3m)\n* need_npqc.m: Determine if need a non photochemical quenching correction\n* scatter3m.m: 4D visualization with earth map (latitude, longitude, depth and measure)\n\nRequirements\n------------\nTo work properly the toolbox need those features in matlab path (addpath):\n* betasw_ZHH2009.m from Xiaodong Zhang available [here](https://github.com/ooici/ion-functions/blob/master/ion_functions/data/matlab_scripts/flort/betasw_ZHH2009.m)\n* gsw_matlab_v3_04 from TEOS-10 available [here](https://github.com/TEOS-10/GSW-Matlab/releases)\n* lr2.m a robust linear regression type II, can substitute it by regress() instead\n"
 },
 {
  "repo": "VIAME/VIAME",
  "language": "Python",
  "readme_contents": "\n<img src=\"http://www.viametoolkit.org/wp-content/uploads/2016/08/viami_logo.png\" alt=\"VIAME Logo\" width=\"200\" height=\"78\">\n\nVIAME is a computer vision application designed for do-it-yourself artificial intelligence including\nobject detection, object tracking, image/video annotation, image/video search, image mosaicing,\nsize measurement, rapid model generation, and tools for the evaluation of different algorithms.\nBoth a desktop and web version exist for deployments in different types of environments, with\nan open annotation archive and example of the web platform available at\n[viame.kitware.com](https://viame.kitware.com). Originally targetting marine species analytics,\nVIAME now contains many common algorithms and libraries, and is also useful as a generic computer\nvision toolkit. It contains a number of standalone tools for accomplishing the above, a pipeline\nframework which can connect C/C++, python, and matlab nodes together in a multi-threaded fashion,\nand, lastly, multiple algorithms resting on top of the pipeline infrastructure.\n\n\nDocumentation\n-------------\n\nThe [User's Quick-Start Guide](https://data.kitware.com/api/v1/item/5fdaf1dd2fa25629b99843f8/download),\n[Tutorial Videos](https://www.youtube.com/channel/UCpfxPoR5cNyQFLmqlrxyKJw), \nand [Developer's Manual](http://viame.readthedocs.io/en/latest/) are more comprehensive,\nbut select entries are also listed below broken down by individual functionality:\n\n\n[Documentation Overview](https://viame.readthedocs.io/en/latest/section_links/documentation_overview.html) <>\n[Install or Build Instructions](examples/building_and_installing_viame) <>\n[All Examples](https://github.com/Kitware/VIAME/tree/master/examples) <>\n[DIVE Interface](https://kitware.github.io/dive) <>\n[VIEW Interface](examples/annotation_and_visualization) <>\n[Search and Rapid Model Generation](examples/search_and_rapid_model_generation) <>\n[Object Detector CLI](examples/object_detection) <>\n[Object Tracker CLI](examples/object_tracking) <>\n[Detector Training CLI](examples/object_detector_training) <>\n[Evaluation of Detectors](examples/scoring_and_roc_generation) <>\n[Detection File Formats](https://viame.readthedocs.io/en/latest/section_links/detection_file_conversions.html) <>\n[Calibration and Image Enhancement](examples/image_enhancement) <>\n[Registration and Mosaicing](examples/image_registration)  <>\n[Stereo Measurement and Depth Maps](examples/measurement_using_stereo) <>\n[Pipelining Overview](https://github.com/Kitware/kwiver) <>\n[Core Class and Pipeline Info](http://kwiver.readthedocs.io/en/latest/architecture.html) <>\n[Plugin Integration](examples/hello_world_pipeline) <>\n[Example Plugin Templates](plugins/templates) <>\n[Embedding Algorithms in C++](examples/using_detectors_in_cxx_code)\n\nInstallations\n-------------\n\nFor a full installation guide and description of the various flavors of VIAME, see the\nquick-start guide, above. The full desktop version is provided as either a .msi, .zip or\n.tar file. Alternatively, standalone annotators (without any processing algorithms)\nare available via smaller installers. Lastly, docker files are available for both VIAME\nDesktop and Web (below). For full desktop installs, extract the binaries and place them\nin a directory of your choosing, for example /opt/noaa/viame on Linux\nor C:\\Program Files\\VIAME on Windows. If using packages built with GPU support, make sure\nto have sufficient video drivers installed, version 465.19 or higher. The best way to\ninstall drivers depends on your operating system. This isn't required if just using\nmanual annotators (or frame classifiers only). The binaries are quite large,\nin terms of disk space, due to the inclusion of multiple default model files and\nprograms, but if just building your desired features from source (e.g. for embedded\napps) they are much smaller.\n\n**Installation Requirements:** <br>\n* Up to 8 Gb of Disk Space for the Full Installation <br>\n* Windows 7\\*, 8, 10, or 11 (64-Bit) or Linux (64-Bit, e.g. RHEL, CentOS, Ubuntu) <br>\n  * Windows 7 requires some updates and service packs installed, e.g. [KB2533623](https://www.microsoft.com/en-us/download/details.aspx?id=26764). <br>\n  * MacOS is currently only supported running standalone annotation tools, see below.\n\n**Installation Recommendations:** <br>\n* NVIDIA Drivers (Version 465.19 or above, \nWindows \n[\\[1\\]](https://www.nvidia.com/Download/index.aspx?lang=en-us)\n[\\[2\\]](https://developer.nvidia.com/cuda-downloads)\nUbuntu \n[\\[1\\]](https://linuxhint.com/ubuntu_nvidia_ppa/)\n[\\[2\\]](https://developer.nvidia.com/cuda-downloads)\nCentOS \n[\\[1\\]](https://developer.nvidia.com/cuda-downloads)\n[\\[2\\]](https://www.nvidia.com/Download/index.aspx?lang=en-us)) <br>\n* A [CUDA-enabled GPU](https://developer.nvidia.com/cuda-gpus) with 8 Gb or more VRAM <br>\n\n**Windows Full Desktop Binaries:** <br>\n* VIAME v0.19.4 Windows, GPU Enabled, Wizard (.msi) (Coming Soon...) <br>\n* [VIAME v0.19.4 Windows, GPU Enabled, Mirror1 (.zip)](https://drive.google.com/file/d/1XaKWnaL3K-j98Yvk3diUYEcOC3mOJglj/view?usp=sharing) <br>\n* [VIAME v0.19.4 Windows, GPU Enabled, Mirror2 (.zip)](https://data.kitware.com/api/v1/item/62d580febddec9d0c45bf62f/download) <br>\n* [VIAME v0.19.4 Windows, CPU Only, Mirror1 (.zip)](https://drive.google.com/file/d/106BCdISlTn7FOtd2H38wJ43B5pgAclKZ/view?usp=sharing) <br>\n* [VIAME v0.19.4 Windows, CPU Only, Mirror2 (.zip)](https://data.kitware.com/api/v1/item/62d58096bddec9d0c45bf506/download)\n\n**Linux Full Desktop Binaries:** <br>\n* [VIAME v0.19.4 Linux, GPU Enabled, Mirror1 (.tar.gz)](https://drive.google.com/file/d/1UWxH-m8VPxYMKYuTMMs_ICFwryM92Bhe/view?usp=sharing) <br>\n* [VIAME v0.19.4 Linux, GPU Enabled, Mirror2 (.tar.gz)](https://data.kitware.com/api/v1/item/62d6d727bddec9d0c471a305/download) <br>\n* [VIAME v0.19.4 Linux, CPU Only, Mirror1 (.tar.gz)](https://drive.google.com/file/d/1Ar5nKs9I1lmGSQ2Zd4xxMWu02cThreLD/view?usp=sharing) <br>\n* [VIAME v0.19.4 Linux, CPU Only, Mirror2 (.tar.gz)](https://data.kitware.com/api/v1/item/62d6d5f1bddec9d0c47147fb/download)\n\n**Web Applications**: <br>\n* [VIAME Online Web Annotator and Public Annotation Archive](https://viame.kitware.com/) <br>\n* [VIAME Web Local Installation Instructions](https://kitware.github.io/dive/Deployment-Overview/) <br>\n* [VIAME Web Source Repository](https://github.com/Kitware/dive)\n\n**DIVE Standalone Desktop Annotator:** <br>\n* [DIVE Installers (Linux, Mac, Windows)](https://github.com/Kitware/dive/releases/tag/1.8.0-beta.3)\n\n**SEAL Standalone Desktop Annotator:** <br>\n* [SEAL Windows 7/8/10, GPU Enabled (.zip)](https://data.kitware.com/api/v1/item/602296172fa25629b95482f6/download) <br>\n* [SEAL Windows 7/8/10, CPU Only (.zip)](https://data.kitware.com/api/v1/item/602295642fa25629b9548196/download) <br>\n* [SEAL CentOS 7, GPU Enabled (.tar.gz)](https://data.kitware.com/api/v1/item/6023362a2fa25629b957c365/download) <br>\n* [SEAL Generic Linux, GPU Enabled (.tar.gz)](https://data.kitware.com/api/v1/item/6023359c2fa25629b957c2f3/download)\n\n**Optional Add-Ons and Model Files:** <br>\n* [Arctic Seals Models, Windows](https://data.kitware.com/api/v1/item/5e30b8ffaf2e2eed3545bff6/download) <br>\n* [Arctic Seals Models, Linux](https://data.kitware.com/api/v1/item/5e30b283af2e2eed3545a888/download) <br>\n* [EM Tuna Detectors, All OS](https://viame.kitware.com/api/v1/item/627b326cc4da86e2cd3abb5b/download) <br>\n* [HabCam Models (Scallop, Skate, Flatfish), All OS](https://viame.kitware.com/api/v1/item/627b145487bad2e19a4c4697/download) <br>\n* [Motion Detector Model, All OS](https://viame.kitware.com/api/v1/item/627b326fea630db5587b577b/download) <br>\n* [MOUSS Deep 7 Bottomfish Models, All OS](https://viame.kitware.com/api/v1/item/627b3282c4da86e2cd3abb5d/download) <br>\n* [Penguin Head FF Models, All OS](https://viame.kitware.com/api/v1/item/627b3289ea630db5587b577d/download) <br>\n* [Sea Lion Models, All OS](https://viame.kitware.com/api/v1/item/62d5da95fd05facb6e178bdd/download) <br>\n* [SEFSC 100-200 Class Fish Models, All OS](https://viame.kitware.com/api/v1/item/627b32b1994809b024f207a7/download)\n\nNote: To install Add-Ons and Patches, copy them into an existing VIAME installation folder.\nFolders should match, for example, the Add-On packages contains a 'configs' folder, and the\nmain installation also contains a 'configs' folder so they should just be merged.\n\n\nDocker Images\n-------------\n\nDocker images are available on: https://hub.docker.com. For a default container with just core\nalgorithms, runnable via command-line, see:\n\nkitware/viame:gpu-algorithms-latest\n\nThis image is headless (ie, it contains no GUI) and contains a VIAME desktop (not web)\ninstallation in the folder /opt/noaa/viame. For links to the VIAME-Web docker containers see the\nabove section in the installation documentation. Most add-on models are not included in the\ninstance but can be downloaded via running the script download_viame_addons.sh in the bin folder.\n\nQuick Build Instructions\n------------------------\n\nThese instructions are intended for developers or those interested in building the latest master\nbranch. More in-depth build instructions can be found [here](examples/building_and_installing_viame),\nbut the software can be built either as a super-build, which builds most of its dependencies\nalongside itself, or standalone. To build VIAME requires, at a minimum, [Git](https://git-scm.com/),\n[CMake](https://cmake.org/), and a [C++ compiler](http://www.cplusplus.com/doc/tutorial/introduction/).\nInstalling Python and CUDA is also recommended. If using CUDA, versions 11.5, 11.3, or 10.2 are\npreferred, with CUDNN 8. Other CUDA or CUDNN versions may or may not work. On both Windows and Linux\nit is also recommended to use [Anaconda3 2021.05](https://repo.anaconda.com/archive/) for python,\nwhich is the most tested distribution used by developers. If using other python distributions,\nat a minimum Python3.7 or above, Numpy, and Cython is necessary.\n\nTo build on the command line in Linux, use the following commands, only replacing [source-directory]\nand [build-directory] with locations of your choice. While these directories can be the same,\nit's good practice to have a 'src' checkout then a seperate 'build' directory alongside it:\n\n\tgit clone https://github.com/VIAME/VIAME.git [source-directory]\n\n\tcd [source-directory] && git submodule update --init --recursive\n\nNext, create a build directory and run the following `cmake` command (or alternatively\nuse the cmake GUI if you are not using the command line interface):\n\n\tmkdir [build-directory] && cd [build-directory]\n\n\tcmake -DCMAKE_BUILD_TYPE:STRING=Release [source-directory]\n\nOnce your `cmake` command has completed, you can configure any build flags you want\nusing 'ccmake' or the cmake GUI, and then build with the following command on Linux:\n\n\tmake -j8\n\nOr alternatively by building it in Visual Studio or your compiler of choice on\nWindows. On Linux, '-j8' tells the build to run multi-threaded using 8 threads, this\nis useful for a faster build though if you get an error it can be difficult to see\nit, in which case running just 'make' might be more helpful. For Windows,\ncurrently VS2019 is the most tested compiler.\n\nThere are several optional arguments to viame which control which plugins get built,\nsuch as those listed below. If a plugin is enabled that depends on another dependency\nsuch as OpenCV) then the dependency flag will be forced to on. If uncertain what to turn\non, it's best to just leave the default enable and disable flags which will build most\n(though not all) functionalities. These are core components we recommend leaving turned on:\n\n\n<center>\n\n| Flag                         | Description                                                                    |\n|------------------------------|--------------------------------------------------------------------------------|\n| VIAME_ENABLE_OPENCV          | Builds OpenCV and basic OpenCV processes (video readers, simple GUIs)          |\n| VIAME_ENABLE_VXL             | Builds VXL and basic VXL processes (video readers, image filters)              |\n| VIAME_ENABLE_PYTHON          | Turns on support for using python processes (multiple algorithms)              |\n| VIAME_ENABLE_PYTORCH         | Installs all pytorch processes (detectors, trackers, classifiers)              |\n\n</center>\n\n\nAnd a number of flags which control which system utilities and optimizations are built, e.g.:\n\n\n<center>\n\n| Flag                         | Description                                                                    |\n|------------------------------|--------------------------------------------------------------------------------|\n| VIAME_ENABLE_CUDA            | Enables CUDA (GPU) optimizations across all packages                           |\n| VIAME_ENABLE_CUDNN           | Enables CUDNN (GPU) optimizations across all processes                         |\n| VIAME_ENABLE_DIVE            | Enables DIVE GUI (annotation and training on multiple sequences)               |\n| VIAME_ENABLE_VIVIA           | Builds VIVIA GUIs (VIEW and SEARCH for annotation and video search)            |\n| VIAME_ENABLE_KWANT           | Builds KWANT detection and track evaluation (scoring) tools                    |\n| VIAME_ENABLE_DOCS            | Builds Doxygen class-level documentation (puts in install tree)                |\n| VIAME_BUILD_DEPENDENCIES     | Build VIAME as a super-build, building all dependencies (default)              |\n| VIAME_INSTALL_EXAMPLES       | Installs examples for the above modules into install/examples tree             |\n| VIAME_DOWNLOAD_MODELS        | Downloads pre-trained models for use with the examples and interfaces          |\n\n</center>\n\n\nAnd lastly, a number of flags which build algorithms or interfaces with more specialized functionality:\n\n\n<center>\n\n| Flag                         | Description                                                                    |\n|------------------------------|--------------------------------------------------------------------------------|\n| VIAME_ENABLE_TENSORFLOW      | Builds TensorFlow object detector plugin                                       |\n| VIAME_ENABLE_DARKNET         | Builds Darknet (YOLO) object detector plugin                                   |\n| VIAME_ENABLE_TENSORRT        | Builds TensorRT object detector plugin                                         |\n| VIAME_ENABLE_BURNOUT         | Builds Burn-Out based pixel classifier plugin                                  |\n| VIAME_ENABLE_SMQTK           | Builds SMQTK plugins to support image/video indexing and search                |\n| VIAME_ENABLE_SCALLOP_TK      | Builds Scallop-TK based object detector plugin                                 |\n| VIAME_ENABLE_SEAL            | Builds Seal multi-modality GUI                                                 |\n| VIAME_ENABLE_ITK             | Builds ITK cross-modality image registration                                   |\n| VIAME_ENABLE_UW_CLASSIFIER   | Builds UW fish classifier plugin                                               |\n| VIAME_ENABLE_MATLAB          | Turns on support for and installs all matlab processes                         |\n| VIAME_ENABLE_LANL            | Builds an additional (Matlab) scallop detector                                 |\n\n</center>\n\n\nSource Code Layout\n------------------\n<pre>\n VIAME\n   \u251c\u2500\u2500 cmake               # CMake configuration files for subpackages\n   \u251c\u2500\u2500 docs                # Documentation files and manual (pre-compilation)\n   \u251c\u2500\u2500 configs             # All system-runnable config files and models\n   \u2502   \u251c\u2500\u2500 pipelines       # All processing pipeline configs\n   \u2502   \u2502   \u2514\u2500\u2500 models      # All models, which only get downloaded based on flags\n   \u2502   \u251c\u2500\u2500 prj-linux       # Default linux project files\n   \u2502   \u2514\u2500\u2500 prj-windows     # Default windows project files \n   \u251c\u2500\u2500 examples            # All runnable examples and example tutorials\n   \u251c\u2500\u2500 packages            # External projects used by the system\n   \u2502   \u251c\u2500\u2500 kwiver          # Processing backend infastructure\n   \u2502   \u251c\u2500\u2500 fletch          # Dependency builder for things which don't change often\n   \u2502   \u251c\u2500\u2500 kwant           # Scoring and detector evaluation tools\n   \u2502   \u251c\u2500\u2500 vivia           # Baseline desktop GUIs (v1.0)\n   \u2502   \u2514\u2500\u2500 ...             # Assorted other packages (typically for algorithms)\n   \u251c\u2500\u2500 plugins             # Integrated algorithms or wrappers around external projects\n   \u2502   \u2514\u2500\u2500 ...             # Assorted plugins (detectors, depth maps, filters, etc.)\n   \u251c\u2500\u2500 tools               # Standalone tools or scripts, often building on the above\n   \u2514\u2500\u2500 README.md           # Project introduction page that you are reading\n   \u2514\u2500\u2500 RELEASE_NOTES.md    # A list of the latest updates in the system per version\n</pre>\n\n\nUpdate Instructions\n-------------------\n\nIf you already have a checkout of VIAME and want to switch branches or\nupdate your code, it is important to re-run:\n\n\tgit submodule update --init --recursive\n\nAfter switching branches to ensure that you have on the correct hashes\nof sub-packages within the build. Very rarely you may also need to run:\n\n\tgit submodule sync\n\nJust in case the address of submodules has changed. You only need to\nrun this command if you get a \"cannot fetch hash #hashid\" error.\n\n\nLicense, Citations, and Acknowledgements\n----------------------------------------\n\nVIAME is released under a BSD-3 license.\n\nA non-exhaustive list of relevant papers used within the project alongside contributors\ncan be found [here](docs/citations.md).\n\nVIAME was developed with funding from multiple sources, with special thanks\nto those listed [here](docs/acknowledgements.md).\n"
 },
 {
  "repo": "juoceano/lecture_figures",
  "language": "Jupyter Notebook",
  "readme_contents": "# Notebook gallery\n\nFigures, and the notebooks used to create them, for lectures in oceanography.\n\n\n| Platform       | Status                                                                                                                                             |\n| -------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------|\n| Linux and OS X | [![Build Status](https://travis-ci.org/juoceano/lecture_figures.svg?branch=master)](https://travis-ci.org/juoceano/lecture_figures)                    |\n| Windows        | [![Build status](https://ci.appveyor.com/api/projects/status/o66cvyp766w5bd10?svg=true)](https://ci.appveyor.com/project/juoceano/lecture-figures) |\n\n\nSee the rendered version at https://juoceano.github.io/lecture_figures\n\nTo suggest a notebook or ask questions please open an issue at: https://github.com/juoceano/lecture_figures/issues\n\n## License\n\n<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-sa/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\">Creative Commons Attribution-ShareAlike 4.0 International License</a>.\n\n## Citation\n\n[![DOI](https://zenodo.org/badge/84759695.svg)](https://zenodo.org/badge/latestdoi/84759695)\n"
 },
 {
  "repo": "biofloat/biofloat",
  "language": "Jupyter Notebook",
  "readme_contents": "biofloat\n--------\n\n[![Build Status](https://travis-ci.org/biofloat/biofloat.svg?branch=master)](https://travis-ci.org/biofloat/biofloat)\n[![Coverage Status](https://coveralls.io/repos/biofloat/biofloat/badge.svg?branch=master&service=github)](https://coveralls.io/github/biofloat/biofloat?branch=master)\n[![Code Health](https://landscape.io/github/biofloat/biofloat/master/landscape.svg?style=flat)](https://landscape.io/github/biofloat/biofloat/master)\n[![PyPI version](https://badge.fury.io/py/biofloat.svg)](https://badge.fury.io/py/biofloat)\n[![DOI](https://zenodo.org/badge/21375/biofloat/biofloat.svg)](https://zenodo.org/badge/latestdoi/21375/biofloat/biofloat)\n\nPython module biofloat is designed to simplify using \n[Bio-Argo data](https://en.wikipedia.org/wiki/Argo_(oceanography)) \nin a Python/Jupyter Notebook programming environment.\n\n#### Installation\n\nFrom Anaconda, Canopy, or Unix prompt:\n\n    pip install biofloat\n\nFor plotting data on a map [basemap](http://matplotlib.org/basemap/users/installing.html) needs to be installed.\n\n#### Usage\n\nSee example [Jupyter Notebooks](notebooks) and [scripts](scripts) that demonstrate specific analyses and \nvisualizations.\n\n"
 },
 {
  "repo": "PyHOGS/pyhogs-code",
  "language": "OpenEdge ABL",
  "readme_contents": "# pyhogs-code\n\nRepository of data, code, and notebooks for the UW Python Hour for Oceanography and GeoSciences\n"
 },
 {
  "repo": "jerabaul29/OpenMetBuoy-v2021a",
  "language": "C++",
  "readme_contents": "**A question? => open an issue; An idea of improvement? => open an issue and / or submit a pull request.**\n\n# OpenMetBuoy-v2021a (OMB / OMB-v21a): an easy to build, affordable, customizable, open source instrument for oceanographic measurements.\n\n**Our pledge**: many fields within Geosciences are relying on in-situ data collection. Traditionally, collecting in-situ data is expensive, with commercial instruments easily reaching several thousands of USDs even if only quite simple functionalities are provided. However, the recent emergence of open source communities within micro controllers and low level firmware for these opens new possibilities for developing low-cost, high performance instruments based on widely available and affordable components. This repository is focusing on developing an oceanic buoy based on such open source code and low cost components. Our solution is around 10 times cheaper than the least expensive similar commercial instrument we know of, and is actually superior to the commercial instrument in a number of regards, for example, for measurement in the polar regions. We hope that the present open source release, beyond sharing the design of our specific instrument, will encourage many more actors to share their developments within in-situ instrumentation and participate in building an open source community around geophysical instrumentation.\n\nLink to our paper, if you find our work useful and build up on it, please consider citing it:\n\n```\nRabault, Jean, et al.\n\"OpenMetBuoy-V2021: an easy-to-build, affordable, customizable, open source instrument for\n  oceanographic measurements of drift and waves in sea ice and the open ocean.\"\nGeosciences (2022).\n```\n\n- As a preprint: https://www.researchgate.net/publication/357712696_OpenMetBuoy-v2021_an_easy-to-build_affordable_customizable_open_source_instrument_for_oceanographic_measurements_of_drift_and_waves_in_sea_ice_and_the_open_ocean , and as pdf on this repository (see **latest_preprint_MDPI_2022.pdf**),\n- As a published paper (open access, CC-BY license): https://www.mdpi.com/2076-3263/12/3/110 .\n\nThe fully assembled instrument gets space into a box 12x12x9 cm, including 3 D-size battery holders (at the bottom, empty on the picture, should use typically 3 SAFT LSH20 D-size batteries), all the components, and the 9dof sensor:\n\n<img src=\"https://github.com/jerabaul29/OpenMetBuoy-v2021a/blob/main/instrument_hardware/jpg/from_side.jpg\" width=\"600\" />\n\nThis is a typical example of dataset collected by an instrument that drifted in the Caribbeans: the track of the instrument and the wave measurements are indicated by the black curve, while satellite measurements of significant wave height (from the Sentinel satellites in particular) are shown on intersecting transsects with colored lines:\n\n![caribbeans_deployment](https://user-images.githubusercontent.com/8382834/155947239-19cf49fe-5649-4f91-b513-fb7ae9e2e0ad.jpg)\n\nUsing the standard box and components, and 3 Li LSH20 batteries, the box floats quite well and works just fine, and will behave quite similar to an iSphere surface drifter for example (note that you will have to add some floatability if you use alkaline batteries, that are much heavier):\n\n![box_no_extra_buoy](https://github.com/jerabaul29/OpenMetBuoy-v2021a/blob/main/instrument_hardware/jpg/floating_3LSH20_box.jpg)\n\nWe also describe how to build a standard, upper ocean drifter buoy, which allows to assemble a complete instrument+buoy setup looking like the following picture (to give a sense of scale, the PVC tube is about 1m long); this will drift more like the 1m depth current:\n\n<img src=\"https://github.com/jerabaul29/OpenMetBuoy-v2021a/blob/main/instrument_hardware/jpg/buoy_with_instrument.jpg\" width=\"600\" />\n\n## Key data\n\nThese are explained in more details in the paper, see in particular Tables 1, 2, 3, and the associated text, for more details.\n\n- total cost of the parts: 562USD (Nov. 2021 price).\n- typical assembly time: around 1-1.5hr per buoy when producing a small series efficiently.\n- cost of the iridium communications: 42USD / month if using only GPS, up to 108USD / month when using GPS and high frequency wave measurements (Nov. 2021 price).\n- battery autonomy: i) 4.6 months using 2 Li D-cells, with GPS and wave measurements activated. ii) Over 1 year using 2 Li D-cells and GPS tracking only. iii) Battery life scales linearly with the number of D-cells included in the instrument.\n- typical detection threshold for waves in ice: 0.5cm at 16s period, even better detection threshold is obtained at higher frequency due to using an IMU to perform the measurements.\n\n## Overview of the content and organisation of the repository\n\n- https://github.com/jerabaul29/OpenMetBuoy-v2021a/tree/main/development_environment : this contains the instructions on how to set up your development environment to write firmwares, either for the \"legacy\" (Arduino 1.8-based), or the \"rewrite\" (PlatformIO-based) firmware.\n- https://github.com/jerabaul29/OpenMetBuoy-v2021a/tree/main/instrument_hardware : this describes the hardware needed, and the parts to source, for building different versions of the instrument, and includes the assembly instructions.\n- https://github.com/jerabaul29/OpenMetBuoy-v2021a/tree/main/legacy_firmware : this contains the legacy firmware (version IDE 1.8), which at the moment is the only one ready to use. It provides both the source code (in different versions), and some pre-compiled binaries with default parameters.\n\n## Coding environment\n\nI am using only Linux in the form of Ubuntu LTS distributions. Though the instructions here can likely be adapted to other distributions / OSes (possibly with the Windows Linux Subsystem or similar, see for example https://github.com/jerabaul29/OpenMetBuoy-v2021a/issues/53 ), if you want to use something else than Ubuntu 20.04 LTS or another modern Ubuntu LTS, you are on your own! Also, some of the PlatformIO building flags rely on using a Linux system. These could be adapted to also work on Windows, but I am not interesed in spending the time to get it to work at the moment on neither Windows not MacOS (but contributions can be welcome).\n\n## About the \"legacy\" vs \"rewrite\" firmwares\n\n### \"Legacy\" instrument firmware\n\nThe coding environment for the \"legacy\" instrument was based on Arduino IDE-1.8 and the SparkFun Ambiq Apollo3 Arduino core, in a slightly older version of the \"bare metal\" core. It is well working, but a bit messy as it was developed \"as things went\". For now, this is the only one that is ready for use (see \"Instrument firmware under rework\" section).\n\n### Instrument firmware under rework\n\nThe \"legacy\" instrument firmware has been developed \"as things were going\". It works well and is robust, but the codebase needs a good rework to clean it. The \"reworked firmware\" is a work in progress, without any guarantees on when it will be done, and is available at: https://github.com/jerabaul29/OpenMetBuoy-v2021a/tree/feat/platformio-tracker and \"related\" branches. It is based on:\n\n- C++ with an Arduino flavor,\n- Visual Studio Code and PlatformIO,\n- the SparkFun Ambiq Apollo3 Arduino core, in \"bare metal\" (i.e. no RTOS), flavor.\n\nSee the corresponding branch for more instructions.\n\nIf you want to develop a \"cousin\" instrument of our own, I would recommend that you base yourself on this development environment.\n\n## A note about other programming languages\n\nC++ has some pros and cons, and it is possible to use other languages for low level programming. You can find a project that i) targets similar MCUs and development boards, ii) uses Rust-Lang as an alternative to C++, at: https://github.com/gauteh/sfy . If you want to develop a new project from scratch, it may be worth considering if you want to use our C++ or the Rust firmware as a starting point :) (NOTE: I see the pont of Rust and I want to transition to it in the - hopefully - not so far future, but for now I am still working in C++, the code there is from a colleague :) ).\n\n## Contribution policy\n\nIn case of any question / bug report, please use the Github issues tracker system. I will very likely **not** answer to private emails, as I want all discussions to be open, become part of the repo and, therefore, the \"meta documentation\" of the project, and serve the community.\n\nAll materials on this repo are released under the MIT license, unless explicitly stated otherwise. Nothing on this repository comes with any warranty. I do not guarantee any form of support.\n\n## Quirks / errors\n\nThis is a large code base, that I largely developed on my own with a very limited time budget, so there are some quirks / errors. I will fix the ones that lead to wrong results, but minor quirks may not be fixed fully. This is to list such quirks.\n\n- **dyslexia frequency vs. period**: the instrument, internally, computes the zero crossing and the mean *frequency*, not *period* as the code says (i.e. there is a relation by ```f(x)=1/x``` between what is named in the code and what is actually computed). This would be error prone to fix in all firmwares, so I choose to i) keep it \"as it is\" in the firmware, and ii) fix this a posteriori in the decoder by taking the inverse there. For more discussions, see: https://github.com/jerabaul29/OpenMetBuoy-v2021a/issues/36\n\n## A few deployments I know about\n\nIf you deploy the OMB, feel free to let me know in an issue! These are so far the deployments I know of (no guarantee this is up to date). This is just to show that we do have solid experience with the OMB, and that we have had many successful depoyments:\n\n- 2 test buoys: NABOS expedition, Arctic, September 2021: 1 in the open water in the MIZ, 1 on ice\n- 1 test drifter: OneOcean cruise, Caribbeans, November 2021: open water, freely drifting\n- 3 ice drifters: Japanese Antarctic program, Antarctica, February 2022: 3 instruments on the sea ice\n- 2 ice drifters: Seal pups monitoring cruise 2022, East Greenland sea, March 2022: on the ice in the MIZ, 2 drifters\n- 20 ice drifters: CIRFA 2022 cruise, East Greenland sea, April 2022: 20 instruments on the sea ice\n- 15 drifting buoys: CIRFA 2022 cruise, Barents sea, April 2022: 15 instruments, drifting in open water\n- 20 ice drifters: AWI summer cruise 2022, Barents sea: 20 instruments, on the sea ice\n- 15 ice trackers: Hovercraft 2022 cruise, Yamal plateau / East Greenland sea, August 2022: 15 instruments on the sea ice\n\n## Just for fun - a not so serious corner of the Readme :)\n\nIf you have more fun / goofy / strange pictures or stories that can fit in here, just share them as an issue or open a pull request!\n\n### Floatenstein, the ugly drifter\n\nA buoy that needed to fly by plane (so Li batteries not allowed) had to be equipped with 3 Alkaline D cells in series instead. Alkaline batteries are heavier than Li batteries, so it was not floating. The solution was to add two chunks of styrofoam, wrapped in duct tape, fixed with cable nilon strips, all of it tightened with bathroom silicon. That made for an ugly, sticky, stinky instrument, but meh, it crossed the Caribbeans without any issue, so don't worry about how your instruments look like - as long as they work! :) .\n\n<img src=\"https://github.com/jerabaul29/OpenMetBuoy-v2021a/blob/main/instrument_hardware/jpg/floatenstein_light.jpg\" width=\"600\" />\n\nPicture credits: me :) .\n\n### The Arctic is tough, even if you're expensive\n\nInstrumentation has a tendency to fail early and \"randomly\" in the Arctic - and harsh field conditions do not care about how much money you cost! This is an example where the telemetry from a Datawell Mk II deployed outside of Longyearbyen, Svalbard, was lost after just 2 hours. We were worried that the mooring had failed, or the buoy had sunk. In reality, due to the conditions (1 deg C water, -15 deg C air and some wind and waves), the Mk II had got iced and tilted. So next time your ocean buoy fails in the Arctic, don't take it too hard on yourself - it is likely not your fault, and it does not matter how much your instrument cost - stick to the OpenMetBuoy.\n\n<img src=\"https://github.com/jerabaul29/OpenMetBuoy-v2021a/blob/main/instrument_hardware/jpg/IMG_1099(1).JPG\" width=\"400\" />\n\nPicture credits Atle Jensen.\n\n### There is nothing as tasty as a free lunch\n\nIf you work in the Arctic, don't use bright colors, blinking LEDs, and chunky flags that make your instruments visible from far away - for a polar bear, there is nothing as tasty as a free lunch, even if it tastes plastic. According with some friends who are experts on Arctic expeditions, polar bears have a particular affinity for round instruments that they can play football with, large instruments they can use as a trampoline, and soft plastic / rubber that they can use to sharpen their teeths. Oh, and cables too! Record to break according to my friend: 250kUSD destruction of scientific instrumentation within 12 minutes...\n\n<img src=\"https://github.com/jerabaul29/OpenMetBuoy-v2021a/blob/main/instrument_hardware/jpg/687px-Polar_bears_chewing_cables.jpg\" width=\"600\" />\n\nPicture credits: this is actually from wikimedia commons, I did not have a good picture lying around: https://commons.wikimedia.org/wiki/File:Polar_bears_chewing_cables.jpg .\n\n### Trashing planet Earth, one step at a time - I feel really bad about this one\n\nMost (all?) of the floating trash we throw in the oceans, ultimately comes back on the shores - or get caught in the Vortex. Also floating scientific instruments. I feel really bad about this, but at least our instruments are much smaller than typical commercial instruments (so, for a given number of measurements, we release less volume and mass of trash, so less trash footprint), and, at least, this is for a good cause (advancing human knowledge and protecting the world oceans), and not a random sacrifice to the pagan god of over-consumption and fast fashion... Still, if you have ideas on how to reduce the environmental footprint of the OMB, open an issue and help us get better :) .\n\nThis picture is actually from one of my \"old, v2018 instruments\", that drifted all the way from the Arctic MIZ North of Svalbard, until it got stranded on the Northen coast of Iceland. Someone taking a walk on the beach was kind enough to open it, read the waterproof notice with our contact information, and let us know it was found. Pretty rusted!\n\n<img src=\"https://github.com/jerabaul29/OpenMetBuoy-v2021a/blob/main/instrument_hardware/jpg/electronics_trash.jpg\" width=\"600\" />\n\nCredits: Picture Finn Plny, sharing Atle Jensen; we are going to pick up the box, and try to get the raw data from the SD card.\n\n### Go big or go home\n\nThe aim of the OpenMetBuoy (OMB) is to increase the number of measurement points you can get at constant budget, NOT to reduce your overall field instrumentation budget! So, go big or go home! This is what you get for the cost of 4 or 5 classical commercial instruments: enough components to build 40 OMBs!\n\n<img src=\"https://github.com/jerabaul29/OpenMetBuoy-v2021a/blob/main/instrument_hardware/jpg/go_big_go_home.jpg\" width=\"600\" />\n\nCredits: Takehiko Nose, preparing to assemble 50 OMBs.\n\n### \"Dolly\" the search-and-rescue doll\n\nThe OMB has also been used to test search and rescue systems - hard time floating alone in the middle of the Norwegian sea! Credits: Forsvarets 330 Skv. Avd Sola.\n\n<img src=\"https://github.com/jerabaul29/OpenMetBuoy-v2021a/blob/main/instrument_hardware/jpg/search_and_rescue_doll\" width=\"600\" />\n"
 },
 {
  "repo": "profxj/oceanpy",
  "language": "Python",
  "readme_contents": "# oceanpy\nOceanography tools\n"
 },
 {
  "repo": "isgiddy/ocean-pytools",
  "language": "Jupyter Notebook",
  "readme_contents": "## Python Tools for Oceanography  \n\nAn introduction to oceanographic data analysis in python\n\nAcknowledgements: Much of the content is adapted from and inspired by the work of Ryan Abernathy and J.M. Lilly  \nThe development of this course was funded by SEAmester"
 },
 {
  "repo": "seaflow-uw/popcycle",
  "language": "R",
  "readme_contents": "Popcycle\n========\n**Popcycle** is an R package that offers a reproducible approach to process, calibrate and curate flow cytometry data collected by SeaFlow.\n\n<img src=\"documentation/images/seaflow-workflow.png?raw=true\" alt=\"Popcycle workflow\"\n\ttitle=\"Popcycle workflow\" align=\"right\" style=\"float\" width=\"500\">\nRaw  data are stored every 3 minutes in a custom binary file format (RAW data) consisting of six to eight 16-bit integer channels, along with [metadata](https://github.com/seaflow-uw/seaflow-sfl) provided by the ship's data system (e.g., time, location, sea surface temperature, salinity, light intensity). The files are stored in day-of-year-labeled directories, each containing raw files with the associated log file.\n\nThe software package performs 4 key analyses:\n1. ```Filtering```: Filter raw particle data down to optimally posistioned particles (OPP).\n2. ```Gating```: Classification of phytoplankton cell populations using a mixture of manual gating and a semi-supervized clusterting algorithm.\n3. ```Light scatter conversion```: Convert light scattering of each particle to cell diameter ([fsc-size-calibration](https://github.com/seaflow-uw/fsc-size-calibration)) and carbon content ([fsc-poc-calibration](https://github.com/seaflow-uw/fsc-poc-calibration)).\n4. ```Population data```: Perform aggregate statistics along with error propagation for the different populations.\n\nFiltered OPP data are stored in hourly [Parquet](https://github.com/apache/parquet-format) files.\nGated data (cell population identification) and calibrated data (diameter and carbon content) for each hourly OPP file are saved in separate hourly VCT Parquet files. The metadata, filtering and gating parameters, and aggregated statistics for each step are saved to a SQLite3 database files.\n\n### Analysis\nTo get started with the analysis, go to the [wiki](https://github.com/seaflow-uw/popcycle/wiki/SeaFlow-data-analysis-tutorial)\n"
 },
 {
  "repo": "DFO-Ocean-Navigator/Ocean-Data-Map-Project",
  "language": "Python",
  "readme_contents": "# Ocean Navigator\n\n[![CodeFactor](https://www.codefactor.io/repository/github/dfo-ocean-navigator/ocean-data-map-project/badge)](https://www.codefactor.io/repository/github/dfo-ocean-navigator/ocean-data-map-project)\n[![Lint Python](https://github.com/DFO-Ocean-Navigator/Ocean-Data-Map-Project/actions/workflows/lint_python.yml/badge.svg)](https://github.com/DFO-Ocean-Navigator/Ocean-Data-Map-Project/actions/workflows/lint_python.yml)\n[![Python tests](https://github.com/DFO-Ocean-Navigator/Ocean-Data-Map-Project/actions/workflows/python-tests.yml/badge.svg)](https://github.com/DFO-Ocean-Navigator/Ocean-Data-Map-Project/actions/workflows/python-tests.yml)\n\n## Contents\n* Overview\n* Development\n* Automate CLASS4 pickle generation\n\n---\n\n## Overview\n\nOcean Navigator is a Data Visualization tool that enables users to discover and view 3D ocean model output quickly and easily.\n\nThe model outputs are stored as [NetCDF4](https://en.wikipedia.org/wiki/NetCDF) files. Our file management is now handled by an SQLite3 process that incrementally scans the files for a dataset, and updates a corresponding table so that the Python layer can only open the exact files required to perform computations; as opposed to the THREDDS aggregation approach which serves all the files in a dataset as a single netcdf file. The THREDDS approach was unable to scale to the sheer size of the datasets we deal with.\n\nThe server-side component of the Ocean Navigator is written in Python 3, using the Flask web API. Conceptually, it is broken down into three components:\n\n-\tQuery Server\n\n\tThis portion returns metadata about the selected dataset in JSON format. These queries include things like the list of variables in the dataset, the times covered, the list of depths for that dataset, etc.\n\n\tThe other queries include things such as predefined areas (NAFO divisions, EBSAs, etc), and ocean drifter paths. The drifter paths are loaded from NetCDF files, but all the other queries are loaded from KML files.\n\n-\tPlotting\n\n\tThis portion generates an image plot, which could be a map with surface fields (or fields at a particular depth), a transect through a defined part of the ocean, depth profiles of one or more points, etc. We use the matplotlib python module to generate the plots.\n\n\tBecause the model grid rarely lines up with the map projection, and profiles and transects don't necessarily fall on model grid points, we employ some regridding and interpolation to generate these plots. For example, for a map plot, we select all the model points that fall within the area, plus some extra around the edges and regrid to a 500x500 grid that is evenly spaced over the projection area. An added benefit of this regridding is that we can directly compare across models with different grids. This allows us to calculate anomalies on the fly by comparing the model to a climatology. In theory, this would also allow for computing derived outputs from variables in different datasets with different native grids.\n\n-\tTile Server\n\n\tThis portion is really a special case of the plotting component. The tile server serves 256x256 pixel tiles at different resolutions and projections that can be used by the OpenLayers web mapping API. This portion doesn't use matplotlib, as the tiles don't have axis labels, titles, legends, etc. The same style of interpolation/regridding is done to generate the data for the images.\n\n\tThe generated tiles are cached to disk after they are generated the first time, this allows the user request to bypass accessing the NetCDF files entirely on subsequent requests.\n\nThe user interface is written in Javascript using the React framework. This allows for a single-page, responsive application that offloads as much processing from the server onto the user's browser as possible. For example, if the user chooses to load points from a CSV file, the file is parsed in the browser and only necessary parts of the result are sent back to the server for plotting.\n\nThe main display uses the OpenLayers mapping API to allow the user to pan around the globe to find the area of interest. It also allows the user to pick an individual point to get more information about, draw a transect on the map, or draw a polygon to extract a map or statistics for an area.\n\n---\n\n## Development\n\n### Local Installation\nThe instructions for performing a local installation of the Ocean Data Map Project are available at:\n[https://github.com/DFO-Ocean-Navigator/Navigator-Installer/blob/master/README.md](https://github.com/DFO-Ocean-Navigator/Navigator-Installer/blob/master/README.md)\n\n* While altering Javascript code, it can be actively transpiled using:\n\t* `cd oceannavigator/frontend`\n\t* `yarn run dev`\n* There's also a linter available: `yarn run lint`.\n* For production use the command: \n\t* `rm -r oceannavigator/frontend`\n\t* `cd oceannavigator/frontend`\n\t* `yarn run build`\n\n### SQLite3 backend\nSince we're now using a home-grown indexing solution, as such there is now no \"server\" to host the files through a URL (at the moment). You also need to install the dependencies for the [netcdf indexing tool](https://github.com/DFO-Ocean-Navigator/netcdf-timestamp-mapper). Then, download a released binary for Linux systems [here](https://github.com/DFO-Ocean-Navigator/netcdf-timestamp-mapper/releases). You should go through the README for basic setup and usage details.\n\nThe workflow to import new datasets into the Navigator has also changed:\n1. Run the indexing tool linked above.\n2. Modify `datasetconfig.json` so that the `url` attribute points to the absolute path of the generated `.sqlite3` database.\n3. Restart web server.\n\n### Running the webserver for development\nAssuming the above installation script succeeded, your PATH should be set to point towards `${HOME}/miniconda/3/amd64/bin`, and the `navigator` conda environment has been activated.\n* Debug server (single-threaded):\n\t* `python ./bin/runserver.py`\n* Multi-threaded (via gUnicorn):\n\t* `./bin/runserver.sh`\n\n### Running the webserver for production\nUsing the launch-web-service.sh script will automatically determine how many processors are available, determine the platform's IP address, what port above 5000 can be used, print out the IP and port information. The IP:PORT information can then be copied to a web browser to access the Ocean Navigator web service either locally or shared with others. This script will also copy all information bring written to stdout and place the information in the ${HOME}/launch-on-web-service.log file.\n* Multi-threaded (via gUnicorn):\n        * `./bin/launch-web-service.sh`\n\n### Coding Style (Javascript)\nJavascript is a dynamically-typed language so it's super important to have clear and concise code, that demonstrates it's exact purpose.\n\n* Comment any code whose intention may not be self-evident (safer to have more comments than none at all).\n* Use `var`, `let`, and `const` when identifying variables appropriately:\n\t* `var`: scoped to the nearest function block. Modern ES6/Javascript doesn't really use this anymore because it usually leads to scoping conflicts. However, `var` allows re-declaration of a variable.\n\t* `let`: new keyword introduced to ES6 standard which is scoped to the *nearest block*. It's very useful when using `for()` loops (and similar), so don't predefine loop variable:\n\n\t\t* Bad:\n\t\t\t```\n\t\t\t\tmyfunc() {\n\t\t\t\t\tvar i;\n\t\t\t\t\t...\n\t\t\t\t\t// Some code\n\t\t\t\t\t...\n\t\t\t\t\tfor (i = 0; i < something; ++i) {\n\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t```\n\t\t* Good:\n\t\t\t```\n\t\t\t\tmyFunc() {\n\t\t\t\t\t...\n\t\t\t\t\t// Some code\n\t\t\t\t\t...\n\t\t\t\t\tfor (let i = 0; i < something; ++i) {\n\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t```\n\t\t\n\t\tKeep in mind that `let` *does not* allow re-declaration of a variable.\n\n\t* `const`: functionally identical to the `let` keyword, however disallows variable re-assignment. Just like const-correctness in C++, `const` is a great candidate for most variable declarations, as it immediately states that \"I am not being changed\". This leads to the next rule.\n* Use `const` when declaring l-values with `require()`. Example:\n\t```\n\t\tconst LOADING_IMAGE = require(\"../images/bar_loader.gif\");\n\t```\n* Unless using `for` loops, *DO NOT* use single-letter variables! It's an extreme nuisance for other programmers to understand the intention of the code if functions are littered with variables like: `s`, `t`, etc. Slightly more verbose code that is extremely clear will result in a much lower risk for bugs.\n\t* Bad:\n\t\t```\n\t\t\t$.when(var_promise, time_promise).done(function(v, t) {\n\t\t\t\t// Some code\n\t\t\t\t...\n\t\t\t}\n\t\t```\n\t* Good:\n\t\t```\n\t\t\t$.when(var_promise, time_promise).done(function(variable, time) {\n\t\t\t\t// Some code\n\t\t\t\t...\n\t\t\t}\n\n\t\t```\n* Try to avoid massive `if` chains. Obviously the most important thing is to get a feature/bugfix working. However if it results in a whole bunch of nested `if` statements, or `if`-`for`-`if`-`else`, etc., try to take that working result and incorporate perhaps a `switch`, or hashtable to make your solution cleaner, and more performant. If it's unavoidable, a well-placed comment would reduce the likelihood of a fellow developer trying to optimize it.\n\n### Coding Style (Python)\nComing soon...\n\n## Automate CLASS4 pickle generation\n\nIn order to generate the class4.pickle file daily. You should create a crontab entry for the user account hosting the Ocean Navigator instance. Use the command `crontab -e` to add the string `@daily ${HOME}/Ocean-Data-Map-Project/bin/launch-pickle.sh`. Then once a day at midnight the script launch-pickle.sh will index all the CLASS4 files.\n\n## Proper handling of the datasetconfig.json and oceannavigator.cfg configuration files\n\nIn order to provide a production ready and off-site configuration files. We have implemented a new configurations repository. When people clone the Ocean-Data-Map-Project repository they will need to perform an additional step of updating any defined submodules. The following command changes your working directory to your local Ocean-Data-Map-Project directory and then updates the submodules recursively.\n\n* cd ${HOME}/Ocean-Data-Map-Project ; git submodule update --init --recursive\n"
 },
 {
  "repo": "regeirk/klib",
  "language": "Python",
  "readme_contents": "kLib\n====\n\nA collection of Python functions for applications in oceanography and science \nin general. This module references to the numpy, scipy, pylab and probably \nother Python packages too.\n\nFunctions are grouped in different modules such as statistics, file management, \ngraphics, mapping, interpolation and common functions.\n\nThis library is still in it's infancy and under development. Eventually everything\nwill become depreciated and moved to the `atlantis` module.\n\n\nMODULES\n-------\n\n- common\n- dynamics\n- file (filemngmnt)\n- geostrophy\n- graphics\n- gis\n- interpolate\n- signal\n- stats \n\n\nDISCLAIMER\n----------\n\nThis software may be used, copied, or redistributed as long as it\nis not sold and this copyright notice is reproduced on each copy\nmade. This routine is provided as is without any express or implied\nwarranties whatsoever.\n\n\nINSTALLATION\n------------\n\nCopy all the contents into a location included in the Python search\npath. On Linux distribution one such option is\n\n> `~/.local/lib/python2.x/site-packages/klib`\n\n\nAUTHORS\n-------\n\nSebastian Krieger (sebastian at nublia.com)\n\n\nREVISION\n--------\n\n    8 (2017-02-20 18_05 -0300)\n    7 (2016-06-28 21:59 -0300)\n    6 (2016-02-05 00:00 -0300)\n    5 (2016-05-19 00:00 -0300)\n    4 (2014-02-24 17:26 -0300)\n    3 (2013-03-19 14:52 -0300)\n    2 (2013-01-08 14:09 -0300)\n    1 (2012-02-24 20:37 -0300)\n\n"
 },
 {
  "repo": "gher-ulg/DINCAE",
  "language": "Python",
  "readme_contents": "[![documentation latest](https://img.shields.io/badge/docs-latest-blue.svg)](https://gher-ulg.github.io/DINCAE/)\n[![DOI](https://zenodo.org/badge/193079989.svg)](https://zenodo.org/badge/latestdoi/193079989)\n[![Build Status](https://travis-ci.org/gher-ulg/DINCAE.svg?branch=master)](https://travis-ci.org/gher-ulg/DINCAE)\n[![codecov.io](http://codecov.io/github/gher-ulg/DINCAE/coverage.svg?branch=master)](http://codecov.io/github/gher-ulg/DINCAE?branch=master)\n\n# DINCAE\n\n\nDINCAE (Data-Interpolating Convolutional Auto-Encoder) is a neural network to\nreconstruct missing data in satellite observations which is described in the following open access paper:\nhttps://doi.org/10.5194/gmd-13-1609-2020\n\n\n*Note that this code is no longer maintained and has been superseeded by https://github.com/gher-ulg/DINCAE.jl*\n\n\n\n## Installation\n\nPython 3.6 or 3.7 with the modules:\n* numpy (https://docs.scipy.org/doc/numpy/user/install.html)\n* netCDF4 (https://unidata.github.io/netcdf4-python/netCDF4/index.html)\n* TensorFlow 1.15 with GPU support (https://www.tensorflow.org/install)\n\nTested versions:\n\n* Python 3.6.8\n* netcdf4 1.4.2\n* numpy 1.15.4\n* Tensorflow version 1.15 (DINCAE does not work with TensforFlow 2.0; TensorFlow 1.5 does not work on python 3.8)\n\nYou can install those packages either with `pip3` or with `conda`.\n\n\n## Documentation\n\nThe document is available at https://gher-ulg.github.io/DINCAE/.\n\n## Input format\n\nThe input data should be in netCDF with the variables:\n* `lon`: longitude (degrees East)\n* `lat`: latitude (degrees North)\n* `time`: time (days since 1900-01-01 00:00:00)\n* `mask`: boolean mask where true means the data location is valid\n* `SST` (or any other varbiable name): the data\n\n\nThis is the example output from `ncdump -h`:\n\n```\nnetcdf avhrr_sub_add_clouds {\ndimensions:\n\ttime = UNLIMITED ; // (5266 currently)\n\tlat = 112 ;\n\tlon = 112 ;\nvariables:\n\tdouble lon(lon) ;\n\tdouble lat(lat) ;\n\tdouble time(time) ;\n\t\ttime:units = \"days since 1900-01-01 00:00:00\" ;\n\tint mask(lat, lon) ;\n\tfloat SST(time, lat, lon) ;\n\t\tSST:_FillValue = -9999.f ;\n}\n```\n\nAn example for how to create this file in the examples folder:\n* [python example](https://github.com/gher-ulg/DINCAE/blob/master/examples/create\\_input\\_file.py)\n* [julia example](https://github.com/gher-ulg/DINCAE/blob/master/examples/create\\_input\\_file.jl)\n\n\n## Running DINCAE\n\nCopy the template file `run_DINCAE.py` and adapt the filename, variable name and the output directory and possibly optional arguments for the reconstruction method as mentioned in the [documentation](https://gher-ulg.github.io/DINCAE/).\nThe code can be run as follows:\n\n```bash\npython3 run_DINCAE.py\n```\n## Reducing GPU memory\n\nConvolutional neural networks can require \"a lot\" of GPU memory. These parameters can affect GPU memory utilisation:\n\n* reduce the mini-batch size\n* use fewer layers (e.g. `enc_nfilter_internal` = [16,24,36] or [16,24])\n* use less filters (reduce the values of the optional parameter enc_nfilter_internal)\n* reduce `frac_dense_layer`, a parameter controlling the width of the dense layer in the bottleneck\n* use a smaller domain or lower resolution\n\n\n## Example results\n\n[Link to animation](http://data-assimilation.net/upload/Alex/DINCAE/data-avg-DINCAE-AVHRR.gif)\n\n\nMore information about this result is given in the [linked paper](https://www.geosci-model-dev-discuss.net/gmd-2019-128/).\n"
 },
 {
  "repo": "juoceano/introductiontooceanography",
  "language": "TeX",
  "readme_contents": "<!--pandoc -V geometry:margin=1in --from markdown_github README.md -o README.pdf \n--latex-engine=xelatex -V geometry:margin=1in --smart --normalize --standalone --webtex -->\n\n\n# Plano de Aula #\n# GEO232 Introdu\u00e7\u00e3o \u00e0 Oceanografia - IGEO-UFBA #\n\nDisciplina: GEO232 Introdu\u00e7\u00e3o \u00e0 Oceanografia - Semestre 2015.1\n\nProfessora: Juliana Leonel \n\nE-mail: jleonel@ufba.br\n\nDia/Hor\u00e1rio das aulas: Ter\u00e7as e Quintas-feiras 10:40-12:30 Sala 103B\n\nAtendimento: Sextas-feiras 13:00 - 14:00 - IGEO, 2 andar, sala 10\n\nHomepage: http://juoceano.github.io/introductiontooceanography\n\n## 1. Ementa:\nOceanografia: Defini\u00e7\u00e3o e Conceitos. Hist\u00f3ria da oceanografia e import\u00e2ncia dos oceanos. Origem dos oceanos. Introdu\u00e7\u00e3o \u00e0 oceanografia geol\u00f3gica. Introdu\u00e7\u00e3o \u00e0 oceanografia biol\u00f3gica. Introdu\u00e7\u00e3o \u00e0 oceanografia qu\u00edmica. Introdu\u00e7\u00e3o \u00e0 oceanografia f\u00edsica. Processos oce\u00e2nicos globais. Processos costeiros. Oceanografia como profiss\u00e3o. O mercado de trabalho para ocean\u00f3grafos.\n\n## 2. Objetivos:\n\nNo final dessa disciplina os alunos:\n\na) ter\u00e3o desenvolvido um entendimento inicial de como os oceanos funcionam;\n\nb) saber\u00e3o como usar o vocabul\u00e1rio apropriado para descrever e se referir aos processos oceanogr\u00e1ficos;\n\nc) ter\u00e3o conhecimento cient\u00edfico e pensamento cr\u00edtico para entender como as quest\u00f5es ligadas \u00e0 oceanografia afetam o nosso dia-a-dia.\n\n## 3. Metodologia das Aulas: \n\na) aulas expositivas; \n\nb) discuss\u00f5es em sala de aula; \n\nc) exerc\u00edcios pr\u00e1ticos; \n\nd) solu\u00e7\u00f5es de problemas.\n\nEu encorajo voc\u00eas perguntarem durante as aulas, especialmente se n\u00e3o antederem o assunto que est\u00e1 em discuss\u00e3o. Os principais t\u00f3picos ser\u00e3o apresentados/discutidas em sala de aula. As leituras requeridas ajudar\u00e3o no entendimento das aulas al\u00e9m de trazerem informa\u00e7\u00f5es e discuss\u00f5es complementares. Portanto, leia o material antes das aulas!!!\n\nOs slides de aulas estar\u00e3o dispon\u00edveis na homepage da disciplina. Como eles s\u00e3o apenas um guia para as aulas eu recomendo que voc\u00eas tamb\u00e9m fa\u00e7am suas pr\u00f3prias anota\u00e7\u00f5es. \n\n## 4. Avalia\u00e7\u00f5es\n\na) exerc\u00edcios (25%);\n\nb) semin\u00e1rio (10%);\n\nc) 2 provas (40%);\n\nd) projeto desenvolvido ao longo do semestre (25%);\n\nPontos extras\n\na) carta para um pol\u00edtico local (at\u00e9 3 pontos - 1 por aluno): escreva uma carta para um pol\u00edtico local (prefeito, vereador, governador, etc) falando sobre como a sua gest\u00e3o pode auxiliar com algum problema ligado a oceanografia. Alguns exemplos: pesca, lixo nas praias, mudan\u00e7as clim\u00e1ticas, preserva\u00e7\u00e3o dos manguezais, impacto do turismo. Fa\u00e7a uma pesquisa para obter mais informa\u00e7\u00f5es. A carta deve ter aproximadamente 1 p\u00e1gina e deve ser entregue at\u00e9 uma semana antes da \u00faltima prova e nelas devem constar o endere\u00e7o de envio. Eu me encarregarei de enviar as melhores cartas. \n\nb) breve apresenta\u00e7\u00e3o (at\u00e9 3 pontos - 1 por aluno): fa\u00e7a uma apresenta\u00e7\u00e3o sobre informa\u00e7\u00e3o oceanogr\u00e1fica n\u00e3o tradicional. Algumas ideias: a) um slideshow onde voc\u00ea descreve a oceanografia de um lugar que voc\u00ea morou ou visitou; b) um som (cantado ao vivo) com versos descrevendo um processo/ambiente/problema oceanogr\u00e1fico; c) uma maquete representando uma fei\u00e7\u00e3o e/ou processo oceanogr\u00e1fico, etc. Use a criatividade! Eu devo aprovar as ideias antes delas serem executadas. As apresenta\u00e7\u00f5es ocorreram no final das aulas e devem sempre ser agendadas com anteced\u00eancia de uma semana e s\u00f3 ocorreram uma apresenta\u00e7\u00e3o por semana. \n\nA nota m\u00e1xima que o aluno poder\u00e1 atingir no final do semestre \u00e9 100.\n\n## 5. Conduta \n\n### Assiduidade: \nSer\u00e1 cobrada presen\u00e7a em sala de aula durante a aula pr\u00e1tica atrav\u00e9s da chamada ou assinatura de lista de presen\u00e7a. Alunos que estiverem ausentes n\u00e3o poder\u00e3o entregar os relat\u00f3rios e, se houver alguma atividade avaliada no dia, receber\u00e3o zero na atividade. O limite de faltas em hora aula (Te\u00f3rica, Pr\u00e1tica) \u00e9 17.\n\n### Atividades: \nCuidado com c\u00f3pias (pl\u00e1gio)! Trabalhos que forem c\u00f3pias (integral ou parcial) do trabalho de colegas ou de outras fontes sem referencia desta (livros, artigos, material da internet) ser\u00e3o desconsiderados na hora da corre\u00e7\u00e3o recebendo nota zero.\n\n## 6. Bibliografia recomendada:\n\nGarrison, T. (2006). Esssentials of Oceanography. 5 edi\u00e7\u00e3o. Canad\u00e1: Brooks/Cole, Cengage Learning, 429p.\n\nRinet, P. R (2009). Invitation to Oceanography. 5 edi\u00e7\u00e3o. EUA: Jones and Barlett Publishers, 626p.\n\nTeixeira, W.; Toledo, M.C.M.de; Fairchild, T. R.; Taioli, F. (2003). Decifrando a Terra. 1 edi\u00e7\u00e3o. S\u00e3o Paulo: Oficina de Textos, 597p.\n\nTrujillo, A.P. ; Thurman, H.V. (2005). Essential of Oceanography. 10 edi\u00e7\u00e3o. EUA:Pearson Education, 551p.\n\n\n## 7. Cronograma\n\n| Aula | Data   | Conte\u00fado                             |Leitura | Atividade Avaliada   |\n|:----:|:-------|:-------------------------------------|:-------|---------------------:|\n| 01   |03 mar\u00e7o|Apresenta\u00e7\u00e3o da Disciplina            |[Texto 1](https://github.com/juoceano/introductiontooceanography/raw/master/README.pdf)|  -     |\n| 02   |05 mar\u00e7o|[Pr\u00e1tica 1: Fisiografia](https://github.com/juoceano/introductiontooceanography/raw/master/practices/Pratica1_Fisiografia.pdf)                  | -      | [Reda\u00e7\u00e3o](https://github.com/juoceano/introductiontooceanography/raw/master/evaluations/Redacao.pdf)|\n| 03   |10 mar\u00e7o|[Forma\u00e7\u00e3o/Estrutura da Terra](https://github.com/juoceano/introductiontooceanography/raw/master/classes/Aula03_20151(CC).pdf)|[Texto 2](https://github.com/juoceano/introductiontooceanography/raw/master/chapters/Texto2.pdf)  |[Reportagem](https://github.com/juoceano/introductiontooceanography/raw/master/evaluations/Reportagem.pdf)|\n| 04   |12 mar\u00e7o|Pr\u00e1tica2: Tect\u00f4nica de Placas         | -      | -                    |\n| 05   |17 mar\u00e7o|[Bacias oce\u00e2nicas](https://github.com/juoceano/introductiontooceanography/raw/master/classes/Aula05_20151(CC).pdf)|[Texto 3](https://github.com/juoceano/introductiontooceanography/raw/master/chapters/Texto3.pdf) | -      |\n| 06   |19 mar\u00e7o|[Pr\u00e1tica 3: Sedimentos Marinhos](https://github.com/juoceano/introductiontooceanography/raw/master/practices/Pratica3_Sedimentos.pdf)| -      |[Tema/Dupla Semin\u00e1rio](https://github.com/juoceano/introductiontooceanography/raw/master/evaluations/TemaDuplaSeminario.pdf)  |\n| 07   |24 mar\u00e7o|Discuss\u00e3o Projeto                     | -      | -      |\n| 08   |26 mar\u00e7o|[Sedimentos marinhos](https://github.com/juoceano/introductiontooceanography/raw/master/classes/Aula08_20151(CC).pdf)|[Texto 4](https://github.com/juoceano/introductiontooceanography/raw/master/chapters/Texto4.pdf)|  |\n| 09   |31 mar\u00e7o|APRESENTAC\u00c3O PROJETO                  |        |         |\n| 10   |02 abril|FERIADO                               | -      |         |\n| 11   |07 abril|[Composi\u00e7\u00e3o da \u00e1gua do mar](https://github.com/juoceano/introductiontooceanography/raw/master/classes/Aula11_20151(CC).pdf)|[Texto 5](https://github.com/juoceano/introductiontooceanography/raw/master/chapters/Texto5.pdf)|[Projeto](https://github.com/juoceano/introductiontooceanography/raw/master/evaluations/Projeto.pdf) - [Ex 1 - Sedimentos](https://github.com/juoceano/introductiontooceanography/raw/master/practices/Pratica3_Sedimentos.pdf)|\n| 12   |09 abril|[Pr\u00e1tica 4: temperatura e salinidade](https://github.com/juoceano/introductiontooceanography/raw/master/practices/Pratica4_SalinityTemperature.pdf)|-       |[Ex 2 - Salinidade](https://github.com/juoceano/introductiontooceanography/raw/master/practices/Exercicio2_Salinidade.pdf) |\n| 13   |14 abril|[Temperatura, salinidade e densidade](https://github.com/juoceano/introductiontooceanography/raw/master/classes/Aula13_20151(CC).pdf)|[Texto 5](https://github.com/juoceano/introductiontooceanography/raw/master/chapters/Texto5.pdf)|        |\n| 14   |16 abril|Pr\u00e1tica 5: circula\u00e7\u00e3o superficial      |-       |                      |\n| 15   |21 abril|FERIADO                                | -      |                      |\n| 16   |23 abril|ANDAMENTO PROJETO                      | -      |[Ex 3 - T-S](https://github.com/juoceano/introductiontooceanography/raw/master/practices/Exercicios3_DiagramaTS.pdf)|\n| 17   |28 abril|PROVA I                                | -      |[Revis\u00e3o](https://github.com/juoceano/introductiontooceanography/raw/master/practices/Revisao1.pdf)|\n| 18   |30 abril|[Circula\u00e7\u00e3o superficial](https://github.com/juoceano/introductiontooceanography/raw/master/classes/Aula18_20151(CC).pdf)|[Texto 6](https://github.com/juoceano/introductiontooceanography/raw/master/chapters/Texto6.pdf)|                      |\n| 19   |05 maio |[Pr\u00e1tica 6: circula\u00e7\u00e3o termoalina](https://github.com/juoceano/introductiontooceanography/raw/master/practices/Pratica6_CirculacaoTermoalina.pdf)|-       |                      |\n| 20   |07 maio |[Circula\u00e7\u00e3o termoalina](https://github.com/juoceano/introductiontooceanography/raw/master/classes/IntroOceano_Aula20(CC).pdf)|-       |[Ex 4 - Cir. Termoalina](https://github.com/juoceano/introductiontooceanography/raw/master/practices/Pratica6_CirculacaoTermoalina.pdf)                      |\n| 21   |12 maio |[Pr\u00e1tica 7: ondas e mar\u00e9s](https://github.com/juoceano/introductiontooceanography/raw/master/practices/Pratica7_OndasMares.pdf)|-       |                      |\n| 22   |14 maio |[Ondas e mar\u00e9s](https://github.com/juoceano/introductiontooceanography/raw/master/classes/IntroOceano_Aula22(CC).pdf)|[Texto 7](https://github.com/juoceano/introductiontooceanography/raw/master/chapters/Texto7.pdf) |                      |\n| 23   |19 maio |[Pr\u00e1tica 8: vida no ambiente marinho](https://github.com/juoceano/introductiontooceanography/raw/master/practices/Pratica8_VidaAmbienteMarinho.pdf)|-       |                      |\n| 24   |21 maio |ANDAMENTO PROJETO                      |-       |                      |\n| 25   |26 maio |[Vida no ambiente marinho](https://github.com/juoceano/introductiontooceanography/raw/master/classes/Aula25_20151(CC).pdf)|[Texto 8](https://github.com/juoceano/introductiontooceanography/raw/master/chapters/Texto8.pdf) |                      |\n| 26   |28 maio |Pr\u00e1tica 9: Produtividade Marinha       |-       |                      |\n| 27   |02 junho|[Produtividade prim\u00e1ria](https://github.com/juoceano/introductiontooceanography/raw/master/classes/Aula27_20151(CC).pdf)|[Texto 9](https://github.com/juoceano/introductiontooceanography/raw/master/chapters/Texto9.pdf) |     |\n| 28   |04 junho|FERIADO                                | -      |                      |\n| 29   |20 outubro|Malha Tr\u00f3fica e Transfer\u00eancia de Energia|[Texto 9](https://github.com/juoceano/introductiontooceanography/raw/master/chapters/Texto9.pdf)|    |\n| 30   |22 outubro|Habitats Costeiros                   |[Texto 10](https://github.com/juoceano/introductiontooceanography/raw/master/chapters/Texto10.pdf)|      |\n| 31   |27 outubro|Exerc\u00edcios de Malha tr\u00f3fica          |-       | |\n| 32   |29 outubro |Finaliza\u00e7\u00e3o Projeto                 | -      | |\n| 33   |03 novembro|Finaliza\u00e7\u00e3o Projeto                 | -      | |\n| 34   |05 novembro|Finaliza\u00e7\u00e3o Projeto                 | -      | |\n| 35   |10 novembro |APRESENTA\u00c7\u00c3O PROJETO(FINAL)        | Ex.Malha Tr\u00f3fica [Relat\u00f3rio Final](https://github.com/juoceano/introductiontooceanography/raw/master/evaluations/RelatorioFinal.pdf)|Apresenta\u00e7\u00e3o e Discuss\u00e3o |\n| 36   |12 novembro |APRESENTA\u00c7\u00c3O: SEMIN\u00c1RIOS           | -      |[Semin\u00e1rio](https://github.com/juoceano/introductiontooceanography/raw/master/evaluations/Seminarios.pdf) |\n| 37   |17 novembro|APRESENTA\u00c7\u00c3O: SEMIN\u00c1RIOS (cont.)    | -      |[Semin\u00e1rio](https://github.com/juoceano/introductiontooceanography/raw/master/evaluations/Seminarios.pdf) |\n| 38   |19 novembro|PROVA II                            |-       |PROVA II | \n| 39   |24 novembro|Encerramento da disciplina          | -      | |\n "
 },
 {
  "repo": "juoceano/ChemOceanographyGuide",
  "language": "TeX",
  "readme_contents": "ChemOceanographyGuide\n=====================\n\nSea water, sediment and suspended particulate matter analysis.\n"
 },
 {
  "repo": "ocefpaf/descriptive_oceanography",
  "language": "TeX",
  "readme_contents": "# Curso de Oceanografia F\u00edsica Descritiva lecionado na unimonte no primeiro semestre de 2014.\n\n## Reposit\u00f3rios com aulas, slides, notas e exerc\u00edcios para Oceanografia F\u00edsica Din\u00e2mica\n~ Filipe Fernandes ~\n\nPara compilar as aulas use o script `make_lecture.py`\n\n```python\nMake lecture slides, handout and homework.\n\nUsage:\n    make_lecture (DIR) [--compile=DOC]\n    make_lecture (-h | --help | --version)\n\nExamples:\n    make_lecture Aula_01 --compile=slides\n    make_lecture Aula_01 --compile=handout\n    make_lecture Aula_01 --compile=homework\n    make_lecture Aula_01 --compile=all\n\nArguments:\n  DIR      Lecture directory.\n\nOptions:\n  -c --compile=DOC   slides, handout, homework or all [default: slides]\n  --version   Show version.\n  ---help     Show this screen.\n```\n\n### 1. Plano de Aula\nDisciplina: Oceanografia Descritiva (Curso: Oceanografia / Ciclo/mod. 1/1B)\n\nProfessor: Filipe Fernandes\n\nTurno: Matutino\n\nSemestre: 2\u00ba -- 2013\n\nTurma: OCE1BM-VMA\n\n#### Ementa\nIntrodu\u00e7\u00e3o \u00e0 Oceanografia F\u00edsica. Conceitos, estrutura e caracter\u00edsticas gerais\ndos oceanos.\n\n#### Bibliografia b\u00e1sica\n- GARRISON, Tom; MIYAJI, C\u00edntia (Trad.). Fundamentos de oceanografia. S\u00e3o Paulo: Cengage Learning, 2009. 426 p. ISBN 9788522106776.\n- MIRANDA, L. B., CASTRO, B. M., KJERFVE, B. 2002. Princ\u00edpios de Oceanografia F\u00edsica de Estu\u00e1rios. 1\u00aa. ed. S\u00e3o Paulo : Edusp, 414p.\n- SVERDRUP, K. A., Duxbury A. B., Duxbury, A. C. 2006. Fundamentals of Oceanography. McGraw Hill, 5th edition. N\u00famero de Chamada: 551.46 S968f 2006.\n\n\n#### Bibliografia complementar\n- STEWART, R. H. 2002. Introduction to Physical Oceanography. Department of Oceanography. Texas A. M. University.\n- SOUZA, Maria Cristina de Arruda. A Corrente do Brasil ao largo de Santos: medi\u00e7\u00f5es diretas. 2000. 169p. Disserta\u00e7\u00e3o (mestrado). Instituto Oceanogr\u00e1fico. USP. Dispon\u00edvel em: http://www.teses.usp.br/teses/disponiveis/21/21132/tde-10092003-094250/pt-br.php\n- The Open University. 1989. Ocean Circulation, Pergamon Press, 2nd edition.\n- TIPLER, Paul Allen.  F\u00edsica para cientistas e engenheiros.  4. ed. Rio de Janeiro:  LTC,  2006. v.1.\n- TOMCZAK, Matthias.  Physical oceanography.   Australia  The Flinders University of South Australia,  2002. 1 CD-ROM:  son., color  Dispon\u00edvel em: http://www.es.flinders.edu.au/~mattom/regoc/pdfversion.html.\n\n### Cronograma\n| Aula | Data          | Conte\u00fado                              | Lista/Prova         |\n|:----:|:--------------|:--------------------------------------| -------------------:|\n| 01   | 2013-08-09    | Introdu\u00e7\u00e3o \u00e0 Oceanografia F\u00edsica 1    |                     |\n| 02   | 2013-08-16    | Introdu\u00e7\u00e3o \u00e0 Oceanografia F\u00edsica 2    | Lista 1             |\n| 03   | 2013-08-23    | Dimens\u00f5es e formas dos oceanos 1      | Lista 2             |\n| 04   | 2013-08-30    | Dimens\u00f5es e formas dos oceanos 2      | Lista 3             |\n| 05   | 2013-09-06    | Propriedades f\u00edsicas da \u00e1gua do mar 1 | Lista 4             |\n| 06   | 2013-09-13    | Propriedades f\u00edsicas da \u00e1gua do mar 2 | Lista 5             |\n| 07   | 2013-09-20    | Distribui\u00e7\u00e3o espacial e temporais 1   | Lista 6             |\n| 08   | 2013-09-27    | Distribui\u00e7\u00e3o espacial e temporais 2   | Lista 7             |\n|      | 2013-10-04    |                                       | Prova 1             |\n| 11   | 2013-10-11    | Leis de conserva\u00e7\u00e3o 1                 | Revis\u00e3o Prova 1     |\n| 12   | 2013-10-18    | Leis de conserva\u00e7\u00e3o 2                 | Lista 8             |\n| 13   | 2013-10-25    | Circula\u00e7\u00e3o e massas d'\u00e1gua 1          | Lista 9             |\n| 14   | 2013-11-01    | Circula\u00e7\u00e3o e massas d'\u00e1gua 2          | Lista 10            |\n| 15   | 2013-11-08    | Oc. costeira e estuarina 1            | An\u00e1lise de dados    |\n|      | 2013-11-15    |                                       | Feriado             |\n| 16   | 2013-11-22    | Oc. costeira e estuarina 2            | Lista 11            |\n|      | 2013-11-29    |                                       | Semin\u00e1rio           |\n|      | 2013-12-06    |                                       | Prova 2             |\n|      | 2013-12-13    |                                       | Revis\u00e3o             |\n|      | 2013-12-20    |                                       | Prova Alternativa   |\n"
 },
 {
  "repo": "wylecho/Python_for_Oceanography",
  "language": "Jupyter Notebook",
  "readme_contents": "# Lectures on Python for Oceanography\nThe lectures materials (including slides in pdf format, IPython notebooks) on Python for Oceanography.\n\nIt consists of four lectures. The first two lectures cover introduction for the Python 3 language and the rest two lectures cover common used packages for Scientific Computing and Oceanography respectively.\n\n# License\n## Code\nAll the code in this repository is licensed under the [MIT license](LICENSE-CODE).\n\n## Text\nThe text content is licensed under the [Creative Commons Attribution 4.0 International License.](https://creativecommons.org/licenses/by/4.0/)\n"
 },
 {
  "repo": "MPOcanes/MPO624-2018",
  "language": "Jupyter Notebook",
  "readme_contents": "# MPO 624 CLASS FOR SPRING 2018\n\n## Welcome!\nThe course is all here. Fork it so you can contribute back! \n\n### Software accounts and installs: \n\n1.  https://github.com/MPOcanes/MPO624-2018/blob/master/INSTALL_JUPYTER_UNIDATA.md\n1. https://github.com/MPOcanes/MPO624-2018/blob/master/GITHUB_QUICKSTART.md\n1. Install the IDV\n   * Install the IDV nightly build from Undiata. https://www.unidata.ucar.edu/downloads/idv/nightly/index.jsp You will need to create a light sign-in, just so they know who\u2019s using it. Great free software, my group (and Xingchen in class) are involved with integrating it into Jupyter notebooks. \n1. Matlab \n   * Matlab is free to you while you are at UM. You will need to create an account with them, verify license tokens, etc.  It is very powerful, and very well documented with lots of hand-holding features, but it is commercial and proprietary. I will not use my course to create a generation of addicts to it, but you might end up using it -- lots of scientists do, and a big body of working, legacy code is nothing to sneer at. You can install it from here: http ://it.miami.edu/a-z-listing/matlab/index.html\n\n\n-------\n#### Random links\n\nBeginner's intro to github: https://github.com/Github-Classroom-Cybros/Learn-Git-Github\n\nHow to use repos in teaching https://classroom.github.com/videos\n\nList of emojis you can use in Markdown: https://gist.github.com/rxaviers/7360908\n\nSpecial characters to cut and paste https://en.wikipedia.org/wiki/List_of_XML_and_HTML_character_entity_references\n\n"
 },
 {
  "repo": "iuryt/ocean_gyre_tank",
  "language": "Jupyter Notebook",
  "readme_contents": "# Ocean Gyre in a Tank (Numerical Experiment)\n\nThis is the MITgcm simulation for the Ocean General Circulation in a rotating tank.\n\nBased on \"Insights of the non-linear solution of Munk\u2019s ocean circulation theory from a rotating tank experiment\" - *Ocean and Coastal Research*, 2020. ([10.1590/2675-2824069.20-011psp](https://doi.org/10.1590/2675-2824069.20-011psp))\n\n<img src=\"https://github.com/iuryt/ocean_gyre_tank/blob/master/notebooks/psi_model.png\" data-canonical-src=\"https://github.com/iuryt/ocean_gyre_tank/blob/master/notebooks/psi_model.png\" width=\"400\" height=\"auto\" />\nA: Linear solution. B: Nonlinear solution.\n\n## How to set up the experiment\n\n1. Follow the [Getting Started](https://mitgcm.readthedocs.io/en/latest/getting_started/getting_started.html) section on MITgcm documentation to set up the model.\n2. Clone this experiment to the MITgcm folder (you can also download the repository and extract it to MITgcm folder.)\n3. Create the `build` and `run` folders inside `ocean_gyre_tank`. \n4. Go to `build` and compile the model with `mpi` (see the [MITgcm documentation](https://mitgcm.readthedocs.io/en/latest/)).\n5. Copy the executable `mitgcmuv` to the `run` folder.\n6. Create a symbolic link to the files in `input` for `run` folder.\n\n## How to generate the initial conditions and forcing\n\nIn `notebooks` there is a file called `Init.ipynb` that creates the bathymetry and wind forcing.\nThe data will be saved to `input` folder. You may have to change the grid spacing in `input/data` or number of points in `code/SIZE.h` if you change the code on the notebooks.\n\n## How to configure the experiment\n\nThe file `data` in `input` folder has all the parameters needed for the experiment.\nYou can change to the linear case setting `.FALSE.` for `momAdvection`.\n\n## How to run the experiment\n\nThe current configuration on `code/SIZE.h` works in parallel using 4 cores (see [Documentation](https://mitgcm.readthedocs.io/en/latest/) to learn how to set up for a different number of cores).\nIf the experiment is already configured you just have to run `mpirun -np 4 ./mitgcmuv` in `run` folder.\n\n## How to read the data from the output\n\nThe notebook `notebooks/Analysis.ipynb` its a tutorial that explains how to read and plot the output from this experiment.\n\n"
 },
 {
  "repo": "BjerknesClimateDataCentre/QuinCe",
  "language": "Java",
  "readme_contents": "![Status](https://img.shields.io/uptimerobot/status/m778932366-17f73ee77c432e68e22f5195)\n![Uptime](https://img.shields.io/uptimerobot/ratio/m778932366-17f73ee77c432e68e22f5195)\n\n[![Version](https://img.shields.io/github/v/release/BjerknesClimateDataCentre/QuinCe)](https://quince.bcdc.no)\n![Activity](https://img.shields.io/github/commit-activity/m/BjerknesClimateDataCentre/QuinCe)\n\n[![build](https://github.com/BjerknesClimateDataCentre/QuinCe/workflows/build/badge.svg)](https://github.com/BjerknesClimateDataCentre/QuinCe/actions)\n[![junit](https://github.com/BjerknesClimateDataCentre/QuinCe/workflows/junit/badge.svg)](https://github.com/BjerknesClimateDataCentre/QuinCe/actions)\n[![codecov](https://codecov.io/gh/BjerknesClimateDataCentre/QuinCe/branch/master/graph/badge.svg)](https://codecov.io/gh/BjerknesClimateDataCentre/QuinCe)\n\n[![Issues](https://img.shields.io/github/issues-raw/BjerknesClimateDataCentre/QuinCe)](https://github.com/BjerknesClimateDataCentre/QuinCe/issues)\n[![Bugs](https://img.shields.io/github/issues/BjerknesClimateDataCentre/QuinCe/bug?color=red&label=known%20bugs)](https://github.com/BjerknesClimateDataCentre/QuinCe/issues?q=is%3Aissue+is%3Aopen+label%3Abug)\n[![Pull Requests](https://img.shields.io/github/issues-pr/BjerknesClimateDataCentre/QuinCe)](https://github.com/BjerknesClimateDataCentre/QuinCe/pulls)\n\n[![License](https://img.shields.io/github/license/BjerknesClimateDataCentre/QuinCe)](https://www.gnu.org/licenses/gpl-3.0)\n\n# QuinCe\nQuinCe is a system for the automatic QC of data collected from ship-board scientific instruments.\n\nSee Documentation/Specification for more details of what the project does.\n\n"
 },
 {
  "repo": "jamespatrickmanning/pyocean",
  "language": "Python",
  "readme_contents": "# pyocean\ncollection of tested code related to physical oceanographic observations and modeling \nUnfortunately, it has not been updated in years so there are likely better versions available that what is posted here. \n"
 },
 {
  "repo": "grrrizzzz/numerical_modeling",
  "language": "FORTRAN",
  "readme_contents": "# numerical_modeling\nSome oceanographic numerical modeling code in Matlab and Fortran\n\n* I wrote the lake model in its entirety.  It's based on Lake Kauhako on the island of Molokai.\n* I also wrote much of the barotropic ocean model.\n* The Zebiak-Cane ENSO model was written back in the 80's and is essentially the first numerical model to capture the physics of ENSO realistically.  I modified a few hundred lines of code for my thesis experiments."
 },
 {
  "repo": "briochemc/OceanographyCruises.jl",
  "language": "Julia",
  "readme_contents": "# OceanographyCruises.jl\n\n*An interface for dealing with oceanographic cruises data*\n\n<p>\n  <a href=\"https://github.com/briochemc/OceanographyCruises.jl/actions\">\n    <img src=\"https://img.shields.io/github/workflow/status/briochemc/OceanographyCruises.jl/Mac%20OS%20X?label=OSX&logo=Apple&logoColor=white&style=flat-square\">\n  </a>\n  <a href=\"https://github.com/briochemc/OceanographyCruises.jl/actions\">\n    <img src=\"https://img.shields.io/github/workflow/status/briochemc/OceanographyCruises.jl/Linux?label=Linux&logo=Linux&logoColor=white&style=flat-square\">\n  </a>\n  <a href=\"https://github.com/briochemc/OceanographyCruises.jl/actions\">\n    <img src=\"https://img.shields.io/github/workflow/status/briochemc/OceanographyCruises.jl/Windows?label=Windows&logo=Windows&logoColor=white&style=flat-square\">\n  </a>\n  <a href=\"https://codecov.io/gh/briochemc/OceanographyCruises.jl\">\n    <img src=\"https://img.shields.io/codecov/c/github/briochemc/OceanographyCruises.jl/master?label=Codecov&logo=codecov&logoColor=white&style=flat-square\">\n  </a>\n</p>\n\nCreate a `Station`,\n\n```julia\njulia> using OceanographyCruises\n\njulia> st = Station(name=\"ALOHA\", lat=22.75, lon=-158)\nStation ALOHA (22.8N, 158.0W)\n```\n\na `CruiseTrack` of stations,\n\n```julia\njulia> N = 10 ;\n\njulia> stations = [Station(name=string(i), lat=i, lon=2i) for i in 1:N] ;\n\njulia> ct = CruiseTrack(stations=stations, name=\"TestCruiseTrack\")\nCruise TestCruiseTrack\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Station \u2502 Date \u2502  Lat \u2502  Lon \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502       1 \u2502      \u2502  1.0 \u2502  2.0 \u2502\n\u2502       2 \u2502      \u2502  2.0 \u2502  4.0 \u2502\n\u2502       3 \u2502      \u2502  3.0 \u2502  6.0 \u2502\n\u2502       4 \u2502      \u2502  4.0 \u2502  8.0 \u2502\n\u2502       5 \u2502      \u2502  5.0 \u2502 10.0 \u2502\n\u2502       6 \u2502      \u2502  6.0 \u2502 12.0 \u2502\n\u2502       7 \u2502      \u2502  7.0 \u2502 14.0 \u2502\n\u2502       8 \u2502      \u2502  8.0 \u2502 16.0 \u2502\n\u2502       9 \u2502      \u2502  9.0 \u2502 18.0 \u2502\n\u2502      10 \u2502      \u2502 10.0 \u2502 20.0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\nAnd make a `Transect` of `DepthProfiles` along that `CruiseTrack`\n\n```julia\njulia> depths = [10, 50, 100, 200, 300, 400, 500, 700, 1000, 2000, 3000, 5000] ;\n\njulia> idepths = [rand(Bool, length(depths)) for i in 1:N] ;\n\njulia> profiles = [DepthProfile(station=stations[i], depths=depths[idepths[i]], values=rand(12)[idepths[i]]) for i in 1:N] ;\n\njulia> t = Transect(tracer=\"PO\u2084\", cruise=ct.name, profiles=profiles)\nTransect of PO\u2084\nCruise TestCruiseTrack\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Station \u2502 Date \u2502  Lat \u2502  Lon \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502       1 \u2502      \u2502  1.0 \u2502  2.0 \u2502\n\u2502       2 \u2502      \u2502  2.0 \u2502  4.0 \u2502\n\u2502       3 \u2502      \u2502  3.0 \u2502  6.0 \u2502\n\u2502       4 \u2502      \u2502  4.0 \u2502  8.0 \u2502\n\u2502       5 \u2502      \u2502  5.0 \u2502 10.0 \u2502\n\u2502       6 \u2502      \u2502  6.0 \u2502 12.0 \u2502\n\u2502       7 \u2502      \u2502  7.0 \u2502 14.0 \u2502\n\u2502       8 \u2502      \u2502  8.0 \u2502 16.0 \u2502\n\u2502       9 \u2502      \u2502  9.0 \u2502 18.0 \u2502\n\u2502      10 \u2502      \u2502 10.0 \u2502 20.0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\njulia> t.profiles[3]\nDepth profile at Station 3 (3.0N, 6.0E)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Depth \u2502              Value \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502   50.0 \u2502  0.519255214063679 \u2502\n\u2502  300.0 \u2502 0.6289521421572468 \u2502\n\u2502  500.0 \u2502 0.8564006614918445 \u2502\n\u2502 5000.0 \u2502 0.7610393670925686 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n"
 },
 {
  "repo": "gher-ulg/DIVAnd.jl",
  "language": "Julia",
  "readme_contents": "# DIVAnd\n<div align=\"center\"> <img src=\"docs/src/assets/logo.png\"></img></div>\n\n---\n\n[![Project Status: Active \u2013 The project has reached a stable, usable state and is being actively developed.](https://www.repostatus.org/badges/latest/active.svg)](https://www.repostatus.org/#active)\n[![Build Status](https://github.com/gher-ulg/DIVAnd.jl/workflows/CI/badge.svg)](https://github.com/gher-ulg/DIVAnd.jl/actions)\n[![codecov.io](http://codecov.io/github/gher-ulg/DIVAnd.jl/coverage.svg?branch=master)](http://codecov.io/github/gher-ulg/DIVAnd.jl?branch=master)\n[![documentation stable](https://img.shields.io/badge/docs-stable-blue.svg)](https://gher-ulg.github.io/DIVAnd.jl/stable/)\n[![documentation latest](https://img.shields.io/badge/docs-latest-blue.svg)](https://gher-ulg.github.io/DIVAnd.jl/latest/)\n[![DOI](https://zenodo.org/badge/79277337.svg)](https://zenodo.org/badge/latestdoi/79277337)\n\n\n\n`DIVAnd` (Data-Interpolating Variational Analysis in n dimensions) performs an n-dimensional variational analysis/gridding of arbitrarily located observations. Observations will be interpolated/analyzed on a curvilinear grid in 1, 2, 3 or more dimensions. In this sense it is a generalization of the original two-dimensional DIVA version (still available here https://github.com/gher-ulg/DIVA but not further developed anymore).\n\nThe method bears some similarities and equivalences with Optimal Interpolation or Krigging in that it allows to create a smooth and continous field from a collection of observations, observations which can be affected by errors. The analysis method is however different in practise, allowing to take into account topological features, physical constraints etc in a natural way. The method was initially developped with ocean data in mind, but it can be applied to any field where localized observations have to be used to produce gridded fields which are \"smooth\".\n\nSee also https://gher-ulg.github.io/DIVAnd-presentation/#1\n\nPlease cite this paper as follows if you use `DIVAnd` in a publication:\n\nBarth, A., Beckers, J.-M., Troupin, C., Alvera-Azc\u00e1rate, A., and Vandenbulcke, L.: DIVAnd-1.0: n-dimensional variational data analysis for ocean observations, Geosci. Model Dev., 7, 225-241, doi:[10.5194/gmd-7-225-2014](https://doi.org/10.5194/gmd-7-225-2014), 2014.\n\n(click [here](./data/DIVAnd.bib) for the BibTeX entry).\n\n\n# Installing\n\nYou need [Julia](http://julialang.org) (version 1.6 or 1.7) to run `DIVAnd`. The command line version is sufficient for `DIVAnd`.\nInside a Julia terminal, you can download and install the package by issuing:\n\n```julia\nusing Pkg\nPkg.add(\"DIVAnd\")\n```\n\nIt is not recommended to download the source of `DIVAnd.jl` directly (using the green *Clone or Download* button above) because this by-passes Julia's package manager and you would need to install the dependencies of `DIVAnd.jl` manually.\n\n\nWindows users are required to pin the version of NetCDF_jll until this [issue](https://github.com/JuliaPackaging/Yggdrasil/issues/4511) is resolved (help is more than welcome).\n\n```julia\nusing Pkg\nPkg.add(\"NetCDF_jll\")\nPkg.pin(name=\"NetCDF_jll\", version=\"400.702.400\")\n```\n\n\n# Updating DIVAnd\n\nTo update DIVAnd, run the following command and restart Julia (or restart the jupyter notebook kernel using `Kernel` -> `Restart`):\n\n```julia\nusing Pkg\nPkg.update(\"DIVAnd\")\n```\n\nNote that Julia does not directly delete the previous installed version.\nTo check if you have the latest version run the following command:\n\n```julia\nusing Pkg\nPkg.status()\n```\n\nThe latest version number is available from [here](https://github.com/gher-ulg/DIVAnd.jl/releases).\n\nTo explicitly install a given version `X.Y.Z` you can also use:\n\n```julia\nusing Pkg\nPkg.add(name=\"DIVAnd\", version=\"X.Y.Z\")\n```\nOr the master version:\n\n```julia\nusing Pkg\nPkg.add(name=\"DIVAnd\", rev=\"master\")\n```\n\n# Testing\n\nA test script is included to verify the correct functioning of the toolbox.\nThe script should be run in a Julia session.\nMake sure to be in a directory with write-access (for example your home directory).\nYou can change the directory to your home directory with the `cd(homedir())` command.\n\n```julia\nusing Pkg\nPkg.test(\"DIVAnd\")\n```\n\nAll tests should pass without error (it can take several minutes).\n\n```\nINFO: Testing DIVAnd\nTest Summary: | Pass  Total\n  DIVAnd      |  461    461\nINFO: DIVAnd tests passed\n```\n\nThe test suite will download some sample data.\nYou need to have Internet access and run the test function from a directory with write access.\n\n# Documentation\n\nThe main routine of this toolbox is called `DIVAnd` which performs an n-dimensional variational analysis of arbitrarily located observations. Type the following in Julia to view a list of parameters:\n\n```julia\nusing DIVAnd\n?DIVAndrun\n```\n\nsee also https://gher-ulg.github.io/DIVAnd.jl/latest/index.html\n\n## Example\n\n[DIVAnd_simple_example_4D.jl](https://github.com/gher-ulg/DIVAnd.jl/blob/master/examples/DIVAnd_simple_example_4D.jl) is a basic example in fours dimensions. The call to `DIVAndrun` looks like this:\n\n```julia\nfi,s = DIVAndrun(mask,(pm,pn,po,pq),(xi,yi,zi,ti),(x,y,z,t),f,len,epsilon2);\n\n```\nwhere\n`mask` is the land-sea mask, usually obtained from the bathymetry/topography,\n`(pm,pn,po,pq)` is a *n*-element tuple (4 in this case) containing the scale factors of the grid,\n`(xi,yi,zi,ti)` is a *n*-element tuple containing the coordinates of the final grid,\n`(x,y,z,t)` is a *n*-element tuple containing the coordinates of the observations,\n`f` is the data anomalies (with respect to a background field),\n`len` is the correlation length and\n`epsilon2` is the error variance of the observations.\n\nThe call returns `fi`, the analyzed field on the grid `(xi,yi,zi,ti)`.\n\nMore examples are available in the notebooks from the [Diva Workshop](https://github.com/gher-ulg/Diva-Workshops).\n\n## Note on which analysis function to use\n\n`DIVAndrun` is the core analysis function in n dimensions. It does not know anything about the physical parameters or units you work with. Coordinates can also be very general. The only constraint is that the metrics `(pm,pn,po,...)` when multiplied by the corresponding length scales `len` lead to non-dimensional parameters. Furthermore the coordinates of the output grid `(xi,yi,zi,...)` need to have the same units as the observation coordinates `(x,y,z,...)`.\n\n`DIVAndfun` is a version with a minimal set of parameters (the coordinates and values of observations)  `(x,f)` and provides and interpolation function rather than an already gridded field. \n\n`diva3D` is a higher-level function specifically designed for climatological analysis of data on Earth, using longitude/latitude/depth/time coordinates and correlations length in meters. It makes the necessary preparation of metrics, parameter optimizations etc you normally would program yourself before calling the analysis function `DIVAndrun`.\n\n`DIVAnd_heatmap` can be used for additive data and produces Kernel Density Estimations.\n\n\n`DIVAndgo` is only needed for very large problems when a call to `DIVAndrun` leads to memory or CPU time problems. This function tries to decide which solver (direct or iterative) to use and how to make an automatic domain decomposition. Not all options from `DIVAndrun` are available.\n\n## Note about the background field\n\nIf zero is not a valid first guess for your variable (as it is the case for e.g. ocean temperature), you have to subtract the first guess from the observations before calling `DIVAnd` and then add the first guess back in.\n\n## Determining the analysis parameters\n\nThe parameter `epsilon2` and parameter `len` are crucial for the analysis.\n\n`epsilon2` corresponds to the inverse of the [signal-to-noise ratio](https://en.wikipedia.org/wiki/Signal-to-noise_ratio). `epsilon2` is the normalized variance of observation error (i.e. divided by the background error variance). Therefore, its value depends on how accurate and how representative the observations are.\n`len` corresponds to the correlation length and the value of `len` can sometimes be determined by physical arguments. Note that there should be one correlation length per dimension of the analysis.\n\nOne statistical way to determine the parameter(s) is to do a [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_%28statistics%29).\n\n1. choose, at random, a relatively small subset of observations (about 5%). This is the validation data set.\n2. make the analysis without your validation data set\n3. compare the analysis to your validation data set and compute the RMS difference\n4. repeat steps 2 and 3 with different values of the parameters and try to minimize the RMS difference.\n\nYou can repeat all steps with a different validation data set to ensure that the optimal parameter values are robust.\nTools to help you are included in  ([DIVAnd_cv.jl](https://github.com/gher-ulg/DIVAnd.jl/blob/master/src/DIVAnd_cv.jl)).\n\n\n## Note about the error fields\n\n`DIVAnd` allows the calculation of the analysis error variance, scaled by the background error variance. Though it can be calculated \"exactly\" using the diagonal of the error covariance matrix s.P, it is too costly and approximations are provided. Two version are recommended, `DIVAnd_cpme` for a quick estimate and `DIVAnd_aexerr` for a version closer the theoretical estimate (see [Beckers et al 2014](https://doi.org/10.1175/JTECH-D-13-00130.1) )\n\n## Advanced usage\n\n### Additional constraint\n\nAn arbitrary number of additional constraints can be included to the cost function which should have the following form:\n\n*J*(**x**) = \u2211<sub>*i*</sub> (**C**<sub>*i*</sub> **x**  - **z**<sub>*i*</sub>)\u1d40 **Q**<sub>*i*</sub><sup>-1</sup> (**C**<sub>*i*</sub> **x** - **z**<sub>*i*</sub>)\n\nFor every constrain, a structure with the following fields is passed to `DIVAnd`:\n\n* `yo`: the vector **z**<sub>*i*</sub>\n* `H`: the matrix **C**<sub>*i*</sub>\n* `R`: the matrix **Q**<sub>*i*</sub> (symmetric and positive defined)\n\nInternally the observations are also implemented as constraint defined in this way.\n\n## Run notebooks on a server which has no graphical interface\n\nOn the server, launch the notebook with:\n```bash\njupyter-notebook --no-browser --ip='0.0.0.0' --port=8888\n```\nwhere the path to `jupyter-notebook` might have to be adapted, depending on your installation. The `ip` and `port` parameters can also be modified.\n\nThen from the local machine it is possible to connect to the server through the browser.\n\nThanks to Lennert and Bart (VLIZ) for this trick.\n\n# Example data\n\nSome examples in `DIVAnd.jl` use a quite large data set which cannot be efficiently distributed through `git`. This data can be downloaded from the URL https://dox.ulg.ac.be/index.php/s/Bo01EicxnMgP9E3/download. The zip file should be decompressed and the directory `DIVAnd-example-data` should be placed on the same level than the directory `DIVAnd.jl`.\n\n# Reporting issues\n\nPlease include the following information when reporting an issue:\n\n* Version of Julia\n* Version of DIVAnd\n* Operating system\n* Full screen output preferably obtained by setting `ENV[\"JULIA_DEBUG\"] = \"DIVAnd\"`.\n* Full stack strace with error message\n* A short description of the problem\n* The command and their arguments which produced the error\n\n# Fun\n\nAn [educational web application](http://data-assimilation.net/Tools/divand_demo/html/) has been developed to reconstruct a field based on point \"observations\". The user must choose in an optimal way the location of 10 observations such that the analysed field obtained by `DIVAnd` based on these observations is as close as possible to the original field.\n"
 },
 {
  "repo": "AtlanticR/bioRworkshops",
  "language": "R",
  "readme_contents": "# Bedford Institute of Oceanography R workshops\n\nThis repository collects materials related to a series of R workshops to be run regularly at the Bedford Institute of Oceanography. While meant to be introductory, the workshops will in general be tailored toward the use of R (and other tools) in the ocean sciences. \n\n## 01_git\n\nAn overview of the [Git](www.git-scm.com) source control system, and why you should use it (to write R code).\n\n## 02_Rbasics\n\nSome basics of the R language, including general syntax, data types, objects and classes, package system overview, and how to get help. \"Super Noob\" workshop given on 2017-10-20 and (mostly) repeated on 2017-11-03.\n\n## 03_Datawrangling\n\nA \"Noob\" workshop, giving many of the details of doing complex data wrangling, given on 2017-10-27.\n\n## 04_readingPlottingAnalysis\n\nThe basis for the \"super noob\" workshop, on 2017-11-17 -- reading data (e.g. csv/spreadsheet), working with the data/columns, making plots, and doing some analysis (fitting models)\n\n## 05_plottingBasics\n\nHow to make plots using *base* graphics in R (i.e. not `ggplot2`, `lattice`, etc). \n\n## 06_datesAndTimes\n\nDealing with dates and times (yay, POSIX!) in R\n\n## 07_groupReinvigoration\n\nThe R noob group has been re-kicked off, starting in 2019! Look at all the fun topics we can cover!\n\n## 08_workflowOrganization\n\nHow to organize your work, use RStudio projects, etc\n\n## 09_intro_ocean_data\n\nSome basics of the Rstats `oce` package for analyzing *real* ocean data\n\n## 10_using_git\n\nA re-visit of the crowd-pleaser, Git! Essential for anyone who bashes on keyboards.\n\n## 11_DataManipulation_dplyr\n\nIntroduction to the grammar of data manipulation using `dplyr`. Test application with the rvdata. \n\n## 12_R_Mapping\n\nIntroduction to mapping in R. \n\n## 16_R_dashboards \n\nOutcomes of the Interactive Tools Workshop \n"
 },
 {
  "repo": "cchdo/exchange",
  "language": "Python",
  "readme_contents": "exchange\n========\n![Docs Build](https://github.com/cchdo/exchange/workflows/Docs%20Build/badge.svg)\n[![Documentation Status](https://readthedocs.org/projects/exchange-format/badge/?version=latest)](https://exchange-format.readthedocs.io/en/latest/?badge=latest)\n\n\nDescription of the WHP (CCHDO) Exchange Format for CTD/Hydrographic Data\n"
 },
 {
  "repo": "regeirk/atlantis",
  "language": "Python",
  "readme_contents": "Atlantis\n========\n\nPython library for atmospheric, oceanographic and hydrographic data management\nanalysis and visualization.\n\nAccording to Wikipedia:\n\n    Atlantis (in Greek, \u1f08\u03c4\u03bb\u03b1\u03bd\u03c4\u1f76\u03c2 \u03bd\u1fc6\u03c3\u03bf\u03c2, \"island of Atlas\") is a legendary \n    island first mentioned in Plato's dialogues Timaeus and Critias, written \n    about 360 BC. According to Plato, Atlantis was a naval power lying \"in \n    front of the Pillars of Hercules\" that conquered many parts of Western \n    Europe and Africa 9,000 years before the time of Solon, or approximately \n    9600 BC. After a failed attempt to invade Athens, Atlantis sank into the \n    ocean \"in a single day and night of misfortune\".\n\nThis softwares is intended to apply the \"don't repeat yourself\" (DRY) principle\nto atmosphere, ocean and hydrology scientists, and reduce time spent developing\ncode to analyse and make diagrams speak. It makes use of code from different\nsources and references are given to them where appropriate.\n\n\nDISCLAIMER\n----------\n\nThis software may be used, copied, or redistributed as long as it is not sold \nand this copyright notice is reproduced on each copy made. This routine is \nprovided as is without any express or implied warranties whatsoever.\n\n\nLICENSE\n-------\n\nThis software is licensed under a Creative Commons Attribution-ShareAlike 3.0\nUnported License. More information on this license can be obtained at\nhttp://creativecommons.org/licenses/by-sa/3.0/deed.en_US\n\n\nAUTHOR\n------\n\nThe library was created by Sebastian Krieger (solutions@nublia.com)\n\n\nINSTALLATION\n------------\n\n\nTO-DO\n-----\n.. Write book / manual: \"Data management and analysis in oceanography\"\n... leanpub.com\n.. Data browser:\n.... Log of information\n.. Cruise/expedition planner:\n... Tasks\n... Standard procedures per tasks\n... Map of stations\n... List of variables / instruments\n... Casts per station\n... Identification\n\n\nCONTRIBUTING\n------------\n\nDo you want to contribute by improving code performance or increasing \ncapabilities? Great! Your help is very welcome. Give a look at the source code\nand share your contributions by\n\n1. Fork it.\n2. Create a branch (git checkout -b my_atlantis)\n3. Commit your changes (git commit -am \"Added capabilities\")\n4. Push to the branch (git push origin my_atlantis)\n5. Open a Pull Request\n6. Enjoy a refreshing lemonade and wait\n\n\nREVISION\n--------\n\n. 3 (2017-02-22 13:37 -0300)\n. 2 (2016-06-28 21:49 -0300)\n. 1 (2014-06-10)\n. 0 (2013-05-20 20:32 -0300)\n"
 },
 {
  "repo": "nwfsc-fram/OceanTS",
  "language": "TypeScript",
  "readme_contents": "# OceanTS\nOceanographic data processing in Typescript using NodeJS\n"
 },
 {
  "repo": "amnh/HackTheDeep",
  "language": "Matlab",
  "readme_contents": "# Hack The Deep\nRepository for AMNH's Fourth Annual BridgeUP: STEM Hackathon, Hack the Deep!\n\nThe root directory of this repository contains data, documents, and images that are general purpose and could pertain to multiple challenges. Challenge specific data, documents, and images can be found under the challenges directory in a subdirectory with the same name as that challenge. Cloning this repository will give you local copies of all source files involved with all challenges that are not available on hard drives at the event (see organizers at the event for details).\n\nEveryone attending Hack the Deep must read, agree to, and sign the [Code of Conduct](https://github.com/amnh/HackTheDeep/blob/master/CODE_OF_CONDUCT.md).\n\nDon't forget to check social media for hashtag [#HackTheDeep](https://twitter.com/search?f=tweets&vertical=default&q=hackthedeep&src=typd)!\n\nSee the [**wiki**](https://github.com/amnh/HackTheDeep/wiki) for all the details! PLEASE NOTE: The wiki is still a work in progress, but we are working to supply you with all the challenge details and data we can give you prior to the event.\n\nHackathon project repositories will eventually appear on the [HackTheDeep organization page](https://github.com/HackTheDeep)\n"
 },
 {
  "repo": "ocefpaf/dynamical_oceanography",
  "language": "TeX",
  "readme_contents": "# Curso de Oceanografia F\u00edsica Din\u00e2mica lecionado na unimonte no primeiro semestre de 2014.\n\n## Reposit\u00f3rios com aulas, slides, notas e exerc\u00edcios para Oceanografia F\u00edsica Din\u00e2mica\n~ Filipe Fernandes ~\n\nPara compilar as aulas use o script `make_lecture.py`\n\n```python\nMake lecture slides, handout and homework.\n\nUsage:\n    make_lecture (DIR) [--compile=DOC]\n    make_lecture (-h | --help | --version)\n\nExamples:\n    make_lecture Aula_01 --compile=slides\n    make_lecture Aula_01 --compile=handout\n    make_lecture Aula_01 --compile=homework\n    make_lecture Aula_01 --compile=all\n\nArguments:\n  DIR      Lecture directory.\n\nOptions:\n  -c --compile=DOC   slides, handout, homework or all [default: slides]\n  --version   Show version.\n  ---help     Show this screen.\n```\n\n### 1. Plano de Aula\nDisciplina: Oceanografia Descritiva (Curso: Oceanografia / Ciclo/mod. 1/1B)\n\nProfessor: Filipe Fernandes\n\nTurno: Matutino\n\nSemestre: 2\u00ba -- 2013\n\nTurma: OCE1BM-VMA\n\n#### Ementa:\nIntrodu\u00e7\u00e3o \u00e0 Oceanografia F\u00edsica. Conceitos, estrutura e caracter\u00edsticas gerais\ndos oceanos.\n\n#### Bibliografia b\u00e1sica:\n- GARRISON, Tom; MIYAJI, C\u00edntia (Trad.). Fundamentos de oceanografia. S\u00e3o Paulo: Cengage Learning, 2009. 426 p. ISBN 9788522106776.\n- MIRANDA, L. B., CASTRO, B. M., KJERFVE, B. 2002. Princ\u00edpios de Oceanografia F\u00edsica de Estu\u00e1rios. 1\u00aa. ed. S\u00e3o Paulo : Edusp, 414p.\n- SVERDRUP, K. A., Duxbury A. B., Duxbury, A. C. 2006. Fundamentals of Oceanography. McGraw Hill, 5th edition. N\u00famero de Chamada: 551.46 S968f 2006.\n\n\n#### Bibliografia complementar:\n- STEWART, R. H. 2002. Introduction to Physical Oceanography. Department of Oceanography. Texas A. M. University.\n- SOUZA, Maria Cristina de Arruda. A Corrente do Brasil ao largo de Santos: medi\u00e7\u00f5es diretas. 2000. 169p. Disserta\u00e7\u00e3o (mestrado). Instituto Oceanogr\u00e1fico. USP. Dispon\u00edvel em: http://www.teses.usp.br/teses/disponiveis/21/21132/tde-10092003-094250/pt-br.php\n- The Open University. 1989. Ocean Circulation, Pergamon Press, 2nd edition.\n- TIPLER, Paul Allen.  F\u00edsica para cientistas e engenheiros.  4. ed. Rio de Janeiro:  LTC,  2006. v.1.\n- TOMCZAK, Matthias.  Physical oceanography.   Australia  The Flinders University of South Australia,  2002. 1 CD-ROM:  son., color  Dispon\u00edvel em: http://www.es.flinders.edu.au/~mattom/regoc/pdfversion.html.\n\n### Cronograma\n| Aula | Data          | Conte\u00fado                              | Lista/Prova         |\n|:----:|:--------------|:--------------------------------------| -------------------:|\n| 01   | 2013-08-09    | Introdu\u00e7\u00e3o \u00e0 Oceanografia F\u00edsica 1    |                     |\n| 02   | 2013-08-16    | Introdu\u00e7\u00e3o \u00e0 Oceanografia F\u00edsica 2    | Lista 1             |\n| 03   | 2013-08-23    | Dimens\u00f5es e formas dos oceanos 1      | Lista 2             |\n| 04   | 2013-08-30    | Dimens\u00f5es e formas dos oceanos 2      | Lista 3             |\n| 05   | 2013-09-06    | Propriedades f\u00edsicas da \u00e1gua do mar 1 | Lista 4             |\n| 06   | 2013-09-13    | Propriedades f\u00edsicas da \u00e1gua do mar 2 | Lista 5             |\n| 07   | 2013-09-20    | Distribui\u00e7\u00e3o espacial e temporais 1   | Lista 6             |\n| 08   | 2013-09-27    | Distribui\u00e7\u00e3o espacial e temporais 2   | Lista 7             |\n|      | 2013-10-04    |                                       | Prova 1             |\n| 11   | 2013-10-11    | Leis de conserva\u00e7\u00e3o 1                 | Revis\u00e3o Prova 1     |\n| 12   | 2013-10-18    | Leis de conserva\u00e7\u00e3o 2                 | Lista 8             |\n| 13   | 2013-10-25    | Circula\u00e7\u00e3o e massas d'\u00e1gua 1          | Lista 9             |\n| 14   | 2013-11-01    | Circula\u00e7\u00e3o e massas d'\u00e1gua 2          | Lista 10            |\n| 15   | 2013-11-08    | Oc. costeira e estuarina 1            | An\u00e1lise de dados    |\n|      | 2013-11-15    |                                       | Feriado             |\n| 16   | 2013-11-22    | Oc. costeira e estuarina 2            | Lista 11            |\n|      | 2013-11-29    |                                       | Semin\u00e1rio           |\n|      | 2013-12-06    |                                       | Prova 2             |\n|      | 2013-12-13    |                                       | Revis\u00e3o             |\n|      | 2013-12-20    |                                       | Prova Alternativa   |\n"
 },
 {
  "repo": "kkats/physical-oceanography",
  "language": "Haskell",
  "readme_contents": "# physical-oceanography\nTools for physical oceanographic data analysis.\n\n## oceanogr\n\nModules for IO (binary IO, netcdf), special data handling (SAM index, ETOPO5 topography, WGHC climatology),\nlinear data analysis (EOF, low-pass filters, leastsquare fit), power spectra (PSD),\nparallel matrix computation (RepaExpand), and others (Misc).\n\n## observation\n\nFor visualising CTD data from [GO-SHIP](http://www.go-ship.org)-style, full-depth observation.\n\n1. It is necessary to provide `CTDfileRead`, a set of header information for the CTD output (e.g. `example/SBEproc.hs` -- this is for output from Sea-Bird Scientific CTD process programs).\n2. Build a list of CTD station to plot using `readCTD` (e.g. `example/MakeList.hs`).\n3. Section data along the list can be output by `sectionCTD` (e.g. `example/PlotSection.hs`).\n\n## gamma-n\n\nCalculate an approximation of the [neutral density](http://www.teos-10.org/preteos10_software/neutral_density.html) surfaces, *gamma-n* from measured temperature, salinity, and pressure.\n\n## GSW\n\nIs now in a separate [repo](https://github.com/TEOS-10/GSW-Haskell.git) under [TEOS-10](https://github.com/TEOS-10).\n\n"
 },
 {
  "repo": "FESOM/pyfesom",
  "language": "Jupyter Notebook",
  "readme_contents": "# pyfesom\n\nCollection of tools for basic handling of FESOM ocean model output.\n\n[![MIT licensed](https://img.shields.io/badge/license-MIT-blue.svg)](https://github.com/koldunovn/pyfesom/blob/master/LICENSE) [![Documentation Status](https://readthedocs.org/projects/pyfesom/badge/?version=latest)](http://pyfesom.readthedocs.io/en/latest/?badge=latest)\n\n## Documentation (tools and library)\n\nhttp://pyfesom.readthedocs.io/en/latest/index.html\n\n## Examples of library usage\n\n- [Show fesom mesh](https://github.com/koldunovn/pyfesom/blob/master/notebooks/show_mesh.ipynb)\n- [Plot variable on original grid](https://github.com/koldunovn/pyfesom/blob/master/notebooks/show_variable_on_original_grid.ipynb)\n- [Plot simple diagnostic (mean, std)](https://github.com/koldunovn/pyfesom/blob/master/notebooks/plot_simple_diagnostics.ipynb)\n- [Interpolate to regular grid](https://github.com/koldunovn/pyfesom/blob/master/notebooks/interpolate_to_regular_grid.ipynb)\n- [Compare to climatology](https://github.com/koldunovn/pyfesom/blob/master/notebooks/compare_to_climatology.ipynb)\n\n## Requirements\n\nWe try to support both python 2.7 and possibly 3.4.\n\n- numpy\n- scipy\n- pandas\n- netCDF4\n\n\n\n"
 },
 {
  "repo": "gvoulgaris0/WavePart",
  "language": "HTML",
  "readme_contents": "# WavePart ![image](https://user-images.githubusercontent.com/48567321/126871118-68ae6e82-15fe-48e5-ac1e-2d1d0b673a9b.png)\n\n\nSet of Matlab(r) functions for the partition of directional wave spectra to its wind and different swell components. The partitions are initially identified using a watershed defining algorithm  and then are modified following mostly the method described in Hanson and Phillips (2001).\n\nAuthors:  \n  Douglas Cahl and George Voulgaris  \n  School of the Earth, Ocean and Environment  \n  University of South Carolina  \n  Columbia, SC, 29205, USA.  \n  \nCode Updates:\n  -  1/22/2020 - waveparams.m - the mean freq. (fm) estimate was incorrect; it has been corrected.\n  -  1/22/2020 - partition.m was updated to catch cases when a flat spectrum is given as input.\n  -  1/22/2020 - master_partition.m was updated so that it calls waveparams.m with the correct number of outputs.\n  -  1/21/2020 - waveparams.m - the Hrms estimate was incorrect; also the mean direction now is given in -180 to +180 deg range.\n  \nCitation:  \n   -  Douglas, C. and Voulgaris, G., 2019, WavePART: MATLAB(r) software for the partition of directional ocean wave spectra.: Zenodo, doi:10.5281/zenodo.2638500. \n\nRelevant References:  \n   -  J.L. Hanson and O.M. Philips, 2001. Automated Analysis of Ocean Surface Directional  Wave Spectra. Journal of Oceanic and Atmospheric Technology, 18, 278-293.   \n   -  J. Portilla, F.J. Ocampo-Torres, and J. Monbaliu, 2009. Spectral Partitioning and Identification of Wind Sea and Swell.  Journal of Oceanic and Atmospheric Technology, 26, 107-121. DOI: 10.1175/2008JTECHO609.1   \n   -  E. Cheynet, 2019. Pcolor in Polar Coordinates (https://www.mathworks.com/matlabcentral/fileexchange/49040-pcolor-in-polar-coordinates), MATLAB Central File Exchange. Retrieved March 16, 2019.  \n"
 },
 {
  "repo": "PlanktoScope/PlanktoScope",
  "language": "HTML",
  "readme_contents": "# An open and affordable imaging platform for citizen oceanography\n\n![PlanktoScope Render](docs/readme/planktoscope_cad.webp)\n\n![Plankton collage](docs/readme/plankton_collage.webp)\n\n\n# What is this?\nThe PlanktoScope is an open-source, affordable imaging platform for citizen oceanography. It's built around a Raspberry Pi, a couple of HATs, some stepper motors and a few centimeters of silicon tubes. Its cost is at about $500 in parts.\n\nThe goal of the PlanktoScope is to allow citizen to engage in scientific programs, either at sea or onshore. You can use the PlanktoScope to image the different species of Plankton living in a body of water.\n\n\n## Get the papers!\nThe PlanktoScope has been described in a paper available on the [bioRxiv preprint server](https://www.biorxiv.org/content/10.1101/2020.04.23.056978v1). The first results of this program and its outline are also available as a [preprint](https://www.biorxiv.org/content/10.1101/2020.08.31.263442v1).\n\n|[![PlanktoScope Preprint](docs/readme/planktoscope_pub.webp)](https://www.biorxiv.org/content/10.1101/2020.04.23.056978v1)|[![PlanktonPlanet Preprint](docs/readme/planktonplanet_pub.webp)](https://www.biorxiv.org/content/10.1101/2020.08.31.263442v1)|\n|--------|--------|\n\n\n# Key Features\n- Image small animals and algae living in water\n- Focus stage control\n- Pump control\n- Automatic image capture\n- Automatic segmentation\n\n# How do I build one?\nYou can access the complete documentation on [Read The Docs](https://planktonscope.readthedocs.io/).\n\n# How do I get involved?\nThere are several ways to join the development effort, share your progress with your build or just ask for help.\n\nWe are using slack as a communication platform between interested parties. You can [request to join by filling this form](https://docs.google.com/forms/d/e/1FAIpQLSfcod-avpzWVmWj42_hW1v2mMSHm0DAGXHxVECFig2dnKHxGQ/viewform).\n\nThis repository is also a good way to get involved. Please fill in an issue if you witnessed a bug in the software or hardware. If you are able, you can also join the development effort. Look through the [issues opened](https://github.com/PlanktonPlanet/PlanktoScope/labels/good%20first%20issue) and choose one that piques your interest. Let us know you want to work on it in the comments, we may even be able to guide your beginnings around the code.\n\n# License: Our work is fully open source\n\nDepending on the material they cover, different licenses apply to the files of this repository.\n\nIf you make any change for your use, please fork this repository and publish your improvements. It will help others, and we can integrate those changes in this repository if needed.\n\nAlso, all licenses are contaminating, if you want to use any of this material for a project that you cannot open-source, please contact us using Slack so we can see what can be done to help you and your project.\n\n## Hardware files\nAll hardware files and documentation (everything in the `hardware` directory) is released under a [CERN-OHL-S-2.0 license](https://ohwr.org/cern_ohl_s_v2.txt).\n\n## Software source\nThe source code (everything in the directories `flows` and `scripts`) is released under a [GPL-3.0 license](https://www.gnu.org/licenses/gpl-3.0.en.html).\n\n## Everything else (documentation, pictures, etc...)\nEverything else is released under a [Creative Commons CC-BY-SA license](https://creativecommons.org/licenses/by-sa/4.0/)."
 },
 {
  "repo": "biavillasboas/IdealizedWaveCurrent",
  "language": "Python",
  "readme_contents": "\n\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.4045184.svg)](https://doi.org/10.5281/zenodo.4045184)\n\n# Source code for \nVillas B\u00f4as, A. B., B. D. Cornuelle, M. R. Mazloff, S. T. Gille, and F. Ardhuin, Wave-Current Interactions at Meso and Submesoscales: Insights from Idealized Numerical Simulations. J. Phys. Oceanogr., doi: https://doi.org/10.1175/JPO-D-20-0151.1. \n\n# Abstract\nSurface gravity waves play a major role in the exchange of momentum, heat, energy, and gases between the ocean and the atmosphere. The interaction between currents and waves can lead to variations in the wave direction, frequency, and amplitude. In the present work, we use an ensemble of synthetic currents to force the wave model WAVEWATCH III and assess the relative impact of current divergence and vorticity in modifying several properties of the waves, including direction, period, directional spreading, and significant wave height (Hs). We find that the spatial variability of Hs is highly sensitive to the nature of the underlying current and that refraction-caused vorticity in the rotational component of the flow is the main mechanism leading to gradients of Hs. The results obtained using synthetic currents were used to interpret the response of surface waves to realistic currents by running an additional set of simulations using the llc4320 MITgcm output in the California Current region. Our findings suggest that wave parameters could be used to detect and characterize strong gradients in the velocity field, which is particularly relevant for the Surface Water and Ocean Topography (SWOT) satellite as well as several proposed satellite missions.\n# Authors\n* [Bia Villas Boas](https://biavillasboas.github.io/) <<avillasboas@ucsd.edu>>\n* [Sarah T. Gille](http://www-pord.ucsd.edu/~sgille/)\n* [Matthew R. Mazloff](http://scrippsscholars.ucsd.edu/mmazloff)\n* [Bruce D. Cornuelle](http://scrippsscholars.ucsd.edu/bcornuelle)\n* [Fabrice Ardhuin](https://annuaire.ifremer.fr/cv/16811/en/)\n\n# Data\nAll model output analyzed in this paper is availabe for download here https://doi.org/10.6075/J0X928V6\n\n# Funding\nThis project was partlially funded by the [SWOT](https://swot.jpl.nasa.gov/) program with NASA grant NNX16AH67G and 80NSSC20K1136.\nBia Villas B\u00f4as was also partially funded by NASA grant 80NSSC17K0326.\n\n# How to use this repository\n\nAll figures in Villas B\u00f4as et al. (2020) can be reproduced using the Python scripts from this repository and the [model output](https://library.ucsd.edu/dc/collection/bb6217292w). To do so, follow these steps\n\n1. Make a local copy of this repository by either cloning or downloading it.\n\n2. Download the [model output](https://library.ucsd.edu/dc/collection/bb6217292w), untar the files, and move all three directories to `data` in the project root. After doing so, your directory tree should look like this:\n\n```\nIdealizedWaveCurrent/\n\u251c\u2500\u2500 data\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 llc4320\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 model_stats\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 synthetic\n\u251c\u2500\u2500 figs\n\u251c\u2500\u2500 src\n\u2514\u2500\u2500 tools\n```\n3. Make sure that you create an environment with the package versions specified in `environment.yml`. If you are using [Conda](https://docs.conda.io/en/latest/) you can run \n\n`conda env create -f environment.yml`\n\nfrom the project root.\n\n4. If you follow the steps above you should be able to reproduce all figures, by running `python figXX.py` from the `src` directory without having to adjust any paths.\n\n* **Note on rendering matplotlib text with LaTeX:** To ensure that the math fonts in the figures matched the fonts in the paper, the code in this repository requires a [working LaTeX installation](https://matplotlib.org/3.1.0/tutorials/text/usetex.html). If you encounter any problems with this, you may change the line matplotlib.rcParams['text.usetex']=True  to False or just comment it out. \n\n# How to cite this code\n\nIf you wish to use the code from this repository, you may cite it as \n\nVillas B\u00f4as, Ana B. (2020, September 23). Source code for: 'Wave-Current Interactions at Meso and Submesoscales: Insights from Idealized Numerical Simulations'. Zenodo. https://doi.org/10.5281/zenodo.4045183\n"
 },
 {
  "repo": "OSU-CEOAS-Schmittner/UVic2.9",
  "language": "Fortran",
  "readme_contents": "# UVic2.9 with updated Marine Iron Biogeochemistry\nThis version contains the new marine iron biogeochemistry modifications from Somes et al., (2021; https://doi.org/10.1029/2021GB006948), which has now been implemented into MOBI2.1 updates level 08. The modified files are located in updates/latest directory. The new iron biogeochemistry modifications can be enabled by including the following options in run/mk.in (O_mobi_iron_var_ligands, O_mobi_iron_sed_dale, O_mobi_iron_gesamp_atmfedep, O_mobi_iron_inscav_nonlinear, O_mobi_sedcox_flogel, O_mobi_omz_threshold_smooth). Please note some marine biogeochemical parameter changes in run/control.in. \n\n# UVic2.9\nThis is the base code of the University of Victoria (UVic) climate model version 2.9 used at OSU. The source directory should not be changed. It is the original code without updates. It resides in /usr/local/models/UVic_ESCM/2.9/. Changes should be made to \"updates/latest\", which contains the latest updates. It should reside in /usr/local/models/UVic_ESCM/2.9/updates/. \n\n## Further info\n* [OSU-UVic2.9 webpage](https://osu-ceoas-schmittner.github.io/UVic2.9/)\n* Model of Ocean Biogeochemistry and Isotopes [MOBI](https://github.com/andreasschmittner/UVic2.9/wiki/Model-of-Ocean-Biogeochemistry-and-Isotopes-(MOBI))\n* How to use git and github with this code: [OSU-UVic2.9 wiki](https://github.com/OSU-CEOAS-Schmittner/UVic2.9/wiki)\n"
 },
 {
  "repo": "lukecampbell/gsw-teos",
  "language": "C",
  "readme_contents": "TEOS-10 V3.0 GSW Oceanographic Toolbox in C\n\nThis is a translation of the original Fortran-90 source\ncode into C. You should download the documentation from http://teos-10.org.\nThe functions gsw_saar and gsw_delta_sa_ref\nhave been modified from the original to not use an external\ndata file for global absolute salinity anomaly and absolute\nsalinity anomaly ratio data. The data are instead incorporated\ninto static tables that are used directly.\n\nManifest:\n\nREADME                                                          -- this file.\ngsw_check_functions.c                                           -- C implementation of the check functions\ngsw_data_v3_0.dat.gz                                            -- global absolute salinity anomaly data\ngsw_format.c                                                    -- program to create gsw_saar_data.c from\n                                                                   gsw_data_v3_0.dat\ngsw_oceanographic_toolbox.c                                     -- The C GSW library less gsw_saar\ngsw_saar.c                                                      -- gsw_saar and gsw_delta_sa_ref (modified)\ngsw_saar_data.c                                                 -- static global absolute salinity anomaly data\n\t\t\t\t                                                   used by gsw_saar.c and created by gsw_format\ngswteos-10.h                                                    -- GSW function prototypes\nMakefile                                                        -- basic make file to build gsw_check_functions\n\t\t\t\t                                                   and libgswteos-10.so\n\nYou'll probably want to build gsw_oceanographic_toolbox.c, and gsw_saar.c\ninto a library. \"make library\" will attempt to build a shared library for\ngcc/GNU Linux platforms.\n\nC programs using the GSW Oceanographic Toolbox should include the\nsupplied header file:\n\n#include <gswteos-10.h>\n\nChangeLog:\n\n2012-10-07:\tgsw-3.0.1 New gsw_check_functions.c based on revised f90.\n2011-09-23:\tgsw-3.0 Initial creation.\n\nFrank Delahoyde <fdelahoyde@ucsd.edu>\nShipboard Technical Support, Computing Resources <sts-cr@ucsd.edu>\nScripps Institution of Oceanography\nNimitz Marine Facility, Point Loma\nSan Diego, Ca. 92106-3505\n"
 },
 {
  "repo": "gher-ulg/DivaPythonTools",
  "language": "Jupyter Notebook",
  "readme_contents": "[![DOI](https://zenodo.org/badge/44103456.svg)](https://zenodo.org/badge/latestdoi/44103456)\n\n# Diva Python Tools\n\nA set of python modules to help users with\n1. the preparation of the Diva input files: data, contours, parameters;\n2. the execution of the Diva interpolation tool,\n3. the reading of output files (analysis, finite-element mesh),\n4. the input and output plotting.\n\n## Getting started \n\n### Prerequisites\n\nThe [Diva](https://github.com/gher-ulg/diva) interpolation tool has to be installed and compiled on your machine. See the related [documentation](https://github.com/gher-ulg/DIVA/blob/master/README.md#installing) for the installation.\n\n### Installing\n\nClone the package:\n```bash\ngit clone git@github.com:gher-ulg/DivaPythonTools.git\n```\nor download the latest stable [release](https://github.com/gher-ulg/DivaPythonTools/releases).\n\nInside DivaPythonTools directory execute:\n```python\npip install -r requirements.txt\npython setup.py install\n```\n\nAfter this you should use it as:\n```python\nfrom pydiva import pydiva2d, pydiva4d\n```\n\n## Module description\n\nThe main modules are [`pydiva2d`](./pydiva2d.py) and [`pydiva4d`](./pydiva4D.py), which define the classes for the 2D and 4D version of Diva, respectively.\n\n### pydiva2d\n\nThe module defines classes corresponding to the main Diva input (data, parameters, contours) and output files (analysed and error fields, finite-element mesh).\n\n### pydiva4d\n\nThe module defines classes to run the 4D version of Diva.\n\n## Plots\n\nThe figures can be generated with and without the [Basemap](https://github.com/matplotlib/basemap) module (Plot on map projections). \n\nSome examples obtained with mixed-layer depth (MLD) data are shown below. The complete example to generate these plots is inside the Notebooks directory [(run_diva2D_MLD)](./Notebooks/run_diva2D_MLD.ipynb).\n\nThe [Notebooks](./Notebooks) directory contains additional examples showing how to run 2D and 4D cases.\n\n### Data values\nScatter plot showing the data positions and values.    \n![Data](./figures/datapoints.png)\n\n### Contours\nBy default, each sub-contour is displayed in a different color.     \n![Contour](./figures/contours.png)\n\n### Finite-element mesh\nTriangular mesh covering the region of interest.     \n![Mesh](./figures/mesh.png)\n\n### Analysed fields\nPseudo-color plot of the gridded field obtained by the interpolation.     \n![Analysis](./figures/analysis.png)\n\n### Combined information\nData, contours, mesh and analysis on the same figure.     \n![Combined](./figures/AnalysisMeshData.png)\n\n## Acknowledgments\n\n[Diva](https://github.com/gher-ulg/DIVA) developments have benefited from the users' feedback and numerous comments, especially during the editions of the Diva workshops.\n\nThe present module was initiated in the frame of [SeaDataCloud](SeaDataCloud) project.  \n\n\n\n"
 },
 {
  "repo": "dankelley/ocedata",
  "language": "R",
  "readme_contents": "# ocedata 0.2.0\n\n<!-- badges: start -->\n\n[![R-CMD-check](https://github.com/dankelley/ocedata/workflows/R-CMD-check/badge.svg)](https://github.com/dankelley/ocedata/actions)\n[![CRAN_Status_Badge](https://www.r-pkg.org/badges/version/ocedata)](https://cran.r-project.org/package=ocedata)\n![RStudio CRAN mirror downloads](https://cranlogs.r-pkg.org/badges/last-month/ocedata)\n![RStudio CRAN mirror downloads](https://cranlogs.r-pkg.org/badges/last-week/ocedata)\n![RStudio CRAN mirror downloads](https://cranlogs.r-pkg.org/badges/last-day/ocedata)\n\n<!-- badges: end -->\n\n\n## About ocedata\n\nThe `ocedata` package supplies some oceanographic datasets, for general use and\nas an adjunct to the `oce` package. Indeed, some of the datasets were once\nsupplied by `oce`, but they were moved to `ocedata` to reduce storage pressure\non the CRAN system, based on the assumption that `oce` will be updated more\nfrequently than `ocedata`.\n\n\n## Installing ocedata\n\nStable versions of `ocedata` may be installed from within R, in the same way as\nother packages, with\n```splus\ninstall.packages(\"ocedata\")\n```\nHowever, this version is only updated a few times a year (pursuant to CRAN\npolicy), so some users install `ocedata` from the github.com website instead,\nto get the latest version. This may be done with\n```splus\nlibrary(devtools)\ninstall_github(\"dankelley/ocedata\", ref=\"main\")\n```\n\n"
 },
 {
  "repo": "chouj/JPO_CloudofKeywords",
  "language": "Matlab",
  "readme_contents": "![wordcloud_jpo_keywords_2013-2017](https://github.com/chouj/JPO_CloudofKeywords/blob/master/wordcloud_jpo_keywords_2013-2017.jpg)\n\n# a MATLAB script for generating cloud of keywords of the Journal of Physical Oceanography\n\n## WARNING\nDo not use this code illegally !\n\n## Note\nWindows code.\n\nMATLAB R2017b or newer and associated Text Analytics Toolbox are required.\n\n## Acknowledgements\n\n#### Inspired by \n\n[\u5982\u4f55\u8fc5\u901f\u79ef\u7d2f\u4e24\u4e07\u8bcd\u6c47\u91cf\u5e76\u6d41\u7545\u9605\u8bfb\u7ecf\u6d4e\u5b66\u4eba\uff1f](https://zhuanlan.zhihu.com/p/20713896)\n\n[R2017b Text Analytics Toolbox:\u82f9\u679c\u7684\u5341\u5e74](https://zhuanlan.zhihu.com/p/31054652)\n\n\n#### Thanks to\n\n[free-proxy-list](https://github.com/a2u/free-proxy-list)\n\n[MATLAB \u4e0e \u722c\u866b](https://zhuanlan.zhihu.com/p/35372205)\n\n[How can I save the content of the MATLAB web browser window programmatically? Not the same as WEBREAD....](https://ww2.mathworks.cn/matlabcentral/answers/276583-how-can-i-save-the-content-of-the-matlab-web-browser-window-programmatically-not-the-same-as-webrea)\n\n[How do I set a proxy server to use with the URLREAD and URLWRITE functions in MATLAB?](https://ww2.mathworks.cn/matlabcentral/answers/94117-how-do-i-set-a-proxy-server-to-use-with-the-urlread-and-urlwrite-functions-in-matlab)\n\n## \u4e2d\u6587\n#### \u722c\u5b66\u672f\u520a\u7269JPO\u8bba\u6587\u7684\u5173\u952e\u8bcd \u6839\u636e\u8bcd\u9891\u751f\u6210\u6807\u7b7e\u4e91\n\n## \u6350\u8d60\n[![Donate](https://img.shields.io/badge/Donate-PayPal-green.svg)](https://www.paypal.me/Mesoscale)\n[![Donate](https://img.shields.io/badge/Donate-WeChat-brightgreen.svg)](https://github.com/chouj/donate-page/blob/master/simple/images/WeChatQR.jpg?raw=true)\n[![Donate](https://img.shields.io/badge/Donate-AliPay-blue.svg)](https://github.com/chouj/donate-page/blob/master/simple/images/AlipayQR.jpg?raw=true)\n"
 },
 {
  "repo": "gher-ulg/OAK",
  "language": "Fortran",
  "readme_contents": "Installation\n------------\n\nTo install OAK you need:\n\n* A Fortran 90 compiler\n* NetCDF (with Fortran 90 interface)\n* LAPACK library\n* an implementation of the BLAS library (such as ATLAS, OpenBLAS or MKL) \n* cholmod\n\nOptionally, for parallelization, either one of:\n\n* MPI (with Fortran 90 interface)\n* OpenMP (included in Fortran 90 compilers) \n\n\n# Compilation\n\n## gfortran and default BLAS\n\nTo compile with open-source gfortran, parallelized with MPI, at least the following packages are needed:\n\n```\nsudo apt-get install gfortran libatlas-base-dev liblapack-dev openmpi-bin libopenmpi-dev libnetcdf-dev netcdf-bin\ncp config.mk.template config.mk\nmake\n```\n\n# ifort and MKL\n\n## Without CLODMOD:\n\nSet `$MKLROOT`:\n\n```bash\n$ make FORT=ifort NETCDF_CONFIG=/path/to/bin/nf-config CHOLMOD_LIB= BLAS_LIBDIR=$MKLROOT/lib/intel64/ BLAS_LIB=\"-lmkl_intel_lp64 -lmkl_sequential -lmkl_core\" LAPACK_LIB=\n```\n"
 },
 {
  "repo": "thunderhoser/ai2es_xai_course",
  "language": "Python",
  "readme_contents": "# ai2es_xai_course\nNotebooks for AI2ES (NSF Institute for Research on Trustworthy Artificial Intelligence in Weather, Climate, and Coastal Oceanography) short course on XAI (explainable artificial intelligence).\n"
 },
 {
  "repo": "clstoulouse/motu-client-python",
  "language": "Python",
  "readme_contents": "# Motu Client Python Project \n@author Product owner <tjolibois@cls.fr>  \n@author Scrum master, software architect <smarty@cls.fr>  \n@author Quality assurance, continuous integration manager <smarty@cls.fr>  \n\n>How to read this file? \nUse a markdown reader: \nplugins [chrome](https://chrome.google.com/webstore/detail/markdown-preview/jmchmkecamhbiokiopfpnfgbidieafmd?utm_source=chrome-app-launcher-info-dialog) exists (Once installed in Chrome, open URL chrome://extensions/, and check \"Markdown Preview\"/Authorise access to file URL.), \nor for [firefox](https://addons.mozilla.org/fr/firefox/addon/markdown-viewer/)  (anchor tags do not work)\nand also plugin for [notepadd++](https://github.com/Edditoria/markdown_npp_zenburn).\n\n>Be careful: Markdown format has issue while rendering underscore \"\\_\" character which can lead to bad variable name or path.\n\n\n# Summary\n* [Overview](#Overview)\n* [Build](#Build)\n* [Installation](#Installation)\n    * [Prerequisites](#InstallationPre)\n    * [Using PIP](#InstallationPIP)\n    * [From tar.gz file](#InstallationTGZ)\n* [Configuration](#Configuration)\n* [Usage and options](#Usage)\n    * [Usage from PIP installation](#UsagePIP)\n    * [Usage from tar.gz installation](#UsageTGZ)\n* [Usage examples](#UsageExamples)\n    * [Download](#UsageExamplesDownload)\n    * [GetSize](#UsageExamplesGetSize)\t\n    * [DescribeProduct](#UsageExamplesDescribeProduct)\n* [Licence](#Licence)\n* [Troubleshooting](#Troubleshooting)\n    * [Unable to download the latest version watched on GitHub from PIP](#Troubleshooting)  \n    * [From Windows, Parameter error](#TroubleshootingWinArgErr)\n\n# <a name=\"Overview\">Overview</a>\nMotu client \"motuclient-python\" is a python script used to connect to Motu HTTP server in order to:  \n\n* __extract__ the data of a dataset, with geospatial, temporal and variable criterias (default option)   \n* __get the size__ of an extraction with geospatial, temporal and variable criterias  \n* __get information__ about a dataset  \n\nThis program can be integrated into a processing chain in order to automate the downloading of products via the Motu.  \n  \n  \n# <a name=\"Build\">Build</a>  \nFrom the root folder runs the command:  \n  \n```\n./patchPOMtoBuild.sh  \nmvn clean install -Dmaven.test.skip=true\n[...]\n[INFO] BUILD SUCCESS\n[...]\n```  \n\nThis creates two archives in the target folder:\n\n* motuclient-python-$version-$buildTimestamp-src.tar.gz: Archive containing all the source code\n* motuclient-python-$version-$buildTimestamp-bin.tar.gz: Archive ready to be installed\n\n\n\n# <a name=\"Installation\">Installation</a> \n\n## <a name=\"InstallationPre\">Prerequisites</a>\nSince motuclient release version 3.X.Y, you must use python version 3.7.10 or later.  \n__/!\\__ motuclient does not work with the OpenSSL library release 1.1.1.e. Either use an older version such as the 1.1.1.d or jump to the 1.1.1.f release.  \nThere are two methods to install the client, by using PIP or from a tar.gz file.  \n [setuptools](#InstallationSetuptools) python package has be installed in order to display the motuclient version successfully.    \n  \n## <a name=\"InstallationPIP\">Using PIP</a>\nPython Package Index is used to ease installation.  \nIf your host needs a PROXY set it, for example:  \n```\nexport HTTPS_PROXY=http://myCompanyProxy:8080  \n```  \n\nThen run:  \n  \n```\npip install motuclient --upgrade  \n```\n  \nNow \"motuclient\" is installed, you can [configured it](#Configuration) and [use it](#UsagePIP).\n  \n  \n## <a name=\"InstallationTGZ\">From tar.gz file</a>\nDeploy the archive (file motuclient-python-$version-bin.tar.gz available from [GitHub release](https://github.com/clstoulouse/motu-client-python/releases)) in the directory of your choice.  \n```  \ntar xvzf motuclient-python-$version-$buildTimestamp-bin.tar.gz\n```  \n\nCreate a [configuration file](#Configuration) and set the user and password to use to connect to the CAS server.   \n\n## <a name=\"InstallationSetuptools\">Install setuptools python package</a>\n\"[Setuptools](https://pypi.python.org/pypi/setuptools)\" python package has to be installed in order to display the version with option --version, here is how to install it:    \n \nIf your host needs a PROXY set it, for example:  \n```\nexport HTTPS_PROXY=http://myCompanyProxy:8080  \n```  \n\nThen run:  \n\n```  \nsudo apt install python-pip  \npip install --upgrade setuptools  \n```  \n\n# <a name=\"Configuration\">Configuration file</a>  \n\nAll parameters can be defined as command line options or can be written in a configuration file.  \nThe configuration file is a .ini file, encoded in UTF-8 without BOM. This file is located in the following directory:  \n\n* on __Unix__ platforms: $HOME/motuclient/motuclient-python.ini\n* on __Windows__ platforms: %USERPROFILE%\\motuclient\\motuclient-python.ini\n  \nThe expected structure of file is:  \n``` \n[Main]  \n# Motu credentials  \nuser=john  \npwd=secret  \n\nmotu=http://motu-ip-server:port/motu-web/Motu  \nservice_id=GLOBAL_ANALYSIS_FORECAST_PHY_001_024-TDS   \nproduct_id=global-analysis-forecast-phy-001-024-hourly-t-u-v-ssh  \ndate_min=2019-03-27  \ndate_max=2019-03-27  \nlatitude_min=-30  \nlatitude_max=40.0  \nlongitude_min=-10  \nlongitude_max=179.9166717529297    \ndepth_min=0.493    \ndepth_max=0.4942  \n# Empty or non set means all variables  \n# 1 or more variables separated by a coma and identified by their standard name  \nvariable=sea_water_potential_temperature,sea_surface_height_above_geoid \n# Accept relative or absolute path. The dot character \".\" is the current folder  \nout_dir=./out_dir  \nout_name=test.nc  \n\n# Logging\n# https://docs.python.org/3/library/logging.html#logging-levels  \n# log_level=X {CRITICAL:50, ERROR:40, WARNING:30, INFO:20, DEBUG:10, TRACE:0}   \nlog_level=0   \n\n# block_size block used to download file (integer expressing bytes) default=65535\n# block_size=65535  \nsocket_timeout=120000  \n\n# Http proxy to connect to Motu server\n# proxy_server=proxy.domain.net:8080  \n# proxy_user=john  \n# proxy_pwd=secret  \n``` \n\nA configuration file in another location can be specified by the `--config-file` option. It is even possible to split the configuration into two or more files. This is useful, for example, to keep server configuration in one file and dataset configuration in another:\n```  \n./motuclient.py --config-file ~/server.ini --config-file ~/mercator.ini\n``` \nIf by chance there is a parameter listed in both configuration files, the value in the last file (e.g. `mercator.ini`) is the one actually used.\n\nNote that the password must be encoded in UTF-8.  \nIf it contains UTF-8 special characters, on Windows host only, you only have to double the \"percent\" character. If password is CMS2017@%! then enter   \n\n```  \npwd = CMS2017@%%! \n```  \n\nExample of server.ini on Windows host only, with user password is   \n__Password__:  \n```  \nloginForTesting2 &~#\"'{([-|`_\\^@)]=}\u00a8^\u00a3$ \u00b5*\u00a7!/:.;?,%<>  \n```  \n\n__server.ini__:  \n```  \n[Main]\nuser = loginForTesting2@groupcls.com\npwd = loginForTesting2 &~#\"'{([-|`_\\^@)]=}\u00a8^\u00a3$ \u00b5*\u00a7!/:.;?,%%<>\nauth-mode = cas\nmotu = http://motuURL:80/motu-web/Motu\nout_dir = J:/dev/CMEMS-CIS-MOTU/git/motu-validkit/output/04-python-client/MOTU-208\n```  \n\nExample of server.ini on Linux host only, with user password is   \n__Password__:\n```  \nloginForTesting2 &~#\"'{([-|`_\\^@)]=}\u00a8^\u00a3$ \u00b5*\u00a7!/:.;?,%<>  \n```  \n\n__server.ini__:\n```   \n[Main]\nuser = loginForTesting2@groupcls.com\npwd = loginForTesting2 &~#\"'{([-|`_\\^@)]=}\u00a8^\u00a3$ \u00b5*\u00a7!/:.;?,%<>\nauth-mode = cas\nmotu = http://motuURL:80/motu-web/Motu\nout_dir = J:/dev/CMEMS-CIS-MOTU/git/motu-validkit/output/04-python-client/MOTU-208\n```  \n\n# <a name=\"Usage\">Usage</a>  \nStarts the motu python client.  \n\n## <a name=\"UsagePIP\">Usage from PIP installation</a>  \nSince version 1.8.0:  \n```  \nmotuclient -h  \nmotuclient [options]\n```  \nBefore version 1.8.0:  \n```  \npython -m motu-client -h  \npython -m motu-client [options]\n```  \n  \n[Options](#UsageOptions) are listed below.  \nMethod to used when it has been installed with [PIP method](#InstallationPIP).  \n\n\n## <a name=\"UsageTGZ\">Usage from tar.gz installation</a>  \n```  \n./motuclient.py  -h  \nmotuclient.py [options]\n```  \nMethod to used when it has been installed with [tar.gz method](#InstallationTGZ).  \nUsefull if host is offline and has no Internet access.\n\n### <a name=\"UsageOptions\">__Options:__</a>  \n\n\n* __-h, --help__            show this help message and exit  \n* __-q, --quiet__           print logs with level WARN in stdout, used to prevent any output in stdout  \n* __--noisy__               print logs with level TRACE in stdout  \n* __--verbose__             print logs with level DEBUG in stdout  \n* __--version__             show program's version number and exit, [setuptools](#InstallationSetuptools) python package has be installed to run it successfully    \n\n* __--proxy-server=PROXY_SERVER__ Proxy server (url) used to contact Motu \n* __--proxy-user=PROXY_USER__ Proxy user name (string)\n* __--proxy-pwd=PROXY_PWD__ Proxy password (string)  \n\n* __--auth-mode=AUTH_MODE__  the authentication mode: [default: cas]  \n  * __none__ for no authentication\n  * __basic__ for basic authentication\n  * __cas__ for Central Authentication Service  \n* __-u USER, --user=USER__  User name (string) for the specified authentication mode\n* __-p PWD, --pwd=PWD__ User password (string) for the specified authentication mode. UTF8 special characters can be used but contraints depending of your operating system must be applyied:\n\n  * __Windows__ users, be careful if your password contain once of the following characters:\n    * __percent__: From a Windows batch command, if your password contains a percent character, double the percent character: If password is CMS2017@%! then enter   \n    \n    ```\n    -u username-p CMS2017@%%! \n    ```  \n\n    * __space__: From a Windows batch command, if your password contains a space character, set password between double quotes: If password is CMS2017 @%! then enter  \n    \n    ```\n    -u username-p \"CMS2017 @%%!\"\n    ```  \n  \n  \n    * __double quotes__: From a Windows batch command, if your password contains a double quotes character, double the double quotes character: If password is CMS2017\"@%! then enter  \n    \n    ```\n    -u username-p \"CMS2017\"\"@%%!\"\n    ```  \n  \n  * __Linux__ users, be careful if your password contain once of the following characters:\n    * __space__: From a Linux bash shell command, if your password contains a space character, set password between double quotes: If password is CMS2017 @% then enter  \n    \n    ```\n    -u username-p \"CMS2017 @%\"\n    ```  \n  \n    * __exclamation point__: From a Linux bash shell command, if your password contains an exclamation point character, cut the password in two double quotes strings, and append the exclamation point between simple quote: If password is CMS!2017@% then enter  \n    \n    ```\n    -u username-p \"CMS\"'!'\"2017@%\" \n    ```  \n  \n    * __double quotes__: From a Linux bash shell command, if your password contains a double quotes character, escape the double quotes character by prefixing it with backslash: If password is CMS2017\"@% then enter  \n    \n    ```\n    -u username-p \"CMS2017\\\"@%\"\n    ```  \n\n    * __grave accent__: From a Linux bash shell command, if your password contains a grave accent character, escape the grave accent character by prefixing it with backslash: If password is CMS2017`@% then enter  \n    \n    ```\n    -u username-p \"CMS2017\\`@%\"\n    ``` \n  \n* __-m MOTU, --motu=MOTU__ Motu server url, e.g. \"-m http://localhost:8080/motu-web/Motu\"  \n* __-s SERVICE_ID, --service-id=SERVICE_ID__ The service identifier (string), e.g. -s Mercator_Ocean_Model_Global-TDS  \n* __-d PRODUCT_ID, --product-id=PRODUCT_ID__ The product (data set) to download (string), e.g. -d dataset-mercator-psy4v3-gl12-bestestimate-uv  \n* __-t DATE_MIN, --date-min=DATE_MIN__ The min date with optional hour resolution (string following format YYYY-MM-DD [HH:MM:SS]), e.g. -t \"2016-06-10\" or -t \"2016-06-10 12:00:00\". Be careful to not forget double quotes around the date.     \n* __-T DATE_MAX, --date-max=DATE_MAX__ The max date with optional hour resolution (string following format YYYY-MM-DD  [HH:MM:SS ]), e.g. -T \"2016-06-11\" or -T \"2016-06-10 12:00:00\".  Be careful to not forget double quotes around the date.      \n* __-y LATITUDE_MIN, --latitude-min=LATITUDE_MIN__ The min latitude (float in the interval  [-90 ; 90 ]), e.g. -y -80.5  \n* __-Y LATITUDE_MAX, --latitude-max=LATITUDE_MAX__ The max latitude (float in the interval  [-90 ; 90 ]), e.g. -Y 80.5   \n* __-x LONGITUDE_MIN, --longitude-min=LONGITUDE_MIN__ The min longitude (float), e.g. -x -180      \n* __-X LONGITUDE_MAX, --longitude-max=LONGITUDE_MAX__ The max longitude (float), e.g. -X 355.5      \n* __-z DEPTH_MIN, --depth-min=DEPTH_MIN__ The min depth (float in the interval  [0 ; 2e31 ] or string 'Surface'), e.g. -z 0.49  \n* __-Z DEPTH_MAX, --depth-max=DEPTH_MAX__ The max depth (float in the interval  [0 ; 2e31 ] or string 'Surface'), e.g. -Z 0.50\n* __-v VARIABLE, --variable=VARIABLE__ The variable (list of strings), e.g. -v salinity -v sst  \n* __-S, --sync-mode__ Sets the download mode to synchronous (not recommended). If this parameter is set, Motu server is called with parameter [console](https://github.com/clstoulouse/motu#download-product). Otherwise\n, Motu server is called with parameter [status](https://github.com/clstoulouse/motu#download-product).   \n\n\n* __-o OUT_DIR, --out-dir=OUT_DIR__ The output dir where result (download file) is written (string). If it starts with \"console\", behaviour is the same as with --console-mode.       \n* __-f OUT_NAME, --out-name=OUT_NAME__ The output file name (string)  \n* __--console-mode__ Write result on stdout. In case of an extraction, write the nc file http URL where extraction result can be downloaded. In case of a getSize or a describeProduct request, display the XML result.\n\n* __-D, --describe-product__ Get all updated information on a dataset. Output is in XML format, [API details](https://github.com/clstoulouse/motu#describe-product)  \n* __--size__ Get the size of an extraction. Output is in XML format, [API details](https://github.com/clstoulouse/motu#get-size)\n\n* __--block-size=BLOCK_SIZE__ The block used to download file (integer expressing bytes), default=65535 bytes  \n* __--socket-timeout=SOCKET_TIMEOUT__ Set a timeout on blocking socket operations (float expressing seconds)  \n* __--user-agent=USER_AGENT__ Set the identification string (user-agent) for HTTP requests. By default this value is 'Python-urllib/x.x' (where x.x is the version of the python interpreter)  \n* __--outputWritten=OUTPUT_FORMAT__ Set the output format (file type) of the returned file for download requests. By default this value is 'netcdf' and no other value is supported.  \n  \n  \n# <a name=\"UsageExamples\">Usage examples</a>   \n\nIn the following examples, variable ${MOTU\\_USER} and ${MOTU\\_PASSWORD} are user name and user password used to connect to the CAS server for single sign on.  \n${MOTU\\_SERVER\\_URL} is the URL on the MOTU HTTP(s) server, for example http://localhost:8080/motu-web/Motu.  \nCommands \"./motuclient.py\" has to be replaced by \"python -m motuclient\" if it has been installed with [PIP method](#UsagePIP).  \n\n\n## <a name=\"UsageExamplesDownload\">Download</a>  \n\n### Download and save extracted file on the local machine\nThis command writes the extraction result data in file: /data/test.nc  \n\n```  \n./motuclient.py --verbose --auth-mode=none -m ${MOTU_SERVER_URL} -s HR_MOD_NCSS-TDS -d HR_MOD -z 0.49 -Z 0.50 -x -70 -X 25 -y -75 -Y 10 -t \"2016-06-10\" -T \"2016-06-11\" -v salinity -o /data -f test.nc\n``` \n\n### Display on stdout the HTTP(s) URL of the NC file available on the Motu server\nThe HTTP(s) URL is displayed on stdout. This URL is a direct link to the file which is available to be downloaded.  \n\n```  \n./motuclient.py --quiet --auth-mode=cas -u ${MOTU_USER} -p ${MOTU_PASSWORD}  -m ${MOTU_SERVER_URL} -s HR_MOD_NCSS-TDS -d HR_MOD -z 0.49 -Z 0.50 -x -70 -X 25 -y -75 -Y 10 -t \"2016-06-10\" -T \"2016-06-11\" -v salinity -o console\n``` \n\n## <a name=\"UsageExamplesGetSize\">GetSize</a>  \nSee [https://github.com/clstoulouse/motu#ClientAPI_GetSize](https://github.com/clstoulouse/motu#ClientAPI_GetSize) for more details about XML result.  \n\n### Get the XML file which contains the extraction size on the local machine\n```  \n./motuclient.py --size --auth-mode=cas -u ${MOTU_USER} -p ${MOTU_PASSWORD}  -m ${MOTU_SERVER_URL} -s HR_MOD_NCSS-TDS -d HR_MOD -z 0.49 -Z 0.50 -x -70 -X 25 -y -75 -Y 10 -t \"2016-06-10\" -T \"2016-06-11\" -v salinity -o /data -f getSizeResult.xml\n``` \n\n### Display the extraction size as XML on stdout\n```  \n./motuclient.py --quiet --size --auth-mode=cas -u ${MOTU_USER} -p ${MOTU_PASSWORD}  -m ${MOTU_SERVER_URL} -s HR_MOD_NCSS-TDS -d HR_MOD -z 0.49 -Z 0.50 -x -70 -X 25 -y -75 -Y 10 -t \"2016-06-10\" -T \"2016-06-11\" -v salinity -o console\n``` \n\n\n## <a name=\"UsageExamplesDescribeProduct\">DescribeProduct</a>  \nSee [https://github.com/clstoulouse/motu#describe-product](https://github.com/clstoulouse/motu#describe-product) for more details about XML result.  \n\n### Get the XML file which contains the dataset description on the local machine\n```  \n./motuclient.py -D --auth-mode=cas -u ${MOTU_USER} -p ${MOTU_PASSWORD}  -m ${MOTU_SERVER_URL} -s HR_MOD_NCSS-TDS -d HR_MOD -o /data -f describeProductResult.xml\n``` \n\n### Display the dataset description XML result on stdout\n```  \n./motuclient.py --quiet -D --auth-mode=cas -u ${MOTU_USER} -p ${MOTU_PASSWORD}  -m ${MOTU_SERVER_URL} -s HR_MOD_NCSS-TDS -d HR_MOD -o console\n``` \n\n\n\n\n# <a name=\"Licence\">Licence</a> \nThis library is free software; you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation; either version 2.1 of the License, or (at your option) any later version.  \n  \nThis library is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.  \n  \nYou should have received a copy of the GNU Lesser General Public License along with this library; if not, write to the Free Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.  \n\n# <a name=\"Troubleshooting\">Troubleshooting</a>  \n# <a name=\"TroubleshootingPIPCache\">Unable to download the latest version watched on GitHub from PIP</a>\nExample:  \n```  \npip install motuclient  \nCollecting motuclient  \n  Using cached https://test-files.pythonhosted.org/packages/4a/7d/41c3bdd973baf119371493c193248349c9b7107477ebf343f3889cabbf48/motuclient-X.Y.Z.zip  \nInstalling collected packages: motuclient  \n  Running setup.py install for motuclient ... done  \nSuccessfully installed motuclient-X.Y.Z  \n```  \n  \nClear your PIP cache: On Windows, delete the folder %HOMEPATH%/pip. On archlinux pip cache is located at ~/.cache/pip.\nAfter re run the command:  \n```  \npip install motuclient  \nCollecting motuclient  \n  Using https://test-files.pythonhosted.org/packages/4a/7d/41c3bdd973baf119371493c193248349c9b7107477ebf343f3889cabbf48/motuclient-X.Y.Z.zip  \nInstalling collected packages: motuclient  \n  Running setup.py install for motuclient ... done  \nSuccessfully installed motuclient-X.Y.Z  \n``` \n\n# <a name=\"TroubleshootingWinArgErr\">From Windows, Parameter error</a>\nFrom Windows, the command \"motuclient.py --version\" returns an error.  \n10:44:24 [ERROR] Execution failed: [Excp 13] User (option 'user') is mandatory when 'cas' authentication is set. Please provide it.\n\n__Analyse:__  \nThis issue comes from the fact that Windows command line does not pass parameters to python command.  \n  \n__Solution:__  \n``` \nEdit the Windows Registry Key \"HKEY_CLASSES_ROOT\\py_auto_file\\shell\\open\\command\" and append at the end of the value %*  \nExemple: \"C:\\dvltSoftware\\python\\Python27\\python.exe\" \"%1\" %*  \n``` \n\n# <a name=\"TroubleshootingPythonVersionErr\">Error on all motuclient commands</a>\nFor example the command \"motuclient.py --version\" returns this kind of error:  \n``` \nTraceback (most recent call last):\n  File \"C:\\dvlt\\python\\python2.7.18\\Scripts\\motuclient-script.py\", line 11, in <module>\n    load_entry_point('motuclient==3.0.0.post1', 'console_scripts', 'motuclient')()\n  File \"c:\\dvlt\\python\\python2.7.18\\lib\\site-packages\\motuclient\\motuclient.py\", line 352, in main\n    initLogger()\n  File \"c:\\dvlt\\python\\python2.7.18\\lib\\site-packages\\motuclient\\motuclient.py\", line 336, in initLogger\n    logging.addLevelName(utils_log.TRACE_LEVEL, 'TRACE')\nAttributeError: 'module' object has no attribute 'TRACE_LEVEL'\n``` \n\n__Analyse:__  \nThis issue comes from a too old python installation version.  You must use Python 3.7.10 or higher.  \n  \n__Solution:__  \nFind and install the Python 3 distribution for your operating system.  "
 },
 {
  "repo": "castelao/GSW-rs",
  "language": "Rust",
  "readme_contents": "# TEOS-10 GSW Oceanographic Toolbox in Rust\n\nGSW for microcontrollers.\n\n## Talks\n\nWe presented about goals and progress (as of Apr 2022) at the\n[SEA Improving Scientific Software 2022](https://sea.ucar.edu/conference/2022),\n[slides available here](https://github.com/castelao/GSW-rs/tree/talk_SEAS2022/doc/talks).\n\n## Minimum supported Rust version\n\nCurrently the minimum supported Rust version is 1.46.0\n\n## License\n\nLicensed under either of\n\n * Apache License, Version 2.0\n   ([LICENSE-APACHE](LICENSE-APACHE) or http://www.apache.org/licenses/LICENSE-2.0)\n * MIT license\n   ([LICENSE-MIT](LICENSE-MIT) or http://opensource.org/licenses/MIT)\n\nat your option.\n\n## Contribution\n\nUnless you explicitly state otherwise, any contribution intentionally submitted\nfor inclusion in the work by you, as defined in the Apache-2.0 license, shall be\ndual licensed as above, without any additional terms or conditions.\n"
 },
 {
  "repo": "eiszapfen2000/tgda",
  "language": "C",
  "readme_contents": "# Ocean Surface Generation and Rendering\nA real-time capable implementation of Tessendorf's choppy wave algorithm [Tessendorf1999a], augmented with wave spectrum models from oceanographic research. Master's thesis and respective poster are to be found [here](https://www.cg.tuwien.ac.at/research/publications/2018/GAMPER-2018-OSG/).\n\n## Screenshots\n\n|   |   |\n|---|---|\n|![alt text](Branches/OpenGL33Core/DATU/figures/21-06-2018_10-44-51_complete.png)|![alt text](Branches/OpenGL33Core/DATU/figures/21-06-2018_12-48-51_complete.png)|\n|![alt text](Branches/OpenGL33Core/DATU/figures/28-05-2018_10-56-10_complete.png)|![alt text](Branches/OpenGL33Core/DATU/figures/21-06-2018_15-47-53_complete.png)|\n\n## Videos\n\n| Overview  | Lods |\n| ------------- | ------------- |\n| [![Overview](https://img.youtube.com/vi/op_NVMRhpL0/0.jpg)](https://www.youtube.com/watch?v=op_NVMRhpL0) | [![Lods](https://img.youtube.com/vi/RiBrIPSPOxo/0.jpg)](https://www.youtube.com/watch?v=RiBrIPSPOxo) |\n\n## Ocean Surface Synthesis\nWe implemented the following oceanographic wave spectra:\n* Pierson-Moskowitz [Pierson1964a]\n* JONSWAP [Hasselmann1973a]\n* Donelan [Donelan1985a]\n* Elfouhaily [Elfouhaily1997a]\n\nFor Pierson-Moskowitz and JONSWAP we employ the directional distribution as introduced by Mitsuyasu et al. [Mitsuyasu1975a], and improved by Hasselmann et al. [Hasselmann1980a]. Donelan and Elfouhaily each incorporate their own directional distribution. \n\n## Fourier Transform\nWe compute the Discrete Fourier Transform with the single precision variant of [FFTW](http://www.fftw.org). The Fast Fourier Transform works fastest on power-of-two resolutions, therefore we restrict the ocean's resolution to such resolutions.\n\n## Ocean Surface Rendering\n* Projected Grid [Johanson2004a]\n* Seamless Ocean Surface Lighting [Bruneton2010a]\n* Ocean Whitecaps [Dupuy2012a]\n\n## Skylight\nPreetham sky light [Preetham1999a, Section 3.1] and sun light [Preetham1999a, Section 3.2].\n\n## Tonemapping\nFor tonemapping purposes we implemented the global tonemapping operator by Reinhard et al. [Reinhard2002a, Equation 4], and the temporal luminance adaptation algorithm by Krawczyk et al. [Krawczyk2005a, Equations 5, 6, 7, 12]. The necessary color space conversions are done according to http://www.brucelindbloom.com/Math.html.\n\n## Literature\n[Bruneton2010a] Eric Bruneton, Fabrice Neyret, and Nicolas Holzschuch. Real-time realistic ocean lighting using seamless transitions from geometry to brdf. Computer Graphics Forum, 29(2):487\u2013496, 2010.\n\n[Donelan1985a] M. A. Donelan, J. Hamilton, andW. H. Hui. Directional spectra of wind-generated waves. Phil. Trans. Roy. Soc. London A, 315:509\u2013562, 1985.\n\n[Dupuy2012a] Jonathan Dupuy and Eric Bruneton. Real-time animation and rendering of ocean whitecaps. In SIGGRAPH Asia 2012 Technical Briefs, SA \u201912, pages 15:1\u201315:3. ACM, 2012.\n\n[Elfouhaily1997a] T. Elfouhaily, B. Chapron, K. Katsaros, and D. Vandemark. A unified directional spectrum for long and short wind-driven waves. J. Geophys. Res., 102(C7):15781\u201315796, 1997.\n\n[Hasselmann1973a] K. Hasselman, T. P. Barnett, E. Bouws, D. E. Carlson, and P. Hasselmann. Measurements of wind-wave growth and swell decay during the joint north sea wave project (jonswap). Deutsche Hydrographische Zeitschrift, 8(12), 1973.\n\n[Hasselmann1980a] D. E. Hasselmann, M. Dunckel, and J. A. Ewing. Directional wave spectra observed during JONSWAP 1973. J. Phys. Oceanogr., 10:1264\u20131280, 1980.\n\n[Johanson2004a] Claes Johanson. Real-time water rendering - introducing the projected grid concept. Master\u2019s thesis, Department of Computer Science, Lund University, 2004.\n\n[Krawczyk2005a] Grzegorz Krawczyk, Karol Myszkowski, and Hans-Peter Seidel. Perceptual effects in real-time tone mapping. In Proceedings of the 21st Spring Conference on Computer Graphics, SCCG \u201905, pages 195\u2013202. ACM, 2005.\n\n[Mitsuyasu1975a] H. Mitsuyasu, F. Tasai, T. Suhara, S. Mizuno, M. Ohkusu, T. Honda, and K. Rikiishi. Observations of the Directional Spectrum of Ocean Waves Using a Cloverleaf Buoy. Journal of Physical Oceanography, 5:750, 1975.\n\n[Pierson1964a] Willard J. Pierson and Lionel Moskowitz. A proposed spectral form for fully developed wind seas based on the similarity theory of S. A. Kitaigorodskii. J. Geophys. Res., 69(24), December 1964.\n\n[Preetham1999a] A. J. Preetham, Peter Shirley, and Brian Smits. A practical analytic model for daylight. In Proceedings of the 26th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH \u201999, pages 91\u2013100. ACM Press/Addison-Wesley Publishing Co., 1999.\n\n[Reinhard2002a] Erik Reinhard, Michael Stark, Peter Shirley, and James Ferwerda. Photographic tone reproduction for digital images. In Proceedings of the 29th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH \u201902, pages 267\u2013276. ACM, 2002.\n\n[Tessendorf1999a] Jerry Tessendorf. Simulating ocean water. In SIGGRAPH course notes. ACM, 1999.\n"
 },
 {
  "repo": "tompc35/oceanography-notebooks",
  "language": "Jupyter Notebook",
  "readme_contents": "# oceanography-notebooks\nIPython/Jupyter notebooks showing examples of oceanographic data analysis.\n\n### Physical oceanography\n\n[Line P hydrographic data](Line_P/plot_line_p_profiles.ipynb)\n\n[Abyssal recipes](Abyssal_Recipes/Abyssal_Recipes.ipynb)\n\n### Loading and plotting data\n\n[MLML\\_seawater.ipynb](MLML_seawater.ipynb)\n\n[Monterey Bay bathymetry](montereybay_bathymetry.ipynb)\n\n[ocean\\_color\\_netcdf.ipynb](ocean_color_netcdf.ipynb)\n\n### Time series analysis\n\n[sst\\_harmonic\\_fit.ipynb](sst_harmonic_fit.ipynb)\n\n### Statistics and probablity distributions\n\n[generating\\_t_and\\_chi2.ipynb](generating_t_and_chi2.ipynb)\n\n[central\\_limit\\_theorem.ipynb](central_limit_theorem.ipynb)"
 },
 {
  "repo": "evb123/oceanography-visualizations",
  "language": "MATLAB",
  "readme_contents": "# oceanography-visualizations\nExamples of data visualizations created for my master's dissertation (2018).\n\nData engineering, analysis and visualisation for dissertation work was performed in MATLAB with use of the Thermodynamic Equation of Seawater 2010 Gibbs-SeaWater Oceanographic [Toolbox](http://www.teos-10.org/software.htm) \n\nPlease navigate to the [wiki](https://github.com/evb123/oceanography-visualizations/wiki) for easy viewing of visualizations and code snippets.\n"
 },
 {
  "repo": "lesommer/2017-lectures-godae-ocean-view",
  "language": "HTML",
  "readme_contents": "# 2017-lectures-godae-ocean-view\nLecture material for GODAE Ocean View summer school\n"
 },
 {
  "repo": "cesar-rocha/niwqg",
  "language": "Python",
  "readme_contents": "Code for a special class of solutions of the Xie & Vanneste (2015) coupled model in a doubly periodic domain.\n\n<img src=\"./docs/figs/organogram.png\" alt=\"NIWQG organogram\"  width=\"500\">\n\n# Installation\nThis software is written in `python3` and depends on `numpy` and `h5py`. I strongly\nrecommend the python3 pre-packaged on [Anaconda](https://www.continuum.io/downloads).\nThis package comes with `numpy`. To install h5py, use Anaconda's package manager:\n```bash\nconda install h5py\n```\n\nFor a more comprehensive installation or update with conda, run\n```\nmake install\n```\n\n### Installing `niwqg`\n\nIf you're a git user, fork and clone your fork of the `niwqg` repository.\nAlternatively, just download the repository by clicking on the link on the\nupper-right corner of this page.\n\nInside the root niwqg directory, install the package:\n\n```bash\npython setup.py install\n```\n\nIf you plan to make changes to the code, then setup the development mode:\n\n```bash\npython setup.py develop\n```\n\n### Testing `niwqg`\nIf you have [`pytest`](https://docs.pytest.org/en/latest/), then run\n```\nmake test\n```\nAlternatively, to in the test directory\n```bash\ncd niwqg/tests\n```\nand run all unit tests using nose:\n```bash\nnosetests\n```\n\nYou can also run this simple [example](./examples/LambDipole_CoupledModel.ipynb)\nand verify the energy budget.\n\n# Documentation\nSome documentation is available [here](docs/Index.ipynb).  Also  check the docstrings of\nclasses and methods.\n\n# Development\nThe code is under rapid development by [@crocha700](https://github.com/crocha700)\nas part of the project \"Stimulated Loss of Balance\" (SLOB) with\n[@glwagner](https://github.com/glwagner) and [@wry55](https://github.com/wry55).\n\nPlease, submit contributions via pull-request of\na cut-off branch (not master).\n\n# Funding\nThis project is funded by the [National Aeronautics and Space Administration](https://www.nasa.gov) under grant NNX16AO5OH.\n"
 },
 {
  "repo": "mvdh7/biogeochem-phi",
  "language": "MATLAB",
  "readme_contents": "# biogeochem-phi\n\n[![DOI](https://zenodo.org/badge/95231697.svg)](https://zenodo.org/badge/latestdoi/95231697)\n\n**Documentation:** [biogeochem-phi.readthedocs.io](https://biogeochem-phi.readthedocs.io/en/latest/)\n\nThese scripts should be cited as:\n\nHumphreys MP, Daniels CJ, Wolf-Gladrow DA, Tyrrell T, & Achterberg EP (2017): \"On the influence of marine biogeochemical processes over CO<sub>2</sub> exchange between the atmosphere and ocean\", *Marine Chemistry* 199, 1-11, <a href=\"https://doi.org/10.1016/j.marchem.2017.12.006\">doi:10.1016/j.marchem.2017.12.006</a>.\n\n## bgc_sliso()\n  * Calculates isocapnic quotient (Q) from *p*CO<sub>2</sub>, DIC, temperature and salinity\n  * Requires functions from https://github.com/mvdh7/oceancarb-constants\n\n## bgc_phi()\n  * Calculates Phi from Q (output from `bgc_sliso()`) and biogeochemical process vector\n\n## bgc_normalise()\n  * Optionally normalises a biogeochemical process vector to unit length\n"
 },
 {
  "repo": "NOC-MSM/SEAsia",
  "language": "Fortran",
  "readme_contents": "******************\n# Relocatable NEMO\n******************\n\nAn example configuration of SE Asia, demonstrating how to setup new regional domains in the NEMO framework.\nThis model configuration has been developed through the ACCORD (Addressing Challenges of Coastal Communities through Ocean Research for Developing Economies) Project, funded by [Natural Environment Research Council, under a National Capability Official Development Assistance](http://gotw.nerc.ac.uk/list_full.asp?pcode=NE%2FR000123%2F1).\n\n*************************************************\n## NEMO regional configuration of South East Asia\n*************************************************\n\n### Model Summary\n\nA specific region of focus includes exploring South East Asia (75E to 135E and -20N to +25N)\n\nThe model grid has 1/12&deg; lat-lon resolution and 75 hybrid sigma-z-partial-step vertical levels. Featuring:\n\n* FES2014 tides\n* Boundary conditions from ... (in prog.)\n* Freshwater forcing (in prog.)\n* ERA5 wind and sea level pressure (in prog.)\n\n![SE Asia bathymetry](https://github.com/NOC-MSM/SEAsia/wiki/FIGURES/ACCORD_SEAsia_bathy.png)\n\n### Model Setup\n\n\nThe following process is followed to build and get started with this configuration\n\n``git clone https://github.com/NOC-MSM/SEAsia.git``\n\nThen follow descritptions in: https://github.com/NOC-MSM/SEAsia/wiki\n\nThe example is based on NEMO v4.0.6 and XIOS v2.5:\n\n\n\n### Experiment Summary\n\n* ``EXP_barotropicTide``\nOnly tidal forcing. Constant T and S\n\n* ``EXP_unforced``\nNo forcing. Stratification varies only with depth. Start from rest.\n\n\n...\n\n### Repository structure\n\n* ...\n"
 },
 {
  "repo": "clstoulouse/motu",
  "language": "Java",
  "readme_contents": "# Motu Project\n@author <tjolibois@cls.fr>: Project manager & Product owner  \n@author <smarty@cls.fr>: Scrum master, Software architect, Quality assurance, Continuous Integration manager   \n\n>How to read this file? \nUse a markdown reader: \nplugins [chrome](https://chrome.google.com/webstore/detail/markdown-preview/jmchmkecamhbiokiopfpnfgbidieafmd?utm_source=chrome-app-launcher-info-dialog) exists (Once installed in Chrome, open URL chrome://extensions/, and check \"Markdown Preview\"/Authorise access to file URL.), \nor for [firefox](https://addons.mozilla.org/fr/firefox/addon/markdown-viewer/)  (anchor tags do not work)\nand also plugin for [notepadd++](https://github.com/Edditoria/markdown_npp_zenburn).\n\n>Be careful: Markdown format has issue while rendering underscore \"\\_\" character which can lead to bad variable name or path.\n\n\n# Summary\n* [Overview](#Overview)\n* [Architecture](#Architecture)\n  * [Overall](#ArchitectureOverall)\n       * [One instance](#ArchitectureOneInstance)  \n\t   * [Scalability](#ArchitectureScalability)  \n  * [Interfaces](#ArchitectureInterfaces)\n     * [Server interfaces](#ArchitectureInterfacesServer)  \n     * [External interfaces with other systems or tools](#ArchitectureInterfacesExternal)  \n  * [Design](#ArchitectureDesign)\n  * [Design details](#ArchitectureDesignD)  \n     * [Motu-web project](#ArchitectureDesignDMW)\n     * [Other projects](#ArchitectureDesignDOthers)\t \n  * [Algorithm details](#ArchiAlgo)  \n     * [Downloading 1 point](#ArchiAlgoDownloading1Point)\n* [Development](#Development)\n  * [Source code](#DEVSRC)\n  * [Development environment](#DEV)\n  * [Compilation](#COMPILATION)\n  * [Packaging](#Packaging)\n* [Installation](#Installation)\n  * [Prerequisites](#InstallPrerequisites)\n     * [Hardware settings](#InstallPrerequisitesHard)\n     * [Software settings](#InstallPrerequisitesSoft)\n\t * [External interfaces](#InstallPrerequisitesExternalInterfaces)\n     * [Several Motu instances on a same host](#InstallPrerequisitesSeveralsInstances)\n  * [Upgrade from Motu v2.x](#UpgradeFromMotu2x)\n  * [Install Motu from scratch](#InstallFromScratch)\n  * [Check installation](#InstallCheck)\n  * [CDO manual installation](#InstallCDO)\n  * [Installation folder structure](#InstallFolders)\n  * [Setup a frontal Apache HTTPd server](#InstallFrontal)\n  * [Security](#InstallSecurity)\n     * [Run Motu as an HTTPS Web server](#InstallSecurityRunHTTPs)\n\t * [Motu and Single Sign On](#InstallSecuritySSO)\n  * [Install a scalable Motu over several instances](#InstallationScalability)\n* [Configuration](#Configuration)\n  * [Configuration directory structure](#ConfigurationFolderStructure)\n  * [Business settings](#ConfigurationBusiness)\n  * [System settings](#ConfigurationSystem)\n  * [Log settings](#LogSettings)\n  * [Theme and Style](#ThemeStyle)\n* [Operation](#Operation)\n  * [Start, Stop and other Motu commands](#SS)\n  * [Monitor performance](#ExpMonitorPerf)\n  * [Logbooks](#Logbooks)\n  * [Add a dataset](#AdminDataSetAdd)\n  * [Tune the dataset metadata cache](#AdminMetadataCache)\n  * [Debug view](#ExploitDebug)\n  * [Clean files](#ExploitCleanDisk)\n  * [Log Errors](#LogCodeErrors)\n     * [Action codes](#LogCodeErrorsActionCode)  \n     * [Error types](#LogCodeErrorsErrorType)  \n* [Motu clients & REST API](#ClientsAPI)\n  * [Python client](#ClientPython)\n  * [OGC WCS API](#OGC_WCS_API)\n  * [REST API](#ClientRESTAPI)\n* [Docker image](#Docker)\n  * [Docker image content](#DockerContent)\n  * [Docker mounted directories](#DockerDirectories)\n  * [Run Motu with Docker](#DockerRun)  \n  \n  \n# <a name=\"Overview\">Overview</a>  \nMotu is a robust web server allowing the distribution of met/ocean gridded data files through the web. \nSubsetter allows user to extract the data of a dataset, with geospatial, temporal and variable criterias. \nThus, user download only the data of interest.  \nA graphic web interface and machine to machine interfaces allow to access data and information on data (metadata).\nThe machine-to-machine interface can be used through a client written in python, freely available here https://github.com/clstoulouse/motu-client-python.\nOutput data files format can be netCDF3 or netCDF4.  \nAn important characteristic of Motu is its robustness: in order to be able to answer many users without crashing, Motu manages its incoming requests in a queue server.  \nThe aim is to obtain complete control over the requests processing by balancing the processing load according to criteria (volume of data to extract, number of requests to fulfill \nfor a user at a given time, number of requests to process simultaneously).  \nMoreover, Motu implements a request size threshold. Motu masters the amount of data to extract per request by computing, without any data processing, the result data size of the request.  \nBeyond the allowed threshold, every request is rejected. The threshold is set in the configuration file.\nMotu can be secured behind an authentication server and thus implements authorization. A CAS server can implement the authentication. \nMotu receives with authentication process user information, including a user profile associated with the account. \nMotu is configured to authorize or not the user to access the dataset or group of datasets which user is trying to access.  \nFor administrators, Motu allows to monitor the usage of the server: the logs produced by Motu allow to know who (login) requests what (dataset) and when, with extraction criterias.\n\n# <a name=\"Architecture\">Architecture</a>  \nMotu is a Java Web Application running inside the Apache Tomcat application server. Motu can be run as a [single application](#ArchitectureSingleInstance) or can be scaled over [several instances](#ArchitectureScalability).\n\n\n## <a name=\"ArchitectureOverall\">Architecture overall</a>  \n\n### <a name=\"ArchitectureOneInstance\">Architecture single instance</a>  \nThe clients [\"motu-client-python\"](#ClientPython) or an HTTP client like a [web browser](#ClientRESTAPI) are used to connect to Motu services.  \nA frontal web, [Apache HTTPd](#InstallFrontal) for example, is used as a reverse proxy to redirect request to Motu server and also to serve the [downloaded](#motuConfig-downloadHttpUrl) data from Motu [download folder](#motuConfig-extractionPath).  \nMotu server, runs on a Apache Tomcat server and can serve files either directly [\"DGF\"](#BSconfigServiceDatasetType) or by delegating extraction to Thredds server with NCSS or OpenDap [protocols](#BSconfigServiceDatasetType).  \nA NFS server is used to share the netcdf files between Thredds and Motu DGF when they are not deployed on the same host.  \nAn (SSO CAS server)[#ConfigurationSystemCASSSO] is used for the authentication of users but Motu can also be deployed without any authentication system.  \nThe Apache HTTPd, on the top right corner is used to [serve the graphic chart](#InstallPublicFilesOnCentralServer) when several Motu Web server are deployed.\n\nThe schema below shows an example of Motu scalability architecture. The \"i1, i2\" are the Motu server deployed. They have to share the same [business configuration file](#ConfigurationBusiness) and the [download folder](#motuConfig-extractionPath).      \n\n![Software architecture](./motu-parent/src/doc/softwareArchitecture.png \"Motu software architecture, one instance\")\n\n\n### <a name=\"ArchitectureScalability\">Architecture scalability</a>  \nTo run Motu over several instances, a [Redis server](#RedisServerConfig) has to be deployed in order to share to request id and status. The download folder of Motu has also to be shared between the different Motu instances.  \nIt can be on a NFS server or a GLusterFS server.  \nThe frontal web server \"Apache HTTPd\" must serve the downloaded files and implement the load balencer between all Motu instances.   \nAll other servers, CAS, NFS remains as on the single instance architecture.   \nThe same source code is used to run Motu with a single architecture or with several instances. It is just done by [configuration](#InstallationScalability).  \nWhen Motu is scalable, one Motu server instance can run a download request, another distinct Motu server instance can respond to a \"get status\" request and a last one can respond the URL of the result file. \n\n![Software architecture](./motu-parent/src/doc/softwareArchitectureScalability.png \"Motu software architecture, scalability\")\n\n\n\n\t   \n# <a name=\"ArchitectureInterfaces\">Interfaces</a>  \n## <a name=\"ArchitectureInterfacesServer\">Server interfaces</a>  \nAll ports are defined in [motu.properties](#ConfigurationSystem) configuration file.\n\n* __HTTP__: Apache tomcat manages incoming requests with HTTP protocol.\n* __HTTPs__: Used to manage HTTPs incoming requests. This is delegated to Apache HTTPd frontal web server. Apache Tomcat is not intended to be used with HTTPs.\n* __AJP__: Used to communicate with an Apache HTTPd frontal server\n* __Socket for Shutdown__: Port opened by Tomcat to shutdown the server\n* __JMX__: Used to monitor the application\n* __Debug__: In development mode, used to remotely debug the application\n  \n## <a name=\"ArchitectureInterfacesExternal\">External interfaces with other systems or tools</a>  \nMotu has interfaces with other systems:  \n\n* __DGF__: Direct Get File: Read dataset from the file system. (See how to [configure it](#AdminDataSetAdd).)\n* __Unidata Thredds Data Server__: It connects with the NCSS or OpenDap HTTP REST API to run download request for example. (See how to [configure it](#AdminDataSetAdd).)\n* __HTTP CAS Server__: Use for Single Sign On (SSO) in order to manager user authentication. (See how to [configure it](#ConfigurationSystem) \"CAS SSO server\" and check [profiles](#BSconfigService) attribute set on the dataset.)\n* __CDO command line tool__: [CDO](#InstallCDO) is used to deal with 2 types of download requests, which are not covered by NCSS service of Thredds Data Server:  \n  * a download request on a __range of depths__,  \n  * a download request that come __across the boundary__ of the datasets (for global datasets)  \n* __Redis__: Stores the request id and status into the Redis server when Motu is scaled over several instances.  \n  \n# <a name=\"ArchitectureDesign\">Design</a>  \nThe Motu application has been designed by implementing the Three-Layered Services Application design. It takes many advantages in maintenance cost efficiency and in the ease of its future evolutivity.  \nThree layers are set in the core \"motu-web\" project:  \n\n* __USL__: User Service Layer: This layer manages all incoming actions through HTTP request\n* __BLL__: Business Logic Layer: This layer manages all Motu business\n* __DAL__: Data Access Layer: This layer manages all access to Motu external interfaces: DGF, Unidata server, CDO, ...\n\nEach layer is an entry point of the application designed with a singleton. These three singletons gives access to high level managers which provides services by implementing a Java interface.\nHigh level managers handle for example the configuration, the request, the catalog, the users.\n\nA common package is also defined to provide utilities: log4j custom format, XML, String ...\n\n# <a name=\"ArchitectureDesignD\">Design details</a>  \nThe main project is \"motu-web\". This project is divided in three main layers detailed below:  \n\n## <a name=\"ArchitectureDesignDMW\">Motu-web project</a> \n### <a name=\"ArchitectureDesignDLayers\">Layers</a> \n#### <a name=\"ArchitectureDesignDUSL\">USL</a>  \n* __usl/servlet/context/MotuWebEngineContextListener.java__: Apache tomcat ServletContextListener used to init and stop the application.  \n* __usl/request__: All requests are managed with the \"motu/web/servlet/MotuServlet.java\" by following a command pattern. \"action\" HTTP parameter matches one of the \"usl/request/actions\" classes.  \n* __usl/response__: XML and [Apache velocity](https://velocity.apache.org/) data model.  \n* __usl/wcs__: All WCS requests and responses are managed in this package. The servlet entry point is defined with the class \"motu/web/servlet/MotuWCSServlet.java\".\n\n#### <a name=\"ArchitectureDesignDBLL\">BLL</a>  \n* __bll/catalog__: Catalog and Product managers. Package \"bll/catalog/product/cache\" contains the product cache.\n* __bll/config__: Configuration and version manager.  \n* __bll/exception__: Business exceptions. MotuException is a generic one. MotuExceptionBase defines all other business exceptions.\n* __bll/messageserror__: Message error manager\n* __bll/request__: The queue server used to download requests\n* __bll/users__: User manager\n\n#### <a name=\"ArchitectureDesignDDAL\">DAL</a>    \n* __dal/catalog__: OpenDap, TDS or FILE catalog access.  \n* __dal/config__: Configuration (/motu-web/src/main/resources/schema/MotuConfig.xsd) and version manager (configuration, distribution, products).  \n* __dal/messageserror__: Manage messages for a specific error (/motu-web/src/main/resources/MessagesError.properties).\n* __dal/request__: NCSS, OpenDAP and CDO\n* __dal/tds__: NCSS and OpenDap datamodel\n* __dal/users__: User manager\n\n### <a name=\"ArchitectureDesignDDynamics\">Daemon threads</a> \n\"motu-web\" project starts daemon threads for:  \n\n* __bll/catalog/product/cache__: Keep a product cache and refresh it asynchronously to improve response time \n* __bll/request/cleaner__: Clean files (extracted files, java temp files) and request status (stored into java map or list objects)\n* __bll/request/queueserver__: Contains a thread pool executor to treat download requests\n* __dal/request/cdo__: A queue server used to manage requests using CDO tool to avoid using too much RAM. As CDO uses a lot of RAM memory, \nrequests that require CDO to be processed are in sequence and only one is processed at a time\n\n\n\n\n## <a name=\"ArchitectureDesignDOthers\">Other projects</a>\n* __motu-api-message__: JAXB API: All errors types, status mode, ... /motu-api-message/src/main/schema/XmlMessageModel.xsd  \n* __motu-api-rest__: @Deprecated. Not used since Motu v3.\n* __motu-distribution__: Deployment tools (ant script, install folder structure, ...)\n* __motu-library-cas__: Used to manage JAVA HTTP client with CAS server.\n* __motu-library-converter__: JodaTime, ...\n* __motu-library-inventory__: Used for DGF access. JAXBmodels are /motu-library-inventory/src/main/resources/fr/cls/atoll/motu/library/inventory/Inventory.xsd and CatalogOLA.xsd\n* __motu-parent__: Maven parent, eclipse launchers, documentation\n* __motu-poducts__: Source code and scripts used to build archive motu-products.tar.gz (JDK, Apache tomcat, CDO tools)\n* __motu-scripts__: ./motu bash script\n* __motu-web__: Main Motu project. See [Motu-web project](#ArchitectureDesignDMW) for details.\n  \n  \n## <a name=\"ArchiAlgo\">Algorithm details</a>  \n\n### <a name=\"ArchiAlgoDownloading1Point\">Downloading 1 point</a>  \nSchema below displays a subset of a dataset variable as an array of 2 longitudes and 2 latitudes. At each intersection, we have got 1 real value (10, 11, 12, 13) as defined in the gridded data.  \nBut which result value is returned by Motu when the request target a location between those longitudes and latitudes ?   \nAs we can see below, 4 areas are displayed and the nearest value from the requested location is returned.\n\n![Downloading 1 point](./motu-parent/src/doc/downwloading1point.png \"Motu algorithm: Downloading 1 point\")\n\t \n# <a name=\"Development\">Development</a>  \n\n## <a name=\"DEVSRC\">Source code</a>\nSource code can be downloaded directly from Github.  \n\n```  \nmkdir motugithub  \ncd motugithub  \ngit clone https://github.com/clstoulouse/motu.git  \n    #In order to work on a specific version  \ngit tag -l  \ngit checkout motu-X.Y.Z  \ngit status  \ncd motu  \ncd motu-parent  \n```\n\n## <a name=\"DEV\">Development environment</a>  \n\n### Configure Eclipse development environment\n* Add variable in order to run/debug Motu on your localhost:  \nFrom Eclipse menu bar: Run/Debug > String substitution  \nMOTU_HOME=J:\\dev\\cmems-cis-motu\\motu-install-dir  \nThis variable represent the folder where Motu is installed.  \n\n* From a file explorer, create folders:  \n$MOTU_HOME/log  \n$MOTU_HOME/config  \n$MOTU_HOME/data/public/download  \n\n* Copy configuration files from Eclipse to configuration folder:  \nNote: If you do not have any motu-config folder available, default configuration files are folders are available in the \"/motu-web/src/main/resources\" folder  \nIf \"motu-config\" exists, copy:  \ncp $eclipse/motu-config/src/config/common/config $MOTU_HOME/config  \ncp $eclipse/motu-config/src/config/cls/dev-win7 $MOTU_HOME/config  \n\n \n* Add an application server in Eclipse: Window>Preferences>Server>Runtime environment  \nName=Apache Tomcat 7.0  \nTomcat installation directory=C:\\dvlt\\java\\servers\\tomcat\\apache-tomcat-7.0.65  \n\n* J2EE perspective > Under the Servers view > Right click > New > Server  \nServer Name: Tomcat v7.0 Server at localhost  \n\n* Edit /Servers/Tomcat v7.0 Server at localhost/server.xml and add  \n```\n<Context docBase=\"J:/dev/cmems-cis-motu/motu-install-dir/data-deliveries\" path=\"/mis-gateway/deliveries\" />\n```  \njust under the line:  \n```\n<Host appBase=\"webapps\" autoDeploy=\"true\" name=\"localhost\" unpackWARs=\"true\">\n```  \nNow Tomcat can serve downloaded files directly   \n \n### Run/Debug Motu\n\nClick Debug configurations...> Under Apache Tomcat, debug \"Motu Tomcat v7.0 Server at localhost\"\n\nOpen a web browser and test:  \nhttp://localhost:8080/motu-web/Motu?action=ping  \n\nit displays \"OK - response action=ping\"  \n\n\nFor more details about Eclipse launchers, refers to /motu-parent/README-eclipseLaunchers.md.\n\n\n## <a name=\"COMPILATION\">Compilation</a>  \n\nMaven is used in order to compile Motu.  \nYou have to set maven settings in order to compile.  \nCopy/paste content below in a new file settings.xml and adapt it to your information system by reading comments inside.\n\n```xml  \n    <settings>  \n    <!-- localRepository: Path to the maven local repository used to store artifacts. (Default: ~/.m2/repository) -->  \n    <localRepository>J:/dev/cmems-cis-motu/testGitHub/m2/repository&lt;/localRepository>  \n    <!-- proxies: Optional. Set it if you need to connect to a proxy to access to Internet -->  \n    <!--   \n    <proxies>  \n       <proxy>  \n          <id>cls-proxy</id>  \n          <active>true</active>  \n          <protocol>http</protocol>  \n          <host></host>  \n          <port></port>  \n          <username></username>  \n          <password></password>  \n          <nonProxyHosts></nonProxyHosts>  \n        </proxy>  \n      </proxies>  \n    -->   \n    <!-- Repositories used to download Maven artifacts in addition to https://repo.maven.apache.org   \n         cls-to-ext-thirdparty : contains patched libraries and non public maven packaged libraries  \n         geotoolkit: contains geographical tools libraries  \n    -->  \n    <profiles>    \n       <profile>  \n         <id>profile-cls-cmems-motu</id>  \n         <repositories>  \n            <repository>  \n              <id>cls-to-ext-thirdparty</id>  \n              <name>CLS maven central repository, used for CMEMS Motu project</name>  \n              <url>http://mvnrepo.cls.fr:8081/nexus/content/repositories/cls-to-ext-thirdparty</url>  \n            </repository>  \n            <repository>  \n              <id>geotoolkit</id>  \n              <name>geotoolkit</name>  \n              <url>http://maven.geotoolkit.org/</url>  \n            </repository>  \n        </repositories>  \n       </profile>  \n     </profiles>  \n     </settings>\n```\n\nThis step is used to generate JAR (Java ARchives) and WAR (Web application ARchive).   \n\nIn the GitLab Continuous Integration context, the access to the Maven repository are configured with maven password encryption using the ci/settings-security.xml master key, according to https://maven.apache.org/guides/mini/guide-encryption.html.\nIt is recommended to use maven encryption, and for changing a password, ensure that the encryption is done on a maven repository using the same master key than the one used on the continuous integration server, or update with your local master key of your own settings-security.xml. \n\n```  \nmkdir motu  \ncd motu   \n  #Copy paste the content above inside settings.xml  \nvi settings.xml  \n  #Get source code of the last Motu version  \ngit clone https://github.com/clstoulouse/motu.git  \ncd motu/motu-parent  \n  #  This remove the maven parent artifact from pom.xml, or remove lines below manually:  \n  #  <parent>  \n  #        <artifactId>cls-project-config&lt;/artifactId>  \n  #        <groupId>cls.commons&lt;/groupId>  \n  #        <version>1.2.00&lt;/version>  \n  # </parent>  \nsed -i '6,10d' pom.xml  \n  #Compile the source code  \nmvn -s ../../settings.xml -gs ../../settings.xml -Pprofile-cls-cmems-motu -Dmaven.test.skip=true clean install  \n...  \n[INFO] BUILD SUCCESS  \n...    \n``` \n\nAll projects are built under target folder.  \nThe Motu war is built under \"/motu-web/target/motu-web-X.Y.Z-classifier.war\".  \nIt embeds all necessary jar libraries.  \n\n## <a name=\"Packaging\">Packaging</a>  \nThis packaging process can be run only on CLS development environment.\nThis is an helpful script used to packaged as tar.gz the different projects (products, distribution (server and client), configuration, themes).   \nSo if you try to run it outside of CLS development environment, you will have to tune and remove many things to run it successfully (in particular all which is related to motu-config and motu-products).\nThis step includes the compilation step. Once all projects are compiled, it groups all archives in a same folder in order to easy the final delivery.  \nYou have to set ANT script inputs parameter before running it.  \nSee /motu-distribution/build.xml header to get more details about inputs.  \n```  \ncd /motu-distribution  \nant  \ncd target-ant/delivery/YYYYMMDDhhmmssSSS  \n```  \n\n4 folders are built containing archives:  \n\n* src: contains sources of motu application and the configuration files\n   * motu-$version-src.tar.gz  \n   * motu-client-python-$version-src.tar.gz  \n   * motu-config-$version-src.tar.gz  \n   * motu-web-static-files-$version-src.tar.gz  \n* motu: contains the built application archive and the products (java, tomcat, cdo) archive  \n   * motu-distribution-$version.tar.gz\n   * motu-products-$version.tar.gz\n* config: contains two kind of archives:\n  * motu-config-X.Y.Z-classifier-$timestamp-$target.tar.gz:  the built configurations for each target platform  \n  * motu-web-static-files-X.Y.Z-classifier-$timestamp-$target.tar.gz: The public static files (css, js) for each target platform \n* motu-client  \n  * motu-client-python-$version-bin.tar.gz\n\n# <a name=\"Installation\">Installation</a>  \n\n\n\n## <a name=\"InstallPrerequisites\">Prerequisites</a>  \n\nIn this chapter some paths are set. For example \"/opt/cmems-cis\" is often written to talk about the installation path.\nYou are free to install Motu in any other folder, so in the case, replace \"/opt/cmems-cis\" by your installation folder.  \nThis installation is used to install Motu on a [single instance](#ArchitectureSingleInstance). To scale Motu on [several instances](#ArchitectureScalability), refers to [Install a scalable Motu over several instances](#InstallationScalability).\n\n### <a name=\"InstallPrerequisitesHard\">Motu host, hardware settings</a>\nOS target: Linux 64bits (Tested on centos-7.2.1511)\n\nMinimal configuration for an __operational usage__:  \n\n* __CPU__: 4 CPU, 2,4GHz\n* __RAM__: 32 Gb RAM\n* __Storage__: \n  * Motu installation folder 15Gb: can be install on the OS partition (default folder /opt/cmems-cis)\n  * Motu download folder 200Gb: by default [/opt/cmems-cis/motu/data/public/download](#InstallFolders)  \n    Has to be installed in a dedicated partition to avoid freezing Motu if disk is full.\nNote that the available space of the download folder has to be tuned, depending on:  \n     * The number of users which run requests at the same time on the server\n     * The size of the data distributed\n   \nOnce started, you can [check performance](#ExpMonitorPerf).\n   \nFor __test usage__ we recommend:  \n\n* __CPU__: 2 CPU, 2,4GHz\n* __RAM__: 10 Gb RAM\n* __Storage__: \n  * Motu installation folder 15Gb\n  * Motu download folder 50Gb: by default [motu/data/public/download](#InstallFolders)  \n\n\n### <a name=\"InstallPrerequisitesSoft\">Motu host, software settings</a>\nMotu embeds all its dependencies (Java , Tomcat, CDO). All versions of these dependencies will be visible in the folder name once the Motu product archive is extracted.  \nFor example:  \n```\nls -1 /opt/cmems-cis/motu/products  \napache-tomcat-7.0.69  \ncdo-group  \njdk1.7.0_79  \nREADME  \nversion-products.txt  \n```  \n\nSo bash shell is only required on the Linux host machine.  \n\n### <a name=\"InstallPrerequisitesExternalInterfaces\">External interfaces</a>\nMotu is able to communicate with different external servers:  \n\n* __Unidata | THREDDS Data Server (TDS)__: Motu has been only tested with TDS v4.6.14 2019-07-29. The links to this server are set in the [Business settings](#ConfigurationBusiness) and are used to run OpenDap or subsetter interfaces. If Motu runs only with DGF, this server is not required.  \nNote that some specific characters have to be relaxed, e.g. when TDS is installed on Apache Tomcat, add attribute relaxedQueryChars=\"&lt;&gt;[\\]{|}\" in the connector node by editing conf/server.xml from your TDS tomcat installation folder:  \n```  \n<Connector relaxedQueryChars=\"&lt;&gt;[\\]{|}\" port=\"8080\" ...  \n```\nas reported in this [forum topic](https://groups.google.com/a/opendap.org/d/msg/support/ixTqhDXoLZQ/IT0lvZQ7CAAJ).  \nWithout this configuration Motu server can raised exeptions visible in the Motu \"errors.log\", e.g.:  \n```\nERROR fr.cls.atoll.motu.web.bll.catalog.product.cache.CacheUpdateService.updateConfigService Error during refresh of the describe product cache, config service=..., productId=...  \nfr.cls.atoll.motu.web.bll.exception.MotuException: Error in NetCdfReader open - Unable to aquire dataset - location data:  \nCaused by: java.io.IOException: http://.../thredds/dodsC/$dataset is not a valid URL, return status=400  \n```  \n  \n  \n \n* __Single Sign-On - CAS__: The link to this server is set in the [System settings](#ConfigurationSystem). If Motu does not use SSO, this server is not required.\n\nThe installation of these two servers is not detailed in this document. Refer to their official web site to know how to install them.\n\n  \n### <a name=\"InstallPrerequisitesSeveralsInstances\">Several Motu instances on a same host</a>\nIf you need to instance several instances of Motu server on a same host, you have to:  \n\n* __RAM__: set 32Go of RAM for each instance. For example, two Motu instances on a same host requires 64Go  \n* __Storage__: allocate disk space, in relationship with the Motu usage. Download dedicated partition can be shared or dedicated.  \n* __Folders__: Install each Motu instance in a dedicated folder:  \n  * /opt/motu1/motu, \n  * /opt/motu2/motu, \n  * ..., \n  * /opt/motuN/motu  \n\n  \n## <a name=\"UpgradeFromMotu2x\">Upgrade from Motu v2.x</a>    \n\nCheck this section only if you have installed Motu v2.x and you want to install Motu 3.x.\nIn this section we consider that your Motu installation folder of version 2.x is \"/opt/atoll/misgw/\".  \n\n### Upgrade the software\nFirst stop Motu v2.x: /opt/atoll/misgw/stop-motu.  \nThen install the version 3.x of [Motu from scratch](#InstallFromScratch). Before starting the new Motu version, upgrade its configuration by ready section below.  \nOnce the version 3.x of Motu runs well, you can fully remove the folder of version 2.x is \"/opt/atoll/misgw/\".  \nTo avoid any issue, perhaps backup the folder of Motu v2.x before removing it definitively.  \n``` \nmotu2xInstallFolder=/opt/atoll/misgw  \nrm -rf $motu2xInstallFolder/deliveries  \nrm -rf $motu2xInstallFolder/motu-configuration-common-2.1.16  \nrm -rf $motu2xInstallFolder/motu-configuration-sample-misgw-1.0.5  \nrm -rf $motu2xInstallFolder/motu-web  \nrm -rf $motu2xInstallFolder/start-motu  \nrm -rf $motu2xInstallFolder/stop-motu  \nrm -rf $motu2xInstallFolder/tomcat7-motu  \nrm -rf $motu2xInstallFolder/tomcat-motu-cas  \n```  \n\n\n\n### Upgrade the configuration\n\n#### Business configuration, Product & dataset: motuConfiguration.xml  \nThe new version of Motu is compatible with the motuConfiguration.xml file configured in Motu v3.x.  \nSo you can use exactly the same file, but it is important to update some fields to improve performance and future compatibility. \nCopy your old motuConfiguration.xml file to the folder /opt/cmems-cis/motu/config, for example:  \n```\ncp  /opt/atoll/misgw/motu-configuration-sample-misgw/resources/motuConfiguration.xml /opt/cmems-cis/motu/config\n```  \n\nThen update attributes below:  \n\n* use the TDS subsetter protocol to improve performance of product download  \n   * __motuConfig/configService/catalog__\n      * __ncss__: See [ncss protocol](#BSmotuConfigNCSS)\n      * __urlSite__: Update URL to use the TDS subsetter URL. See [Add a dataset, TDS NCSS protocol](#AdminDataSetAdd)\n* check the attributes used to serve downloaded datasets  \n   * __motuConfig__\n      * __extractionPath__ See [extractionPath](#motuConfig-extractionPath)\n      * __downloadHttpUrl__ Set the URL used to serve files from extractionPath\n* remove all @deprecated attributes listed below to ease future migrations. You can read the attribute description in [Business configuration](#ConfigurationBusiness).  \n   * __XML header__ Remove header below  \n```\n< !DOCTYPE rdf:RDF [  \n<!ENTITY myoceanUrn \"http://purl.org/myocean/ontology/service/database#\">  \n]>  \n```\n   * __motuConfig__\n      * __maxSizePerFile__ This attribute definition has been updated. See [maxSizePerFile parameter configuration](#motuConfig-defaultService)  \n      * __maxSizePerFileTDS__ Rename this attribute to maxSizePerFileSub. See [maxSizePerFileSub parameter configuration](#motuConfig-defaultService)  \n      * __runGCInterval__ Remove the attribute\n      * __httpDocumentRoot__ Remove the attribute\n      * __useAuthentication__ Remove the attribute\n      * __defaultActionIsListServices__ Remove the attribute\n      * __useProxy__, __proxyHost__, __proxyPort__, __proxyLogin__, __proxyPwd__ Remove the attributes\n      * __defaultService__  Remove the attribute, See [defaultService](#motuConfig-defaultService) . It was previously used to declare a default config Service. Now this attribute sets a default action. If the action set is unknown, an error log will be written and user is redirected to the listServices action which is the default one.\n   * __motuConfig/configService__\n      * __defaultLanguage__ Remove the attribute\n      * __defaultPriority__ Remove the attribute\n      * __httpBaseRef__ Remove the attribute\n      * __veloTemplatePrefix__ Remove the attribute\n   * __motuConfig/queueServerConfig__\n      * __defaultPriority__ Remove the attribute\n   * __motuConfig/queues__\n      * __batch__ Remove the attribute\n      * __lowPriorityWaiting__ Remove the attribute\n   * __motuConfig/configFileSystem__ Remove the node\n* after starting Motu, if there is issues with the graphic chart, check the attributes below.\n   * __motuConfig__\n      * __httpBaseRef__ See [httpBaseRef](#motuConfig-httpBaseRef) attribute\n   * __motuConfig/configService__\n      * __httpBaseRef__ Remove the attribute\n\n### Upgrade DGF\nResource URN attributes are not handled in the same way in Motu v3.  \nIn Motu __v2.x__, URN attributes had values set with an ontology URL:  \n``` \n<resource urn=\"http://purl.org/myocean/ontology/product/database#dataset-bal-analysis-forecast-phys-V2-dailymeans\">\n```   \n\nIn Motu __v3.x__, you have to upgrade URN attributes with only the value found after the # character:  \n``` \n<resource urn=\"dataset-bal-analysis-forecast-phys-V2-dailymeans\">\n```   \n  \n\n\n\n#### Log files\nIn CMEMS-CIS context the log file motuQSlog.xml is stored in a specific folder in order to be shared.  \nYou have so to check that with the new version this log file is well written in the shared folder.\nHere is where this log files were written in Motu v2.x:  \n``` \ngrep -i \"motuQSlog.xml\" /opt/atoll/misgw/motu-configuration-sample-misgw-1.0.5/resources/log4j.xml\n<param name=\"file\" value=\"/opt/atoll/misgw/tomcat-motu-cas/logs/motuQSlog.xml\" />\n```  \nThe folder set in the value attribute shall be the same as the one defined in new the Motu configuration file. Replace $path below by the folder path:  \n``` \ngrep -i \"motuQSlog.xml\" /opt/cmems-cis/motu/config/log4j.xml\nfileName=\"$path/motuQSlog.xml\"\nfilePattern=\"$path/motuQSlog.xml.%d{MM-yyyy}\"\n``` \n\n\n\n\n\n\n  \n## <a name=\"InstallFromScratch\">Install Motu from scratch</a>  \n\nMotu installation needs two main step: the software installation and optionally the theme installation.  \nThe software installation brings by default the CLS theme. The theme installation is there to customize or change this default theme.  \n\n### Install Motu software, for example on a <a name=\"IntallDU\">Dissemination Unit</a>    \n\nCopy the installation archives and extract them.  \n```\ncd /opt/cmems-cis  \ncp motu-products-A.B.tar.gz .  \ncp motu-distribution-X.Y.Z.tar.gz .  \ncp motu-config-X.Y.Z-$BUILDID-$TARGET-$PLATFORM.tar.gz .  \ntar xzf motu-products-A.B.tar.gz  \ntar xzf motu-distribution-X.Y.Z.tar.gz  \ntar xzf motu-config-X.Y.Z-$BUILDID-$TARGET-$PLATFORM.tar.gz  \ncd motu\n```\n\nAt this step, Motu is able to start. But static files used for customizing the web theme can be installed.  \nIn the CMEMS context, the installation on a dissemination unit is ended, static files are installed on a [central server](#InstallPublicFilesOnCentralServer).  \n\nNow you can configure the server:  \n* Set the [system properties](#ConfigurationSystem): http port, ...\n* Configure [dataset](#ConfigurationBusiness)\n* Configure the [logs](#LogSettings)\n  \nRefer to [configuration](#Configuration) in order to check your configuration settings.  \n\nMotu is installed and configured. You can [start Motu server](#SS).  \nThen you can [check installation](#InstallCheck).\n\n\n### Install Motu theme (public static files)\n\nAs a dissemination unit administrator, in CMEMS context, this section is not applicable.  \n\nPublic static files are used to customized Motu theme. When several Motu are installed, a central server eases the installation and the update by \nreferencing static files only once on a unique machine. This is the case in the CMEMS context, where each dissemination unit host a Motu server, and \na central server hosts static files.  \nIf you runs only one install of Motu, you can install static files directly on Motu Apache tomcat server.\n\n#### <a name=\"InstallPublicFilesOnCentralServer\">On a central server</a>    \nExtract this archive on a server.\n```\ntar xvzf motu-web-static-files-X.Y.Z-classifier-$timestamp-$target.tar.gz  \n```\nThen use a server to make these extracted folders and files accessible from an HTTP address.\n \nExample: The archive contains a motu folder at its root. Then a particular file is \"motu/css/motu/motu.css\" and this file is served by the URL   http://resources.myocean.eu/motu/css/motu/motu.css in the CMEMS CIS context.   \n\n\n\n#### <a name=\"IntallPublicFilesOnMotuTomcat\">Directly on Motu Apache tomcat server</a>    \n \nIf you do not use a central entity to serve public static files, you can optionally extract the archive \nand serve files directly by configuring Motu.  \nFirst extract the archive:   \n```\ntar xzf motu-web-static-files-X.Y.Z-classifier-$timestamp-$target.tar.gz -C /opt/cmems-cis/motu/data/public/static-files   \n```\n\nThen edit \"motu/tomcat-motu/conf/server.xml\" in order to serve files from Motu.  \nAdd then \"Context\" node as shown below. Note that severals \"Context\" nodes can be declared under the Host node.  \n```\n[...]  \n<Host appBase=\"webapps\" [...]  \n        <!-- Used to serve public static files -->  \n        <Context docBase=\"/opt/cmems-cis/motu/data/public/static-files/motu\" path=\"/motu\"/>    \n [...]  \n```  \n  \nFinally in motuConfiguration.xml, remove all occurrences of the attribute named: [httpBaseRef](#motuConfig-httpBaseRef) in motuConfig and configService nodes. (Do not set it empty, remove it).\n\n\nIf you want to set another path instead of \"/motu\", you have to set also the business configuration parameter named [httpBaseRef](#motuConfig-httpBaseRef).  \n\n \n## <a name=\"InstallCheck\">Check installation</a>  \n\n### Start motu\n```\n./motu start \n```\n\n### Check messages on the server console\n\nWhen you start Motu, the only message shall be:  \n```\ntomcat-motu - start\n```\n\nOptionaly, when this is your first installation or when a software update is done, an INFO message is displayed:  \n```\nINFO: War updated: tomcat-motu/webapps/motu-web.war [$version]  \n```  \n  \n  \nIf any other messages appear, you have to treat them.\n\nAs Motu relies on binary software like CDO, error could be raised meaning that CDO does not runs well.  \n```\nERROR: cdo tool does not run well: $cdo --version  \n[...]\n```  \n\nIn this case, you have to install CDO manually.  \n\n### Check Motu web site available\n\nOpen a Web browser, and enter:\nhttp://$motuUrl/motu-web/Motu?action=ping  \nWhere $motuUrl is: ip adress of the server:tomcat port\nRefer to [configuration](#Configuration) regarding the tomcat port\n\nResponse has to be:   \n```  \nOK - response action=ping\n```  \n\nOpen a Web browser, and enter:\nhttp://$motuUrl/motu-web/Motu  \nIf nothing appears, it is because you have to [add dataset](#AdminDataSetAdd).  \n\n\n### Check Motu logs\nCheck that no error appears in Motu [errors](#LogbooksErrors) log files.\n\n## <a name=\"InstallCDO\">CDO manual installation</a>  \nThis section has to be read only if Motu does not start successfully.  \nSelect one option below to install \"cdo\". If you have no idea about cdo installation, choose the Default option.\n\n* [Default option: Install cdo](#InstallCDOHelp)\n* [cdo is already installed on this machine](#InstallCDOAlreadyInstalled)\n* [Try MOTU without cdo installation](#InstallCDONoInstall)\n* [How cdo is built?](#InstallCDOUnderstand)  \n  \n### <a name=\"InstallCDOHelp\">Install cdo</a>  \n\"cdo\" (Climate Data operators) are commands which has to be available in the PATH when Motu starts.   \nBy default, Motu provides a built of CDO and add the \"cdo\" command to the PATH, but with some Linux distribution it is necessary to install it.  \nMotu provides some help in order to install CDO.  \n\nFirst check your GLibC version:  \n```\nldd --version  \nldd (GNU libc) 2.12  \n[...]  \n```  \n\nIf your GlibC lower than 2.14, you have to install GLIBC 2.14, but to highly recommend to upgrade your Linux operating system to get an up to date GLIBC version:  \n__INSTALL GLIBC 2.14__  \n```\nexport MotuInstallDir=/opt/cmems-cis  \ncd $MotuInstallDir/motu/products/cdo-group  \nwget http://ftp.gnu.org/gnu/glibc/glibc-2.14.tar.gz  \ntar zxvf glibc-2.14.tar.gz  \ncd glibc-2.14  \nmkdir build  \ncd build  \nmkdir $MotuInstallDir/motu/products/cdo-group/glibc-2.14-home  \n../configure --prefix=$MotuInstallDir/motu/products/cdo-group/glibc-2.14-home  \nmake -j4  \nmake install  \ncd $MotuInstallDir/motu  \n```  \n\n\n__Now check if \"cdo\" runs well__:  \n```\nexport MotuInstallDir=/opt/cmems-cis  \n$MotuInstallDir/motu/products/cdo-group/cdo.sh --version  \nClimate Data Operators version x.y.z (http://mpimet.mpg.de/cdo)  \n[...]  \n```  \n\nIf error appear like ones below, it certainly means that GLIC is not in the LD_LIBRARY_PATH.\n```\n$MotuInstallDir/motu/products/cdo-group/cdo-x.z.z-home/bin/cdo: error while loading shared libraries: libhdf5.so.10: cannot open shared object file: No such file or directory\n```  \nor  \n```\ncdo: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by cdo)\ncdo: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /opt/cmems-cis-validation/motu/products/cdo-group/hdf5-1.8.17-home/lib/libhdf5.so.10)\n```\n\nIn this case, edit $MotuInstallDir/products/cdo-group/cdo.sh and add \"$GLIBC-home/lib\" to LD\\_LIBRARY\\_PATH.   \n\nNow check again if \"cdo\" runs well.\n\nIf it runs well, you can now start Motu.  \n\n\n### <a name=\"InstallCDOAlreadyInstalled\">cdo is already installed on this machine</a>  \nIf \"cdo\" is installed in another folder on the machine, you can add its path in \"$MotuInstallDir/motu/motu\" script:  \n\n```  \n__setPathWithCdoTools() {  \n  PATH=$MOTU_PRODUCTS_CDO_HOME_DIR/bin:$PATH  \n}  \n```  \n\nOptionnaly set LD_LIBRAY_PATH in $MotuInstallDir/products/cdo-group/setLDLIBRARYPATH.sh  \n\n\n\n### <a name=\"InstallCDONoInstall\">Try MOTU without cdo installation</a>  \nNote that without CDO, some functionalities on depth requests or on download product won't work successfully.\nIf any case, you can disable the CDO check by commented the check call:  \n\n* Disable check:  \n```\ncd /opt/cmems-cis/motu/  \nsed -i 's/  __checkCDOToolAvailable/#  __checkCDOToolAvailable/g' motu\n```  \n* Enable check:  \n```\ncd /opt/cmems-cis/motu/  \nsed -i 's/#  __checkCDOToolAvailable/  __checkCDOToolAvailable/g' motu\n```  \n  \n  \n  \n  \n### <a name=\"InstallCDOUnderstand\">How cdo is built?</a>  \nCDO is automaticcly build from the script $MotuInstallDir/motu/products/cdo-group/install-cdo.sh\nAlso in order to get full details about CDO installation, you can get details in /opt/cmems-cis/motu/products/README and\nsearch for 'Download CDO tools'.  \n\n\n\n## <a name=\"InstallFolders\">Installation folder structure</a>  \n  \nOnce archives have been extracted, a \"motu\" folder is created and contains several sub-folders and files:  \n__motu/__  \n\n* __config:__ Folder which contains the motu configuration files. Refers to [Configuration](#Configuration) for more details.\n* __data:__ Folder used to managed Motu data.\n  * __public__: Folders which contain files exposed to public. It can be published through a frontal Apache HTTPd Web server, through Motu Apache Tomcat or any other access.\n     * __download__: Folder used to save the products downloaded. This folder is sometimes elsewhere, for example in Motu v2: /datalocal/atoll/mis-gateway/deliveries/. A best practice is to create a symbolic link to a dedicated partition to avoid to freeze Motu when there is no space left.   \n     * __inventories__: This folder can be used to store the DGF files.  \n     * __transaction__: This folder is used to serve the [transaction accounting logs](#LogbooksTransactions)\n     * __static-files__: Used to store public static files. This folder can be served by a frontal Apache HTTPd Web server or Motu Apache Tomcat. In the CMEMS-CIS context, it is not used as static files are deployed on a central web server.      \n* __log:__ Folder which contains all log files. Daily logging are suffixed by yyyy-MM-dd.\n  * __logbook.log:__ Motu application logs (errors and warning are included)\n  * __warnings.log:__ Motu application warnings\n  * __errors.log:__ Motu application errors\n  * __motuQSlog.xml,motuQSlog.csv:__ Motu queue server logs messages (transaction accounting logs), format is either xml or csv\n* __motu file:__ Script used to start, stop Motu application.  Refers to [Start & Stop Motu](#SS) for more details.\n* __pid:__ Folder which contains pid files of the running Motu application.\n  * __tomcat-motu.pid:__ Contains the UNIX PID of the Motu process.\n* __products:__ Folder which contains Java, Tomcat ($CATALINE_BASE folder) and CDO products.\n  * __apache-tomcat-X.Y.Z:__ Apache tomcat default installation folder from http://tomcat.apache.org/\n  * __cdo-group:__ CDO tools from https://code.zmaw.de/projects/cdo\n  * __jdkX.Y.Z:__ Oracle JDK from http://www.oracle.com/technetwork/java\n  * __version-products.txt:__ Contains the version of the current Motu products.\n* __tomcat-motu:__ Tomcat is deployed inside this folder. This folder is built by setting the $CATALINE_HOME environment variable.\n* __version-distribution.txt file:__ Contains the version of the current Motu distribution.\n\n\n\n\n\n## <a name=\"InstallFrontal\">Setup a frontal Apache HTTPd server</a>  \nApache HTTPd is used as a frontal HTTP server in front of Motu Http server.\nIt has several aims:  \n\n* Delegate HTTP requests to Motu HTTP server    \n* Serve directly extracted dataset files written after a download action. The folder in which requests are written is [configurable](#motuConfig-extractionPath). URL used to download those files is \"http://$ipMotuServer/mis-gateway/deliveries\". This URL is [configurable](#motuConfig-downloadHttpUrl).  \n* Serve the download transaction logbook files. The folder in which log files are written is [configurable](#LogSettings).  \n* Manage errors like 403, 404. Motu server manages errors web page by displaying a custom message. Redirect to the URL \"http://$ipMotuServer/motu-web/Motu?action=httperror&code=$errorCode\" by replacing $errorCode with the HTTP error code.  \n* Optionally acts as a load balancer when several instances of Motu.\n\n\nSee sample of the Apache HTTPd configuration in the Motu installation folder: __config/apache-httpd-conf-sample__  \nThe configuration is described for Apache2 contains files:\n\n* __001_httpd-vhosts-motu.conf:__ Main apache configuration file used for Motu, replace __$serverHostName__ by the server host, and $webmasterEmail by the webmaster or administrator email.\n* __apache2.conf:__ Use to show how timeout parameter shall be set\n* __enableApacheModules.sh__ Describe the Apache modules to enable\n\nWhen an SSO cas server is used, you have to st the property [cas-auth-serverName](#ConfigurationSystemCASSSO) to http://$serverHostName\n\nApache HTTPd can be used at different levels. The Apache HTTPd above is the one installed on the same machine as Motu.\nAn Apache HTTPd can be used as a frontal to manage Apache HTTPd load balancing. In the case, you can set up with the following example:  \n\n```  \n # Use to authenticate users which want to download transaction files  \n< Location /datastore-gateway/transactions/* >  \n|--AuthType Basic  \n|--AuthName \"XXX\"  \n|--AuthUserFile /XXX/password.conf  \n|--Require valid-user  \n< / Location>   \n  \n # Used to serve URL requested after a CAS authentication  \n # Because Motu SSO client set a redirection URL directly to its webapp name  \n # so we have to take into account the webapp name in Apache HTTPd  \nProxyPass /motu-web http://$motuTomcatIp:$motuTomcatPort/motu-web  \nProxyPassReverse /motu-web http://$motuTomcatIp:$motuTomcatPort/motu-web  \n        \n # Used to serve Motu requests   \n    # /mis-gateway-servlet These rules are used for retro compatibility between Motu v2.x and Motu v3.x  \nProxyPass /mis-gateway-servlet http://$motuTomcatIp:$motuTomcatPort/motu-web  \nProxyPassReverse /mis-gateway-servlet http://$motuTomcatIp:$motuTomcatPort/motu-web  \nProxyPreserveHost On  \n    # /motu-web-servlet This URL is sometimes used.  \n    # It can be customized depending of your current installation. If you have any doubt, keep this rule.  \nProxyPass /motu-web-servlet http://$motuTomcatIp:$motuTomcatPort/motu-web  \nProxyPassReverse /motu-web-servlet http://$motuTomcatIp:$motuTomcatPort/motu-web  \n                \nProxyPass /datastore-gateway/transactions http://$apacheHTTPdOnMotuHost/datastore-gateway/transactions  \nProxyPassReverse /datastore-gateway/transactions http://$apacheHTTPdOnMotuHost/datastore-gateway/transactions  \n\nProxyPass /datastore-gateway/deliveries http://$apacheHTTPdOnMotuHost/datastore-gateway/deliveries  \nProxyPassReverse /datastore-gateway/deliveries http://$apacheHTTPdOnMotuHost/datastore-gateway/deliveries  \n\n< Location /motu-web-servlet/supervision>  \n|--Order allow,deny  \n|--Allow from All  \n< /Location>  \n```  \n\n\n\n## <a name=\"InstallSecurity\">Security</a>  \n\n### <a name=\"InstallSecurityRunHTTPs\">Run Motu as an HTTPS Web server</a>  \nMotu is a web server based on Apache Tomcat.  \nIn order to secure HTTP connections with a client, HTTPs protocol can be used.  \nYou have two choices:  \n\n* __Motu as a standalone web server__  \n  In this case only Motu is installed.  \n  Refers to the Apache Tomcat official documentation to know how to set SSL certificates: [SSL/TLS Configuration HOW-TO](#https://tomcat.apache.org/tomcat-9.0-doc/ssl-howto.html)\n* __Motu with an Apache HTTPd frontal web server__  \n  In this case Motu is installed and also a frontal Apache HTTPd server.  \n  Refers to the Apache HTTPd official documentation: [SSL/TLS Strong Encryption: How-To](#https://httpd.apache.org/docs/2.4/ssl/ssl_howto.html)  \n  [Apache HTTPd](#InstallFrontal) communicates with Apache Tomcat with the [AJP protocol](#ConfigurationSystem).\n\n### <a name=\"InstallSecuritySSO\">Motu and Single Sign On</a>  \nIn order to manage SSO (Single Sign On) connections to Motu web server, Motu uses an HTTPs client.  \nAll documentation about how to setup is written in chapter [CAS SSO server](#ConfigurationSystemCASSSO).\n\n\n## <a name=\"InstallationScalability\">Install a scalable Motu over several instances</a>  \nScalability is provided with two main components, the Redis database, for sharing the status of the requests between Motu instances, and Tomcat with the session replication mechanism to share login and authentication data.\n### Scalability prerequisites:\n* install a [Redis server](https://redis.io/). (Motu has been tested with Redis version 4.0.8, 64 bit). Redis shares the request ids and status between all Motu instances\n* share the [download folder](#motuConfig-extractionPath) between all instances with a NFS mount, GlusterFS or any other file sharing system.\n* set a frontal web server to serve the [downloaded](#motuConfig-downloadHttpUrl) files from the Motu server and to load balance the requests between all Motu servers.  \n  \n### Scalability configuration:\n\n#### Motu configuration:\n* $installDir/motu/config/__motuConfiguration.xml__: set the Redis settings in the [business configuration file](#RedisServerConfig). For example:\n```xml\n<motuConfig dataBlockSize=\"1024000\" ...>\n   ....\n   <redisConfig host=\"localhost\" port=\"16379\"/>\n   ....\n</motuConfig>\n```  \n  \n#### Motu apache Tomcat configuration:\n* $installDir/motu/tomcat-motu/conf/__web.xml__ file: add the _distributable_ element inside of the _web-app_ element\n```xml\n<web-app xmlns=\"http://xmlns.jcp.org/xml/ns/javaee\"\n  xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n  xsi:schemaLocation=\"http://xmlns.jcp.org/xml/ns/javaee\n                      http://xmlns.jcp.org/xml/ns/javaee/web-app_4_0.xsd\"\n  version=\"4.0\">\n    ....\n    <distributable />\n    ....\n</web-app>\n```  \n  \n* $installDir/motu/tomcat-motu/conf/__server.xml__ file\n  * add a _Cluster_ element in the _Engine_ element\n```xml  \n<Server port=\"16005\" shutdown=\"SHUTDOWN\">\n  <Service name=\"Catalina\">\n    <Engine name=\"Catalina\" defaultHost=\"localhost\">\n      ....\n      <Cluster className=\"org.apache.catalina.ha.tcp.SimpleTcpCluster\"\n                       channelSendOptions=\"6\">\n        <Channel className=\"org.apache.catalina.tribes.group.GroupChannel\">\n          <Membership className=\"org.apache.catalina.tribes.membership.McastService\"\n                      address=\"228.0.0.4\"\n                      port=\"45564\"\n                      frequency=\"500\"\n                      dropTime=\"3000\"/>\n          <Receiver className=\"org.apache.catalina.tribes.transport.nio.NioReceiver\"\n                    address=\"auto\"\n                    port=\"4000\"\n                    autoBind=\"100\"\n                    selectorTimeout=\"5000\"\n                    maxThreads=\"6\"/>\n      \n          <Sender className=\"org.apache.catalina.tribes.transport.ReplicationTransmitter\">\n            <Transport className=\"org.apache.catalina.tribes.transport.nio.PooledParallelSender\"/>\n          </Sender>\n          <Interceptor className=\"org.apache.catalina.tribes.group.interceptors.TcpFailureDetector\"/>\n          <Interceptor className=\"org.apache.catalina.tribes.group.interceptors.MessageDispatchInterceptor\"/>\n        </Channel>\n        <Valve className=\"org.apache.catalina.ha.tcp.ReplicationValve\" filter=\"\"/>\n        <Valve className=\"org.apache.catalina.ha.session.JvmRouteBinderValve\"/>\n        <Deployer className=\"org.apache.catalina.ha.deploy.FarmWarDeployer\"\n                  tempDir=\"/tmp/war-temp/\"\n                  deployDir=\"/tmp/war-deploy/\"\n                  watchDir=\"/tmp/war-listen/\"\n                  watchEnabled=\"false\"/>\n        <ClusterListener className=\"org.apache.catalina.ha.session.ClusterSessionListener\"/>\n      </Cluster>\n      ....\n    </Engine>\n  </Service>\n</Server>\n```  \n  Ensure that the configured multicast ip is authorized for multicast (here `228.0.0.4`). The configured broadcast port (here `45564`) has to be available.  \n  Each Motu instance will get allocated a port in the configurable range starting at the port `4000` as configured in the _Receiver_ element. Ensure those ports are available.  \n  >_channelSendOptions_ at `6` ensures that a request creating a session on a server of the cluster is shared among all other servers and acknowledged, before of being responded. \n  * Ensure no `jvmRoute` field is configured in the _Engine_ element:\n```xml\n<Server port=\"16005\" shutdown=\"SHUTDOWN\">\n  <Service name=\"Catalina\">\n     ....\n     <Engine name=\"Catalina\" defaultHost=\"localhost\">\n     <!-- instead of <Engine name=\"Catalina\" defaultHost=\"localhost\" jvmRoute=\"server1\"> -->\n         ....\n     </Engine>\n     ....\n  </Service>\n</Server>\n```  \n  \n* $installDir/motu/tomcat-motu/conf/__context.xml__: add a _Manager_ in the _Context_ element.\n```xml\n<Context>\n  ....\n  <Manager className=\"org.apache.catalina.ha.session.DeltaManager\"\n           expireSessionsOnShutdown=\"false\"\n           notifyListenersOnReplication=\"true\"\n           persistAuthentication=\"true\"/>\n  ....\n</Context>\n```  \n  \n#### Frontal load balancer configuration (httpd or other)\nDeactivate sticky session mechanisms:\n* for __httpd__ _VirtualHost_ blocks\n  * no _route_ attribute in `BalancerMember` directives:\n```xml\n<Proxy balancer://cluster/>\nBalancerMember http://my-host:6080/motu-web\nBalancerMember http://my-host:7080/motu-web\n</Proxy>\n```  \n  * no _stickysession_ attribute in `ProxyPass` and `ProxyPassReverse` directives:\n```xml\nProxyPass /motu-web/ balancer://cluster/\nProxyPassReverse /motu-web/ balancer://cluster/\n``` \n* for __HAProxy__ configurations, do not use `stick on ` directives on IP or session  \n  \n# <a name=\"Configuration\">Configuration</a>  \n\nThis chapter describes the Motu configuration settings.  \nAll the configuration files are set in the $installDir/motu/config folder.  \n\n* [Configuration directory structure](#ConfigurationFolderStructure)\n* [Business settings](#ConfigurationBusiness)\n* [System settings](#ConfigurationSystem)\n* [Log settings](#LogSettings)\n\n  \n## <a name=\"ConfigurationFolderStructure\">Configuration directory structure</a>  \ncd $installDir/motu/config\n  \n* __config:__ Folder which contains the motu configuration files.\n  * __[motu.properties](#ConfigurationSystem):__ JVM memory, network ports of JVM (JMX, Debug) and Tomcat (HTTP, HTTPS, AJP, SHUTDOWN). CAS SSO server settings.\n  * __[motuConfiguration.xml](#ConfigurationBusiness):__ Motu settings (Service, Catalog via Thredds, Proxy, Queues, ....)\n  * __[log4j.xml](#LogSettings):__ Log4j v2 configuration file\n  * __[standardNames.xml](#ConfigStandardNames):__ Contains the standard names\n  * __version-configuration.txt:__ Contains the version of the current Motu configuration.\n  \n## <a name=\"ConfigurationBusiness\">Business settings</a>  \n### motuConfiguration.xml: Motu business settings  \nThis file is watched and updated automatically. This means that when Motu is running, this file has to be written in a atomic way.  \n\nYou can configure 3 main categories:  \n\n* [MotuConfig node : general settings](#BSmotuConfig)\n* [ConfigService node : catalog settings](#BSconfigService)\n* [QueueServerConfig node : request queue settings](#BSqueueServerConfig)\n* [RedisConfig node : Redis server config](#RedisServerConfig)\n  \n  \nIf you have not this file, you can extract it (set the good version motu-web-X.Y.Z.jar):  \n```\n/opt/cmems-cis/motu/products/jdk1.7.0_79/bin/jar xf /opt/cmems-cis/motu/tomcat-motu/webapps/motu-web/WEB-INF/lib/motu-web-X.Y.Z.jar motuConfiguration.xml\n```   \n    \n  \nIf you have this file from a version anterior to Motu v3.x, you can reuse it. In order to improve global performance, you have to upgrade some fields:  \n* [ncss](#BSmotuConfigNCSS) Set it to \"enabled\" to use a faster protocol named subsetter rather than OpenDap to communicate with TDS server. ncss must be enabled only with regular grid. The datasets using curvilinear coordinates (like ORCA grid) can not be published with ncss. Thus, ncss option must be set to disable or empty.  \n* [httpBaseRef](#motuConfig-httpBaseRef) shall be set to the ULR of the central repository to display the new theme  \n* [ExtractionFilePatterns](#BSmotuConfigExtractionFilePatterns) to give a custom name to the downloaded dataset file  \n  \n  \n\n#### <a name=\"BSmotuConfig\">Attributes defined in motuConfig node</a>  \n\n##### <a name=\"motuConfig-defaultService\">defaultService</a>  \nA string representing the default action in the URL /Motu?action=$defaultService  \nThe default one is \"listservices\".  \nAll values can be found in the method USLRequestManager#onNewRequest with the different ACTION_NAME.  \n\n##### dataBlockSize\nAmount of data in Ko that can be requested in a single query from Motu to TDS. Default is 2048Kb.  \nIf this amount is lower than the [maxSizePerFile](#maxSizePerFile) (in MegaBytes), Motu will launch several sub-requests to TDS to gather all the data.  \nHigher value (up to the [maxSizePerFile](#maxSizePerFile)) leads to less requests, but with higher data volume to transfer by request from TDS to Motu. And the [tds.http.sotimeout](#TdsHttpSoTimeout) has to be long enough for letting TDS the time to read and transfer the whole data to Motu.  \nLower values will imply more requests, but shorter. It consumes more CPU on both Motu and TDS with the advantages to allow TDS to answer to other parallel request it would receive, and to reduce communication times for each request.  \nConcerning performance, we tried 200Mb and 1024Mb for a 1024Mb request, and durations were similar, but note that the shapes of the sub-requests may not match the shapes of the netcdf files, and that could imply a supplementary delay for hard drive data storage.  \n>If the [tds.http.sotimeout](#TdsHttpSoTimeout) can be set to a high value (such as 900s for a 1Gb max size request), the safest is to use the [maxSizePerFile](#maxSizePerFile) value for the *dataBlockSize* parameter (mind the units Kb/Mb).  \n>Else from the max timeout the environment can support (external constraints or \"004-27\" error, see [tds.http.sotimeout](#TdsHttpSoTimeout)), extrapolate a volume of data that can be transfered during this delay, lower it to keep a margin, and use it as the *dataBlockSize*.\n\n##### maxSizePerFile\nThis parameter is only used with a catalog type set to \"FILE\" meaning a DGF access.  \nIt allows download requests to be executed only if data extraction is lower than this parameter value.  \nUnit of this value is MegaBytes.  \nDefault is 1024 MegaBytes.  \nExample: maxSizePerFile=\"2048\" to limit a request result file size to 2GB.  \n\n##### maxSizePerFileSub\nThis parameter is only used with a catalog type used with Opendap or Ncss.  \nIt allows download requests to be executed only if data extraction is lower that this parameter value.  \nUnit of this value is MegaBytes.  \nDefault is 1024 MegaBytes.  \nExample: maxSizePerFileSub=\"2048\" to limit request result file size to 2GB.\n\n##### maxSizePerFileTDS\n@Deprecated from v3 This parameter is not used and has been replaced by maxSizePerFile and maxSizePerFileSub.   \nNumber of data in Megabytes that can be written and download for a Netcdf file. Default is 1024Mb.\n\n##### <a name=\"motuConfig-extractionPath\">extractionPath</a>  \nThe absolute path where files downloaded from TDS are stored.  \nFor example: /opt/cmems-cis/motu/data/public/download\nIt is recommended to set this folder on an hard drive with very good performances in write mode.\nIt is recommended to have a dedicated partition disk to avoid freezing Motu if the hard drive is full.\nBy default value is $MOTU_HOME/data/public/download, this folder can be a symbolic link to another folder.  \nString with format ${var} will be substituted with Java property variables. @See System.getProperty(var)  \n\n##### <a name=\"motuConfig-downloadHttpUrl\">downloadHttpUrl</a>\nHttp URL used to download files stored in the \"extractionPath\" described above. It is used to allow users to download the result data files.  \nThis URL is concatenated to the result data file name found in the folder \"extractionPath\".  \nWhen a frontal HTTPd server is used, it is this URL that shall be configured to access to the folder \"extractionPath\".  \nString with format ${var} will be substituted with Java property variables. @See System.getProperty(var)  \n\n##### <a name=\"motuConfig-httpBaseRef\">httpBaseRef</a>  \nHttp URL used to serve files from to the path where archive __motu-web-static-files-X.Y.Z-classifier-buildId.tar.gz__ has been extracted.  \nFor example: \n\n* When __httpBaseRef__ is set to an __URL__, for example __\"http://resources.myocean.eu/motu\"__, this URL serves a folder which contains ./css/motu/motu.css.  \nFor example, it enables to serve the file http://resources.myocean.eu/motu/css/motu/motu.css  \n* When __httpBaseRef__ is set to __\".\"__, it serves static files which are included by default in Motu application\n* When __httpBaseRef__ is __removed__ (not just empty but attribute is removed), it serves a path accessible from URL $motuIP/${motuContext}/motu\n\n__IMPORTANT__: When Motu URL starts with \"HTTPS\", if you set an URL in __httpBaseRef__, this URL has also to start with \"HTTPS\". On the contrary, \nwhen Motu URL starts with \"HTTP\", if you set an URL in __httpBaseRef__, this URL can start with \"HTTP\" or \"HTTPS\".\n\n        \n##### <a name=\"#BScleanExtractionFileInterval\">cleanExtractionFileInterval</a>\nIn minutes, oldest result files from extraction request which are stored in the folder set by [extractionpath](#extractionpath) are deleted. This check is done each \"runCleanInterval\" minutes.    \nDefault = 60min\n\n##### <a name=\"BScleanRequestInterval\">cleanRequestInterval</a>  \nIn minutes, oldest status (visible in [debug](#ExploitDebug) view) than this time are removed from Motu. This check is done each \"runCleanInterval\" minutes.  \nDefault = 60min\n\n##### <a name=\"#BSrunCleanInterval\">runCleanInterval</a>\nIn minutes, the waiting time between each clean process. The first clean work is triggered when Motu starts.  \nA clean process does:  \n\n* delete files inside java.io.tmpdir\n* delete all files found in extractionFolder bigger than extractionFileCacheSize is Mb\n* delete all files found in extractionFolder oldest than cleanExtractionFileInterval minutes\n* remove all status oldest than [cleanRequestInterval](#BScleanRequestInterval) minutes\n\nDefault = 1min\n\n##### <a name=\"BSmotuConfigExtractionFilePatterns\">extractionFilePatterns</a>  \nPatterns (as regular expression) that match extraction file name to delete in folders:\n\n* java.io.tmpdir\n* extractionPath\n\nDefault is \".*\\.nc$|.*\\.zip$|.*\\.tar$|.*\\.gz$|.*\\.extract$\"  \n\n\n##### extractionFileCacheSize\nSize in Mbytes.  \nA clean job runs each \"runCleanInterval\". All files with a size higher than this value are deleted by this job.\nIf value is zero, files are not deleted.  \nDefault value = 0.\n\n##### <a name=\"describeProductCacheRefreshInMilliSec\">describeProductCacheRefreshInMilliSec</a>\nProvide the delay to wait to refresh the meta-data of products cache after the last refresh.  \nMotu has a cache which is refreshed asynchronously. Cache is first refreshed as soon as Motu starts.   \nThen Motu waits for this delay before refreshing again the cache.  \nThis delay is provided in millisecond.  \nThe default value is 60000 meaning 1 minute.  \n  \nLogbook file (motu/log/logbook.log) gives details about time taken to refresh cache, for example:   \n```  \nINFO  CatalogAndProductCacheRefreshThread.runProcess Product and catalog caches refreshed in 2min 19sec 75msec  \n```\nLogbook file gives details per config service ($configServiceId) about dedicated time taken to refresh cache, for example:   \n```  \nINFO  CatalogAndProductCacheRefreshThread.runProcess Refreshed statistics: $configServiceId@Index=0min 34sec 180msec, $configServiceId@Index=0min 31sec 46msec, ...   \n```  \nThey are sorted by config service which has taken the most time first.  \n@Index All config services are refreshed sequentially. This index is the sequence number for which this cached has been refreshed.\n  \nExample of archived data with several TB of data. Cache is refreshed daily: describeProductCacheRefreshInMilliSec=86400000   \nExample of real time data with several GB of data. Cache is refreshed each minute: describeProductCacheRefreshInMilliSec=60000    \n\n##### runGCInterval\n@Deprecated from v3 This parameter is not used. \n\n##### httpDocumentRoot\n@Deprecated from v3 This parameter is not used. \nDocument root of the servlet server.   \n\n##### wcsDcpUrl \nOptional attribute. Used to set the tag value \"DCP\" in the response of the [WCS GetCapabilities](#GetCapabilities) request with a full URL.\nThe WCS DCP URL value is define using the following priority order:\n\t- The value of this parameter defines on the motuConfiguration.xml file. The value can be directly the URL to use or the name of a java property define between {} which contains the value of the URL.\n\t- The java property \"wcs-dcp-url\" value\n\t- The URL of the web server on which Motu webapps is deployed \nThis attribute can be set when you use a frontal web server to serve the WCS requests, e.g. http://myFrontalWebServer/motu/wcs and your frontal is an HTTP proxy to http://motuWebServer/motu-web/wcs.  \n        \n##### useAuthentication\n@Deprecated from v3 This parameter is not used. It is redundant with parameter config/motu.properties#cas-activated.\n\n\n##### defaultActionIsListServices\n@Deprecated from v3 This parameter is not used.  \n\n##### Configure the Proxy settings  \n@Deprecated from v3 This parameter is not used.\nTo use a proxy in order to access to a Threads, use the [JVM properties](#ConfigurationSystem), for example:  \n\n```  \ntomcat-motu-jvm-javaOpts=-server -Xmx4096M  ... -Dhttp.proxyHost=monProxy.host.fr -Dhttp.proxyPort=XXXX -Dhttp.nonProxyHosts='localhost|127.0.0.1'\n```  \n\n\n* __useProxy__  \n* __proxyHost__  \n* __proxyPort__  \n* __proxyLogin__  \n* __proxyPwd__ \n\n\n##### <a name=\"refreshCacheToken\">refreshCacheToken</a>   \n\nThis token is a key value which is checked to authorize the execution of the cache refresh when it is request by the administrator .\nIf the token value provided by the administrator doesn't match the configured token value, the refresh is not executed and an error is returned.\nA default value \"a7de6d69afa2111e7fa7038a0e89f7e2\" is configured but it's hardly recommended to change this value. If this token is not changed, it is a security breach and \na log ERROR will be written while the configuration will be loaded.\nThe value can contains the characters [A-Za-z] and specials listed here ( -_@$*!:;.,?()[] )\nIt's recommended to configure a token with a length of 29 characters minimum.\n\n##### downloadFileNameFormat  \nFormat of the file name result of a download request.  \n2 dynamic parameters can be used to configure this attribute:  \n\n* __@@requestId@@__: this pattern will be replaced in the final file name by the id of the request.  \n* __@@productId@@__: this pattern will be replaced in the final file name by the id of the requested product.  \n\nIf this attribute is not present, default value is: \"@@productId@@_@@requestId@@.nc\"\n\n##### motuConfigReload  \nConfigure how motu configuration is reloaded.  \nArguments are only 'inotify' or an 'integer in seconds'. 'inotify' is the default value.  \n* __'inotify'__: reload as soon as the file is updated (works only on local file system, not for NFS file system).  \n* __'integer in seconds'__: reload each X second the configuration in 'polling' mode. If this integer is equals or lower than 0, it disables the refresh of the configuration.  \n\n\n#### <a name=\"BSconfigService\">Attributes defined in configService node</a>  \n\n##### <a name=\"BSconfigServiceName\">name</a>  \nString to set the config service name\nIf the value of this attribute contains some special caracters, those caracters have not to be encoded.\nFor example, if the value is an URL, the caracters \":\" and \"/\" have not to be encoded like \"%2E\" or \"%3A\".\n\n##### group\nString which describes the group\n\n##### description\nString which describes the service\n\n##### profiles\nOptional string containing one value, several values separated by a comma or empty (meaning everybody can access).  \nUsed to manage access right from a SSO cas server.  \nIn the frame of CMEMS, three profiles exist:  \n\n* internal: internal users of the CMEMS project  \n* major: major accounts  \n* external: external users  \n\nOtherwise, it's possible to configure as many profiles as needed.  \nProfiles are configured in LDAP within the attribute \"memberUid\" of each user. This attribute is read by CAS and is sent to Motu \nonce a user is logged in, in order to check if it matches profiles configured in Motu to allow a user accessing the data.  \nIn LDAP, \"memberUid\" attribute can be empty, contains one value or several values separated by a comma.  \n\n##### veloTemplatePrefix\nOptional, string used to target the default velocity template. It is used to set a specific theme.  \nValue is the velocity template file name without the extension.  \nDefault value is \"index\".\n\n##### <a name=\"refreshCacheAutomaticallyEnabled\">refreshCacheAutomaticallyEnabled</a>\nOptional, boolean used to determine if the current config service have its cache updated automatically by Motu or not.\nDefault value is \"true\". \n\"true\" means that the config service cache update is executed automatically by Motu.\n\n##### httpBaseRef\nOptional, used to override [motuConfig httpBaseRef](#motuConfig-httpBaseRef) attribute for this specific service.\n\n##### defaultLanguage\n@Deprecated from v3 This parameter is not used.\n\n\n#### Attributes defined in catalog node\n\n##### <a name=\"BSconfigServiceDatasetName\">name</a>  \nThis catalog name refers a TDS catalog name available from the URL: http://$ip:$port/thredds/m_HR_MOD.xml\nExample: m_HR_OBS.xml \n\n##### <a name=\"BSconfigServiceDatasetType\">type</a>    \n* tds: Dataset is downloaded from TDS server. In this case, you can use [Opendap or NCSS protocol](#BSmotuConfigNCSS).\n* file: Dataset is downloaded from DGF\n\nExample: tds\n\n##### <a name=\"BSmotuConfigNCSS\">ncss</a>  \nOptional parameter used to enable or disable the use of NetCDF Subset Service (NCSS) in order to request the TDS server.\nncss must be enabled only with regular grid. The datasets using curvilinear coordinates (like ORCA grid) can not be published with ncss. Thus, ncss option must be set to disable or empty.\nWithout this attribute or when empty, Motu connects to TDS with Opendap protocol. If this attribute is set to \"enabled\" Motu connects to TDS with NCSS protocol in order to improve performance.   \nWe recommend to use \"enabled\" for regular grid datasets. \nValues are: \"enabled\", \"disabled\" or empty.\n\n##### urlSite\n* TDS URL  \nFor example: http://$ip:$port/thredds/  \n\n* DGF URL  \nFor example: file:///opt/publication/inventories\n\n#### <a name=\"BSqueueServerConfig\">Attributes defined in queueServerConfig node</a>  \n\n##### maxPoolAnonymous\nMaximum number of request that an anonymous user can send to Motu before throwing an error message.  \nValue of -1 means no check is done so an unlimited number of user can request the server.  \nDefault value is 10  \nIn case where an SSO server is used for authentication, this parameter is not used. In this you you will be able to fix a limit by setting \"maxPoolAuth\" parameter value.  \n\n##### maxPoolAuth\nMaximum number of request that an authenticated user can send to Motu before throwing an error message.  \nValue of -1 means no check is done so an unlimited number of user can request the server.  \nDefault value is 1\nIn case where no SSO server is used for authentication, this parameter is not used. In this you you will be able to fix a limit by setting \"maxPoolAnonymous\" parameter value.  \n\n##### defaultPriority\n@Deprecated from v3 This parameter is not used.\n\n\n#### Attributes defined in queues\n##### id\nAn id to identify the queue.\n\n##### description\nDescription of the queue.\n\n##### batch\n@Deprecated from v3 This parameter is not used.\n\n##### Child node: maxThreads\nUse to build a java.util.concurrent.ThreadPoolExecutor an to set \"corePoolSize\" and \"maximumPoolSize\" values.  \nDefault value is 1  \nThe total number of threads should not be up to the total number of core of the processor on which Motu is running.  \n\n##### Child node: maxPoolSize\nRequest are put in a queue before being executed by the ThreadPoolExecutor. Before being put in the queue, the queue size\nis checked. If it is upper than this value maxPoolSize, an error message is returned.\nValue of -1 means no check is done.  \nDefault value is -1\n\n\n##### Child node: dataThreshold\nSize in Megabyte. A request has a size. The queue in which this request will be processed is defined by the request size.\nAll queues are sorted by size ascending. A request is put in the last queue which has a size lower than the request size.\nIf the request size if higher than the bigger queue dataThreshold, request is not treated and an error message is returned.  \nThis parameter is really useful when a Motu is used to server several kind of file size and you want to be sure that file with a specific size does no slow down request of small data size.  \nIn this case you can configure two queues and set a number of threads for each in order to match the number of processors. The JVM, even if requests for high volume are running, will be able to\nprocess smallest requests by running the thread on the other processor core. Sp processing high volume requests will not block the smallest requests.  \n\n\n##### Child node: lowPriorityWaiting\n@Deprecated from v3 This parameter is not used.\n\n#### <a name=\"RedisServerConfig\">Attributes defined in redisConfig node</a>  \nThis optional node is used to run Motu in a [scalable architecture](#ArchitectureScalability). Do not add this node when you just run one single Motu instance.  \nOnce this node is added, Motu stores all its request ids and status in Redis.  \n\n##### host\nDefine the host (ip or server name) where is deployed the Redis server od Redis cluster used by Motu to share the RequestId and RequestStatus data.\nDefault value is localhost\n\n##### port\nDefine the port used by the Redis server or Redis cluster used by Motu to share the requestId and RequestStatus data.\nDefault value is 6379  \n\n##### prefix\nDefine the prefix used to build the RequestId value of the shared RequestStatus data.\nDefault value is requestStatus\n\n##### isRedisCluster \nDefine if the redis server in in cluster mode.\nThis is a boolean value.\nBy default is set to false and the cluster mode is not activate.\nTo activate the cluster, the value have to be set on true.\n\n## <a name=\"ConfigurationSystem\">System settings</a>  \n\n### motu.properties: Motu system settings  \n\nSystem settings are configured in file config/motu.properties  \nAll parameters can be updated in the file.  \n\n* [Java options](#ConfigurationSystemJavaOptions)\n* [Tomcat network ports](#ConfigurationSystemTomcatNetworkPorts)\n* [CAS SSO server](#ConfigurationSystemCASSSO)\n\n#### <a name=\"ConfigurationSystemJavaOptions\">Java options</a>\nThe three parameters below are used to tune the Java Virtual Machine, and the __tomcat-motu-jvm-javaOpts__ parameter can include any Java property in the form \"-D\\<java property name\\>=\\<value\\>\":  \n   &#35; -server: tells the Hostspot compiler to run the JVM in \"server\" mode (for performance)  \n__tomcat-motu-jvm-javaOpts__=-server -Xmx4096M -Xms512M -XX:MetaspaceSize=128M -XX:MaxMetaspaceSize=512M  \n__tomcat-motu-jvm-port-jmx__=9010  \n__tomcat-motu-jvm-address-debug__=9090  \n__tomcat-motu-jvm-umask__=tomcat|umask|0000 [(More details...)](#ConfigurationSystemTomcatUmask)\n\n##### <a name=\"ConfigurationSystemTomcatUmask\">Tomcat umask</a>\nBy default, if tomcat-motu-jvm-umask is not set, motu sets the umask with result of the command `umask`  \n__tomcat-motu-jvm-umask__=umask|tomcat|0000  \n* __umask__:  By default, if tomcat-motu-jvm-umask is not set, motu sets the umask with result of the command `umask`  \n* __tomcat__: Apache Tomcat process forces umask to 0027 (https://tomcat.apache.org/tomcat-8.5-doc/security-howto.html)  \n* __0000__:   Custom umask value  \nValues 0002 or umask are recommended if Motu download results are served by a frontal web server\n\n##### <a name=\"TdsHttpSoTimeout\">Java property tds.http.sotimeout</a>\nBy default this parameter is at \"300\". It represents the maximum delay in seconds for TDS to answer a MOTU request (reading timeout of the socket).  \nFor queries involving lots of files, TDS might need more than the default 5 minutes to answer, and to avoid the error \"004-27 : Error in NetcdfWriter finish\", this parameter can be set to a higher value in:  \n__tomcat-motu-jvm-javaOpts__=-server [...] -XX:MaxPermSize=512M -Dtds.http.sotimeout=4000\n\n##### <a name=\"TdsHttpConnTimeout\">Java property tds.http.conntimeout</a>\nBy default this parameter is at \"60\". It represents the maximum delay in seconds for TDS to accept a MOTU request (connection timeout on the socket).  \nThe paramater can be customized and added in:  \n__tomcat-motu-jvm-javaOpts__=-server [...] -XX:MaxPermSize=512M -Dtds.http.conntimeout=100\n\n#### <a name=\"ConfigurationSystemTomcatNetworkPorts\">Tomcat network ports</a>\nThe parameters below are used to set the different network ports used by Apache Tomcat.  \nAt startup, these ports are set in the file \"$installdir/motu/tomcat-motu/conf/server.xml\".    \nBut if this file already exist, it won't be replaced. So in order to apply these parameters, remove the file \"$installdir/motu/tomcat-motu/conf/server.xml\".  \n  \n__tomcat-motu-port-http__=9080  \n  &#35; HTTPs is in a common way managed from a frontal Apache HTTPd server. If you really need to use it from Tomcat, you have to tune the SSL certificates and the protocols directly in the file \"$installdir/motu/tomcat-motu/conf/server.xml\".  \n__tomcat-motu-port-https__=9443  \n__tomcat-motu-port-ajp__=9009  \n__tomcat-motu-port-shutdown__=9005  \n\n#### <a name=\"ConfigurationSystemCASSSO\">CAS SSO server</a>\n\n   &#35;  true or false to enable the SSO connection to a CAS server  \n__cas-activated__=false  \n  \n   &#35;  Cas server configuration to allow Motu to access it  \n   &#35;  @see https://wiki.jasig.org/display/casc/configuring+the+jasig+cas+client+for+java+in+the+web.xml  \n     \n   &#35;  The  start of the CAS server URL, i.e. https://cas-cis.cls.fr/cas  \n__cas-server-url__=https://cas-cis.cls.fr/cas   \n\n   &#35;  The Motu HTTP server URL, for example: http://misgw-ddo-qt.cls.fr:9080 or http://motu.cls.fr   \n   &#35;  If you use a frontal HTTPd server, you have to known if its URL will be called once the user will be login on CAS server.  \n   &#35;  In this case, set the Apache HTTPd server. The value will be http://$apacheHTTPdServer/motu-web/Motu So, in Apache HTTPd, you have to redirect this URL to the Motu Web server  \n__cas-auth-serverName__=http://$motuServerIp:$motuServerPort   \n\n   &#35;  The proxy callback HTTPs URL of the Motu server ($motuServerIp is either the Motu host or the frontal Apache HTTPs host ip or name. $motuServerHttpsPort is optional if default HTTPs port 443 is used, otherwise it is the same value as defined above with the key \"tomcat-motu-port-https\", or it is the port defined for the HTTPs server on the frontal Apache HTTPd)  \n__cas-validationFilter-proxyCallbackUrl__=https://$motuServerIp:$motuServerHttpsPort/motu-web/proxyCallback  \n  \n  \n__IMPORTANT__: Motu uses a Java HTTPs client to communicate with the CAS server. When the CAS server has an untrusted SSL certificate, you have to add it to Java default certificates or to add the Java property named \"javax.net.ssl.trustStore\" to target a CA keystore which contains the CAS Server SSL CA public key.\nFor example, add this property by setting Java option [tomcat-motu-jvm-javaOpts](#ConfigurationSystem):  \n```\ntomcat-motu-jvm-javaOpts=-server -Xmx4096M -Xms512M -XX:MetaspaceSize=128M -XX:MaxMetaspaceSize=512M -Djavax.net.ssl.trustStore=/opt/cmems-cis/motu/config/security/cacerts-with-cas-qt-ca.jks\n```\n\nThe following part is not relevant in the CMEMS context as the SSO CAS server has been signed by a known certification authority.  \nIf you need to run tests with your own SSO CAS server without any certificate signed by a known certification authority, you have to follow the following steps.  \n\nHow to build the file cacerts-with-cas-qt-ca.jks on Motu server?  \n\n* Download the certificate file (for example \"ca.crt\") of the authority which has signed the CAS SSO certificate on the CAS server machine (/opt/atoll/ssl/ca.crt) and copy it to \"${MOTU_HOME}/config/security/\", then rename it \"cas-qt-ca.crt\"\n* Copy the default Java cacerts \"/opt/cmems-cis-validation/motu/products/jdk1.7.0_79/jre/lib/security/cacerts\" file into ${MOTU_HOME}/config/security/\n  and rename this file to \"cacerts-with-cas-qt-ca.jks\"  \n  ```\n  cp /opt/cmems-cis-validation/motu/products/jdk1.7.0_79/jre/lib/security/cacerts /opt/cmems-cis/motu/config/security/  \n  mv /opt/cmems-cis/motu/config/security/cacerts /opt/cmems-cis/motu/config/security/cacerts-with-cas-qt-ca.jks  \n  ```\n* Then import \"cas-qt-ca.crt\" inside \"cacerts-with-cas-qt-ca.jks\", Trust the certificate=yes  \n  ```\n  /opt/cmems-cis-validation/motu/products/jdk1.7.0_79/bin/keytool -import -v -trustcacerts -alias $CAS_HOST_NAME -file cas-qt-ca.crt -keystore cacerts-with-cas-qt-ca.jks -keypass XXX  \n  ```  \n\n#### <a name=\"ConfigStandardNames\">NetCdf standard names</a>  \nWhen NetCdf variables are read in data files, either by Threads or directly by Motu, Motu wait for a standard name metadata sttribute to be found for each variable as requiered by the [CF convention](#http://cfconventions.org/Data/cf-standard-names/docs/guidelines.html).\nDue to any production constraints, some netcdf files does not have any standard_name attribute.  \nIn the case, you can add directly in the configuration folder, a file named standardNames.xml in order to map a standard_name to a netcdf variable name.  \nYou can find an example in Motu source: /motu-web/src/main/resources/standardNames.xml  \n\n#### Supervision\nTo enable the status supervision, set the parameter below:  \n__tomcat-motu-urlrewrite-statusEnabledOnHosts__=localhost,*.cls.fr\n\nThis parameter is used to set the property below in the WEB.XML file:  \n```\n        <!-- Documentation from http://tuckey.org/urlrewrite/manual/3.0/\n        you may want to allow more hosts to look at the status page\n        statusEnabledOnHosts is a comma delimited list of hosts, * can\n        be used as a wildcard (defaults to \"localhost, local, 127.0.0.1\") -->\n        <init-param>  \n            <param-name>statusEnabledOnHosts</param-name>  \n            <param-value>${tomcat-motu-urlrewrite-statusEnabledOnHosts}</param-value>  \n        </init-param>  \n```  \n\nFor more detail read:  \norg.tuckey UrlRewriteFilter FILTERS : see http://tuckey.org/urlrewrite/manual/3.0/  \n\n  \n## <a name=\"LogSettings\">Log settings</a>  \n\nLog are configured by using log4j2 in file config/log4j.xml  \n\n### Motu queue server logs: motuQSlog.xml, motuQSlog.csv\n\nThis log files are used to compute statistics about Motu server usage.  \nTwo format are managed by this log, either XML or CSV.  \nTo configure it, edit config/log4j.xml  \n\n##### Log format: XML or CSV  \nUpdate the fileFormat attribute of the node \"MotuCustomLayout\": <MotuCustomLayout fileFormat=\"xml\">\nA string either \"xml\" or \"csv\" to select the format in which log message are written.  \nAlso update the log file name extension of the attributes \"fileName\" and \"filePattern\" in order to get a coherent content in relationship with value set for MotuCustomLayout file format.  \nIf this attribute is not set, the default format is \"xml\".  \n``` \n        <RollingFile name=\"log-file-infos.queue\"   \n            fileName=\"${sys:motu-log-dir}/motuQSlog.xml\"   \n            filePattern=\"${sys:motu-log-dir}/motuQSlog.xml.%d{MM-yyyy}\"    \n            append=\"true\">   \n            <!-- fileFormat=xml or csv -->  \n            <MotuCustomLayout fileFormat=\"xml\" />  \n            <Policies>  \n                <TimeBasedTriggeringPolicy interval=\"1\" modulate=\"true\"/>  \n            </Policies>  \n        </RollingFile>  \n``` \n\n##### Log path\nIn the dissemination unit, Motu shares its log files with a central server.  \nLog files have to be save on a public access folder.  \nSet absolute path in \"fileName\" and \"filePattern\" attributes. This path shall be serve by the frontal Apache HTTPd or Apache Tomcat.\n  \nFor example, if you want to share account transaction log files, you edit config/log4j.xml. \nUpdate content below:  \n``` \n<RollingFile name=\"log-file-infos.queue\" fileName=\"${sys:motu-log-dir}/motuQSlog.xml\"\n            filePattern=\"${sys:motu-log-dir}/motuQSlog.xml.%d{MM-yyyy}\"\n```   \n with:  \n``` \n<RollingFile name=\"log-file-infos.queue\" fileName=\"/opt/cmems-cis/motu/data/public/transaction/motuQSlog.xml\"\n            filePattern=\"/opt/cmems-cis/motu/data/public/transaction/motuQSlog.xml.%d{MM-yyyy}\"\n```   \nNote that both attributes __fileName__ and __filePattern__ have been updated.  \nThen the frontal [Apache HTTPd server](#InstallFrontal) has to serve this folder.\n\n\n\n## <a name=\"ThemeStyle\">Theme and Style</a>  \nIn Motu you can update the theme of the website. There is 2 mains things in order to understand how it works?  \n\n* [Template] velocity: The velocity templates are used to generated HTML pages from Java objects.  \n* [Style] CSS, Images and JS: These files are used to control style and behaviour of the web UI.\n\nBy default, the template and style are integrated in the \"war\". But the Motu design enable to customize it easily.\n\n* [Template] velocity: You can change all templates defined in:\nmotu/tomcat-motu/webapps/motu-web/WEB-INF/lib/motu-web-2.6.00-SNAPSHOT.jar/velocityTemplates/*.vm\nby defining them in motu/config/velocityTemplates.\n\nThe main HTML web page structure is defined by the index.vm velocity template. For example, in you create a file motu/config/velocityTemplates/index.vm containing an empty html page, website will render empty web pages.  \n\"index.vm\" is the default theme. The name can be updated for each motuConfig#configService by setting veloTemplatePrefix=\"\".\nBy default veloTemplatePrefix=\"index\".\n\n\n* [Style] CSS, Images and JS: Those files are integrated with the default theme motu-web-2.6.00-SNAPSHOT.war/css/*, motu-web-2.6.00-SNAPSHOT.war/js/*. These files can be downloaded from an external server which enable to benefit to several mMotu server at he same time. The external server name can be updated for each motuConfig#configService by setting httpBaseRef=\"\".  \nBy default httpBaseRef search static files from the Motu web server, for example:  \n``` \nservice.getHttpBaseRef()/css/motu/screen/images/favicon.ico\"\n``` \n\n\n\n\n\n# <a name=\"Operation\">Operation</a>    \n\n## <a name=\"SS\">Start, Stop and other Motu commands</a>    \nAll operations are done from the Motu installation folder.  \nFor example:  \n``` \ncd /opt/cmems-cis/motu \n```\n\n### Start Motu\nStart the Motu process.  \n``` \n./motu start  \n```\n\n### Stop Motu  \nAt the shutdown of Motu, the server waits for none of the pending or in progress request to be in execution.  \nIf it's the case, the server waits the end of the request before shutdown.  \nNote that after waiting 10 minutes, server will automatically shutdown without waiting any running requests.  \nSo command below can respond quickly if no requests are in the queue server or takes time to process them.  \n``` \n./motu stop\n``` \n\nIf you needs to understand what Motu is waiting for, you can check the logbook:  \n``` \ntail -f log/logbook.log  \nStop in progress...  \nStop: Pending=0; InProgress=2  \nStop: Pending=0; InProgress=2  \n...  \nStop: Pending=0; InProgress=1  \nStop: Pending=0; InProgress=0  \n...  \nStop done  \n``` \n\nDuring the stop step, from a web browser, the user will be able to ends its download request if a front web server (Apache HTTPd) serves the statics files and the downloaded product.\nIn case where Motu is installed as a standalone web server, user will get a 500 HTTP error. For example in development or qualification environment, \nthis could lead to block the download of the files if Motu is used to serve both static and requested product files.\n\n\n### Advanced commands\n#### Restart Motu\n``` \n./motu restart\n``` \n\n#### Status of the Motu process\n``` \n./motu status\n``` \n\nStatus are the following: \n \n* __tomcat-motu started__ A pid file exists\n* __tomcat-motu stopped__ No pid file exists\n\n#### Help about Motu parameters\n``` \n./motu ?\n``` \n\n## <a name=\"ExpMonitorPerf\">Monitor performance</a> \n\nOnce started, you can use the Linux command \"top\" to check performance:  \n\n* __load average__ the three numbers shall be low and under the number of CPU (lscpu | grep Proc). For example if you have 4 processors this indicator can rise up to 4 but not above. If it is above, you have to add more CPU power.\n* __%CpuX, parameter wa__ This indicator shall be near 0 to indicate that processes does not wait to access to the disks. When this number is above 0.5 you have to improve access disk performance.\n* __KiB Mem__ Be sure that free memory is available. If it is less than 5000000, meaning less than 5GB, you have to add RAM memory in order to manage pic load. \n\nExample of top command:  \n\n``` \ntop - 11:07:01 up 19:46,  3 users,  ***load average: 0,05, 0,09, 0,25***  \nTasks: 395 total,   2 running, 393 sleeping,   0 stopped,   0 zombie  \n%Cpu0  :  1,0 us,  1,0 sy,  0,0 ni, 98,1 id,   ***0,0 wa***,  0,0 hi,  0,0 si,  0,0 st  \n%Cpu1  :  1,0 us,  0,0 sy,  0,0 ni, 99,0 id,  ***0,0 wa***,  0,0 hi,  0,0 si,  0,0 st  \nKiB Mem : 10224968 total,  ***4034876 free***,  3334576 used,  2855516 buff/cache    \n...\n``` \n\n  \n## <a name=\"Logbooks\">Logbooks</a>    \n\nLog messages are generated by Apache Log4j 2. The configuration file is \"config/log4j.xml\".  \nBy default, log files are created in the folder $MOTU_HOME/log. This folder contains Motu log messages.  \nTomcat log messages are generated in the tomcat-motu/logs folder.  \n\n* __Motu log messages__\n  * __logbook.log__: All Motu log messages including WARN and ERROR(without stacktrace) messages.\n  * __warnings.log__: Only Motu log messages with a WARN level\n  * <a name=\"LogbooksErrors\">__errors.log__</a>: Only Motu log messages with an ERROR level. When this file is not empty, it means that at least an error has been generated by the Motu application.\n  * <a name=\"LogbooksTransactions\">__motuQSlog.xml__, __motuQSlog.csv__</a>: Either a \"CSV\" or \"XML\" format which logs all queue events.\n     * CSV: On one unique line, writes:  \n    [OK | ERR;ErrCode;ErrMsg;ErrDate];  \n    queueId;queueDesc;\n    requestId;  \n    elapsedWaitQueueTime;elapsedRunTime;elapsedTotalTime;totalIOTime;preparingTime;readingTime;  \n    inQueueTime;startTime;endTime;  \n    amountDataSize;  \n    downloadUrlPath;extractLocationData;  \n    serviceName;TemporalCoverageInDays;ProductId;UserId;UserHost;isAnonymousUser;  \n    variable1;variable2;...;variableN;  \n    temporalMin,temporalMax;  \n    LatitudeMin;LongitudeMin;LatitudeMax;LongitudeMax:\n    DepthMin;DepthMax;\n     * XML: XStream is used to serialized a Java Object to XML from fr.cls.atoll.motu.web.bll.request.queueserver.queue.log.QueueLogInfo  \n     Same data are represented.\n     * Field details\n         * queueId, queueDesc: Queue used to process the request. Id and description found in config/motuConfiguration.xml\n         * requestId: A timestamp representing the request id.\n         * inQueueTime: Timestamp with format \"yyyy-MM-dd' 'HH:mm:ss.SSS\" when the request has been put in the queue\n         * startTime: Timestamp with format \"yyyy-MM-dd' 'HH:mm:ss.SSS\" when the request has been started to be processed\n         * endTime: Timestamp with format \"yyyy-MM-dd' 'HH:mm:ss.SSS\" when the request has been ended to be processed\n         * elapsedWaitQueueTime: Duration in milliseconds, [startTime - inQueueTime]\n         * elapsedRunTime: Duration in milliseconds, [endTime - startTime]\n         * elapsedTotalTime: Duration in milliseconds, [endTime - inQueueTime]\n         * totalIOTime: Duration in nanoseconds: reading + writing + copying + compressing times.\n         * readingTime: Duration in nanoseconds.\n         * writingTime: Duration in nanoseconds.\n         * preparingTime: Duration in nanoseconds, same value as reading time.\n         * copyingTime: Duration in nanoseconds, only set in DGF mode.\n         * compressingTime: Duration in nanoseconds, only set in DGF mode.\n         * amountDataSize: Size in MegaBytes\n         * downloadUrlPath: URL to download the product\n         * extractLocationData: Absolute path on the server\n         * serviceName: The service name found in the configuration file motuConfiguration.xml\n         * TemporalCoverageInDays: duration in days\n         * ProductId: Product id\n         * UserId: User login if user is not anonymous, otherwise its host or IP address from which he is connected\n         * UserHost: Host or ip address from which user is connected\n         * isAnonymousUser: true or false                \n         * variable1;variable2;...;variableN; Extracted variable names\n         * temporalMin,temporalMax: Temporal coverage\n         * LatitudeMin;LongitudeMin;LatitudeMax;LongitudeMax: Geographical coverage (latitude:-90;+90; longitude:180;+180)\n         * DepthMin;DepthMax;: Depth coverage   \n  * __velocity.log__: Logs generated by the http://velocity.apache.org/ technology to render HTML web pages.\n\n* __Tomcat log messages__: This folder contains all Apache Tomcat log files. The file below is important to check startup logs:  \n  * __catalina.out__: Catalina output matching the environment variable CATALINA_OUT.\n    \n\n## <a name=\"AdminDataSetAdd\">Add a dataset</a>    \nIn order to add a new Dataset, you have to add a new configService node in the [Motu business configuration](#ConfigurationBusiness).  \nWhen Motu read data through TDS (Opendap or NCSS service) url, the data shall be configured in TDS before this configuration is saved in Motu. The [TDS configuration](https://www.unidata.ucar.edu/software/thredds/v4.6/tds/catalog/index.html) is not explained here.  \n\nWithin CMEMS, the datasets are organized in a tree structure, where the product granularity appears above the dataset granularity.  \nTo be noticed:  \n\n* All gridded dataset shall be configured in TDS, to be served through the subsetter of Motu  \n* A product is a coherent group of datasets. The product is the granularity used in the catalogue of CMEMS  \n* In the XML tree structure of the TDS configuration, each product shall be configured through a unique node  \n* This node shall correspond to one XML file in the TDS configuration (for example GLOBAL_ANALYSIS_PHYS_001_016.xml) and shall be further referenced in the motuConfiguration.xml file as one <catalog name> (for example <catalog name=\" GLOBAL_ANALYSIS_PHYS_001_016.xml>)  \n* The value of the \"name\" attribute of the element <dataset> shall be identical to the Product Name (from CMEMS Product Information Table). \nIn the example below named \u201cCMEMS DU xxx Thredds Catalog\u201d there are three datasets. The following catalog tree presents a hierarchical organization for this catalog.\n      \n```       \n<  CMEMS DU xxx Thredds Catalog >  \n| ------ < GLOBAL_ANALYSIS_PHYS_001_016  >   \n|------- < dataset-armor-3d-v5-myocean >  \n|----------------- < GLOBAL_REP_PHYS_001_013  >   \n|------- < dataset-armor-3d-rep-monthly-v3-1-myocean >                                                                     \n|------- < dataset-armor-3d-rep-weekly-v3-1-myocean>   \n``` \n\nThe Motu configuration (motuConfiguration.xml) should reference the node corresponding to one XML file in the TDS configuration.\n\n  \nExamples:  \n\n* __TDS NCSS protocol__:  \nThis is the fastest protocol implemented by Motu. Motu select this protocol because type is set to \"tds\" and ncss is set to \"enabled\".  \n\n``` \n<configService description=\"Free text to describe your dataSet\" group=\"HR-Sample\" httpBaseRef=\"\" name=\"HR_MOD-TDS\" veloTemplatePrefix=\"\" profiles=\"external\">  \n        <catalog name=\"m_HR_MOD.xml\" type=\"tds\" ncss=\"enabled\" urlSite=\"http://$tdsUrl/thredds/\"/>  \n</configService>  \n```  \n  \n* __TDS Opendap protocol__:  \nHere OpenDap is used because it is the default protocol when tds type is set and ncss is not set or is disable.  \n\n``` \n<configService description=\"Free text to describe your dataSet\" group=\"HR-Sample\" httpBaseRef=\"\" name=\"HR_MOD-TDS\" veloTemplatePrefix=\"\" profiles=\"external\">  \n        <catalog name=\"m_HR_MOD.xml\" type=\"tds\" ncss=\"\" urlSite=\"http://$tdsUrl/thredds/\"/>  \n</configService>  \n```  \n\n* __DGF protocol__:   \nThis protocol is used to access to local files. With this protocol user download the full data source file and can run only temporal extractions on the dataset (As a reminder, a dataset is temporal aggregation of several datasource files.    \n\n```\n<configService description=\"Free text to describe your dataSet\" group=\"HR-Sample\" profiles=\"internal, external, major\" httpBaseRef=\"\" name=\"HR_MOD-TDS\" veloTemplatePrefix=\"\">  \n           <catalog name=\"catalogFILE_GLOBAL_ANALYSIS_PHYS_001_016.xml\" type=\"file\" urlSite=\"file:///opt/cmems-cis-data/data/public/inventories\"/>  \n</configService>  \n```\n\nAn an example, the file __catalogFILE_GLOBAL_ANALYSIS_PHYS_001_016.xml__ contains:  \n\n```\n< ?xml version=\"1.0\" encoding=\"UTF-8\"?>  \n<!DOCTYPE rdf:RDF [  \n<!ENTITY atoll \"http://purl.org/cls/atoll/ontology/individual/atoll#\">  \n]>  \n<catalogOLA xmlns=\"http://purl.org/cls/atoll\" name=\"catalog GLOBAL-ANALYSIS-PHYS-001-016\">  \n        <resourcesOLA>  \n                <resourceOLA urn=\"dataset-armor-3d-v5-myocean\" inventoryUrl=\"file:///opt/cmems-cis-data/data/public/inventories/dataset-armor-3d-v5-myocean-cls-toulouse-fr-armor-motu-rest-file.xml\"/>  \n        </resourcesOLA>  \n</catalogOLA>    \n```\n\nFile __dataset-armor-3d-v5-myocean-cls-toulouse-fr-armor-motu-rest-file.xml__:  \n\n```\n< ?xml version=\"1.0\" encoding=\"UTF-8\"?>  \n<!DOCTYPE rdf:RDF [  \n<!ENTITY atoll \"http://purl.org/cls/atoll/ontology/individual/atoll#\">  \n<!ENTITY cf \"http://purl.org/myocean/ontology/vocabulary/cf-standard-name#\">  \n<!ENTITY cu \"http://purl.org/myocean/ontology/vocabulary/cf-unofficial-standard-name#\">  \n<!ENTITY ct \"http://purl.org/myocean/ontology/vocabulary/forecasting#\">  \n<!ENTITY cp \"http://purl.org/myocean/ontology/vocabulary/grid-projection#\">  \n]>  \n<inventory lastModificationDate=\"2016-01-27T00:10:10+00:00\" xmlns=\"http://purl.org/cls/atoll\" updateFrequency=\"P1D\">  \n  <service urn=\"cls-toulouse-fr-armor-motu-rest-file\"/>  \n  <resource urn=\"dataset-armor-3d-v5-myocean\">  \n    <access urlPath=\"file:///data/atoll/armor/armor-3d-v3/\"/>  \n    <geospatialCoverage south=\"-82\" north=\"90\" west=\"0\" east=\"359.75\"/>  \n    <depthCoverage min=\"0\" max=\"5500\" units=\"m\"/>  \n    <timePeriod start=\"2014-10-01T00:00:00+00:00\" end=\"2016-01-26T23:59:59+00:00\"/>  \n    <theoricalTimePeriod start=\"2014-10-01T00:00:00+00:00\" end=\"2016-01-26T23:59:59+00:00\"/>  \n    <variables>  \n      <variable name=\"zvelocity\" vocabularyName=\"http://mmisw.org/ont/cf/parameter/eastward_sea_water_velocity\" units=\"m/s\"/>  \n      <variable name=\"height\" vocabularyName=\"http://purl.org/myocean/ontology/vocabulary/cf-standard-name#height_above_geoid\" units=\"m\"/>  \n      <variable name=\"mvelocity\" vocabularyName=\"http://mmisw.org/ont/cf/parameter/northward_sea_water_velocity\" units=\"m/s\"/>  \n      <variable name=\"salinity\" vocabularyName=\"http://mmisw.org/ont/cf/parameter/sea_water_salinity\" units=\"1e-3\"/>  \n      <variable name=\"temperature\" vocabularyName=\"http://mmisw.org/ont/cf/parameter/sea_water_temperature\" units=\"degC\"/>  \n    </variables>  \n  </resource>  \n  <files>  \n    <file name=\"ARMOR3D_TSHUV_20141001.nc\" weight=\"327424008\" modelPrediction=\"http://www.myocean.eu.org/2009/resource/vocabulary/forecasting#\" startCoverageDate=\"2014-10-01T00:00:00+00:00\" endCoverageDate=\"2014-10-07T23:59:59+00:00\" creationDate=\"2015-03-17T00:00:00+00:00\" availabilitySIDate=\"2016-01-27T00:10:10+00:00\" availabilityServiceDate=\"2016-01-27T00:10:10+00:00\" theoreticalAvailabilityDate=\"2015-03-17T00:00:00+00:00\"/>  \n    <file name=\"ARMOR3D_TSHUV_20141008.nc\" weight=\"327424008\" modelPrediction=\"http://www.myocean.eu.org/2009/resource/vocabulary/forecasting#\" startCoverageDate=\"2014-10-08T00:00:00+00:00\" endCoverageDate=\"2014-10-14T23:59:59+00:00\" creationDate=\"2015-03-17T00:00:00+00:00\" availabilitySIDate=\"2016-01-27T00:10:10+00:00\" availabilityServiceDate=\"2016-01-27T00:10:10+00:00\" theoreticalAvailabilityDate=\"2015-03-17T00:00:00+00:00\"/>  \n    ...  \n    <file name=\"ARMOR3D_TSHUV_20160120.nc\" weight=\"327424008\" modelPrediction=\"http://www.myocean.eu.org/2009/resource/vocabulary/forecasting#\" startCoverageDate=\"2016-01-20T00:00:00+00:00\" endCoverageDate=\"2016-01-26T23:59:59+00:00\" creationDate=\"2016-01-26T11:11:00+00:00\" availabilitySIDate=\"2016-01-27T00:10:10+00:00\" availabilityServiceDate=\"2016-01-27T00:10:10+00:00\" theoreticalAvailabilityDate=\"2016-01-26T11:11:00+00:00\"/>  \n  </files>  \n</inventory>  \n```  \n\n\n## <a name=\"AdminMetadataCache\">Tune the dataset metadata cache</a>  \nIn order to improve response time, Motu uses an in-memory cache which stores datasets metadata. This cache is indexed by config service.    \nYou can tune the cache behaviour in order to manage both real time and archived datasets effectively.      \nAt startup, Motu loads datasets metadata of each configService by turn. Once done, cache is refreshed either periodically in an automatic manner or either when asked by triggering a [specific action](#ClientAPI_RefreshCache).  \nThe cache is kept in memory and all Motu requests are based on it. When a cache refresh is asked, a second cache loads new metadata and when fully loaded, Motu main cache is replaced. So until the full loading, old cache is used in Motu responses.   \n\nFor config services which manages real time datasets, meaning datasets which are daily updated, you can set the following configuration in motuConfiguration.xml:\n```  \n<?xml version=\"1.0\"?>\n<motuConfig  ...\n<configService ... refreshCacheAutomaticallyEnabled=\"true\"\n...\n```  \n\nFor config services which manages archived datasets, meaning dataset which not updated frequently for example only once a week, you can set the following configuration in motuConfiguration.xml:\n```  \n<?xml version=\"1.0\"?>\n<motuConfig  ...\n<configService ... refreshCacheAutomaticallyEnabled=\"false\"\n...\n```  \nIn this case, when you want to refresh metadata cache of these datasets, you can use this dedicated [action](#ClientAPI_RefreshCache).\n\n\n## <a name=\"ExploitDebug\">Debug view</a>  \nFrom a web browser access to the Motu web site with the URL:  \n``` \n/Motu?action=debug  \n``` \n\nYou can see the different requests and their [status](#ClientAPI_Debug).  \nYou change the status order by entering 4 parameters in the URL:  \n``` \n/Motu?action=debug&order=DONE,ERROR,PENDING,INPROGRESS\n``` \n\n\n \n \n## <a name=\"ExploitCleanDisk\">Clean files</a>  \n\n## <a name=\"ExploitCleanDiskLogbook\">Logbook files</a>   \nLogbook files are written by Apache Tomcat server and Motu application.  \n \n### <a name=\"ExploitCleanDiskLogbookTomcat\">Apache Tomcat Logbook files</a>  \nTomcat writes log files in folder tomcat-motu/logs.  \nYou can customize this default configuration by editing tomcat-motu/conf/logging.properties  \nThis file is the default file provided by Apache Tomcat.\nThere is a daily rotation so you can clean those files to fullfill the harddrive.   \ncrontab -e   \n0 * * * * find /opt/cmems-cis/motu/tomcat-motu/logs/*.log* -type f -mmin +14400 -delete >/dev/null 2>&1   \n0 * * * * find /opt/cmems-cis/motu/tomcat-motu/logs/*.txt* -type f -mmin +14400 -delete >/dev/null 2>&1   \n\n### <a name=\"ExploitCleanDiskLogbookMotu\">Motu Logbook files</a>  \nLogbook files are written in the folder(s) configured in the log4j.xml configuration file.  \nAll logs are generated daily except for motuQSLog (xml or csv) which are generated monthly.  \nYou can clean those files to avoid to fullfill the harddrive.   \ncrontab -e   \n0 * * * * find /opt/cmems-cis/motu/log/*.log* -type f -mmin +14400 -delete >/dev/null 2>&1  \n0 * * * * find /opt/cmems-cis/motu/log/*.out* -type f -mmin +14400 -delete >/dev/null 2>&1  \n0 * * * * find /opt/cmems-cis/motu/log/*.xml* -type f -mmin +144000 -delete >/dev/null 2>&1  \n0 * * * * find /opt/cmems-cis/motu/log/*.csv* -type f -mmin +144000 -delete >/dev/null 2>&1  \n  \nNote that Motu is often tuned to write the motuQSLog in a dedicated folder. So you have to clean log files in this folder too. For example:  \n0 * * * * find /opt/cmems-cis/motu/data/public/transaction/*.xml* -type f -mmin +144000 -delete >/dev/null 2>&1  \n0 * * * * find /opt/cmems-cis/motu/data/public/transaction/*.csv* -type f -mmin +144000 -delete >/dev/null 2>&1 \n\n## <a name=\"LogCodeErrors\">Log Errors</a>   \n\n### The code pattern\nThe error codes of Motu as the following format \"XXXX-Y\":\n  \n* [XXXX](#LogCodeErrorsActionCode) code matching the action which is executed when the error is raised. This part is the \"ActionCode\".  The action is in general a HTTP request and matches the following HTTP parameter http://$server/motu-web/Motu?action=.\n* [Y](#LogCodeErrorsErrorType) code which identifies the part of the program from which the error was raised. This part is the \"ErrorType\".\n  \n  \nFor example, the web browser can display:  \n011-1 : A system error happened. Please contact the administrator of the site. \n\nHere, we have the error code in order to understand better what happens. But the end user has a generic message and no detail is given to him. These end user messages are described in the file \"/motu-web/src/main/resources/MessagesError.properties\". The file provided with the project is a default one and can be customized for specific purposes. Just put this file in the \"config\" folder, edit it and restart Motu to take it into account. So when a user has an error, it just have to tell you the error code and you can check the two numbers with the descriptions below.  \n\n\n### <a name=\"LogCodeErrorsActionCode\">Action codes</a>  \n\nThe Action Code        =>    A number matching the HTTP request with the action parameter.\n\n001        =>    UNDETERMINED\\_ACTION           \n002        =>    PING\\_ACTION                   \n003        =>    DEBUG\\_ACTION                  \n004        =>    GET\\_REQUEST\\_STATUS\\_ACTION     \n005        =>    GET\\_SIZE\\_ACTION               \n006        =>    DESCRIBE\\_PRODUCT\\_ACTION       \n007        =>    TIME\\_COVERAGE\\_ACTION          \n008        =>    LOGOUT\\_ACTION                 \n010        =>    DOWNLOAD\\_PRODUCT\\_ACTION       \n011        =>    LIST\\_CATALOG\\_ACTION           \n012        =>    PRODUCT\\_METADATA\\_ACTION       \n013        =>    PRODUCT\\_DOWNLOAD\\_HOME\\_ACTION  \n014        =>    LIST\\_SERVICES\\_ACTION              \n015        =>    DESCRIBE\\_COVERAGE\\_ACTION         \n016        =>    ABOUT\\_ACTION  \n018        =>    WELCOME\\_ACTION  \n019        =>    REFRESH\\_CACHE\\_ACTION  \n020        =>    HEALTHZ\\_ACTION  \n021        =>    CACHE\\_STATUS\\_ACTION  \n\n### <a name=\"LogCodeErrorsErrorType\">Error types</a>  \n\nThe Error Type Code    =>    A number defining a specific error on the server.\n\n0         =>    No error.  \n1         =>    There is a system error. Please contact the Administrator.    \n2         =>    There is an error with the parameters. There are inconsistent.         \n3         =>    The date provided into the parameters is invalid.         \n4         =>    The latitude provided into the parameters is invalid.  \n5         =>    The longitude provided into the parameters is invalid.         \n6         =>    The range defined by the provided dates is invalid.         \n7         =>    The memory capacity of the motu server is exceeded.         \n8         =>    The range defined by the provided latitude/longitude parameters is invalid.         \n9         =>    The range defined by the provided depth parameters is invalid.         \n10        =>    The functionality is not yet implemented.         \n11        =>    There is an error with the provided NetCDF variables.         \n12        =>    There is not variables into the variable parameter.         \n13        =>    NetCDF parameter error. Example: Invalid date range, invalid depth range, ...         \n14        =>    There is an error with the provided NetCDF variable. Have a look at the log file to have more information.         \n15        =>    The number of maximum request in the queue server pool is reached. it's necessary to wait that some requests are finished.         \n16        =>    The number of maximum request for the user is reached. It's necessary to wait that some requests are finished for the user.         \n18        =>    The priority of the request is invalid in the queue server manager. Have a look at the log file to have more information.         \n19        =>    The id of the request is not know by the server. Have a look at the log file to have more information.         \n20        =>    The size of the request is greater than the maximum data managed by the available queue. It's impossible to select a queue for this request. It's necessary to narrow the request.         \n21        =>    The application is shutting down. it's necessary to wait a while before the application is again available.         \n22        =>    There is a problem with the loading of the motu configuration file. Have a look at the log file to have more information.         \n23        =>    There is a problem with the loading of the catalog configuration file. Have a look at the log file to have more information.         \n24        =>    There is a problem with the loading of the error message configuration file. Have a look at the log file to have more information.         \n25        =>    There is a problem with the loading of the netcdf file. Have a look at the log file to have more information.         \n26        =>    There is a problem with the provided parameters. Have a look at the log file to have more information.         \n27        =>    There is a problem with the NetCDF generation engine. Have a look at the log file to have more information.         \n28        =>    The required action is unknown. Have a look at the log file to have more information.  \n29        =>    The product is unknown.  \n30        =>    The service is unknown.  \n31        =>    The request cut the ante meridian. In this case, it's not possible to request more than one depth. It's necessary to change the depth selection and to select in the \"from\" and the \"to\" the values that have the same index into the depth list.  \n32        =>  \tDue to a known bug in Thredds Data Server, a request cannot be satisfied wit netCDF4. User has to request a netCDF3 output file.  \n101\t\t  =>\tWCS specific error code : A WCS mandatory parameter is missing  \n102\t\t  =>\tWCS specific error code : A WCS parameter doesn't match the mandatory format  \n103\t\t  =>\tWCS specific error code : The WCS version parameter is not compatible with the Motu WCS server  \n104\t\t  =>\tWCS specific error code : A system error append.  \n105\t\t  =>\tWCS specific error code : The coverage ident doesn't exist  \n106\t\t  =>\tWCS specific error code : The list of coverage id is empty  \n107\t\t  =>\tWCS specific error code : The provided parameter used to define a subset is invalid  \n108\t\t  =>\tWCS specific error code : The provided axis label doesn't match any available label  \n\n  \n# <a name=\"ClientsAPI\">Motu clients & REST API</a>  \n\nYou can connect to Motu by using a web browser or a client.\n\n## <a name=\"ClientPython\">Python client</a>   \nMotu offers an easy to use Python client. Very useful in machine to machine context, it enables to download data by running a python script.   \nProject and all its documentation is available at [https://github.com/clstoulouse/motu-client-python](https://github.com/clstoulouse/motu-client-python).\n  \n## <a name=\"OGC_WCS_API\">OGC WCS API</a>  \nMotu offers a Web Service interface which implements the OGC WCS standard described, in particular, by the two following documents on the OGC web site:  \n\n* [09-110r4_WCS_Core_2.0.1.pdf](https://portal.opengeospatial.org/files/09-110r4)\n* [09-147_WCS_2.0_Extension_--_KVP_Protocol.pdf](http://portal.opengeospatial.org/files/?artifact_id=36263&format=pdf)\n\nAvailable Web Services are:  \n\n* [Get capabilities](#GetCapabilities)\n* [Describe coverage](#DescribeCoverage)\n* [Get coverage](#GetCoverage)\n\nParameters can be added to each request and they are described with their cardinality [x,y].  \n\n* [0,1] is an optional parameter.   \n* [1] is a mandatory parameter.  \n* [0,n] is an optional parameter which can be set several times.  \n* [1,n] is a mandatory parameter which can be set several times. \n\n### <a name=\"GetCapabilities\">WCS: Get Capabilities</a>\n\nThe GetCapabilities request retrieves all available products defined on Motu server.\n\n\n__URL__: http://localhost:8080/motu-web/wcs?service=WCS&version=2.0.1&request=GetCapabilities\n\n__Parameters__:  \n\n* __service [1]:__ Value is fixed to \"WCS\"\n* __version [1]:__ Value is fixed to \"2.0.1\"\n* __request [1]:__ Value is fixed to \"GetCapabilties\"\n\n__Return__: \nA XML document as shown below:\n\n<pre>\n  <code>\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?&gt;\n&lt;ns3:Capabilities version=\"2.0.1\" xmlns:ns6=\"http://www.opengis.net/swe/2.0\" xmlns:ns5=\"http://www.opengis.net/gmlcov/1.0\" xmlns:ns2=\"http://www.w3.org/1999/xlink\" xmlns:ns1=\"http://www.opengis.net/ows/2.0\" xmlns:ns4=\"http://www.opengis.net/gml/3.2\" xmlns:ns3=\"http://www.opengis.net/wcs/2.0\"&gt;\n    &lt;ns1:ServiceIdentification&gt;\n        &lt;ns1:Title&gt;Motu&lt;/ns1:Title&gt;\n        &lt;ns1:Abstract&gt;Motu WCS service&lt;/ns1:Abstract&gt;\n        &lt;ns1:ServiceType&gt;OGC WCS&lt;/ns1:ServiceType&gt;\n        &lt;ns1:ServiceTypeVersion&gt;2.0.1&lt;/ns1:ServiceTypeVersion&gt;\n        &lt;ns1:Profile&gt;http://www.opengis.net/spec/WCS/2.0/conf/core&lt;/ns1:Profile&gt;\n        &lt;ns1:Profile&gt;http://www.opengis.net/spec/WCS_protocol-binding_get-kvp/1.0/conf/get-kvp&lt;/ns1:Profile&gt;\n    &lt;/ns1:ServiceIdentification&gt;\n    &lt;ns1:OperationsMetadata&gt;\n        &lt;ns1:Operation name=\"GetCapabilities\"&gt;\n            &lt;ns1:DCP&gt;\n                &lt;ns1:HTTP&gt;\n                    &lt;ns1:Get ns2:href=\"http://localhost:8080/motu-web/wcs\"/&gt;\n                &lt;/ns1:HTTP&gt;\n            &lt;/ns1:DCP&gt;\n        &lt;/ns1:Operation&gt;\n        &lt;ns1:Operation name=\"DescribeCoverage\"&gt;\n            &lt;ns1:DCP&gt;\n                &lt;ns1:HTTP&gt;\n                    &lt;ns1:Get ns2:href=\"http://localhost:8080/motu-web/wcs\"/&gt;\n                &lt;/ns1:HTTP&gt;\n            &lt;/ns1:DCP&gt;\n        &lt;/ns1:Operation&gt;\n        &lt;ns1:Operation name=\"GetCoverage\"&gt;\n            &lt;ns1:DCP&gt;\n                &lt;ns1:HTTP&gt;\n                    &lt;ns1:Get ns2:href=\"http://localhost:8080/motu-web/wcs\"/&gt;\n                &lt;/ns1:HTTP&gt;\n            &lt;/ns1:DCP&gt;\n        &lt;/ns1:Operation&gt;\n    &lt;/ns1:OperationsMetadata&gt;\n    &lt;ns3:ServiceMetadata&gt;\n        &lt;ns3:formatSupported&gt;application/netcdf&lt;/ns3:formatSupported&gt;\n    &lt;/ns3:ServiceMetadata&gt;\n    &lt;ns3:Contents&gt;\n        &lt;ns3:CoverageSummary&gt;\n            &lt;ns3:CoverageId&gt;HR_MOD_NCSS-TDS@HR_MOD&lt;/ns3:CoverageId&gt;\n            &lt;ns3:CoverageSubtype&gt;ns3:GridCoverage&lt;/ns3:CoverageSubtype&gt;\n        &lt;/ns3:CoverageSummary&gt;\n\n            ...\n\n\t\t&lt;/ns3:Contents&gt;\n&lt;/ns3:Capabilities&gt;\n  </code>\n</pre>\n\n\n\n### <a name=\"DescribeCoverage\">WCS: Describe Coverage</a>\n\nThe DescribeCoverage request retrieves the parameters description and the list of available  variables.\nFor the parameters description, low and high values are provided.\n\n__URL__: http://localhost:8080/motu-web/wcs?service=WCS&version=2.0.1&request=DescribeCoverage&coverageId=$coverageId\n\n__Parameters__:  \n\n* __service [1]:__ Value is fixed to \"WCS\"\n* __version [1]:__ Value is fixed to \"2.0.1\"\n* __request [1]:__ Value is fixed to \"DescribeCoverage\"\n* __coverageId [1]:__ list of identifiers of the required coverages. Each coverage identifiers are separated by a comma (,). CoverageId are returned by the [GetCapabilities](#GetCapabilities) service.\n\n__Return__: \nA XML document as shown below:\n\n<pre>\n  <code>\n  &lt;?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?&gt;\n&lt;ns4:CoverageDescriptions xmlns:ns6=\"http://www.opengis.net/ows/2.0\"\n\txmlns:ns5=\"http://www.opengis.net/swe/2.0\" xmlns:ns2=\"http://www.w3.org/1999/xlink\"\n\txmlns:ns1=\"http://www.opengis.net/gml/3.2\" xmlns:ns4=\"http://www.opengis.net/wcs/2.0\"\n\txmlns:ns3=\"http://www.opengis.net/gmlcov/1.0\"&gt;\n\t&lt;ns4:CoverageDescription ns1:id=\"$covergaeId\"&gt;\n\t\t&lt;ns1:boundedBy&gt;\n\t\t\t&lt;ns1:Envelope\n\t\t\t\tuomLabels=\"latitude longitude depth date in seconds since 1970, 1 jan\"\n\t\t\t\taxisLabels=\"Lat Lon Height Time\"&gt;\n\t\t\t\t&lt;ns1:lowerCorner&gt;-80.0 -180.0 0.0 1.3565232E9&lt;/ns1:lowerCorner&gt;\n\t\t\t\t&lt;ns1:upperCorner&gt;90.0 180.0 5728.0 1.4657328E9&lt;/ns1:upperCorner&gt;\n\t\t\t&lt;/ns1:Envelope&gt;\n\t\t&lt;/ns1:boundedBy&gt;\n\t\t&lt;ns4:CoverageId&gt;$covergaeId&lt;/ns4:CoverageId&gt;\n\t\t&lt;ns1:domainSet&gt;\n\t\t\t&lt;ns1:Grid dimension=\"4\"\n\t\t\t\tuomLabels=\"latitude longitude depth date in seconds since 1970, 1 jan\"\n\t\t\t\taxisLabels=\"Lat Lon Height Time\" ns1:id=\"Grid000\"&gt;\n\t\t\t\t&lt;ns1:limits&gt;\n\t\t\t\t\t&lt;ns1:GridEnvelope&gt;\n\t\t\t\t\t\t&lt;ns1:low&gt;-80 -180 0 1356523200&lt;/ns1:low&gt;\n\t\t\t\t\t\t&lt;ns1:high&gt;90 180 5728 1465732800&lt;/ns1:high&gt;\n\t\t\t\t\t&lt;/ns1:GridEnvelope&gt;\n\t\t\t\t&lt;/ns1:limits&gt;\n\t\t\t&lt;/ns1:Grid&gt;\n\t\t&lt;/ns1:domainSet&gt;\n\t\t&lt;ns3:rangeType&gt;\n\t\t\t&lt;ns5:DataRecord&gt;\n\t\t\t\t&lt;ns5:field name=\"uice\"&gt;\n\t\t\t\t\t&lt;ns5:Quantity&gt;\n\t\t\t\t\t\t&lt;ns5:uom code=\"m s-1\" /&gt;\n\t\t\t\t\t&lt;/ns5:Quantity&gt;\n\t\t\t\t&lt;/ns5:field&gt;\n\t\t\t\t&lt;ns5:field name=\"salinity\"&gt;\n\t\t\t\t\t&lt;ns5:Quantity&gt;\n\t\t\t\t\t\t&lt;ns5:uom code=\"1e-3\" /&gt;\n\t\t\t\t\t&lt;/ns5:Quantity&gt;\n\t\t\t\t&lt;/ns5:field&gt;\n\t\t\t\t&lt;ns5:field name=\"vice\"&gt;\n\t\t\t\t\t&lt;ns5:Quantity&gt;\n\t\t\t\t\t\t&lt;ns5:uom code=\"m s-1\" /&gt;\n\t\t\t\t\t&lt;/ns5:Quantity&gt;\n\t\t\t\t&lt;/ns5:field&gt;\n\t\t\t\t&lt;ns5:field name=\"hice\"&gt;\n\t\t\t\t\t&lt;ns5:Quantity&gt;\n\t\t\t\t\t\t&lt;ns5:uom code=\"m\" /&gt;\n\t\t\t\t\t&lt;/ns5:Quantity&gt;\n\t\t\t\t&lt;/ns5:field&gt;\n\t\t\t\t&lt;ns5:field name=\"u\"&gt;\n\t\t\t\t\t&lt;ns5:Quantity&gt;\n\t\t\t\t\t\t&lt;ns5:uom code=\"m s-1\" /&gt;\n\t\t\t\t\t&lt;/ns5:Quantity&gt;\n\t\t\t\t&lt;/ns5:field&gt;\n\t\t\t\t&lt;ns5:field name=\"v\"&gt;\n\t\t\t\t\t&lt;ns5:Quantity&gt;\n\t\t\t\t\t\t&lt;ns5:uom code=\"m s-1\" /&gt;\n\t\t\t\t\t&lt;/ns5:Quantity&gt;\n\t\t\t\t&lt;/ns5:field&gt;\n\t\t\t\t&lt;ns5:field name=\"temperature\"&gt;\n\t\t\t\t\t&lt;ns5:Quantity&gt;\n\t\t\t\t\t\t&lt;ns5:uom code=\"K\" /&gt;\n\t\t\t\t\t&lt;/ns5:Quantity&gt;\n\t\t\t\t&lt;/ns5:field&gt;\n\t\t\t\t&lt;ns5:field name=\"ssh\"&gt;\n\t\t\t\t\t&lt;ns5:Quantity&gt;\n\t\t\t\t\t\t&lt;ns5:uom code=\"m\" /&gt;\n\t\t\t\t\t&lt;/ns5:Quantity&gt;\n\t\t\t\t&lt;/ns5:field&gt;\n\t\t\t\t&lt;ns5:field name=\"fice\"&gt;\n\t\t\t\t\t&lt;ns5:Quantity&gt;\n\t\t\t\t\t\t&lt;ns5:uom code=\"1\" /&gt;\n\t\t\t\t\t&lt;/ns5:Quantity&gt;\n\t\t\t\t&lt;/ns5:field&gt;\n\t\t\t&lt;/ns5:DataRecord&gt;\n\t\t&lt;/ns3:rangeType&gt;\n\t&lt;/ns4:CoverageDescription&gt;\n&lt;/ns4:CoverageDescriptions&gt;\n  </code>\n</pre>\n\n  \n### <a name=\"GetCoverage\">WCS: Get Coverage</a>\n\nThe GetCoverage request is used to run an extraction on a dataset using some filtering parameters and a list of required variables.\n\n__URL__:  \n\n* __DGF__: http://localhost:8080/motu-web/wcs?service=WCS&version=2.0.1&request=GetCoverage&coverageId=$coverageId&subset=Time(1412157600,1412244000)\n* __Subetter__: http://localhost:8080/motu-web/wcs?service=WCS&version=2.0.1&request=GetCoverage&coverageId=$coverageId&subset=Time(1412157600,1412244000)&subset=Lat(50,70)&subset=Lon(0,10)&subset=Height(0,5728)&rangeSubset=temperature,salinity\n\n\n__Parameters__:   \n\n* __service [1]:__ Value is fixed to \"WCS\"\n* __version [1]:__ Value is fixed to \"2.0.1\"\n* __request [1]:__ Value is fixed to \"GetCoverage\"\n* __coverageId [1]:__ the identifier of the required coverage. CoverageId are returned by the [GetCapabilities](#GetCapabilities) service.\n* __subset [1,n]:__ the list of filtering parameters.  \n\t* To define one filtering parameter, the following format have to be respected:<br/>\n\tFor the Time parameter:\n\t```\n\tSUBSET=Time(lowTimeValue,highTimeValue)\n\t```\n\tUnit is epoch since 1st January 1970, in UTC. E.g. Thu Dec 01 2016 00:00:00 is set to 1480550400000.  \n\t* To define multiple filtering parameters, the following format have to be respected:<br/>\n\tFor the Latitude and the Longitue:\n\t```\n\tSUBSET=Lat(lowLatValue,highLatValue)&SUBSET=Lon(lowLonValue,highLonValue)\n\t```  \n\tIn order to know which subset filters can be applied, you have to run a [DescribeCoverage](#DescribeCoverage) request.\n* __rangesubset:__ the list of required variables for the coverage. Each variable have to be separated by a comma (,)\n\n__Return__: \nA Netcdf file. When you request for one point, a specific algorithm is used, see [Downloading 1 point](#ArchiAlgoDownloading1Point).\n\n\n## <a name=\"ClientRESTAPI\">MOTU REST API</a>   \n__MOTU REST API__ defines a set of services accessible from an HTTP URLs.  \nAll URLs have always the same pattern: http://motuServer/${context}/Motu?action=$actionName  \nOther parameters can be added and they are described with their cardinality [x,y].  \n\n* [0,1] is an optional parameter.   \n* [1] is a mandatory parameter.  \n* [0,n] is an optional parameter which can be set several times.  \n* [1,n] is a mandatory parameter which can be set several times.  \n\n__$actionName is an action, they are all listed below:__  \n  \n* XML API\n   * [Describe coverage](#ClientAPI_DescribeCoverage)  \n   * [Describe product](#ClientAPI_DescribeProduct)  \n   * [Request status](#ClientAPI_RequestStatus)  \n   * [Get size](#ClientAPI_GetSize)  \n   * [Time coverage](#ClientAPI_GetTimeCov)  \n   * [Download product](#ClientAPI_DownloadProduct)  \n* HTML Web pages\n   * [About](#ClientAPI_About)  \n   * [Debug](#ClientAPI_Debug)  \n   * [Download product](#ClientAPI_DownloadProduct)  \n   * [List catalog](#ClientAPI_ListCatalog)  \n   * [List services](#ClientAPI_ListServices)  \n   * [Product download home](#ClientAPI_ProductDownloadHome)  \n   * [Product medatata](#ClientAPI_ProductMetadata)    \n   * [Welcome](#ClientAPI_welcome)  \n* Plain Text \n   * [Ping](#ClientAPI_Ping)  \n   * [Refresh config services metadata cache](#ClientAPI_RefreshCache) \n   * [Healthz](#ClientAPI_healthz)  \n* JSON\n   * [Supervision](#ClientAPI_supervision)  \n   * [CacheStatus](#ClientAPI_CacheStatus)  \n\n\n \n### <a name=\"ClientAPI_About\">About</a>    \nDisplay version of the archives installed on Motu server  \n__URL__: http://localhost:8080/motu-web/Motu?action=about  \n\n__Parameters__: No parameter.  \n\n__Return__: An HTML page. Motu-static-files (Graphic chart) is refreshed thanks to Ajax because its version file can be installed on a distinct server.   \nExample:  \n```\nMotu-products: 3.0  \nMotu-distribution: 2.6.00-SNAPSHOT  \nMotu-configuration: 2.6.00-SNAPSHOT-20160623173246403  \nMotu-static-files (Graphic chart): 3.0.00-RC1-20160914162955422  \n```\n\n\n### <a name=\"ClientAPI_Debug\">Debug</a>    \nDisplay all requests status managed by Motu server in the last [cleanRequestInterval](#BScleanRequestInterval) minutes.\nTables are sorted by time ascending.  \n4 status are defined:\n\n* __DONE__: Request has been processed successfully. Result file can be downloaded.  \n* __ERROR__: Request has not been processed successfully. No result file is available.  \n* __PENDING__: Request has been received by the server. Server computes its size and runs some checks, it can take a while.  \n* __INPROGRESS__: Request has been delegated to the queue server which is currently processing it.  \n\n\n__URL__: http://localhost:8080/motu-web/Motu?action=debug  \n\n__Parameters__:  \n* __order__ [0,1]: Change the order of items INPROGRESS,PENDING,ERROR,DONE. All items shall be set.  \nexample: http://localhost:8080/motu-web/Motu?action=Debug&order=DONE,ERROR,PENDING,INPROGRESS  \nWithout this parameter, default order is: INPROGRESS,PENDING,ERROR,DONE  \n\n__Return__: An HTML page  \n  \n  \n### <a name=\"ClientAPI_DescribeCoverage\">Describe coverage</a>    \nGet coverage data in relationship with a dataset.  \n__URL__: http://localhost:8080/motu-web/Motu?action=describecoverage&service=HR_MOD-TDS&datasetID=HR_MOD  \n\n__Parameters__:  \n\n* __service__ [1]: The [service name](#BSconfigServiceName)  \n* __datesetID__ [1]: The [dataset ID](#BSconfigServiceDatasetName)  \n\n__Return__: A XML document  \n\n```\n<dataset xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"  xsi:noNamespaceSchemaLocation=\"describeDataset.xsd\" name=\"HR_MOD\" id=\"HR_MOD\">  \n<boundingBox>  \n  <lon min=\"-180.0\" max=\"179.91668701171875\" units=\"degrees_east\"/>\n  <lat min=\"-80.0\" max=\"-80.0\" units=\"degrees_north\"/>\n</boundingBox>\n<dimension name=\"time\" start=\"2012-12-26T12:00:00.000+00:00\" end=\"2016-06-12T12:00:00.000+00:00\" units=\"ISO8601\"/>  \n<dimension name=\"z\" start=\"\" end=\"\" units=\"m\"/>  \n<variables>  \n<variable id=\"northward_sea_water_velocity\" name=\"v\" description=\"Northward velocity\" standardName=\"northward_sea_water_velocity\" units=\"m s-1\">  \n<dimensions></dimensions>  \n</variable>  \n...  \n</variables>  \n</dataset>  \n```\n  \n  \n### <a name=\"ClientAPI_DescribeProduct\">Describe product</a>    \nDisplay the product meaning dataset description. Result contains notably: the datasetid, the time coverage, the geospatial coverage, the variable(s) (with the standard_name and unit), eventually the vertical coverage.  \nThere is 2 ways to call describe product, both returning a same response.  \n\n\n#### Way 1   \n  \n__URL__: http://localhost:8080/motu-web/Motu?action=describeproduct&service=HR_MOD-TDS&product=HR_MOD  \n\n__Parameters__:  \n\n* __service__ [1]: The [service name](#BSconfigServiceName)  \n* __product__ [1]: The product id  \n  \n  \n#### Way 2  (Deprecated) \n\n__URL__: http://localhost:8080/motu-web/Motu?action=describeproduct&data=http://$tdsServer/thredds/dodsC/path_HR_MOD&xmlfile=http://$tdsServer/thredds/m_HR_MOD.xml  \n\n__Parameters__:  \n\n* __xmlfile__ [1]: The Thredds dataset, example: http://$tdsServer/thredds/m_HR_MOD.xml  \n* __data__ [1]: The Thredds data, example http://$tdsServer/thredds/dodsC/path_HR_MOD  \n    \n__Return__: An XML document  \n\n```\n<productMetadataInfo code=\"OK\" msg=\"OK\" lastUpdate=\"Not Available\" title=\"HR_MOD\" id=\"HR_MOD\">  \n<timeCoverage code=\"OK\" msg=\"OK\"/>  \n<availableTimes code=\"OK\" msg=\"OK\">  \n1993-01-15T12:00:00Z/2001-01-01T00:00:00Z/P2D,2001-01-01T00:00:00Z/2012-03-01T00:00:00Z/PT6H\n</availableTimes>  \n<availableDepths code=\"OK\" msg=\"OK\">  \n0.49402;1.54138;2.64567;...  \n</availableDepths>  \n<geospatialCoverage code=\"OK\" msg=\"OK\"/>  \n<variablesVocabulary code=\"OK\" msg=\"OK\"/>  \n<variables code=\"OK\" msg=\"OK\">  \n<variable description=\"Northward velocity\" units=\"m s-1\" longName=\"Northward velocity\" standardName=\"northward_sea_water_velocity\" name=\"v\" code=\"OK\" msg=\"OK\"/>  \n<variable description=\"Eastward velocity\" units=\"m s-1\" longName=\"Eastward velocity\" standardName=\"eastward_sea_water_velocity\" name=\"u\" code=\"OK\" msg=\"OK\"/>  \n...  \n</variables>  \n<dataGeospatialCoverage code=\"OK\" msg=\"OK\">  \n<axis code=\"OK\" msg=\"OK\" description=\"Time (hours since 1950-01-01)\" units=\"hours since 1950-01-01 00:00:00\" name=\"time_counter\" upper=\"582468\" lower=\"552132\" axisType=\"Time\"/>  \n<axis code=\"OK\" msg=\"OK\" description=\"Longitude\" units=\"degrees_east\" name=\"longitude\" upper=\"179.91668701171875\" lower=\"-180\" axisType=\"Lon\"/>  \n<axis code=\"OK\" msg=\"OK\" description=\"Latitude\" units=\"degrees_north\" name=\"latitude\" upper=\"90\" lower=\"-80\" axisType=\"Lat\"/>  \n<axis code=\"OK\" msg=\"OK\" description=\"Depth\" units=\"m\" name=\"depth\" upper=\"5727.9169921875\" lower=\"0.4940249919891357421875\" axisType=\"Height\"/>  \n</dataGeospatialCoverage>  \n</productMetadataInfo>  \n```\n\n\n#### availableTimes XML tag \nIn the XML result file the tag \"availableTimes\" provides the list of date where data are available for the requested product.\nThe format of the date follows the convention ISO_8601 used to represent the dates and times. (https://en.wikipedia.org/wiki/ISO_8601)\nForeach available time period, the period definition format is \"StartDatePeriod/EndDatePeriod/DurationBetweenEachAvailableData\".  \nThe \"availableTimes\" contains a list of time period separated by a \",\".\n* __StartDate__ : this the first date of the period where data are available.\n* __EndDate__ : this the last date of the period where data are available.\n* __DurationBetweenEachAvailableData__ : This the period duration between each available data in the interval defined by the the \"StartDate\" and \"EndDate\" date.  \n\n>For DGF datasets, the list of available times is built from the start and end date of each file of the dataset, ignoring the other time values if any. As a consequence, **there might be more available times than those listed in this attribute for DGF datasets** with there are more than 2 time values per file.\n\n##### StartDate and EndDate format\nThe format of the StartDate and EndDate is YYYY-MM-DDThh:mm:ssZ where:\n* __YYYY__ : is the year defined on 4 digits\n* __MM__ : is the number of the month defined on 2 digits\n* __DD__: is the number of the day in the month on 2 digits\n* __hh__: is the hour of the day on 2 digits\n* __mm__: is the minutes of the hour on 2 digits\n* __ss__: is the seconds of the minutes on 2 digits\n\nExamples:\n* 1993-01-15T12:00:00Z\n* 2016-07-25T06:35:45Z\n* 2017-08-31T15:05:08Z\n\n##### DurationBetweenEachAvailableData\nThe formation of the duration is P*nbyers*Y*nbmonths*M*nbdays*DT*nbhours*H*nbminutes*M*nbseconds*S.*nbmillisec* where:\n* __nbyears__ : is the number of years. The ISO_8601 is ambiguous on the number of days in the year. For the Motu project, the number of days is fixed to 365 as in the most of projects.\n* __nbmonths__ : is the number of month. The ISO_8601 is ambiguous on the number of days in the month. For the Motu project, the number of days is fixed to 30 as in the most of projects.\n* __nbdays__ : is the number of day. One day is 24 hours.\n* __nbhours__ : is the number of hours. One hour is 60 minutes.\n* __nbseconds__: is the number of seconds. One seconds is 1000 milliseconds.\n* __nbmillisec__ : is the number of milliseconds.\n\nBy convention, P1M defines a duration of 1 month and PT1M defines a duration of 1 minutes.\n\nExamples:\n* each minute => PT1M\n* each hour => PT1H\n* each 12 hours => PT12H\n* each day => P1D\n* each 15 days => P15D\n* each month => P1M\n \n#### \"lastUpdate\" XML attribute\nNote about \"lastUpdate\" attribute of the \"productMetadataInfo\" field: it has the same value than \"[Last update](#ClientAPI_ListCatalog)\" field of the list catalog page.  \n \n### <a name=\"ClientAPI_DownloadProduct\">Download product</a>    \nRequest used to download a product  \n\n__URL__: http://localhost:8080/motu-web/Motu?action=productdownload  \nexample:  \nhttp://localhost:8080/motu-web/Motu?action=productdownload&service=HR_MOD-TDS&product=HR_MOD&x_lo=-2&x_hi=2&y_lo=-2&y_hi=2&output=netcdf&t_lo=2016-06-12+12%3A00%3A00&t_hi=2016-06-12+12%3A00%3A00&z_lo=0.49&z_hi=5727.92  \n\n__Parameters__:  \n\n* __service__ [1]: The [service name](#BSconfigServiceName)  \n* __product__ [1]: The product id  \n* __variable__ [0,n]: physical variables to be extracted from the product. When no variable is set, all the variables of the dataset are extracted.  \n* __y_lo__ [0,1]: low latitude of a geographic extraction. Default value is -90.  \n* __y_hi__ [0,1]: high latitude of a geographic extraction. Default value is 90.  \n* __x_lo__ [0,1]: low longitude of a geographic extraction. Default value is -180.  \n* __x_hi__ [0,1]: high longitude of a geographic extraction. Default value is 180.  \n* __z_lo__ [0,1]: low vertical depth . Default value is the min available depth. If the lo value is greater than the hi value, the 2 values are switched. If the depth range is out of the available range, Motu computes the best range into the available range. Value of this parameter is a double or \"Surface\" string which has a value of 0.0.   \n* __z_hi__ [0,1]: high vertical depth. Default value is the max available depth. If the hi value is lower than the lo value, the 2 values are switched. If the depth range is out of the available range, Motu computes the best range into the available range. Value of this parameter is a double or \"Surface\" string which has a value of 0.0.   \n* __t_lo__ [0,1]: Start date of a temporal extraction. If not set, the default value is the first date/time available for the dataset. Format is  \"yyy-MM-dd\" or \"yyyy-MM-dd HH:mm:ss\" or \"yyyy-MM-ddTHH:mm:ss\" and depends on the requested dataset.  \n* __t_hi__ [0,1]: End date of a temporal extraction. If not set, the default value is the last date/time available for the dataset. Format is \"yyy-MM-dd\" or \"yyyy-MM-dd HH:mm:ss\" or \"yyyy-MM-ddTHH:mm:ss\" and depends on the requested dataset.    \n* __output__ [0,1]: netcdf. Due to a TDS issue, only netcdf is available. netcdf4 will be available as soon as TDS will have resolved its issue.\n* __mode__ [0,1]: Specify the desired result mode. Enumeration value from [url, console, status] represented as a string. If no mode, \"url\" value is the default mode.  \n\n   * mode=__url__: URL of the delivery file is directly returned in the HTTP response as an HTML web page. Then Javascript read this URL to download file. The request is processed in a synchronous mode.  \n   * mode=__console__: the response is a 302 HTTP redirection to the delivery file to be returned as a binary stream. The request is processed in a synchronous mode.  \n   * mode=__status__: request is submitted and [the status](#ClientAPI_RequestStatus) of the request processing is immediately returned as an XML. The request is processed in an asynchronous mode.  \n   Web Portal submits the request to the Dissemination Unit Subsetter and gets an immediate response of the Subsetter. \n   This response contains the identifier and the status of the order (pending, in progress, done, error).\n   So long as the order is not completed (done or error), Web Portal requests the status of the order at regular and fair intervals (> 5 seconds) \n   and gets an immediate response. When the status is \u201cdone\u201d, Web Portal retrieves the url of the file to download, from the status response. \n   Then Web Portal redirects response to this url. \n   The Web Browser opens a binary stream of the file to download and shows a dialog box to allow the user saving it as a local file.  \n\n__Return__: Several ways depending of the selected http parameter mode. When you request for one point, a specific algorithm is used, see [Downloading 1 point](#ArchiAlgoDownloading1Point).  \n\n\n\n \n### <a name=\"ClientAPI_RequestStatus\">Request status</a>    \nGet a request status to get more details about a download state.  \n\n__URL__: http://localhost:8080/motu-web/Motu?action=getreqstatus&requestid=123456789  \n\n__Parameters__:  \n\n* __requestid__ [1]: A request id.  \n\n__Return__: An XML document or an HTML page if requestId does not exists.    \nValidated by the schema /motu-api-message/src/main/schema/XmlMessageModel.xsd#StatusModeResponse  \nExample:  \n\n```\n<statusModeResponse code=\"004-0\" msg=\"\" scriptVersion=\"\" userHost=\"\" userId=\"\" dateSubmit=\"2016-09-19T16:56:22.184Z\" localUri=\"/$pathTo/HR_MOD_1474304182183.nc\" remoteUri=\"http://localhost:8080/motu/deliveries/HR_MOD_1474304182183.nc\" size=\"1152.0\"dateProc=\"2016-09-19T16:56:22.566Z\" requestId=\"1474304182183\" status=\"1\"/>\n```\n  \nSize is in MegaBytes or at NaN while still not estimated.\n\n\n\n \n### <a name=\"ClientAPI_GetSize\">Get size</a>    \nGet the size of a download request. Result contains the size of the potential result file, with a unit, and the maximum allowed size for this service.  \n\n__URL__: http://localhost:8080/motu-web/Motu?action=getsize  \n\n__Parameters__:  \n\nParameters below are exactly the same as for [Download product](#ClientAPI_DownloadProduct)   \n\n* __service__ [1]: The [service name](#BSconfigServiceName)  \n* __product__ [1]: The product id  \n* __variable__ [0,n]: physical variables to be extracted from the product. When no variable is set, all the variables of the dataset are extracted.  \n* __y_lo__ [0,1]: low latitude of a geographic extraction. Default value is -90.  \n* __y_hi__ [0,1]: high latitude of a geographic extraction. Default value is 90.  \n* __x_lo__ [0,1]: low longitude of a geographic extraction. Default value is -180.  \n* __x_hi__ [0,1]: high longitude of a geographic extraction. Default value is 180.  \n* __z_lo__ [0,1]: low vertical depth . Default value is the min available depth. If the lo value is greater than the hi value, the 2 values are switch. If the depth range is out of the available range, Motu compute the best range into the available range.  \n* __z_hi__ [0,1]: high vertical depth. Default value is the max available depth. If the hi value is less than the lo value, the 2 values are switch. If the depth range is out of the available range, Motu compute the best range into the available range.\n* __t_lo__ [0,1]: Start date of a temporal extraction. If not set, the default value is the first date/time available for the dataset. Format is yyy-mm-dd or yyyy-dd h:m:s or yyyy-ddTh:m:s.  \n* __t_hi__ [0,1]: End date of a temporal extraction. If not set, the default value is the last date/time available for the dataset. Format is yyy-mm-dd or yyyy-dd h:m:s or yyyy-ddTh:m:s.  \n  \n__Return__: An XML document.    \nThe unit is \"KB\" means Kilobyte.\nValidated by the schema /motu-api-message/src/main/schema/XmlMessageModel.xsd#RequestSize  \nExample:  \n\n```\n<requestSize code=\"005-0\" msg=\"OK\" unit=\"kb\" size=\"1.5104933E8\" maxAllowedSize=\"9.961472E8\"/>  \n```  \n\n### <a name=\"ClientAPI_ListCatalog\">List catalog</a>    \nDisplay information about a catalog (last update timestamp) and display link to access to download page and dataset metadata.\n\nNote about field \"__Last update__\" : On MOTU start-up, this date is set to the most recent date of the dataset if it is before of the current day, or to the \"last update date\" returned by the TDS server also if it is before current day, or else to \"Not available\".  \nOn MOTU cache update, if the most recent available date of the dataset changes, the \"last update date\" is set with the current date.  \n\n__URL__: http://localhost:8080/motu-web/Motu?action=listcatalog&service=HR_MOD-TDS  \n\n__Parameters__:   \n\n* __service__ [1]: The [service name](#BSconfigServiceName)  \n\n__Return__: An HTML page   \n\n\n### <a name=\"ClientAPI_ListServices\">List services</a>    \nDisplay the service web page \n__URL__: http://localhost:8080/motu-web/Motu?action=listcatalog&service=HR_MOD-TDS  \n\n__Parameters__:  \n\n* __catalogtype__ [0,1]: The [catalog type](#BSconfigServiceDatasetType) used to filter by type.  \n\n__Return__: An HTML page   \n\n\n### <a name=\"ClientAPI_Ping\">Ping</a>    \nUsed to be sure that server is up. You can also use the [supervision](#ClientAPI_supervision) URL.  \n\n__URL__: http://localhost:8080/motu-web/Motu?action=ping  \n\n__Parameters__: No parameter.  \n\n__Return__: An plain text  \n\n```  \nOK - response action=ping    \n```     \n\n### <a name=\"ClientAPI_RefreshCache\">Refresh config services metadata cache</a>    \nForce the refresh of the cache of [config service](#BSconfigService) metadata instead of waiting the [automatic refresh](#describeproductcacherefreshinmillisec).   \nThis action is secured and is only triggered when a valid [token](#refreshCacheToken) is given.    \nMoreover a list of config services needed to be refreshed is shared with the automatic update process.     \nThis add robustness because a job refreshes only cache of the config services which are is the list. So when this action is called several times, if a config service in already in this waiting list, it is not added a second time.\nA soon as a cache for a config service is refreshed, config service is removed from this waiting list.   \n\n__URL__: http://localhost:8080/motu-web/Motu?action=refreshcache&token=tokenValid&configServiceNames=all  \n\n__Parameters__:\n\n* __token__ [1] : Used to secure this action. The [token](#refreshCacheToken) configured in the motuConfiguration.xml file which allowed the execution of the refresh. See this section for [the token configured](#refreshCacheToken)\n* __configServiceNames__ [1] : [all,onlyauto,$configServiceNames] 3 options to tune how the cache will be resfreshed.  \n   * __all__ : Refresh immediately all the config service. \n   * __onlyauto__ : Refresh immediately only the config services which enable the [automatic refresh](#refreshCacheAutomaticallyEnabled).\n   * __$configServiceNames__ : Refresh immediately all the config services listed. Value of this parameter is a list of all [config service](#BSconfigServiceName) name is separated by a comma character, e.g. configServiceNames=AAA,BBB,CCC\n\n__Return__: A plain text which specify if the refresh is launched or if an error occurred, e.g. \"OK: config service AAA cache refresh in progress\" or \"ERROR: Unknwon config service UnknownConfigService\"\n\n```  \nOK cache refresh in progress   \n```  \n\n\n### <a name=\"ClientAPI_healthz\">Healthz</a>  \nGives healthz information about Motu server health. \n\n__URL__: http://localhost:8080/motu-web/Motu?action=healthz\n\n__Parameters__: No parameter\n  \n__Return__: An http status and a short message:\n - http status **202** (*accepted*) when started and cache still not refreshed, with the message \"*Server started and refresh in progress (remaining  X / Y).*\"  Where X is the number of Catalog to put in the cache, over the total number Y.  \n   This message is also displayed when the Web context gets destroyed and cache gets build again, or also when the configuration file is modified and reloaded.\n - http status **200** when running and ready, with the message \"*Server is ready.*\"\n\n\n### <a name=\"ClientAPI_welcome\">Welcome</a>  \nHTML page which gives access to several web pages, in particular the Motu listservices web page.\n\n__URL__:  \n* http://localhost:8080/motu-web/  \n* http://localhost:8080/motu-web/Motu?action=welcome\n\n__Parameters__: No parameter\n  \n__Return__: An HTML web page  \n\n### <a name=\"ClientAPI_ProductDownloadHome\">Product download home</a>    \nDisplay an HTML page in order to set the download parameters.  \n\n__URL__: http://localhost:8080/motu-web/Motu?action=productdownloadhome&service=HR_OBS-TDS&product=HR_OBS  \n\n__Parameters__:  \n\n* __service__ [1]: The [service name](#BSconfigServiceName)  \n* __product__ [1]: The product id  \n  \n__Return__: An HTML page  \n\n\n\n### <a name=\"ClientAPI_ProductMetadata\">Product metadata Home</a>    \nDisplay an HTML page with the geographical and temporal coverage, the last dataset update and the variables metadata.  \n\nNote about field \"Date\" of \"__Last dataset update__\" section: this date has the same value than the \"[Last update](#ClientAPI_ListCatalog)\" field.  \n\n__URL__: http://localhost:8080/motu-web/Motu?action=listproductmetadata&service=HR_OBS-TDS&product=HR_OBS  \n\n__Parameters__:  \n\n* __service__ [1]: The [service name](#BSconfigServiceName)  \n* __product__ [1]: The product id  \n  \n__Return__: An HTML page  \n\n\n\n### <a name=\"ClientAPI_GetTimeCov\">Time coverage</a>    \nDisplay an HTML page with the geographical and temporal coverage, the last dataset update and the variables metadata.   \n\n__URL__: http://localhost:8080/motu-web/Motu?action=gettimecov&service=HR_MOD-TDS&product=HR_MOD  \n\n__Parameters__:  \n\n* __service__ [1]: The [service name](#BSconfigServiceName)  \n* __product__ [1]: The product id  \n  \n__Return__: A XML document  \n\n```xml  \n<timeCoverage code=\"007-0\" msg=\"OK\" end=\"2016-09-17T00:00:00.000Z\" start=\"2007-05-13T00:00:00.000Z\"/>\n```  \n\n### <a name=\"ClientAPI_supervision\">Supervision</a>  \nGives information about Motu server.  \nFor more details, see [https://jolokia.org/reference/html/agents.html].\n\n__URL__: http://localhost:8080/motu-web/supervision\n\n__Parameters__: No parameter\n  \n__Return__: A JSON document  \n\n```json  \n{\"timestamp\":1474638852,\"status\":200,\"request\":{\"type\":\"version\"},\"value\":{\"protocol\":\"7.2\",\"config\":{\"agentId\":\"10.1.20.198-18043-2df3a4-servlet\",\"agentType\":\"servlet\"},\"agent\":\"1.3.3\",\"info\":{\"product\":\"tomcat\",\"vendor\":\"Apache\",\"version\":\"7.0.69\"}}}\n```  \n\n### <a name=\"ClientAPI_CacheStatus\">CacheStatus</a>  \nGives the status of the dataset cache of Motu.  \nOn start-up, Motu reads its configuration file, and gets a list of \"configService\" nodes referencing a dataset catalog with an URL, and starts caching them. \n\n__URL__: http://localhost:8080/motu-web/Motu?action=cachestatus\n\n__Parameters__: No parameter\n  \n__Return__: A JSON document  \n\n```json  \n{\"cachestatus\":\n\t{\"state\":\n\t\t{\"nbTotal\":8,\"nbSuccess\":6,\"nbFailure\":2,\"lastUpdate\":\"2019-11-19T10:56:37.357Z\",\"lastUpdateDuration\":\"PT1M17.561S\"},\n \t \"configServices\": [\n \t \t{\"Sea_Surface_Temperature_Global-TDS\":\n \t \t\t{\"state\":\n \t \t\t\t{\"status\":\"FAILURE\",\"lastUpdate\":\"2019-11-19T10:57:56.398Z\",\"lastUpdateDuration\":\"\"},\n \t \t\t \"conf\":\n \t \t\t \t{\"refreshCacheAutomaticallyEnabled\":true,\"type\":\"tds\",\"ncss\":\"enabled\"}\n \t \t\t}\n \t \t},\n \t \t{\"HR_MOD_NCSS-TDS\":\n \t \t\t{\"state\":\n \t \t\t\t{\"status\":\"SUCCESS\",\"lastUpdate\":\"2019-11-19T10:57:01.436Z\",\"lastUpdateDuration\":\"PT1.688S\"},\n \t \t\t \"conf\":\n \t \t\t \t{\"refreshCacheAutomaticallyEnabled\":true,\"type\":\"tds\",\"ncss\":\"enabled\"}\n \t \t\t}\n \t \t},\n \t \t.....]\n \t },\n \"version\":\n \t{\"motu-products\":\"Unknow version\",\"motu-distribution\":\"Unknow version\",\"motu-configuration\":\"3.11.04-20190716151835979\"}\n }\n```  \nThe ***nbTotal*** is the total number of ConfigServices.  \nThe ***nbSuccess*** and the ***nbFailure*** are the number of currently loaded ConfigServices in success/failure.  \nNote that ***lastUpdate*** and ***lastUpdateDuration*** fields can be empty if the system hasn't still refreshed the cache or if the access failed.\n\n### <a name=\"ClientAPI_welcome\">Welcome</a>  \nHTML page which gives access to several web pages, in particular the Motu listservices web page.\n\n__URL__:  \n* http://localhost:8080/motu-web/  \n* http://localhost:8080/motu-web/Motu?action=welcome\n\n__Parameters__: No parameter\n  \n__Return__: An HTML web page  \n\n  \n# <a name=\"Docker\">Motu Docker distribution</a>  \n\nMotu comes in a Docker release based on CentOS.\n\n## <a name=\"DockerContent\">Docker image content</a>   \nThe Motu specific elements are located under /opt/motu.  \nThe required libraries are installed on the Docker image.  \nThe Tomcat is located under /opt/motu/tomcat-motu.  \nThe CDO scripts (merge.sh and cdo.sh) can be found in the folder /opt/motu/products/cdo-group.\n  \n## <a name=\"DockerDirectories\">Docker mounted directories</a>  \nTo configure the Motu Docker image, some of the folders have to be mounted from the execution environment:\n* the **motu configuration** folder to mount to /opt/motu/config\n  * log4j.xml\n  * motuConfiguration.xml\n  * motu.properties\n  * standarNames.xml\n  * version-configuration.txt\n  * velocityTemplates/motu.vm\n  * security (folder with the configuration elements for cas authentification)\n* the produced **data files** folder outputed on /opt/motu/data/download/public\n\nOther folders for logs and Apache configuration are optionnals:\n* the **log** folder to mount to /opt/motu/log  \n  The produced logs (errors.log, logbook.log, wrnings.log and motuQSlog.xml) will be available in this folder.\n* the **apache tomcat log** folder to mount to /opt/motu/tomcat-motu/logs  \n  In this folder the host-manager, catalina, and access logs will be created.\n* the **tomcat configuration** folder to mount to /opt/motu/tomcat-product/conf  \n  If this volume is not mounted, the default Apache configuration is used. In that case, the downloads have to be handled by another element of the installation, to which the ddownloadHttpUrl property of the motuConfiguration file will refer.  \n  * logging.properties\n  * server.xml\n    For example to set the Apache HTTPd \"Context\" directive to handle the result files downloading with the MOTU server.\n  * web.xml\n  * other files that could be used for specific needs (configuring catalina, jaspic or the tomcat users)  \n  \nConfiguration files are similar to standard Motu configuration files with some exceptions to take into account:\n* Motu is installed under /opt/motu (and not /opt/cmems-cis/motu)\n* The download directory is under /opt/motu/data/download/public \n\n## <a name=\"DockerRun\">Run Motu with Docker</a>  \n\nThe following command:\n\n```  \ndocker run -d -v /data/shared/motu/result:/opt/motu/data/download/public -v /home/motu/motu/config_qt/motu_config:/opt/motu/config -v /home/motu/motu/config_qt/apache_config:/opt/motu/tomcat-motu/conf -v /home/motu/motu/config_qt/log_motu:/opt/motu/log -v /home/motu/motu/config_qt/log_apache:/opt/motu/tomcat-motu/logs -p 8080:8080 -p 8443:8443 -p 8009:8009 -p 8005:8005 registry-ext.cls.fr:443/motu/motu/motu-distribution\n\n```  \n  \nWill run the Motu server. Logs (from logbook, warning and error MOTU files) can be accessed using:\n```\ntail /var/log/syslog\n```\n\nIf a reverse proxy for downloading the results is to be added in the platform where MOTU server docker image is deployed, use:\n```  \ndocker run -d -v /data/shared/motu/result:/var/www -v /home/motu/motu/config_qt/motu_config:/opt/motu/config -p $NGINX_PORT:8070 -e NGINX_PORT=$NGINX_PORT -e MOTU_DOWNLOAD_PATH=/motu-web -e MOTU_URL=http://$(hostname):18080 -d registry-ext.cls.fr:443/motu/motu/motu-nginx\n\n```  \n\n"
 },
 {
  "repo": "hydroffice/hyo2_openbst",
  "language": "Jupyter Notebook",
  "readme_contents": "The Open BackScatter Toolchain Project\n======================================\n\n.. image:: https://github.com/hydroffice/hyo2_openbst/raw/master/resources/png/openbst.png\n    :alt: OpenBST logo\n\n|\n\n.. image:: https://img.shields.io/badge/docs-latest-brightgreen.svg\n    :target: https://www.hydroffice.org/manuals/openbst/index.html\n    :alt: Latest Documentation\n\n.. image:: https://ci.appveyor.com/api/projects/status/e73gnrt9n50cu2k5?svg=true\n    :target: https://ci.appveyor.com/project/giumas/hyo2-openbst\n    :alt: AppVeyor Status\n\n.. image:: https://travis-ci.org/hydroffice/hyo2_openbst.svg?branch=master\n     :target: https://travis-ci.org/hydroffice/hyo2_openbst\n     :alt: Travis-CI Status\n\n.. image:: https://api.codacy.com/project/badge/Grade/13c4893c0a7e45ddb40cfdbddd7091a3\n    :target: https://www.codacy.com/app/hydroffice/hyo2_openbst/dashboard\n    :alt: Codacy\n\n.. image:: https://coveralls.io/repos/github/hydroffice/hyo2_openbst/badge.svg?branch=master\n    :target: https://coveralls.io/github/hydroffice/hyo2_openbst?branch=master\n    :alt: Coveralls\n\n|\n\n* Code Repository: `github <https://github.com/hydroffice/hyo2_openbst>`_\n* Project Page: `url <https://www.hydroffice.org/openbst/main>`_\n* License: `LGPLv3 <https://github.com/hydroffice/hyo2_openbst/raw/master/LICENSE>`_\n\n|\n\nGeneral info\n------------\n\nThe Open BackScatter Toolchain (OpenBST) package provides a library and an app for processing acoustic backscatter.\n\n|\n\nCopyright Notice\n----------------\n\nCopyright (c) 2019, University of New Hampshire, Center for Coastal and Ocean Mapping; IFREMER. All rights reserved.\nPortions of this project were developed under a cooperative agreement with NOAA Coast Survey Development\nLaboratory, and contain NOAA-developed code in the public domain.\n"
 },
 {
  "repo": "asascience-open/QARTOD",
  "language": "Python",
  "readme_contents": "**Library has been archived; new projects wishing to use QARTOD and other QC algorithms should use https://github.com/ioos/ioos_qc instead.**\n\n.. image:: https://travis-ci.org/asascience-open/QARTOD.svg?branch=master\n   :target: https://travis-ci.org/asascience-open/QARTOD\n   :alt: build_status\n\n\nQARTOD\n======\n\nCAVEAT: this module is deprecated!\nWe recommend https://github.com/ioos/ioos_qc as an alternative QARTOD python implementation.\n``ioos_qc`` is actively developed and supported by IOOS.\n\nCollection of utilities, scripts and tests to assist in automated\nquality assurance and quality control for oceanographic datasets and\nobserving systems.\n"
 },
 {
  "repo": "nitrogenlab/oceanography_colab_notebooks",
  "language": "Jupyter Notebook",
  "readme_contents": "# oceanography_colab_notebooks\nFor oceanography-related colab notebooks\n"
 },
 {
  "repo": "redouanelg/qgsw-DI",
  "language": "Jupyter Notebook",
  "readme_contents": "***** for QGNET the PyTorch version of the code https://arxiv.org/abs/1911.08856 please check the QGNET folder ******\n\n# qgsw-DI\nQuasigeostrophic shallow-water model for Dynamic Interpolation of SSH\n\nThis is the supplementary material of the publication \"Dynamic Interpolation of Sea Surface Height and Potential Applications for Future High-Resolution Altimetry Mapping\", from C. Ubelmann et al., [pdf](https://journals.ametsoc.org/doi/abs/10.1175/JTECH-D-14-00152.1), please cite this article if you use the code in your works.\n\nThe backbone of the codes was written by Cl\u00e9ment Ubelmann (ubelmann@cls.fr). I commented and prepared the codes for sharing, I also added a Jupyter notebook for illustration.\n\n# Example\n\nPlease check the folder \"notebooks\" for more details\n\n![](example_qgSSH.png)\n"
 },
 {
  "repo": "pvthinker/argopy",
  "language": "Python",
  "readme_contents": "# argopy\nPython tools to perform statistics with [Argo data](http://www.argo.ucsd.edu/) and produce global atlases of statistics. The tools allow to brew the over two millions of profiles in the Argo database with hundreds of cores. \n\nThe resulting atlas is about to be published on [SEANOE](https://www.seanoe.org)\n\n![ScreenShot](/images/compensated_density_1000m.png)"
 },
 {
  "repo": "seaflow-uw/seaflowpy",
  "language": "Python",
  "readme_contents": "# Seaflowpy\n\nA Python package for SeaFlow flow cytometer data.\n\n## Table of Contents\n\n1. [Install](#install)\n1. [Read EVT/OPP/VCT Files](#evtoppvct)\n1. [Command-line Interface](#cli)\n1. [Configuration](#configuration)\n1. [Integration with R](#rintegration)\n1. [Testing](#testing)\n1. [Development](#development)\n\n<a name=\"install\"></a>\n\n## Install\n\nThis package is compatible with Python 3.7 and 3.8.\n\n### Source\n\nThis will clone the repo and create a new virtual environment `seaflowpy`.\n`venv` can be replaced with `virtualenv`, `conda`, etc.\n\n```sh\ngit clone https://github.com/armbrustlab/seaflowpy\ncd seaflowpy\n[[ -d ~/venvs ]] || mkdir ~/venvs\npython3 -m venv ~/venvs/seaflowpy\nsource ~/venvs/seaflowpy/bin/activate\npip3 install -U pip setuptools wheel\npip3 install -r requirements-test.txt\npip3 install .\n# Confirm the seaflowpy command-line tool is accessible\nseaflowpy version\n# Make sure basic tests pass\npytest\n# Leave the new virtual environment\ndeactivate\n```\n\n### PyPI\n\n```sh\npip3 install seaflowpy\n```\n\n### Docker\n\nDocker images are available from Docker Hub at `ctberthiaume/seaflowpy`.\n\n```sh\ndocker pull ctberthiaume/seaflowpy\ndocker run -it ctberthiaume/seaflowpy seaflowpy version\n```\n\nThe Docker build file is in this repo at `/Dockerfile`. The build process for the Docker image is detailed in `/build.sh`.\n\n<a name=\"evtoppvct\"></a>\n\n## Read EVT/OPP/VCT Files\n\nAll file reading functions will return a `pandas.DataFrame` of particle data.\nGzipped EVT, OPP, or VCT files can be read if they end with a \".gz\" extension.\nFor these code examples assume `seaflowpy` has been imported as `sfp`\nand `pandas` has been imported as `pd`, e.g.\n\n```python\nimport pandas as pd\nimport seaflowpy as sfp\n```\n\nand `*_filepath` has been set to the correct data file.\n\nRead an EVT file\n\n```python\nevt = sfp.fileio.read_evt_labview(evt_filepath)\n```\n\nRead an OPP file as an Apache Arrow Parquet file, select the 50% quantile, and subset columns.\nVCT files created with `popcycle` are also standard Parquet files and can be read in a similar fashion.\n\n```python\nopp = pd.read_parquet(opp_filepath)\nopp50 = opp[opp[\"q50\"]]\nopp50 = opp50[['fsc_small', 'chl_small', 'pe']]\n```\n\n<a name=\"cli\"></a>\n\n## Command-line interface\n\nAll `seaflowpy` CLI tools are accessible from the `seaflowpy` executable.\nRun `seaflowpy --help` to begin exploring the CLI usage documentation.\n\n### SFL validation workflow\n\nSFL validation sub-commands are available under the `seaflowpy sfl` command.\nThe usage details for each command can be accessed as `seaflowpy sfl <cmd> -h`.\n\nThe basic worfkflow should be\n\n1) If starting with an SDS file, first convert to SFL with `seaflowpy sds2sfl`\n\n2) If the SFL file is output from `sds2sfl` or is a raw SeaFlow SFL file,\nconvert it to a normalized format with `seaflowpy sfl print`.\nThis command can be used to concatenate multiple SFL files,\ne.g. merge all SFL files in day-of-year directories.\n\n3) Check for potential errors or warnings with `seaflowpy sfl validate`.\n\n4) Fix errors and warnings. Duplicate file errors can be fixed with `seaflowpy sfl dedup`.\nBad lat/lon errors may be fixed with`seaflowpy sfl convert-gga`,\nassuming the bad coordinates are GGA to begin with.\nThis can be checked with with `seaflowpy sfl detect-gga`.\nOther errors or missing values may need to be fixed manually.\n\n5) (Optional) Update event rates based on true event counts and file duration\nwith `seaflowpy sfl fix-event-rate`.\nTrue event counts for raw EVT files can be determined with `seaflowpy evt count`.\nIf filtering has already been performed then event counts can be pulled from\nthe `all_count` column of the opp table in the SQLITE3 database.\ne.g. `sqlite3 -separator $'\\t' SCOPE_14.db 'SELECT file, all_count ORDER BY file'`\n\n6) (Optional) As a check for dataset completeness,\nthe list of files in an SFL file can be compared to the actual EVT files present\nwith `seaflowpy sfl manifest`. It's normal for a few files to differ,\nespecially near midnight. If a large number of files are missing it may be a\nsign that the data transfer was incomplete or the SFL file is missing some days.\n\n7) Once all errors or warnings have been fixed, do a final `seaflowpy validate`\nbefore adding the SFL file to the appropriate repository.\n\n\n<a name=\"configuration\"></a>\n\n## Configuration\n\nTo use `seaflowpy sfl manifest` AWS credentials need to be configured.\nThe easiest way to do this is to install the `awscli` Python package\nand go through configuration.\n\n```sh\npip3 install awscli\naws configure\n```\n\nThis will store AWS configuration in `~/.aws` which `seaflowpy` will use to\naccess Seaflow data in S3 storage.\n\n<a name=\"rintegration\"></a>\n\n## Integration with R\n\nTo call `seaflowpy` from R, update the PATH environment variable in\n`~/.Renviron`. For example:\n\n```sh\nPATH=${PATH}:${HOME}/venvs/seaflowpy/bin\n```\n\n<a name=\"testing\"></a>\n\n## Testing\n\nSeaflowpy uses `pytest` for testing. Tests can be run from this directory as\n`pytest` to test the installed version of the package, or run `tox` to install\nthe source into a temporary virtual environment for testing.\n\n<a name=\"development\"></a>\n\n## Development\n\n### Source code structure\n\nThis project follows the [Git feature branch workflow](https://www.atlassian.com/git/tutorials/comparing-workflows/feature-branch-workflow).\nActive development happens on the `develop` branch and on feature branches which are eventually merged into `develop`.\n\n### Build\n\nTo build source tarball, wheel, and Docker image, run `./build.sh`. This will\n\n* create `seaflowpy-dist` with source tarball and wheel file (created during Docker build)\n\n* Docker image named `seaflowpy:<version>`\n\nTo remove all build files, run `rm -rf ./seaflowpy-dist`.\n\n### Updating requirements files\n\nCreate a new virtual environment\n\n```sh\npython3 -m venv newenv\nsource newenv/bin/activate\n```\n\nUpdate pip, wheel, setuptools\n\n```sh\npip3 install -U pip wheel setuptools\n```\n\nAnd install `seaflowpy`\n\n```sh\npip3 install .\n```\n\nThen freeze the requirements\n\n```sh\npip3 freeze | grep -v seaflowpy >requirements.txt\n```\n\nThen install test dependencies, test, and freeze\n\n```sh\npip3 install pytest pytest-benchmark\npytest\npip3 freeze | grep -v seaflowpy >requirements-test.txt\n```\n\nThen install dev dependencies, test, and freeze\n\n```sh\npip3 install pylint twine\npytest\npip3 freeze | grep -v seaflowpy >requirements-dev.txt\n```\n\nLeave the virtual environment\n\n```sh\ndeactivate\n```\n"
 },
 {
  "repo": "janosh/ocean-artup-gatsby",
  "language": "JavaScript",
  "readme_contents": "> ### \u26a0\ufe0f This repo was superseded by [the new Ocean artUp site](https://github.com/janosh/ocean-artup) written in Svelte.\n\n<p align=\"center\">\n  <a href=\"https://ocean-artup.eu\"><img src=\"src/assets/favicon.png\" alt=\"Favicon\" width=150></a>\n</p>\n\n<h1 align=\"center\">\n  <a href=\"https://ocean-artup.eu\">Ocean artUp</a>\n</h1>\n\n<h3 align=\"center\">\n\n[![License](https://img.shields.io/github/license/janosh/ocean-artup?label=License)](/license)\n![GitHub Repo Size](https://img.shields.io/github/repo-size/janosh/ocean-artup?label=Repo+Size)\n![GitHub last commit](https://img.shields.io/github/last-commit/janosh/ocean-artup?label=Last+Commit)\n\n</h3>\n\nThis repo powers the [Gatsby](https://gatsbyjs.org) site hosted at [ocean-artup.eu](https://ocean-artup.eu). The design and layout make heavy use of [CSS grid](https://css-tricks.com/snippets/css/complete-guide-grid) and [styled-components](https://styled-components.com). It is fully responsive, supports a two-level navbar animated with [`react-spring`](https://react-spring.io) (on mobile), [fluid typography](https://css-tricks.com/snippets/css/fluid-typography), [Algolia search](https://algolia.com) and [Contentful](https://contentful.com). This site is maintained by [Janosh Riebesell](https://janosh.dev).\n\nOcean artUp is a research project funded by an [Advanced Grant](https://cordis.europa.eu/project/rcn/205206_en.html) of the European Research Council. It aims to study the feasibility, effectiveness, associated risks and potential side effects of artificial upwelling in increasing ocean productivity, raising fish production, and enhancing oceanic CO<sub>2</sub> sequestration.\n\n## Installation\n\nTo get this site running locally, you need installed [`git`](https://git-scm.com), [`gatsby-cli`](https://gatsbyjs.org/packages/gatsby-cli) and [`yarn`](https://yarnpkg.com) (or [`npm`](https://npmjs.com)). Then follow these steps:\n\n1. Clone the repo to your machine and change into its directory.\n\n   ```sh\n   git clone https://github.com/janosh/ocean-artup \\\n   && cd ocean-artup\n   ```\n\n2. Optionally setup `git` hooks (recommended if you intend to open a PR).\n\n   ```sh\n   git config core.hooksPath src/utils/gitHooks \\\n   && chmod -R u+x src/utils/gitHooks\n   ```\n\n3. Install dependencies.\n\n   ```sh\n   yarn\n   ```\n\n4. Start the dev server.\n\n   ```sh\n   gatsby develop\n   ```\n\n## Deployment\n\nThe easiest way to get this site published is as follows:\n\n1. Create an account with [netlify](https://netlify.com).\n2. Install the [`netlify-cli`](https://netlify.com/docs/cli).\n3. Login to your account.\n\n   ```sh\n   netlify login\n   ```\n\n4. Connect your GitHub repo with your netlify account for [continuous deployment](https://netlify.com/docs/cli/#continuous-deployment).\n\n   ```sh\n   netlify init\n   ```\n\n5. Finally deploy the site with\n\n   ```sh\n   netlify deploy\n   ```\n"
 },
 {
  "repo": "gher-ulg/Ocot-notebook",
  "language": "Jupyter Notebook",
  "readme_contents": "# Coastal Oceanography\n\nClick on the \"launch binder\" icon to start the notebooks. Setting-up the working environement on the binder service can take a couple of minutes. Binder will automatically shut down user sessions that have more than 10 minutes of inactivity.\n\n* Data processing [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/gher-ulg/Ocot-notebook/master?filepath=Data\\_processing\\_blank.ipynb)\n\n* Linear regression [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/gher-ulg/Ocot-notebook/master?filepath=LinearRegression\\_blank.ipynb)\n\n* Air-Sea heat fluxes [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/gher-ulg/Ocot-notebook/master?filepath=OceanHeatFluxes.ipynb)\n\n## Downloading\n\nTo download a file (like a figure), select `File -> Open...` in the menu. This opens a new window with a file manager. Tick the check-box of the file and select `Download`.\n\n![download1](Figures/download1.png)\n\n![download2](Figures/download2.png)\n\nMore information about Binder is available in the [Binder FAQ](https://mybinder.readthedocs.io/en/latest/faq.html).\n"
 },
 {
  "repo": "oiip/oiip-data-viewer",
  "language": "JavaScript",
  "readme_contents": "## Welcome to the OIIP Data Viewer\n\nThis web application was developed as part of the Oceanographic In-Situ data Interoperability Project (OIIP) at NASA/JPL in collaboration with UCAR/Unidata, the Large Pelagics Research Center, and University of Massachusetts, Boston. The primary goal of the application is progress the architecture and design of applications that pair in-situ and remote sensing data products. To that end, it visualizes a select number of oceanographic remote sensing datasets (such as sea surface temperature and ocean current speed) as well as vertically resolved animal tag and ship based sensor tracks.\n\nFor more information on the project, please visit: [https://oiip.jpl.nasa.gov/](https://oiip.jpl.nasa.gov/)\n\nFor a demonstration of the tool, please see: [https://www.youtube.com/watch?v=CgOTwWWMhdc](https://www.youtube.com/watch?v=CgOTwWWMhdc)\n\nThis application is built off of the [Common Mapping Client](https://github.com/nasa/common-mapping-client)\n\n### Development Instructions\n\n**Requirements**\n\nNodeJS\n\n**Install Dependencies**\n\n`npm install`\n\n**Start Dev Server**\n\n`npm start`\n\n**Build for Prod**\n\n`npm run build`\n"
 },
 {
  "repo": "pymoc/pymoc",
  "language": "Python",
  "readme_contents": "[![CircleCI](https://circleci.com/gh/pymoc/pymoc/tree/master.svg?style=shield)](https://circleci.com/gh/pymoc/pymoc/tree/master)\n[![Test Coverage](https://api.codeclimate.com/v1/badges/b03ff00b5c86d7afc364/test_coverage)](https://codeclimate.com/github/pymoc/PyMOC/test_coverage)\n[![Maintainability](https://api.codeclimate.com/v1/badges/b03ff00b5c86d7afc364/maintainability)](https://codeclimate.com/github/pymoc/PyMOC/maintainability)\n[![PyPI version](https://badge.fury.io/py/py-moc.svg)](https://badge.fury.io/py/py-moc)\n[![Documentation](https://img.shields.io/badge/docs-PyMOC-informational)](https://pymoc.github.io)\n[![License](https://img.shields.io/badge/license-MIT-informational)](LICENSE)\n\nPyMOC is a suite of python modules to build simple \"toy\" models for ocean's\nMeridional Overturning Circulation (MOC). \n\nThe model suite consists of several independent modules representing\nvarious ocean regions and dynamics. Specifically, there are modules\nfor calculating the 1-D advective-diffusive buoyancy tendencies averaged \nover ocean basins, given the net isopycnal transports in and out of the column.\nThe isopycnal transports are computed as diagnostic  relationships, with modules\nto calculate the wind- and eddy-driven residual circulation in a southern-ocean-like\nre-entrant channel, as well as the geostrophic exchange between different basins or\nbasin regions. These modules may be coupled to study the circulation in a wide range\nof different configurations.\n\nThe intended audiences for this model are researchers, educators and students\nin the geophysical sciences. The goal is to provide an accessible\nmodel appropriate for newcomers to geophysical modeling, but with physics\nthat reflect the current state of our theoretical understanding of the deep ocean\noverturning circulation.\n\nConfiguration and execution of the PyMOC suite requires relatively little\ntechnical knowledge or computational resources. All modules are written\nin pure Python, and the only core dependencies are the NumPy and SciPy\nlibraries. If configuration of your base system environment is undesirable,\na preconfigured Docker container has been made available with all required\nsoftware libraries pre-installed. \n\nAnybody is more than welcome to contibute to the development of PyMOC,\nbut is asked to adhere to the goal of keeping PyMOC well tested, stable,\nmaintainable, and documented. Further details on installation, configuration,\ncontribution, and issue reporting is available in the [documentation](https://pymoc.github.io).\n"
 },
 {
  "repo": "janjaapmeijer/oceanpy",
  "language": "Python",
  "readme_contents": "# OceanPy\nPython package for Oceanography\n\n```\noceanpy\n\u251c\u2500\u2500 docs\n\u251c\u2500\u2500 models\n\u251c\u2500\u2500 obs\n\u2502   \u251c\u2500\u2500 ctd.py\n\u2502   \u2514\u2500\u2500 satellite.py\n\u251c\u2500\u2500 plot\n\u251c\u2500\u2500 stats\n\u251c\u2500\u2500 tools\n\u2502   \u251c\u2500\u2500 contour.py\n\u2502   \u251c\u2500\u2500 netcdf.py\n\u2502   \u2514\u2500\u2500 projections.py\n```\n\n## Polynomials\n\n\\begin{equation}\nL_t = e^x\n\\end{equation}\n\n\n<script type=\"text/javascript\" async\n  src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML\">\n</script>\n"
 },
 {
  "repo": "iwensu0313/oceanographyinR",
  "language": "R",
  "readme_contents": "# oceanographyinR\nPractice exercises and notes from Oceanographic Analysis in R, by Dan E. Kelley\n"
 },
 {
  "repo": "pboris84/DataShore",
  "language": "JavaScript",
  "readme_contents": "![DataShore Logo](https://github.com/pboris84/DataShore/blob/master/src/img/Full_Logo_Small.png)\r\n# DataShore Final Documentation\r\n\r\n## Overview of Project\r\nDataShore allows users to understand, explore and predict environmental data. It aims to help scientists reduce time spent in treacherous research conditions and eliminate mass amounts of money spent on research equipment. Our tool will be able to fill in missing data for oceanographic and environmental datasets that have variables that are proven to be linked based on science and regression results. It allows users to sign in and also create accounts, choose the variable they want to predict, upload data for the variables required to complete this action and then view the output and customize visuals according to the outut data. \r\n\r\n## List of Contents\r\n#### For more information, please visit individual file for documentation.\r\n#### Main Folder:  \r\n.DS_Store  \r\nREADME.md   \r\nindex.html  \r\nsignin.html  \r\nvisualize.html  \r\n#### SRC/ Folder:    \r\njquery.csv.js  \r\njscolor.js  \r\ntest.ipynb  \r\n#### SRC/CSS/ Folder:  \r\nindex.css  \r\nsignin.css  \r\nsinging.scss  \r\nvisualize.css  \r\n#### SRC/DATA/ Folder:   \r\ntest.csv      \r\n#### SRC/IMG/ Folder:  \r\nDataShore_Logo_BG.png  \r\nDataShore_Logo_Slogan.png  \r\nLanding Page Image 1.png  \r\nLanding Page Image 2.png  \r\nLanding Page Image 3.png  \r\nProblem.svg  \r\nblue-macro-water-wave-4813.jpg  \r\nbox plot.png  \r\ncontour_plot.png  \r\nheatmap.png  \r\nhistogram.png  \r\nline_chart.png  \r\nscatter_plot.ong  \r\nsmoke-waves-wallpaper-1.jgp  \r\ntut_1.1png  \r\ntut_1.png  \r\ntut_2.png  \r\ntut_img_2.png  \r\nurl.html  \r\n#### SRC/JS/ Folder:  \r\ncommon.js  \r\nindex.js  \r\nsignin.js  \r\ntutorial.js     \r\nvisualize.js \r\n\r\n## Summary of the Major Technology Decisions You Made\r\nOn the landing page we mention the who, what, and how for the user scenario because some users may not know about oceanography. We catered our project to Oceanographic researchers but we wanted this to be user-friendly for anyone who may want to use our product so we added a description here.\r\nWe chose to use a non-relational database instead of a relational database because the data did not need to be stored in a relational matter. We used Firebase for this purpose because it was free and easy to interact with via JavaScript calls. It stores our user's login informaiton (hasing the passwords for security measures) as well as the data uploaded by a user and the charts they update later so they can view them again upon next login.\r\nWe chose to use HTML and CSS for our front end development along with a combination of publically available libraries such as Bootstrap to help us with UX and streamlining our design. We stuck with the blues found in our logo becuase this stays true to the themes of Oceanography and also gives our site a calming feel. We also tried to have minimal text on our pages to not overwhelm users but added enough that it would be clear what actions are required.\r\nWe used JavaScript to make our pages interactive and to make calls to the database.\r\nOur algorithm was written in Python, and we experimented with the SeaWater package, and ultimately ended up translating it into a JavaScript file so we could make calls to it easily enough interacting with our other pages.\r\n\r\n## Contact Information\r\n#### For questions or concerns regarding existing code, please contact:  \r\nSukhman Tiwana: tiwans@uw.edu  \r\nTara Wilson: wwtara@uw.edu  \r\nLinda Fan Yang: yangf6@uw.edu  \r\nBoris Pavlov: borisp@uw.edu  \r\n"
 },
 {
  "repo": "bdelorme/OH21_DownscalingOcean",
  "language": "Jupyter Notebook",
  "readme_contents": "# OH21_DownscalingOcean\nDownscaling Oceanographic Data - Ocean Hackathon 2021 San Francisco\n\ndata available at https://sextant.ifremer.fr/Donnees/Catalogue#/metadata/85f3fa1c-4f6e-48c0-b6c2-4237c672769a\n\npapers/ contains a few papers exploring this challenge with different approaches\n\nmovie_vort_netcdf.ipynb shows an example of how to open a netcdf file, calculate the vorticity and plot it.\n"
 },
 {
  "repo": "csherwood-usgs/FI_processing",
  "language": "MATLAB",
  "readme_contents": "# FI_processing\nMatlab scripts to process Fire Island oceanographic data. This repo is in ..\\proj\\2019_FireIslandAsymmetry\\FI_processing on my desktop.\n---\n### Overall goals of this repo\n* Compare asymmetry deduced from bulk wave statistics via Ursell number with asymmetry measure with near-bottom instruments\n* But first, demonstrate/test routines for reading/processing instrument data\n---\n### Asymmetry\n* Measures of asymmetry vary. Some are best calculated for a singe wave, others are amenable to field measurements.\n* TODO: summarize asymmetry formulae and code\n---\n### Progress so far\n* The stats file has some missing depths...we skip these in processing, but have not investigated the underlying bursts. Some of the bursts where depth is missing are also missing some pressure measurements. These break the calcs.\n* TODO: Add some basic QA/QC to the bursts to replace missing or bad data where possible, and to skip bursts that don't meet QA standards.\n* TODO: Complete analysis of rotation into wave-propagation direction. I think `wds` returns the correct direction, and `puvq` returns a direction that is 180 deg. out...the direction waves are coming *from*.\n* TODO: Complete calcs. of comparable statistics. Best version of *du/dt* for acceleration statistics? Hilbert transform (per Malarky and Davies, 2012) for acceleration asymmetry?\n### Code so far:\n* `puv_proc_FI`    - Main script to read in ADV data and process burst by burst\n* `puvq`           - Function to calculate wave parameters from burst measurements of *u, v, p*\n* `ubstatsr`       - Function to calculate a range of burst statistics\n* `ruessink_asymm` - Function to calculate asymmetry parameters from Ursell number\n* `urotate`        - Unfinished function to rotate velocities into direction of wave propagation\n* `ustats`         - Unfinished function to calculate statistics on u returned by `urotate`\n\n* `compare_advs`   - Reads and plots from both ADV statistics files.\n---\n### Nortek scripts\nWe thank NortekUSA for several scripts written by Lee Gordon and distributed here:\n\nhttp://www.nortekusa.com/usa/knowledge-center/table-of-contents/waves\n\nThese are:\n\n`wds.m` - Directional wave spectra calcs\n\n`hs.m`  - Summary statistics of output of `wds`\n\n`wavek` and `logavg` - called by `wds`\n"
 },
 {
  "repo": "BigelowLab/oharvester",
  "language": "Python",
  "readme_contents": "# oharvester\n\n**Goal**: A repository with a library of functions, scripts, and tutorials for harvesting oceanographic data.\n\n* Each tool works basically like this:\n\n```\ntoolX(time range, spatial range, data source)\n  {\n    gets [data] from specified source\n    returns [data]\n    (or possibly saves [data] to a file)\n  }\n```\n    \nSometimes the needs change a little bit, so the arguments to the functions can change, too.  For example, if you need to retrieve point data the too might work as shown below. In this case, `points` might be a data frame with `lon`, `lat`, and `date` columns.\n\n```\ntoolX(points, data source)\n  {\n    gets [data] from specified source\n    returns [data]\n    (or possibly saves [data] to a file)\n  }\n```\n\n\n## Other resources\n\nCath Mitchell's [oc-satellite-misc](https://github.com/cathmmitchell/oc-satellite-misc) repos."
 },
 {
  "repo": "gher-ulg/Diva-Workshops",
  "language": "Jupyter Notebook",
  "readme_contents": "[![Build Status](https://github.com/gher-ulg/Diva-Workshops/workflows/CI/badge.svg)](https://github.com/gher-ulg/Diva-Workshops/actions)\n[![DOI](https://zenodo.org/badge/108153788.svg)](https://zenodo.org/badge/latestdoi/108153788)\n\n# DIVA Workshops and training\n\nThis page provides the [Jupyter](https://jupyter.org/) notebooks (examples and exercises) for the `DIVAnd` user workshops and training sessions organised in the frame of H2020 [SeaDataCloud](https://www.seadatanet.org/) project.     \n\n[`Diva`](https://github.com/gher-ulg/DIVA) and [`DIVAnd`](https://github.com/gher-ulg/divand.jl) are software tools designed to generate gridded fields from in-situ observations.\n\nNotebooks can be previewed with the service [nbviewer](https://nbviewer.jupyter.org/github/gher-ulg/Diva-Workshops/tree/master/notebooks/).\n\n## Objectives\n\n* The [1st workshop](https://gher-ulg.github.io/Diva-Workshops/Previous/Diva-workshop-2018-Liege.html) (Li\u00e8ge, 3-6 April 2018) was focused on the creation of gridded products and climatologies using `DIVAnd`. The organizational details are available.\n\n* Within the 2nd SeaDataCloud training course (19-26 June 2019), the objective is to introduce participants to the `Julia` language, the Jupyter notebooks and the new Virtual Research Environment.\n\n* The [2nd workshop](https://gher-ulg.github.io/Diva-Workshops/2020/) (Bologna, 27-30 January 2020) was attended by beginners, intermediate and advanced users, and the goal was to help them create new products with `DIVAnd`.\n\n## Recommendation for the products\n\nA list of [recommendations](./tricks/climato_recommendation.md) for the preparation of products.\n\n## Tips and tricks\n\n### Export ODV to netCDF\n\n[Instructions](./tricks/ODV_netCDF_export.md) and screen-shots detailing how to create a netCDF\nfrom an ODV collection.\n\n### Adding EDMO code and CDI to CORA data sets\n\n[Instructions](./tricks/ODV_EDMO_CDI.md) and screen-shots to show how to add metadata to a CORA\ndataset.\n\n### Manipulation of a netCDF with nco\n\n[Examples](./tricks/cutting_netcdf.md)\n\n### Viewing netCDF file\n\n[netCDF](./tricks/netCDV_viewing.md)\n\n### Saving the attributes in a text file\n\nThis is useful when working on a machine with no internet connexion:      \n[save_attributes_file.ipynb](notebooks/postprocessing/save_attributes_file.ipynb)\n\n# Binder\n\nMost notebooks need more resources that what is can currently available on Binder. The introduction notebooks (introduction to OI and variationa analysis) however work\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/gher-ulg/Diva-Workshops/master?filepath=notebooks%2F1-Intro%2F04-OI-variational-analysis-introduction.ipynb).\n"
 },
 {
  "repo": "chouj/POCalendar",
  "language": null,
  "readme_contents": "# POCalendar\nA (Physical) Oceanography Calendar based on <strike>Airtable</strike> [Notion](https://notion.so).\n\n## Address: [Here](https://www.notion.so/pocalendar/Calendar-for-Physical-Oceanographer-814d0e162904439a932fac0b4ee9b87a)\n\n### Why I create and maintain a public calendar while everyone has his/her own ?\n\n- Event seeking of course. Especially, deadlines are marked on the calendar.\n- might be useful to community. Just like another project of mine: [POPapers](https://github.com/chouj/popapers)\n- A test whether community is willing to collaborate on such kind of project. \n- convenient to revisit / recall all the events in the past.\n\n### Who might be interested ?\n\n- Researchers in the fields of (Physical) Oceanography and Climate Change.\n- Media / Press chasing scientific achievements and academic events.\n- Those who cares about or study history of ocean sciences.\n\n### Special Features\n\n- Associated deadlines are marked, not only events.\n- - There are \"Event list\" in which you can sort according to certain `Property` and the \"Calendar view\" is absolutely instinctive and convenient.\n- Events with livestream capability are labeled with different color.\n\n### Content Sources\n\n- [\u6d77\u6d0b\u77e5\u5708](https://mp.sohu.com/profile?xpt=c29odW1wOGh2a2NnQHNvaHUuY29t&_f=index_pagemp_1&spm=smpc.content.author.2.15680372448367pEIA27)\n- [Conferences and Meetings on Oceanography](https://www.conference-service.com/conferences/oceanography.html)\n- [https://usclivar.org/us-clivar-newsgrams](https://usclivar.org/us-clivar-newsgrams)\n- [https://www.solas-int.org/activities/conferences.html](https://www.solas-int.org/activities/conferences.html)\n- [https://www.egu.eu/meetings/calendar/](https://www.egu.eu/meetings/calendar/)\n- [https://www.jcomm.info/index.php?option=com_oe&task=eventCalendar&start=2021-03-01&end=2021-06-30&headGroupID=78](https://www.jcomm.info/index.php?option=com_oe&task=eventCalendar&start=2021-03-01&end=2021-06-30&headGroupID=78)\n- [GEOMAR events](https://events.geomar.de/)\n- [North Pacific Marine Science Organization meetings](https://meetings.pices.int/meetings)\n\n### Notice\n\n- Time zone issue exists. Please pay attention to. Any suggestions about this are appreciated.\n- Any adjustment about event time, for example, deadline getting extended, would not be modified/revised accordingly, if I might not be aware of it.\n\n### Seeking collaborators and financial support.\n\nTeamwork and cooperation will make its management a lot easier. However, as defined by Notion, more collaborators, more charges. Thus, please use my [referral link](https://www.notion.so/?r=d94e7c7a384f486185f8296abe5ea15b) to register (You will get $10 credit while I will get $5).\n\n### \u4e2d\u6587\u7b80\u4ecb (Introduction in chinese)\n\n[(\u7269\u7406)\u6d77\u6d0b\u9886\u57df\u65e5\u5386](https://xuchi.name/calendar-for-oceanographer/)\n\n### Changelog\n\n- Moved from [Airtable](https://airtable.com/shrc9MSotEPF7zYAv/tbl4cC2OM65kinAka/viwxx55XjBsRGEeIj?blocks=hide&date=2020-04-17&mode=month) to Notion on Mar 11, 2020. [Airtable](https://airtable.com/shrc9MSotEPF7zYAv/tbl4cC2OM65kinAka/viwxx55XjBsRGEeIj?blocks=hide&date=2020-04-17&mode=month) is such an elegante tool but simply costs too much for full features.\n\n#### Buy me a cup of coffee\n\n[![Donate](https://img.shields.io/badge/Donate-PayPal-green.svg)](https://www.paypal.me/Mesoscale)\n[![Donate](https://img.shields.io/badge/Donate-WeChat-brightgreen.svg)](https://github.com/chouj/donate-page/blob/master/simple/images/WeChatQR.jpg?raw=true)\n[![Donate](https://img.shields.io/badge/Donate-AliPay-blue.svg)](https://github.com/chouj/donate-page/blob/master/simple/images/AlipayQR.jpg?raw=true)\n"
 },
 {
  "repo": "gher-ulg/Documentation",
  "language": null,
  "readme_contents": "# GHER documentation\n\nThis wiki is meant to contain all the information that does not fit specific projects, software tools or personal pages.\n\nTypically the content of the [Software section](http://modb.oce.ulg.ac.be/mediawiki/index.php/Software) of the previous wiki.\nOnce a section has been moved to the current wiki, the corresponding link should be added in the former wiki.\n\n## Conversion from mediawiki\n\nTo convert from `mediawiki` format to `markdown`, use [pandoc](http://pandoc.org/) conversion tool.   \nAn example for the conversion of the Python section:\n```bash\npandoc -f mediawiki -t markdown Python.wiki -o Python.md\n```\nwhere `Python.wiki` is a file whose content is obtained as a copy/paste of the mediawiki source\n\n## Other contents\n\n**Projects:** should be listed at http://labos.ulg.ac.be/gher/projects/ with a link to the project web page.    \n**Personal pages:** links at http://labos.ulg.ac.be/gher/people/.    \n**Software tools:** links to [github](https://github.com/gher-ulg) from http://labos.ulg.ac.be/gher/software-2/.    \n\n## Table of contents\n\nUse [gh-md-toc](https://github.com/ekalinin/github-markdown-toc) on the main markdown file to generate the table of contents (not dynamically). Then copy the result in the sidebar (right-hand side).\n\n\n"
 },
 {
  "repo": "EPauthenet/fda.oceM",
  "language": "MATLAB",
  "readme_contents": "# fda.oceM\nFunctional Data Analysis of oceanographic profiles\n\n**Functional Data Analysis** is a set of tools to study curves or functions. Here we see vertical hydrographic profiles of several variables (temperature, salinity, oxygen,...) as curves and apply a functional principal component analysis (FPCA) in the multivaraite case to reduce the dimensionality of the system. The classical case is done with couples of temperature and salinity. It can be used for front detection, water mass identification, unsupervised or supervised classification, model comparison, data calibration ...\nThis repository is also available in an R package [fda.oce](https://github.com/EPauthenet/fda.oce) with a doi [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.4073123.svg)](https://doi.org/10.5281/zenodo.4073123).\n\n*Dependencies*:\nThe method uses the fdaM Toolbox by Jim Ramsay.\nhttp://www.psych.mcgill.ca/misc/fda/downloads/FDAfuns/Matlab/\nYou will need to install this toolbox and add it to the matlab path to use this software\n\n*References*: \n- Pauthenet et al. (2019) The thermohaline modes of the global ocean. Journal of Physical Oceanography, [10.1175/JPO-D-19-0120.1](https://doi.org/10.1175/JPO-D-19-0120.1)\n- Pauthenet et al. (2018) Seasonal meandering of the Polar Front upstream of the Kerguelen Plateau. Geophysical Research Letters, [10.1029/2018GL079614](https://doi.org/10.1029/2018GL079614)\n- Pauthenet et al. (2017) A linear decomposition of the Southern Ocean thermohaline structure. Journal of Physical Oceanography, [10.1175/JPO-D-16-0083.1](http://dx.doi.org/10.1175/JPO-D-16-0083.1)\n- Ramsay, J. O., and B. W. Silverman, 2005: Functional Data Analysis. 2nd Edition Springer, 426 pp., Isbn : 038740080X.\n\n\n# Demo\nHere is an example of how to use these functions. We compute the modes for temperature and salinity profiles of the reanalysis [GLORYS](http://marine.copernicus.eu/services-portfolio/access-to-products/) in the Southern Ocean for December of 2015.\n\nFirst we load the data and fit the Bsplines on the 1691 profiles of the example. By default the function fit 20 Bsplines. It returns a fd object named 'fdobj' :\n``` Matlab\nload GLORYS_SO_2015-12.mat\nfdobj = bspl(Pi,Xi);\n```\n\nThen we apply the FPCA on the fd object :\n``` Matlab\nfpca(fdobj);\n```\n\nThe profiles can be projected on the modes defined by the FPCA, to get the principal components (PCs) :\n``` Matlab\nproj(fdobj,pca);\n```\n\nVisualisation of the 2 first PCs :\n``` Matlab\npc_plot(pca,pc);\n```\n<img src=\"https://github.com/EPauthenet/fda.oceM/blob/master/figures/pc_plot.png\" alt=\"drawing\" width=\"1000px\"/>\n\nVisualisation of the 2 first eigenfunctions effect on the mean profile (red (+1) and blue (-1)) :\n``` Matlab\neigenf_plot(pca,1);\neigenf_plot(pca,2);\n```\n<img src=\"https://github.com/EPauthenet/fda.oceM/blob/master/figures/eigenf1.png\" alt=\"drawing\" width=\"370px\"/> <img src=\"https://github.com/EPauthenet/fda.oceM/blob/master/figures/eigenf2.png\" alt=\"drawing\" width=\"370px\"/>\n\nThe profiles can then be reconstructed with less PCs than the total number, removing the small variability. For example with only 5 modes :\n``` Matlab\nte = 5;\nfdobj_reco = reco(pca,pc,te);\n```\n\nTo transform fd objects back in a the variable space, we use the function eval.fd (\"fda\" package) :\n``` Matlab\nX = eval_fd(Pi,fdobj);\nX_reco = eval_fd(Pi,fdobj_reco);\n```\n\nAnd finally we can represent the profiles reconstructed compared to the original data :\n``` Matlab\ni = 600  %index of a profile\nfigure(1),clf\nfor k = 1:pca.ndim    %Loop for each variable                        \n  subplot(1,2,k)\n  plot(Xi(:,i,k),-Pi,'ko')              %Plot of the raw data\n  xlim([min([Xi(:,i,k);X_reco(:,i,k)]) max([Xi(:,i,k);X_reco(:,i,k)])])\n  ylim([-max(Pi) 0]);\n  xlabel(char([pca.fdnames{3}{k}]))\n  ylabel(pca.fdnames(1))\n  hold on\n  plot(X(:,i,k),-Pi,'r')           %Plot of the B-spline fit\n  plot(X_reco(:,i,k),-Pi,'g')     %Plot of the reconstructed profiles\nend\nlegend('raw','spline','reconstructed')\n```\n<img src=\"https://github.com/EPauthenet/fda.oceM/blob/master/figures/reco_prof600.png\" alt=\"drawing\" width=\"1000px\"/>\n\n"
 },
 {
  "repo": "Sam-Saarinen/WHOI-ML",
  "language": "Jupyter Notebook",
  "readme_contents": "# WHOI-ML: About This Repository\nThis Repository contains notes, resources, and assignments for a Machine Learning Bootcamp for the Woods Hole Oceanographic Institution.\n\n# About This Document\nThis ReadMe outlines the content of the bootcamp, provides installation instructions for the required software packages, and suggests background reading material for extended study.\n\n# Course Structure\nThe course will run over 3 days in six 4-hour sessions. Each session will be broken up into 3-4 subsections consisting of presentation content and a practical component. Practical components are designed so that the minimal learning goals can be comfortably met within the allotted time, but stretch goals will allow participants to gain additional practice and deepen their understanding.\n\nThe six sections are:\n1. **The Machine Learning Pantry** - Machine Learning comes in several different flavors (supervised, unsupervised, reinforcement, etc.) and there are several ready-made libraries that will give us 80% of the functionality in 20% of the time. In this section, you will use scikit-learn and matplotlib to make predictions.\n2. **Neural Networks and Supervised Training** - Artificial Neural Networks have been used to great effect in almost every area of machine learning, thanks to their flexibility, their simplicity of use, and the explosion of available training data. In this section you will use pytorch to define and train several types of neural networks to solve several types of machine learning problem. Although why they work is still something of a mystery, this section will elucidate when they work, and what to try when they don't.\n3. **Structured and Spatio-Temporal Data** - Data often have structure that constrains what is possible or what is likely. In this section, you will learn to use clustering algorithms on discrete spatial categories, represent continuous functions using Gaussian Processes, represent sequential and temporal data, and encode translational symmetry using convolutional structures.\n4. **Unsupervised Learning and Anomaly Detection** - Unsupervised (and self-supervised) techniques can be used to summarize data, providing interpretability, computational simplification, and measures of typicality. In this section, you will use PCA, t-SNE, and mixture models to summarize data, and you will use data reconstruction error and likelihoods to identify outliers.\n5. **Reinforcement Learning and Automation** - Machine Learning can be used to optimize decisions and improve them over time. In this section you will apply several versions of Thompson Sampling to decision problems with uncertainty, and you will apply REINFORCE to decisions with long-term effects.\n6. **Machine Learning in the Real World** - In this section you will apply what you've learned to a dataset of your choice. This section will also describe some approaches to scalability, addressing time, memory, and data constraints.\n\n# System Requirements\nAny machine with an up-to-date popular desktop operating system (Mac OS, Windows, Ubuntu, e.g.) and a modern web browser installed will be sufficient for this course. That said, machine learning can be computationally intensive, so higher-end CPU's will save time, and machines with NVidia GPUs _may_ be able to leverage them for faster training of neural networks, in the section of the course pertaining to that. However, modest hardware will still be sufficient for the purpose of the course.\n\n# Software Packages\nThis program uses many software packages, but fortunately will only require two installs, as a large number of the packages we will use come packaged with Anaconda.  \n1. [Install Anaconda](https://www.anaconda.com/distribution/). If offered a choice, download and install the package with the most recent version of Python (3.7). Anaconda is a scientific computing package that bundles many software packages and libraries we will be using in the course, including Python, Jupyter Notebooks, the conda package manager, matplotlib, numpy, pandas, scikit-learn, tqdm, and many more.\n2. [Install PyTorch](https://pytorch.org/). PyTorch is a deep-learning library (comparable to Tensorflow) with a shallow learning curve and high flexibility. This is easiest to do through the command line, using the conda package manager. In the \"Getting Started\" section, you can choose the install command that fits your environment. Note that if you are not running on a CUDA-supported NVidia GPU, you should choose \"none\" to install only the CPU version of PyTorch. Windows users may need to run the installation command through Power Shell or by going through Anaconda Navigator > Environments > base (root) > Open Terminal.\n3. Test the installation by opening a new Jupyter notebook and running  \n`import torch`  \n`print(torch.__version__)`    \nJupyter notebooks can be opened by either running `jupyter notebook` from a shell/terminal or by going through the Anaconda Navigator > Jupyter Notebooks. Either option should open a page in your web browser that will allow you to browse files and create a new notebook by clicking new > Python 3. Type the code above into a cell, and then execute the cell by pressing Shift+Enter.\n\n# Background Reading:\nNo prior knowledge of Machine Learning is assumed in this course, but a strong programming background is assumed. The course will be exclusively in Python, and participants may want to review some of the following pages so that they can focus exclusively on Machine Learning once the course begins.  \n- [Python Cheat Sheet](https://perso.limsi.fr/pointal/_media/python:cours:mementopython3-english.pdf). Feel free to search or follow a tutorial if anything needs clarification.\n- [Python Classes/Objects](https://www.w3schools.com/python/python_classes.asp)\n- [Python Special Methods](https://micropyramid.com/blog/python-special-class-methods-or-magic-methods/) (`__init__`, `__str__`, `__repr__`, etc.). There is a gentler introduction [here](https://dbader.org/blog/python-dunder-methods).\n- [Functional Programming](https://kite.com/blog/python/functional-programming/)\n- [Debugging in Python](https://realpython.com/python-debugging-pdb/). Although Jupyter notebooks make interactive programming and introspection through `print` statements very flexible, `pdb` may still be useful to developers aiming at 10x engineer status.\n\n# Additional Resources:\nThese resources will not be used in this course, but those hoping to explore some areas in greater detail may find them worth looking into.\n\n### Troubleshooting Neural Networks\n- Josh Tobin - [Troubleshooting Deep Neural Networks](http://josh-tobin.com/troubleshooting-deep-neural-networks.html)\n- Andrej Karpathy - [A Recipe for Training Neural Networks](http://karpathy.github.io/2019/04/25/recipe/)\n    - See also: Karpathy's [tweet thread](https://twitter.com/karpathy/status/1013244313327681536) on common neural net mistakes\n- Matt Holt & Daniel Ricks - [Practical Advice for Building Deep Neural Networks](https://pcc.cs.byu.edu/2017/10/02/practical-advice-for-building-deep-neural-networks/)\n\n### Parallel/Distributed Training\n- Thomas Wolf - [Training Neural Nets on Larger Batches: Practical Tips for 1-GPU, Multi-GPU & Distributed Setups](https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255)\n\n### Recurrent Networks\n- Andrej Karpathy - [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n- Chris Olah - [Understanding LSTMs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n\n### Reinforcement Learning\nFor those interested in doing more reinforcement learning experiments (on Markov Decision Processes, not Bandit Problems), the libraries `gym` and `simple_rl` may be helpful. These can be installed through a python package manager, such as `conda` or `pip`.\n\n### Other Courses\n- fast.ai - [Practical Deep Learning for Coders](https://course.fast.ai/index.html)\n- Stanford - [Convolutional Neural Networks for Visual Recognition](http://cs231n.github.io/)\n\n# License Information\nAll content in this repository is \u00a92019 (S)am. It has been made available for educational purposes, but copying or redistributing this content is not permitted. If you would like to suggest changes to this repository or correct errors, please contact the owner of this repository or submit a pull request.\n\n# Instructor Contact:\n`sam_saarinen@brown.edu`\n"
 },
 {
  "repo": "cgentemann/python4oceanography.github.io",
  "language": "SCSS",
  "readme_contents": "## Introduction\nThis template utilizes Jekyll, an open source static website generator, as well as a theme based largely off of the Minimal Mistakes theme by Michael Rose. The purpose of this template is to provide you with a simple, well designed website that is optimized for hosting on Github pages. We aim to reduce the technological know-how and time that is usually required for maintaining a personal or professional website.\n\n#### Why Should I Use This?\nBy using this template you will have a website that is well designed, easy to maintain, free to host and easy to update. While there are many options out there for personal and professional websites, most are dependant on the platform on which they were built, and cannot be easily migrated. This template, while built for Github Pages integration, provides flexibility should you choose to host it elsewhere.\n\n## [Getting Started Guide](https://ncsu-libraries.github.io/jekyll-academic-docs/)\nComplete documentation for getting started as well as advanced features of Jekyll Academic can be found at [https://ncsu-libraries.github.io/jekyll-academic-docs/](https://ncsu-libraries.github.io/jekyll-academic-docs/).\n\n## Migrating to a new default branch name\nWe've decided to change this project's default branch name to 'main'.  If you've forked this repository prior to July 20th, 2021, then you should a message with update instructions when you go to your fork in github: \n\n![fork renamed message](https://user-images.githubusercontent.com/3514165/126372022-ae4c07fa-dec7-427c-a4b5-cdd73aec75eb.png)\n\nIn your fork on GitHub, go to the branches view, and click on the edit icon next to the 'master' branch.  Change the branch name to main.  Underneath the input box where you change the name you will be presented with the commands that you will need to run on your local copy of your fork.\n\n![local instructions for default branch name change ](https://user-images.githubusercontent.com/3514165/126372635-208fbc4b-698e-4938-bdae-5ff19eed2c96.png)\n\n\n## Upgrade Notes for June 2021 release\nIf you are running a fork of Jekyll Academic before June 2021, we made some breaking changes to upgrade the underlying Jekyll version and to address the constant github/dependabot security notices mentioned in issue #4.\n\nWe have updated Jekyll to version 4 and removed reveal.js as an included library. We still want to support reveal.js presentations, so we have taken the suggestion from issue #4 and made the reveal.js directory a [git submodule](https://git-scm.com/book/en/v2/Git-Tools-Submodules). If you are running Jekyll Academic as a Github Page, this should hopefully be a minor change.\n\nIf, however, you are running Jekyll Academic locally or on a custom server, after merging this repo's commits in to your fork, you will need to go to the command line in your local or custom instance and perform the following command:\n\n  `$ git submodule update --init`\n\nIf you have any reveal.js presentations posted, you may need to make some updates for them to display properly using reveal.js version 4.  See [Jekyll's documentation](https://revealjs.com/upgrading/) for details.\n\n## Keeping reveal.js up to date\nMoving forward, if you'd like to update reveal.js you will need to run the following commands:\n\n  `$ git submodule update --remote`\n"
 },
 {
  "repo": "pmlmodelling/oceandata",
  "language": "Python",
  "readme_contents": "# oceandata\nMethods to access oceanographic data available through the likes of thredds servers.\n\n\n\n"
 },
 {
  "repo": "dislu/oceanography",
  "language": "Python",
  "readme_contents": "# oceanography\nWork done as a Project Scientist in indian national centre for ocean information services, a autonomous institute under ministry of earth science of indian government.\n"
 },
 {
  "repo": "Herpinemmanuel/Oceanography",
  "language": "Python",
  "readme_contents": "# Oceanography\nOceanography - Internship M1 Fluid Mechanics - Agulhas current and vortex \n\nI could study the influence of the Aiguilhas current on the circulation of the South Atlantic gyre. We are interested in the ocean / atmosphere interactions. The Aguilhas current, which is a saline and hot stream, meets the cold Antarctic circumpolar current. This encounter led to the formation of eddies in south of South Africa, which ascended to the Atlantic and drifted westward.\n\nAgulhas current : https://commons.wikimedia.org/wiki/File:Indian_Ocean_Gyre.png?uselang=fr\n\nAntarctic Circumpolar current  : https://commons.wikimedia.org/wiki/File:Antarctic_Circumpolar_Current.jpg?uselang=fr\n\nAgulhas vortex  : https://www.ird.fr/var/ird/storage/images/media/ird.fr/dic/fas/semestre2_2012/fas-408/courant-des-aiguilles/383551-1-fre-FR/courant-des-aiguilles.jpge\n\nTo study theese eddies, we used MITgcm (MIT General Circulation Model) which allow us to resolve mouvements equations in ocean and atmosphere with finite volume method. \n\nTo store results we used a SSH server.\n\nTo exploit them, we worked on a UNIX interface with Python computation language.\n\n"
 },
 {
  "repo": "eas2655-taka/oceanography",
  "language": "Matlab",
  "readme_contents": "# oceanography\n\nThis repository includes MATLAB scripts used in EAS4305/6305\n"
 },
 {
  "repo": "anhleduypham/oceanography",
  "language": null,
  "readme_contents": "oceanography\n============\n\noceanography\n"
 },
 {
  "repo": "Kayla425/Oceanography",
  "language": null,
  "readme_contents": "# Oceanography\n4/30 - .nc.bz2 large global file, but we only need a section of it. Download from url - unzip - ncl lat/lon specifics.\n"
 },
 {
  "repo": "Neryneto/Oceanography",
  "language": "Python",
  "readme_contents": "\"#Oceanography\" \n"
 },
 {
  "repo": "KingSeaStar/Oceanography-Underwater-Glider-Survey-Amelia-2016-Wilimington-Canyon",
  "language": "MATLAB",
  "readme_contents": "# Oceanography\n\nScripts, tools, data sources for analyzing and visualizing ocean data from:\n\n- Slocum underwater gliders. (underwater temperature, conductivity, pressure, dissolved oxygen, optical properties etc.)\n- Marine buoys (winds)\n- Moorings and vertical profilers\n- Satellites (sea surface temperature, sea surface height)\n- Ocean models (most oceanographic parameters)\n- Atmospheric models (winds)\n\nThis repository contains the MATLAB scripts I have written in my graduate school research (M.S. & Ph.D.).\nI plan to translate some of the scripts from MATLAB to Phython.\nI have learned from or directly used the scripts written by others in the oceanography community, meteology community, earth science community, and on the internet. \nBugs might exist in some of the scripts.\n"
 },
 {
  "repo": "SalishSeaCast/tools",
  "language": "Jupyter Notebook",
  "readme_contents": "***********************\nSalish Sea MEOPAR Tools\n***********************\n:License: Apache License, Version 2.0\n\nThis is a collection of tools for working with the Salish Sea MEOPAR NEMO model,\nits results,\nand associated data.\n\nDocumentation for the tools is included in the docs/ directory and is rendered at https://salishsea-meopar-tools.readthedocs.io/en/latest/.\n\n.. image:: https://readthedocs.org/projects/salishsea-meopar-tools/badge/?version=latest\n   :target: https://readthedocs.org/projects/salishsea-meopar-tools/?badge=latest\n   :alt: Documentation Status\n\n\nLicenses\n========\n\nUnless otherwise specified,\nthe Salish Sea MEOPAR tools and documentation are copyright 2013-2021 by the `Salish Sea MEOPAR Project Contributors`_ and The University of British Columbia.\n\nThey are licensed under the Apache License, Version 2.0.\nhttps://www.apache.org/licenses/LICENSE-2.0\nPlease see the LICENSE file for details of the license.\n\n.. _Salish Sea MEOPAR Project Contributors: https://github.com/SalishSeaCast/docs/blob/master/CONTRIBUTORS.rst\n\nThe `salishsea_tools.namelist` and `salishsea_tools.tests.test_namelist` modules are based on https://gist.github.com/krischer/4943658.\nThey are copyright 2013 by Lion Krischer <krischer@geophysik.uni-muenchen.de> and are licensed under the GNU Lesser General Public License, Version 3 (http://www.gnu.org/copyleft/lesser.html).\n"
 },
 {
  "repo": "janadr/time_series_analysis",
  "language": "Jupyter Notebook",
  "readme_contents": "# Time series analysis\nPython equivalent code for a course in data analysis in oceanography.\n"
 },
 {
  "repo": "jaysongiroux/Oceanography",
  "language": "JavaScript",
  "readme_contents": "# Oceanography Tool Kit\n### Objective\nCreate a computer program that allows oceanographers and researchers to enter data\n on a user friendly UI that will parse the data and output the visualized data into an \n intuitive PDF document that will be intuitive and easy to understand for the general public and other researchers\n___\n\n### Setup:\n```npm i``` \n<br>\n```npm start```\n\n___\n\n### Packaging for deployment\n```npm install electron-packager --save-dev```<br>\n```cd ..```<br>\n```electron-packager Oceanography/ OceanographyToolKit```\n<br>\nor\n<br>\n```npm i && npm run package```\na folder will appear in the same folder the github repo is stored with the packaged program\n\n\nDocumentation: https://github.com/electron/electron-packager \n<br>\n<br>Format for custom packaging: <br>\n```electron-packager <sourcedir> <appname> --platform=<platform> --arch=<arch> [optional flags...]```\n\n___\n\n### Development \n#### Tools\n- Add the link in the tools.html page then create a separate tool HTML and JS file. This will allow easy scalability for multiple tools in this program\n- include the navbar in each new html file you add\n\n### Information: \nDependencies: <br>\n    ```\n    \"electron-reload\": \"latest\",\n    \"nan\": \"^2.14.1\",\n    \"plotly\": \"^1.0.6\",\n    \"react\": \"^16.13.1\",\n    \"react-dom\": \"^16.13.1\"\n    ```"
 },
 {
  "repo": "RiccardoCar/Oceanography",
  "language": null,
  "readme_contents": "Oceanography\n============\n"
 },
 {
  "repo": "colterbrooks/Oceanography",
  "language": null,
  "readme_contents": "# Oceanography"
 },
 {
  "repo": "davidtran470/Oceanography",
  "language": "HTML",
  "readme_contents": "# Oceanography\nScientific Communication Project\n"
 },
 {
  "repo": "dan-fern/Oceanography",
  "language": null,
  "readme_contents": "# Oceanography\nAssignments for Coastal/Estuarine Oceanography Class, Jack Barth, Spring 2014\n"
 },
 {
  "repo": "hafez-ahmad/Oceanography",
  "language": "Jupyter Notebook",
  "readme_contents": "# Oceanography [My personal web page ](https://hafez-ahmad.github.io/HafezAhmadOceanographer.github.io/)\nOceanic , atmospheric and climate data analysis, and visualization by python. this repo is only for learning . \n"
 },
 {
  "repo": "BarshisLab/CBASS-vs-RSS-Physiology",
  "language": "R",
  "readme_contents": "Electronic notebook for \"Remarkably high and consistent tolerance of a Red Sea coral to rapid and prolonged thermal stress\"\n\nOpen Access publication accessible here: https://aslopubs.onlinelibrary.wiley.com/doi/full/10.1002/lno.11715 \n\nContains two folders: one with all the main figures and one with the supplementary figures\n\nEach Folder contains a PDF of the figure, the data and R code used to produce the figure, and the R code detailing how the statistical analyses were conducted.\n\nNote that figures 3, 4, and S2 were produced with Prism by Graphpad (v8) using the provided data.\n"
 },
 {
  "repo": "dankelley/oceanography",
  "language": "Julia",
  "readme_contents": "This repository holds some tests that might shed light on the utility and\npracticality of creating a Julia package for oceanographic analysis.\n\n# Struct ideas\n\nThis might work\n```julia\nabstract type Oce end\nstruct Adp<:Oce\n    metadata::Dict\n    data::Dict\nend\na = Adp(Dict(), Dict())\na.metadata[\"filename\"] = \"food\"\na.data[\"u\"] = Array{Float64}(undef, 4, 10, 200);\na\n```\n\n"
 },
 {
  "repo": "dptorri/Oceanography",
  "language": null,
  "readme_contents": "# Oceanography\nSample website using the PHP framework - Laravel \n\n\nOceanography\n\nOceanography (compound of the Greek words \u1f60\u03ba\u03b5\u03b1\u03bd\u03cc\u03c2 meaning \"ocean\" and \u03b3\u03c1\u03ac\u03c6\u03c9 meaning \"write\"), also known as oceanology, is the study of the physical and the biological aspects of the ocean.\nIt is also the topic of this exercise.\n"
 },
 {
  "repo": "susopeiz/Oceanography",
  "language": "Matlab",
  "readme_contents": "# Oceanography\nSome code of physical oceanography calculations\n"
 },
 {
  "repo": "ActionMailman/oceanography",
  "language": "Jupyter Notebook",
  "readme_contents": "# Oceanography: Predicting Salinity in Water\nA Jupyter Notebook where I explore correlations with salinity in water using over 60 years worth of oceanographic data. More info can be found in the Jupyter Notebook. Throughout the notebook, I conduct exploratory data analysis of the data, look for correlations with different variables, and create a model using linear regression to predict salinity levels based off other factors.\n\nMake sure to download the dataset from (https://www.kaggle.com/sohier/calcofi) if you want to recreate the Notebook on your machine for whatever reason.\n"
 },
 {
  "repo": "bellemscott/oceanography",
  "language": "Ruby",
  "readme_contents": "# Belle Scott and Melissa Rothenberg's Oceanography Final Project\n\nThis application will be an interactive and informative experience for users, made up of mostly static pages and apis with hurricane/relavent ocean hazard data. \n\nLink to deployed website: <will enter once deployed>\n\nSpecs and Instructions:\n\n* Ruby version: 2.7.2\n\n* Rails version: 6.1.1\n\n* Database: Postgres version 13\n\n* To run: clone this reposity, install all dependencies with bundle install, then run with rails s\n"
 },
 {
  "repo": "blaqlamb/oceanography",
  "language": null,
  "readme_contents": "# oceanography\n"
 },
 {
  "repo": "firesinger/oceanography",
  "language": "Jupyter Notebook",
  "readme_contents": "This is New one for MD"
 },
 {
  "repo": "nasa/podaac_tools_and_services",
  "language": "Python",
  "readme_contents": "podaac_tools_and_services\n=========================\nThis is a meta-repository which lists locations of code related to all tools and services software for `NASA JPL's Physical Oceanography Distributed Active Archive Center (PO.DAAC) <https://podaac.jpl.nasa.gov>`__.\n\n|image7|\n\nWhat is PO.DAAC?\n----------------\nThe `PO.DAAC <https://podaac.jpl.nasa.gov>`__ is an element of the Earth Observing System Data and Information System (`EOSDIS <https://earthdata.nasa.gov/>`__). The EOSDIS provides science data to a wide community of users for NASA's Science Mission Directorate. `PO.DAAC <https://podaac.jpl.nasa.gov>`__ has become the premier data center for measurements focused on ocean surface topography (OST), sea surface temperature (SST), ocean winds, sea surface salinity (SSS), gravity, ocean circulation and sea ice.\n\nWhat's in this repository?\n--------------------------\nThis repository reflects an active catalog of all tools and services software pertaining to `PO.DAAC data access <https://podaac.jpl.nasa.gov/dataaccess>`__. If you have a suggestion for a new tool or would like to update the content here, please `open an issue <https://github.com/nasa/podaac_tools_and_services/issues>`__ or `send a pull request <https://github.com/nasa/podaac_tools_and_services/pulls>`__.\n\nWhere do I find detailed information on tools and services included in this repository?\n---------------------------------------------------------------------------------------\nEach repository has it's own README file e.g. `data_animation/README.rst <https://github.com/nasa/podaac_tools_and_services/blob/master/data_animation/README.rst>`__\n\nKeeping Git submodules up-to-date\n---------------------------------\nIn order to keep the submodules as defined in [.gitmodules](https://github.com/nasa/podaac_tools_and_services/blob/master/.gitmodules) up-to-date it is necessary to periodically push updates. You can safely execute this command to do so::\n\n\n    $ git submodule foreach git pull origin master\n    $ git status //you will then see the changes which have been mode\n    $ git add -A\n    $ git commit -m \"Update submodules\"\n    $ git push origin master\n\n\nLicense\n-------\n| Unless noted explicitly, all code in this repository is licensed permissively under the `Apache License\n  v2.0 <http://www.apache.org/licenses/LICENSE-2.0>`__.\n| A copy of that license is distributed with each software project.\n\nCopyright and Export Classification\n-----------------------------------\n\n::\n\n    Copyright 2019, by the California Institute of Technology. ALL RIGHTS RESERVED. \n    United States Government Sponsorship acknowledged. Any commercial use must be \n    negotiated with the Office of Technology Transfer at the California Institute \n    of Technology.\n    This software may be subject to U.S. export control laws. By accepting this software, \n    the user agrees to comply with all applicable U.S. export laws and regulations. \n    User has the responsibility to obtain export licenses, or other export authority \n    as may be required before exporting such information to foreign countries or \n    providing access to foreign persons.\n\n.. |image7| image:: https://podaac.jpl.nasa.gov/sites/default/files/image/custom_thumbs/podaac_logo.png\n"
 },
 {
  "repo": "EmriN5347/sep11-freedom-project",
  "language": null,
  "readme_contents": "# sep11-freedom-project\nby Emri Nesimi\n\n## Context\nText\n\n## Content\nText\n\n## Links\n\nProduct\n\nPresentation\n\n## Implications\nText\n\n---\n\n* [Blog Entry 1](entries/entry01.md)\n"
 },
 {
  "repo": "fcarvalhopacheco/CODAS",
  "language": null,
  "readme_contents": "# ``CODAS`` ADCP PROCESSING GUIDE\n\nThis is a guide to complement and/or simplify some steps required to install [CODAS software](https://currents.soest.hawaii.edu/docs/adcp_doc/codas_setup/index.html)\n\n>*A **CODAS** (Common Oceanographic Data Access System) database is a way to store and access oceanographic data. CODAS was developed in the late 1980's as a portable, self-describing format for oceanographic data, with emphasis on processed ADCP data.*\n\n\n>*The \"**CODAS processing**\" refers to the ADCP data processing software and procedures that were developed around the CODAS format. The processing steps have become increasingly automated, but human judgement is still required for the final product, and the software is highly flexible in allowing manual configuration and execution of individual steps.* [(Hummon, 2009)](https://currents.soest.hawaii.edu/docs/adcp_doc/)\n\n\n## For detailed  information, please visit:\n- [Currents Group at the University of Hawaii](https://currents.soest.hawaii.edu/home/)\n\n- [UHDAS+CODAS Documentation](https://currents.soest.hawaii.edu/docs/adcp_doc/)\n\n\n\n## Installation Options:\n\n***Recommended***\n\n- [Virtualbox](https://currents.soest.hawaii.edu/docs/adcp_doc/codas_setup/virtual_computer/index.html)\n\n\n**Harder/Tricky**\n\n- [Ubuntu - Anaconda3 - Python 2.7 Environment](https://github.com/fcarvalhopacheco/CODAS-installation/blob/master/installation/anaconda3_py27.md)\n\n- [Ubuntu - Anaconda3 - Python 3.6 Environment](https://github.com/fcarvalhopacheco/CODAS-installation/blob/master/installation/anaconda3_py36.md)\n\n\n## Processing Example:\n\n- [Shipboard ADCP Processing](https://github.com/fcarvalhopacheco/CODAS/blob/master/processing/kilo_moana_processing.md)\n\n\n"
 },
 {
  "repo": "max-simon/master-thesis",
  "language": "Jupyter Notebook",
  "readme_contents": "# On the Impact of Submesoscale Fronts on Mesoscale Eddies and Biological Productivity in the California Current System - Masterthesis in Physics\n\n[![CC BY 4.0][cc-by-shield]][cc-by]\n\n**Title**: On the Impact of Submesoscale Fronts on Mesoscale Eddies and Biological Productivity in the California Current System\n\n**Author**: Max Simon\n\n**Institute**: _Department of Environmental Systems Science_, ETH Z\u00fcrich and _Department of Physics and Astronomy_, University of Heidelberg\n\n**Submission date**: 18.12.2020\n\n**Supervisors:** [Prof. Dr. Norbert Frank (University of Heidelberg)](https://www.iup.uni-heidelberg.de/de/institut/mitarbeiter/prof-norbert-frank), [Prof. Dr. Nicolas Gruber (ETH Z\u00fcrich)](https://usys.ethz.ch/personen/profil.nicolas-gruber.html) and [Dr. Matthias M\u00fcnnich (ETH Z\u00fcrich)](https://up.ethz.ch/people/person-detail.NDY0NDk=.TGlzdC8xMDg5LC0zMDYxNTA1MjU=.html)\n\n**Abstract**: Submesoscale motions are often not resolved in numerical models, although recent studies suggest that they interact with mesoscale processes. This might be particularly relevant for regions like the California Current System (CCS) where mesoscale processes redistribute nutrients and organic matter to offshore regions. In this study, the impact of submesoscale fronts on mesoscale eddies and biological productivity is examined by comparing two models of the CCS with different horizontal resolutions: a conventional (7.0 km) and a front-permitting resolution (2.8 km). A novel detection algorithm was developed which allows quantifying the area covered by submesoscale fronts. The algorithm reveals that fronts occur more often in anticyclones than in cyclones. This results in a weakening of the density anomaly associated with anticyclones by 40% during winter for the increased resolution. Further, the energy cascade of mesoscale eddies is better resolved contributing to the seasonal evolution of eddy kinetic energy. Finally, the biological productive band at the coast broadens, presumably driven by enhanced lateral transport of nutrients. The results demonstrate that submesoscale and mesoscale motions are inextricably linked and that regional numerical models should aim to resolve submesoscale fronts for future studies.\n\n**Full text**: [Digital Version](thesis/print/document_signed.pdf)\n\n\n**License**: This work is licensed under a\n[Creative Commons Attribution 4.0 International License][cc-by].\n\n[![CC BY 4.0][cc-by-image]][cc-by]\n\n[cc-by]: http://creativecommons.org/licenses/by/4.0/\n[cc-by-image]: https://i.creativecommons.org/l/by/4.0/88x31.png\n[cc-by-shield]: https://img.shields.io/badge/License-CC%20BY%204.0-lightgrey.svg"
 },
 {
  "repo": "SalishSeaCast/SalishSeaNowcast",
  "language": "Python",
  "readme_contents": "****************\nSalishSeaNowcast\n****************\n\n:License: Apache License, Version 2.0\n\n.. image:: https://img.shields.io/badge/license-Apache%202-cb2533.svg\n    :target: https://www.apache.org/licenses/LICENSE-2.0\n    :alt: Licensed under the Apache License, Version 2.0\n.. image:: https://img.shields.io/badge/python-3.10-blue.svg\n    :target: https://docs.python.org/3.10/\n    :alt: Python Version\n.. image:: https://img.shields.io/badge/version%20control-git-blue.svg?logo=github\n    :target: https://github.com/SalishSeaCast/SalishSeaNowcast\n    :alt: Git on GitHub\n.. image:: https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&logoColor=white\n   :target: https://github.com/pre-commit/pre-commit\n   :alt: pre-commit\n.. image:: https://img.shields.io/badge/code%20style-black-000000.svg\n    :target: https://black.readthedocs.io/en/stable/\n    :alt: The uncompromising Python code formatter\n.. image:: https://readthedocs.org/projects/salishsea-nowcast/badge/?version=latest\n    :target: https://salishsea-nowcast.readthedocs.io/en/latest/\n    :alt: Documentation Status\n.. image:: https://github.com/SalishSeaCast/SalishSeaNowcast/workflows/sphinx-linkcheck/badge.svg\n      :target: https://github.com/SalishSeaCast/SalishSeaNowcast/actions?query=workflow:sphinx-linkcheck\n      :alt: Sphinx linkcheck\n.. image:: https://github.com/SalishSeaCast/SalishSeaNowcast/workflows/CI/badge.svg\n    :target: https://github.com/SalishSeaCast/SalishSeaNowcast/actions?query=workflow%3ACI\n    :alt: pytest and test coverage analysis\n.. image:: https://codecov.io/gh/SalishSeaCast/SalishSeaNowcast/branch/master/graph/badge.svg\n    :target: https://codecov.io/gh/SalishSeaCast/SalishSeaNowcast\n    :alt: Codecov Testing Coverage Report\n.. image:: https://github.com/SalishSeaCast/SalishSeaNowcast/actions/workflows/codeql-analysis.yaml/badge.svg\n      :target: https://github.com/SalishSeaCast/SalishSeaNowcast/actions?query=workflow:CodeQL\n      :alt: CodeQL analysis\n.. image:: https://img.shields.io/github/issues/SalishSeaCast/SalishSeaNowcast?logo=github\n    :target: https://github.com/SalishSeaCast/SalishSeaNowcast/issues\n    :alt: Issue Tracker\n\nThe ``SalishSeaNowcast`` package is a collection of Python modules associated with running\nthe SalishSeaCast ocean models system of daily nowcasts and forecasts.\nThe runs use as-recent-as-available\n(typically previous day)\nforcing data for the western boundary sea surface height and the Fraser River flow,\nand atmospheric forcing from the four-times-daily produced forecast results from the\nEnvironment and Climate Change Canada High Resolution Deterministic Prediction System\n(HRDPS) operational GEM 2.5km resolution model.\n\nThe model runs are automated using an asynchronous,\nmessage-based architecture that:\n\n* obtains the forcing datasets from web services\n* pre-processes the forcing datasets into the formats expected by NEMO and the other models\n  in the automation system\n* uploads the forcing dataset files to the HPC or cloud-computing facility where the runs\n  will be executed\n* executes the run\n* downloads the results\n* prepares a collection of plots from the run results for monitoring purposes\n* publishes the plots and the processing log to the web\n\nDocumentation for the package is in the ``docs/`` directory and is rendered at https://salishsea-nowcast.readthedocs.io/en/latest/.\n\n.. image:: https://readthedocs.org/projects/salishsea-nowcast/badge/?version=latest\n    :target: https://salishsea-nowcast.readthedocs.io/en/latest/\n    :alt: Documentation Status\n\n\nLicense\n=======\n\n.. image:: https://img.shields.io/badge/license-Apache%202-cb2533.svg\n    :target: https://www.apache.org/licenses/LICENSE-2.0\n    :alt: Licensed under the Apache License, Version 2.0\n\nThe SalishSeaCast ocean model automation system code and documentation are copyright 2013 \u2013 present\nby the `SalishSeaCast Project Contributors`_ and The University of British Columbia.\n\n.. _SalishSeaCast Project Contributors: https://github.com/SalishSeaCast/docs/blob/master/CONTRIBUTORS.rst\n\nThey are licensed under the Apache License, Version 2.0.\nhttp://www.apache.org/licenses/LICENSE-2.0\nPlease see the LICENSE file for details of the license.\n"
 },
 {
  "repo": "CoherentStructures/OceanTools.jl",
  "language": "Julia",
  "readme_contents": "# OceanTools.jl\n\n[![build status][build-img]][build-url] [![coverage][codecov-img]][codecov-url]\n\nThis package was previously known as `CopernicusUtils.jl`.\nUtilities for working with oceanographic datasets from the [Copernicus](http://marine.copernicus.eu/)\nproduct family. This package can be used (amongst other things) to generate large test\ncases for use with [CoherentStructures.jl](https://github.com/CoherentStructures/CoherentStructures.jl).\n\n## Main features\n\nAbility to load velocity fields of arbitrary space-time cubes from Copernicus CMES data files.\n\nFast, allocation free interpolation on regular grids supporting periodic boundaries.\n\n## Documentation\n\n[![stable docs][docs-stable-img]][docs-stable-url] [![dev docs][docs-dev-img]][docs-dev-url]\n\n## Example Pictures\n\nFTLE field for a 90 day period off the coast of Japan.\n\n![FTLE field off the coast of Japan](https://raw.githubusercontent.com/CoherentStructures/OceanTools.jl/master/examples/ftle_plot.jpg)\n\nCalculating ``material barriers'' on freely choosable domain, more details in the\ncorresponding [documentation page](https://coherentstructures.github.io/OceanTools.jl/dev/example/):\n\n![material barriers](https://github.com/natschil/misc/raw/master/images/oceantools3.png)\n\n## Misc\n\nThis package is in no way affiliated with the Copernicus Marine Environment Monitoring Service.\n\n[build-img]: https://github.com/CoherentStructures/OceanTools.jl/workflows/CI/badge.svg?branch=master\n[build-url]: https://github.com/CoherentStructures/OceanTools.jl/actions?query=workflow%3ACI+branch%3Amaster\n\n[codecov-img]: http://codecov.io/github/CoherentStructures/OceanTools.jl/coverage.svg?branch=master\n[codecov-url]: http://codecov.io/github/CoherentStructures/OceanTools.jl?branch=master\n\n[docs-dev-img]: https://img.shields.io/badge/docs-dev-blue.svg\n[docs-dev-url]: http://coherentstructures.github.io/OceanTools.jl/dev\n\n[docs-stable-img]: https://img.shields.io/badge/docs-stable-blue.svg\n[docs-stable-url]: http://coherentstructures.github.io/OceanTools.jl/stable\n"
 },
 {
  "repo": "oceanhackweek/ohw19-projects-Trackpy",
  "language": "Jupyter Notebook",
  "readme_contents": "# Trackpy\n\n## Connecting animal tracking and mesoscale oceanographic data\n***\n\n#### Collaborators\n\n+ Hannah Blondin (hblondin at stanford dot edu)\n+ James Fahlbusch (musculus at stanford dot edu)\n+ Will Oestreich (woestreich at stanford dot edu)\n\n***\n\n### Background\n\nUnderstanding the underlying oceanographic drivers of the movements of pelagic marine organisms has been a major area of research interest for both primary and applied marine ecologists in recent years. Several recent studies leveraging animal tracking data and remotely-sensed oceanographic data have elucidated the importance of mesoscale oceanographic features, including eddies and filaments, to movements of highly-mobile pelagic organisms (e.g. Braun et al., 2019; Gaube et al., 2018; Abrahms et al., 2018). Open-source tools for linking such animal movements to oceanographic remote sensing data are of interest (see Dodge et al., 2013 for an R-based tool), yet simple tools for linking animal movements to databases of value-added mesoscale oceanographic features derived from remote sensing products are not readily available.\n\n#### Resources\n\n- https://www.aviso.altimetry.fr/fileadmin/documents/data/tools/hdbk_eddytrajectory_2.0exp.pdf\n\n### Goals\n\nProvide an illustrative example for Python beginners to link animal movement data with remotely sensed oceanographic data (SST) and databases of mesoscale oceanographic features and visualize these interactions.\n\n### Datasets\n1. Simulated loggerhead turtle (Caretta caretta) satellite tag tracks.\n\n2. AVISO+ Mesoscale Eddy Trajectory Atlas Product \n\n3. Sea surface temperature, blended, global, 2002-2014, EXPERIMENTAL 5-day composite\n \n### Workflow\n1. Upload animal track data\n\n2. Link animal movements in space and time to remotely-sensed SST\n\n3. Access AVISO+ mesoscale eddy data product\n\n4. Slice mesoscale eddy dataset for spatiotemporal domain of animal tracks\n\n5. Visualize animal-feature interactions in space and time\n\n\n### References\nAbrahms, B., Scales, K.L., Hazen, E.L., Bograd, S.J., Schick, R.S., Robinson, P.W. and Costa, D.P., 2018. Mesoscale activity facilitates energy gain in a top predator. Proceedings of the Royal Society B: Biological Sciences, 285(1885), p.20181101.\n\nBraun, C.D., Gaube, P., Sinclair-Taylor, T.H., Skomal, G.B. and Thorrold, S.R., 2019. Mesoscale eddies release pelagic sharks from thermal constraints to foraging in the ocean twilight zone. Proceedings of the National Academy of Sciences, p.201903067.\n\nChelton, D.B., Schlax, M.G. and Samelson, R.M., 2011. Global observations of nonlinear mesoscale eddies. Progress in Oceanography, 91(2), pp.167-216.\n\nDodge, S., Bohrer, G., Weinzierl, R., Davidson, S.C., Kays, R., Douglas, D., Cruz, S., Han, J., Brandes, D. and Wikelski, M., 2013. The environmental-data automated track annotation (Env-DATA) system: linking animal tracks with environmental data. Movement Ecology, 1(1), p.3.\n\nGaube, P., Braun, C.D., Lawson, G.L., McGillicuddy, D.J., Della Penna, A., Skomal, G.B., Fischer, C. and Thorrold, S.R., 2018. Mesoscale eddies influence the movements of mature female white sharks in the Gulf Stream and Sargasso Sea. Scientific reports, 8(1), p.7363.\n\n\n\n### Required Packages\nInstall:\nconda install package -c conda-forge\n\nPackages:\nfolium\nmatplotlib\nftplib\ngetpass\nnumpy\nnetCDF4\ndatetime\npandas\nos\nxarray\n\n\n"
 },
 {
  "repo": "metno/moxml",
  "language": null,
  "readme_contents": "============================================================\nMOX - Meteorological and Oceanographic XML\n============================================================\n\nNorwegian Meteorological Institute (met.no)\nBox 43 Blindern\n0313 OSLO\nNORWAY\n\nemail: wdb@met.no\n\nThis is a meteorological/oceanographic data exchange format \ndeveloped at met.no. It is developed as an application \nschema of the Geographic Markup Language (GML) version \n3.2.1. The mox schema is used in the wdb weather and water \ndatabase.\n\n\n"
 },
 {
  "repo": "euroargodev/argodmqc_owc",
  "language": "HTML",
  "readme_contents": "# pyowc: OWC salinity calibration in Python\n\n![build](https://github.com/euroargodev/argodmqc_owc/workflows/build/badge.svg)\n[![codecov](https://codecov.io/gh/euroargodev/argodmqc_owc/branch/refactor-configuration/graph/badge.svg)](https://codecov.io/gh/euroargodev/argodmqc_owc)\n[![Requirements Status](https://requires.io/github/euroargodev/argodmqc_owc/requirements.svg?branch=master)](https://requires.io/github/euroargodev/argodmqc_owc/requirements/?branch=refactor-configuration)\n[![Python 3.6](https://img.shields.io/badge/python-3.6-blue.svg)](https://www.python.org/downloads/release/python-360/)\n\n[![Gitter](https://badges.gitter.im/Argo-floats/owc-python.svg)](https://gitter.im/Argo-floats/owc-python?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)\n\nThis software is a python implementation of the \"OWC\" salinity calibration method used in Argo floats Delayed Mode Quality Control.\n\nThis software is in very active development and its usage may change any time. Please [post an issue to get involved if you're interested](https://github.com/euroargodev/argodmqc_owc/issues/new/choose).\n\n# Installation\n\nIf you are using Linux, Windows or macOS, you can simply install this package using `pip` in a virtual environment.\nAssuming your virtual environment is activated:\n\n```bash\npip install pyowc\n```\n\n# Software usage\n\n[![badge](https://img.shields.io/static/v1.svg?logo=Jupyter&label=Pangeo+Binder&message=Click+here+to+try+this+software+online+!&color=blue&style=for-the-badge)](https://binder.pangeo.io/v2/gh/euroargodev/argodmqc_owc/master?filepath=docs%2FTryit.ipynb)\n\nOtherwise, you can run the pyowc code in your local machine in this way:\n\nIn start_with_pycharm.py code, you can specify the WMO float number that you want to do analysis.\nYou can also add more float numbers, then the calculations of all floats will be done at the\nsame time.\n\n```python\nimport pyowc as owc\n\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\nif __name__ == '__main__':\n\n    FLOAT_NAMES = [\"3901960\"]  # add float names here e.g. [\"3901960\",\"3901961\",\"3901962\"]\n    USER_CONFIG = owc.configuration.load()  # fetch the default configuration and parameters\n    print(owc.configuration.print_cfg(USER_CONFIG))\n```\n\n## Parameters for your analysis\n\nParameters for the analysis are set in a configuration.py python code. \nThe configuration has the same parameters as the Matlab software (https://github.com/ArgoDMQC/matlab_owc).\n\n- You can change the default directories to locations of your historical data.\n```python\n        #    Climatology Data Input Paths\n        'HISTORICAL_DIRECTORY': \"data/climatology/\"\n        'HISTORICAL_CTD_PREFIX': \"/historical_ctd/ctd_\"\n        'HISTORICAL_BOTTLE_PREFIX': \"/historical_bot/bot_\"\n        'HISTORICAL_ARGO_PREFIX': \"/historical_argo/argo_\"\n```\n- To run the analysis,you need to have the float source file in .mat format. \n```python\n        #    Float Input Path\n        'FLOAT_SOURCE_DIRECTORY': \"data/float_source/\"\n        'FLOAT_SOURCE_POSTFIX': \".mat\"\n```\n- The output from the analysis will be saved in default directory of the code.You can change \nthe default directories to locations of your constants.\n```python\n        #    Constants File Path\n        'CONFIG_DIRECTORY': \"data/constants/\"\n        'CONFIG_COASTLINES': \"coastdat.mat\"\n        'CONFIG_WMO_BOXES': \"wmo_boxes.mat\"\n        'CONFIG_SAF': \"TypicalProfileAroundSAF.mat\"\n```\n- Final step is to set your objective mapping parameters, e.g.\n```python\n        'MAP_USE_PV': 0\n        'MAP_USE_SAF': 0\n\n        'MAPSCALE_LONGITUDE_LARGE': 8\n        'MAPSCALE_LONGITUDE_SMALL': 4\n        'MAPSCALE_LATITUDE_LARGE': 4\n        'MAPSCALE_LATITUDE_SMALL': 2\n ```\n- Additionally, you can set a specific ranges of theta bounds for salinity anomaly plot.\nThe code will crete two separate plots with set ranges.\n```python \n     #    Plotting Parameters\n        # Theta bounds for salinity anomaly plot\n        'THETA_BOUNDS': [[0, 5], [5, 20]]\n```\n\n## Plots\nThe plots are automatically generated. It is worth to note that only one plot will be \ndisplayed at one time in the PyCharm. The next plot will be displayed after closing\nthe window of the current plot. \n\nThe number of generated plots at specific theta levels (from 1 to 10 theta levels) can be\ncurrently changed in the dashboard.py code. The default is set to 2. The plots will be \ngenerated separately for each theta level.\n\n```python\ndef plot_diagnostics(float_dir, float_name, config, levels=2):\n```\n\n# Building the documentation\n\nIf you wish to build the documentation locally, you will need a virtual environment.\nAssuming your virtual environment is activated, follow these steps:\n\n1. Install the required documentation packages\n    ```bash\n    pip install -r requirements-docs.txt\n    ```\n2. Change directory to the `docs` directory, for example on Linux:\n    ```bash\n    cd docs\n    ```\n3. Run the `sphinx-build` command:\n    ```bash\n    sphinx-build -M html source build -W\n    ```\n\nThis will build the HTML documentation under the `docs/build/html` directory and can be viewed\nusing your normal web browser.\n\n```{admonition} Note\n:class: note\n\nIf you make modifications to the code or documentation configuration, you may need to delete\nthe `docs/source/generated` directory for the documentation to build correctly.\n```\n\n# Software history\n\n- Major refactoring of the software for performance optimisation and to fully embrace the Pythonic way of doing this !\n\n- [UAT: Phase 1, 2, 3](https://github.com/euroargodev/User-Acceptance-Test-Python-version-of-the-OWC-tool) \n\n- Migration of the code from from BODC/NOC git to euroargodev/argodmqc_owc. See https://github.com/euroargodev/User-Acceptance-Test-Python-version-of-the-OWC-tool/issues/10 for more details on the migration. Contribution from [G. Maze](https://github.com/gmaze)\n\n- Alpha experts user testings with [feedbacks available here](https://github.com/euroargodev/User-Acceptance-Test-Python-version-of-the-OWC-tool/issues). Contributions from: [K. Walicka](https://github.com/kamwal), [C. Cabanes](https://github.com/cabanesc), [A. Wong](https://github.com/apswong)\n\n- BODC created [the first version of the code](https://git.noc.ac.uk/bodc/owc-software-python), following the [Matlab implementation](https://github.com/ArgoDMQC/matlab_owc).\n  Contributions from: [M. Donnelly](https://github.com/matdon17), [E. Small](https://github.com/edsmall-bodc),\n   [K. Walicka](https://github.com/kamwal), [A. Hale](https://github.com/halebodc), [T. Gardner](https://github.com/thogar-computer).\n\n\n## New positioning of functions \nNote that functions name are not changed !\n\n- **pyowc/core**\n  - **stats.py**: brk_pt_fit, build_cov, covarxy_pv, covar_xyt_pv, noise_variance, signal_variance, fit_cond, nlbpfun\n  - **finders.py**: find_10thetas, find_25boxes, find_besthit, find_ellipse, nearest_neighbour\n\n- **pyowc/data**\n  - **fetchers.py**: get_region_data, get_region_hist_locations, get_data, get_topo_grid, frontal_constraint_saf\n  - **wrangling.py**: interp_climatology, map_data_grid \n\n- **pyowc/plot**\n  - **dashboard.py**: plot_diagnostics\n  - **plots.py**: cal_sal_curve_plot, sal_var_plot, t_s_profile_plot, theta_sal_plot, trajectory_plot\n  - **utils.py**: create_dataframe\n\n- **pyowc/calibration.py**: update_salinity_mapping, calc_piecewisefit\n\n- **pyowc/configuration.py**: load_configuration, set_calseries, print_cfg\n\n- **pyowc/tests**  # Contain all the unit tests !\n\n- **pyowc/utilities.py**: change_dates, cal2dec, potential_vorticity, wrap_longitudes, sorter, spatial_correlation\n"
 },
 {
  "repo": "chouj/POPapers",
  "language": null,
  "readme_contents": "# [POPapers](https://twitter.com/geomatlab)\n\n![](https://img.shields.io/badge/dynamic/json?label=Twitter%20Followers&query=%24.data.totalSubs&url=https%3A%2F%2Fapi.spencerwoo.com%2Fsubstats%2F%3Fsource%3Dtwitter%26queryKey%3Dgeomatlab&color=1da1f2&logo=twitter&longCache=true&style=flat-square)\n\n###### A twitter account to auto-share new papers in the field of (Physical) Oceanography\n\n## Introduction\n\n![https://img.shields.io/badge/Powered%20by-RSS-orange.svg?style=plastic&logo=rss](https://img.shields.io/badge/Powered%20by-RSS-orange.svg?logo=rss)\n\n\"**POPapers**\" means Pysical Oceanography Papers. Actually, it also means \"papers pop out\". ^_^ \n\nThe account was created on 6 Apr 2013, when I wanted to utilize the `RSS` feeds provided by my preferred journals to help myself keep an eye on research frontiers. Soon I found it might be useful to other researchers, so the idea emerged natually when one was aware of the automation / information flow services like [IFTTT](https://ifttt.com) and [Zapier](https://zapier.com). Currently, it has been followed by [600+ worldwide researchers](https://twitter.com/geomatlab/followers) in the fields of oceanography, atmospheric sciences or climate change.\n\nActually, I have been harnessing useful information by using `RSS` technology since 2007. There is a category called \"[About RSS](https://chouj.github.io/categories/rss%E7%9B%B8%E5%85%B3/)\" on my blog. `RSS` is not dead apparently.\n\nIf you like this idea or have been benifited from it, please kindly **star this repo**. Cheers.\n\n## How to setup a similar one in your field\n\n<blockquote class=\"twitter-tweet\" data-partner=\"tweetdeck\"><p lang=\"en\" dir=\"ltr\">It&#39;s pretty easy to create your own as long as your preferred journals provide RSS feeds for recently-published papers. Then follow the instructions in this post [ <a href=\"https://t.co/3epPIhBdN3\">https://t.co/3epPIhBdN3</a>] with a ready twitter account for auto-posting.</p>&mdash; POPapers (@geomatlab) <a href=\"https://twitter.com/geomatlab/status/1125692588302819329?ref_src=twsrc%5Etfw\">May 7, 2019</a></blockquote>\n\n## Scrape RSS feeds of these journals (no particular order)\n\n- AGU journals including _Geophys. Res. Lett._, _J. Geophys. Res. -Ocean_, etc.\n- _J. Phys. Oceanogr._\n- Nature Publish Group journals including _Nature_, _Nat. Geosci._, _Nat. Comm._, _Nat. Clim. Change_, etc.\n- _Acta Oceanol. Sin._\n- _J. Mar. Sci._\n- _Prog. Oceanogr._\n- _Deep Sea Res._\n- _Cont. Shelf Res._\n- _Ocean Dynam._\n- _Dynam. Atmos. Oceans_\n- _Front. Mar. Sci._\n- _J. Fluid Dynam._\n- _Ann. Rev. Mar. Sci._\n- _Ann. Rev. Fluid Dynam._\n- _J. Clim._\n- _J. Oceanogr._\n- _Tellus A_\n- _Ocean Model._\n- _Ocean Sci._\n- _J. Atmos. Ocean. Tech._\n- _PlosOne: Oceanography_\n- _Clim. Dynam._\n- _J. Oper. Oceanogr._\n- _AGU EOS_\n- _ESSOAr_\n- ASLO pubs. eg. _Limnol. Oceanogr._\n- _Oceanogr._\n- _Remote Sen._ Ocean Remote Sensing Section\n- _Geosci. Model Dev._ Subject: Oceanography ![](https://img.shields.io/badge/NEW-APR%2012%202020-green)\n- _Earth Syst. Sci. Data_ Subject: Oceanography ![](https://img.shields.io/badge/NEW-APR%2012%202020-green)\n\n## Acknowledgement\n\n- [Twitter](https://twitter.com) | [IFTTT](https://ifttt.com) | [Zapier](https://zapier.com) | [Integromat](https://www.integromat.com/)\n\nThanks for solidary and reliable services.\n\n- [SUBSTATS](https://substats.spencerwoo.com/)\n\n## Else\n\nI also maintain a [Twitter list](https://twitter.com/chouj/lists/oceanography) full of researchers / organizations / projects in the field of (Physical) Oceanography.\n\n#### Buy me a cup of coffee\n\n[![Donate](https://img.shields.io/badge/Donate-PayPal-green.svg)](https://www.paypal.me/Mesoscale)\n[![Donate](https://img.shields.io/badge/Donate-WeChat-brightgreen.svg)](https://github.com/chouj/donate-page/blob/master/simple/images/WeChatQR.jpg?raw=true)\n[![Donate](https://img.shields.io/badge/Donate-AliPay-blue.svg)](https://github.com/chouj/donate-page/blob/master/simple/images/AlipayQR.jpg?raw=true)\n"
 },
 {
  "repo": "mvdh7/pytzer",
  "language": "Python",
  "readme_contents": "# Pytzer\n\n![Tests](https://github.com/mvdh7/pytzer/workflows/Tests/badge.svg)\n[![Coverage](https://github.com/mvdh7/pytzer/blob/master/.misc/coverage.svg)](https://github.com/mvdh7/pytzer/blob/master/.misc/coverage.txt)\n[![pypi badge](https://img.shields.io/pypi/v/pytzer.svg?style=popout)](https://pypi.org/project/pytzer/)\n[![DOI](https://img.shields.io/badge/DOI-10.5281%2Fzenodo.2637914-informational)](https://doi.org/10.5281/zenodo.2637914)\n[![Docs](https://readthedocs.org/projects/pytzer/badge/?version=latest&style=flat)](https://pytzer.readthedocs.io/en/latest/)\n[![License: GPL v3](https://img.shields.io/badge/License-GPLv3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n\nPytzer is a Python implementation of the Pitzer model for chemical activities in aqueous solutions [[P91](https://pytzer.readthedocs.io/en/jax/refs/#p)] plus solvers to determine the equilibrium state of the system.\n\n**Pytzer is in beta!  Use at your own peril.**\n\n- [Pytzer](#pytzer)\n  - [Installation](#installation)\n    - [For general use](#for-general-use)\n    - [For development](#for-development)\n  - [Documentation](#documentation)\n  - [Citation](#citation)\n\n## Installation\n\nDue to its dependency on [JAX](https://github.com/google/jax), Pytzer can only be installed on Unix systems, although it does work on Windows via [WSL](https://docs.microsoft.com/en-us/windows/wsl/).\n\n### For general use\n\nInstall with pip:\n\n    pip install pytzer\n\nNote that you should also install the dependencies (especially NumPy) using pip, not conda, for best performance.  This happens automatically with the above command if the dependencies are not already installed.\n\nOnce installed, you will need to set the environment variable `JAX_ENABLE_X64=True`.  For example, using conda:\n\n    conda env config vars set JAX_ENABLE_X64=True\n\n### For development\n\nUse the [environment.yml](https://github.com/mvdh7/pytzer/blob/master/environment.yml) file to create a new environment with Conda:\n\n    conda env create -f environment.yml\n\nThen, fork and/or clone this repo to somewhere that your Python can see it.\n\n## Documentation\n\nA work in progress at [pytzer.readthedocs.io](https://pytzer.readthedocs.io/en/latest/).\n\n## Citation\n\nPytzer is maintained by [Dr Matthew P. Humphreys](https://humphreys.science) at the [NIOZ Royal Netherlands Institute for Sea Research](https://www.nioz.nl/en) (Texel, the Netherlands).\n\nFor now, the appropriate citation is:\n\n> Humphreys, Matthew P. and Schiller, Abigail J. (2021). Pytzer: the Pitzer model for chemical activities and equilibria in aqueous solutions in Python (beta).  *Zenodo.*  [doi:10.5281/zenodo.2637914](https://doi.org/10.5281/zenodo.2637914).\n\nPlease report which version of Pytzer you are using.  You can find this in Python with:\n\n```python\nimport pytzer as pz\npz.hello()\n```\n"
 },
 {
  "repo": "jchawarski/arctic-oceanography",
  "language": "R",
  "readme_contents": "# arctic-oceanography\n# This is a repository for publishing my research code\n\nIt includes:\n  CTD_summary- a file used to summarize CTD data from .cnv files \n  \n"
 },
 {
  "repo": "selina24228/physical_oceanography",
  "language": "MATLAB",
  "readme_contents": "# physical_oceanography\nThese are homeworks I done in course introduction to physical oceanography. <br>\nAll of them are written in matlab.\n\n## hw1\nIn this homework, I get data from database of NOAA (National Oceanic and Atmospheric Administration).<br>\nAfter that, I use contourf to plot the data, and observer how temperature and salinity change with latitude and longitude.\n\n## hw2\nIn this homework, I use seawater toolbox to draw a T-S diagram and mark AAIW, NADW, and AABW on it.<br>\nAlso, I count the in-situ temperature and potential temperature of a specific station, as well as in-situ density. \n\n## hw3\nAssuming the ocean has constant salinity gradients in both y (north-south) and z direction, I compute the density and plot the distribution. <br>\nAnd then, I use thermal wind relation to infer geostrophic flow.\n\n## hw4\nModeling Ekman sprial by ode and Euler method.\n\n## final\n1. plot the density distribution and identify upwelling and downwelling regions.<br>\n2. use thermal wind relation to infer along-channel geostrophic flow. <br>\n3. plot the vertical structure of ageostrophic velocity, and end up a Ekman spiral.\n"
 },
 {
  "repo": "amirvt1234/oceanography_notebooks",
  "language": "Jupyter Notebook",
  "readme_contents": "#Oceanography notebooks\n\nSome IPython notebooks to help undergraduate students in Oceanography class.\n"
 },
 {
  "repo": "wpinheiro99/oo",
  "language": null,
  "readme_contents": "# oo\nOceanographic Observatory\n"
 },
 {
  "repo": "DocOtak/gsw-xarray",
  "language": "Python",
  "readme_contents": ".. |CI Status| image:: https://github.com/docotak/gsw-xarray/actions/workflows/ci.yml/badge.svg\n  :target: https://github.com/DocOtak/gsw-xarray/actions/workflows/ci.yml\n  :alt: CI Status\n.. |Documentation Status| image:: https://readthedocs.org/projects/gsw-xarray/badge/?version=latest\n  :target: https://gsw-xarray.readthedocs.io/en/latest/?badge=latest\n  :alt: Documentation Status\n.. |pypi| image:: https://badge.fury.io/py/gsw-xarray.svg\n   :target: https://badge.fury.io/py/gsw-xarray\n   :alt: pypi package\n.. |conda forge| image:: https://img.shields.io/conda/vn/conda-forge/gsw-xarray\n   :target: https://anaconda.org/conda-forge/gsw-xarray\n\ngsw-xarray: Wrapper for gsw that adds CF attributes\n===================================================\n|CI Status| |Documentation Status| |pypi| |conda forge|\n\ngsw-xarray is a wrapper for `gsw python <https://github.com/TEOS-10/GSW-python>`_\nthat will add CF attributes to xarray.DataArray outputs.\nIt is meant to be a drop in wrapper for the upstream GSW-Python library and will only add these attributes if one argument to a function is an xarray.DataArray.\n\nYou can find the documentation on `gsw-xarray.readthedocs.io <https://gsw-xarray.readthedocs.io/>`_.\n\nUsage\n-----\n\n.. code:: python\n\n   import gsw_xarray as gsw\n\n   # Create a xarray.Dataset\n   import numpy as np\n   import xarray as xr\n   ds = xr.Dataset()\n   id = np.arange(3)\n   ds['id'] = xr.DataArray(id, coords={'id':id})\n   ds['CT'] = ds['id'] * 10\n   ds['CT'].attrs = {'standard_name':'sea_water_conservative_temperature'}\n   ds['SA'] = ds['id'] * 0.1 + 34\n   ds['SA'].attrs = {'standard_name':'sea_water_absolute_salinity'}\n\n   # Apply gsw functions\n   sigma0 = gsw.sigma0(SA=ds['SA'], CT=ds['CT'])\n   print(sigma0.attrs)\n\nOutputs\n\n::\n\n   {'standard_name': 'sea_water_sigma_t', 'units': 'kg/m^3'}\n\nDon't worry about usage with non xarray array objects, just use in all places you would the upstream library:\n\n.. code:: python\n\n   sigma0 = gsw.sigma0(id * 10, id * 0.1 + 34)\n   print(type(sigma0), sigma0)\n\nOutputs\n\n::\n\n   <class 'numpy.ndarray'> [-5.08964499  2.1101098   9.28348219]\n\n\nWe support (and convert the unit if necessary) the usage of pint.Quantities and the usage of xarray wrapped Quantities.\nSupport for pint requires the installation of two optional dependencies: ``pint`` and ``pint-xarray``.\nIf all the inputs to a gsw function are Quantities, the returned object will also be a Quantity belonging to the same UnitRegistry.\n\n.. warning::\n\n   Quantities must all belong to the same pint.UnitRegistry, a ValueError will be thrown if there are mixed registries.\n\n.. warning::\n\n   If one input is a Quantity, all inputs must be Quantities (and/or xarray wrapped Quantities), except for the `axis` and `interp_method` arguments.\n   For mixed usage of Quantities and non Quantities, a ValueError will be thrown.\n\n.. code:: python\n\n   import pint_xarray\n   import gsw_xarray as gsw\n\n   # Create a xarray.Dataset\n   import numpy as np\n   import xarray as xr\n   ds = xr.Dataset()\n   id = np.arange(3)\n   ds['id'] = xr.DataArray(id, coords={'id':id})\n   ds['CT'] = ds['id'] * 10\n   # make sure there are unit attrs this time\n   ds['CT'].attrs = {'standard_name':'sea_water_conservative_temperature', 'units': 'degC'}\n   ds['SA'] = ds['id'] * 0.1 + 34\n   ds['SA'].attrs = {'standard_name':'sea_water_absolute_salinity', 'units': 'g/kg'}\n\n   # use the pint accessor to quantify things\n   ds = ds.pint.quantify()\n\n   # Apply gsw functions\n   sigma0 = gsw.sigma0(SA=ds['SA'], CT=ds['CT'])\n   # outputs are now quantities!\n   print(sigma0)\n\nOutputs\n\n::\n\n   <xarray.DataArray 'sigma0' (id: 3)>\n   <Quantity([27.17191038 26.12820162 24.03930887], 'kilogram / meter ** 3')>\n   Coordinates:\n     * id       (id) int64 0 1 2\n   Attributes:\n       standard_name:  sea_water_sigma_t\n\nThe usage of xarray wrapped Quantities is not required, you can use pint directly (though the ``pint-xarray`` dep still needs to be installed).\n\n.. code:: python\n\n   import gsw_xarray as gsw\n   import pint\n   ureg = pint.UnitRegistry()\n   SA = ureg.Quantity(35, ureg(\"g/kg\"))\n   CT = ureg.Quantity(10, ureg.degC)\n   sigma0 = gsw.sigma0(SA=SA, CT=CT)\n   print(sigma0)\n\nOutputs\n\n::\n\n   26.824644457868317 kilogram / meter ** 3\n\nAs gsw-xarray converts arguments to the proper unit when Quantities are used, we can e.g. use the temperature in Kelvin:\n\n.. code:: python\n\n   CT = ureg.Quantity(10, ureg.degC).to('kelvin')\n   sigma0 = gsw.sigma0(SA=SA, CT=CT)\n   print(sigma0)\n\nOutputs\n\n::\n\n   26.824644457868317 kilogram / meter ** 3\n\n.. note::\n   If you do not wish to use the unit conversion ability, you need to pass dequantified Quantities\n   (e.g. `da.pint.dequantify()` for pint-xarray or `arg.magnitude` for pint.Quantity).\n\n.. warning::\n   On the opposite, gsw-xarray will not check the units if non Quantity arguments are used.\n   If you wish to use unit conversion, please pass quantified arguments (if your xarray.Dataset /\n   xarray.DataArray has the 'units' attribute, you can use `da.pint.quantify()`)\n\n.. note::\n   We recommend that you use the `cf-xarray <https://cf-xarray.readthedocs.io/en/latest/units.html>`_ registry for units,\n   as it implements geophysical units as `degree_north`, `degrees_north`, etc.\n   gsw-xarray internally uses `degree_north` and `degree_east` for latitude and longitude unit names.\n   If they are not found in the unit registry, they will be replaced by `degree`.\n\n   The function `gsw.SP_from_SK` uses part per thousand for SK. 'ppt' is already used for picopint,\n   so the expected unit is replaced by '1'.\n\n\nInstallation\n------------\nPip\n...\n\n.. code:: bash\n\n    pip install gsw-xarray\n\n\nConda\n.....\n\nInside a conda environment:  ``conda install -c conda-forge gsw-xarray``.\n\nPipenv\n......\n\nInside a pipenv environment: ``pipenv install gsw-xarray``.\n\n\nContributor guide\n-----------------\n\nAll contributions, bug reports, bug fixes, documentation improvements,\nenhancements, and ideas are welcome.\nIf you notice a bug or are missing a feature, fell free\nto open an issue in the `GitHub issues page <https://github.com/DocOtak/gsw-xarray/issues>`_.\n\nIn order to contribute to gsw-xarray, please fork the repository and\nsubmit a pull request. A good step by step tutorial for starting with git can be found in the\n`xarray contributor guide <https://xarray.pydata.org/en/stable/contributing.html#working-with-the-code>`_.\nA main difference is that we do not use conda as python environment, but poetry.\n\nSet up the environment\n......................\n\nYou will first need to `install poetry <https://python-poetry.org/docs/#installation>`_.\nThen go to your local clone of gsw-xarray and launch installation:\n\n.. code:: bash\n\n   cd /path/to/your/gsw-xarray\n   poetry install\n\nYou can then activate the environment by launching a shell\nwithin the virtual environment:\n\n.. code:: bash\n\n   poetry shell\n\nYou can check that the tests pass locally:\n\n.. code:: bash\n\n   pytest gsw_xarray/tests\n\nRelease (for maintainers only)\n..............................\n\nTODO...\n"
 },
 {
  "repo": "gyuler83821/Oceanography_ENSO",
  "language": "Jupyter Notebook",
  "readme_contents": "# Oceanography_ENSO\n\n107\u5b78\u5e74\u7b2c\u4e00\u5b78\u671f\u6d77\u6d0b\u5b78\u8ab2\u7a0b\u671f\u672b\u5831\u544a\n\n\u8056\u5b30\u73fe\u8c61\uff1aSOI\u503c\u5448\u73fe\n\n\u6578\u64da\u4f86\u6e90:http://www.cpc.ncep.noaa.gov/data/indices/  \n\u53c3\u8003\u8cc7\u6599:http://www.geostory.tw/enso-elnino-math-soi/\n"
 },
 {
  "repo": "NIVANorge/Okokyst_oceanography",
  "language": "HTML",
  "readme_contents": "# Okokyst_oceanography\nReading all \u00f8kokyst data from the DATABASE folder in K (using [script 02](https://github.com/DagHjermann/Okokyst_oceanography/blob/master/02_Read_all.md)) and comparing _Chl a_ at 5 m depth (Ferrybox depth) with _Chl a_ at 10 m depth ([script 04](https://github.com/DagHjermann/Okokyst_oceanography/blob/master/04_ChlA_difference_10m_vs_5m.md)) and 0 m depth ([script 05](https://github.com/DagHjermann/Okokyst_oceanography/blob/master/05_ChlA_difference_0m_vs_5m.md)).  \n  \n_Note that_ the html files made by RStudio look a bit better.   \n\n"
 },
 {
  "repo": "whigg/FortranOceanography",
  "language": "Fortran",
  "readme_contents": "# FortranOceanography\nUtilities for oceanography\n\n* _convert_SMAP_v1_SSS_L3_netcdf.f90_: convert the binary files obtained from [the SMAP salinity product](http://www.remss.com/missions/smap) available on FTP at ftp://ftp.remss.com/smap/L3/v1.0/8day_running into [netCDF](http://www.unidata.ucar.edu/software/netcdf/). \nTo compile:\n```bash\n gfortran -I/usr/include convert_SMAP_v1_SSS_L3_netcdf.f90 -L/usr/lib/ -lnetcdf -lnetcdff -o convert_SMAP_v1_SSS_L3_netcdf.a\n```\n\n<img src=\"https://cloud.githubusercontent.com/assets/11868914/17514539/6f048808-5e33-11e6-9f56-9ffa9b944d8c.png\" width=\"500\">\n"
 },
 {
  "repo": "lugoga/oceanography_wio",
  "language": "JavaScript",
  "readme_contents": "\"# oceanography_wio\" \n"
 },
 {
  "repo": "nbalbed/physical-oceanography",
  "language": "Jupyter Notebook",
  "readme_contents": "# Physical Oceanography Excercise \n\n## This repository is a bunch of code to understand physical oceanography\n"
 },
 {
  "repo": "dankelley/sliderules",
  "language": "R",
  "readme_contents": "---\ntitle: Oceanographic slide rules\nauthor: Dan Kelley\n---\n\n**Overview**\n\nThis repository is for the development of oceanographic slide rules, akin to\nthe general slide rules that were popular until handheld calculators became\ncommon. Although direct computation with software offers more accurate results,\nthese slide rules have a didactic advantage, in the sense that they can yield\nenhanced intuition regarding the essence of the underlying formulae.\n\n**How a slide rule works**\n\nFor those who have not used a conventional side rule, a little bit of\nexplanation is in order.  It's simple. The basic idea is to use sliding rulers\nto combine functions in an additive way.\n\n![Definition sketch of a slide rule set up to add 2 and 3; the answer is under the 3 on the upper ruler.](fig.png)\n\nConsider the diagram above, which sketches two rulers made of\nrectangular pieces of wood.  The orang ruler has a linear scale on the upper\nedge, and blue ruler has an identical scale, but on its lower edge. The rulers\nare placed on a table, or held within a container, in such a way that the\nscales touch.  Then, adding two numbers is a simple matter of sliding the\nrulers.  For example, the sum of 2 and 3 may be found by placing the 0 of the\nupper ruler alongside 2 on the lower ruler, and then looking over to 3 on the\nupper ruler, which will be immediately above 5 on the lower ruler.  In a slide\nrule, there is a transparent pointer that has a line marked orthogonal to the\nscales, which makes it easy to line up the 3 of the upper scale with the 5 of\nthis example.\n\nIn this example, the rulers are a form of mechanical (analogue) computer for\naddition.  It should be obvious that the system also works for subtraction.\nAnd, if the scales are switched from linear to logarithmic form, then\nmultiplication and division are possible.  For this reason, a conventional\nslide rule has a sequence of scales, some linear (for $x+y$ and $x-y$), some\nlogarithmic (for $xy$ and $x/y$), and some with other functions, such as the\ntrigonometric functions.\n\n**Oceanographic slide rules**\n\nIn the oceanographic case, there are some specialized functions that do not fit\nthese general forms. For example, the density of seawater is a function of\nsalinity $S$, temperature $T$, and pressure $p$.  This function has cross terms\nthat depend on combinations of $S$, $T$ and $p$. However, an approximate\nfunction can be constructed (through regression of data from the actual\nfunction) as a sum of three functions, some $f_1(S)$ plus some $f_2(T)$ plus\nsome $f_3(p)$.\n\nThis is the basic idea of the slide rules presented in this repository.  Each\nslide rule gets its own subdirectory here, with at least a brief explanation of\nthe analysis in a README file in the directory.\n\n**Progress**\n\n*Slide rules that are done and tested*\n\n* $\\sigma_\\theta$ from $S$, $T$ and $p$: see [sigthe](sigthe)\n* sound speed from $S$, $T$ and $p$: see [sound_speed](sound_speed)\n\n*Slide rules that under development*\n\n* wave properties given wind speed and fetch\n\n\n*Ideas for new slide rules*\n\n* distance from delta-lon and delta-lat\n* buoy knockdown from radius and water speed\n* something to do with instrument time-constants\n* something to do with waves. NB: the dispersion relationship is hard because\n  it is not in the form $z=f_1(x_1)+f_2(x_2)$.\n* something to do with gliders, perhaps buoyancy adjustment or adjusting\n  waypoints for currents.\n\n"
 },
 {
  "repo": "blaqlamb/oceanography-site",
  "language": "CSS",
  "readme_contents": "# oceanography\n"
 },
 {
  "repo": "manjunath5496/Oceanography-Books",
  "language": null,
  "readme_contents": "1. Biological Oceanography: An Introduction</br>\n                <a href=\"https://github.com/manjunath5496/Oceanography-Books/blob/master/og(1).pdf\" target=\"_blank\" style=\"text-decoration:none\"> <font color=\"blue\"> <center> Download</center></font> </a></br>\n                \n2. Atmosphere, Ocean and Climate Dynamics: An Introductory Text</br>\n                <a href=\"https://github.com/manjunath5496/Oceanography-Books/blob/master/og(2).pdf\" target=\"_blank\" style=\"text-decoration:none\"> <font color=\"blue\"> <center> Download</center></font> </a></br>\n                \n3. Octopus: The Ocean's Intelligent Invertebrate</br>\n                <a href=\"https://github.com/manjunath5496/Oceanography-Books/blob/master/og(3).pdf\" target=\"_blank\" style=\"text-decoration:none\"> <font color=\"blue\"> <center> Download</center></font> </a></br>\n                \n4. Global Physical Climatology</br>\n                <a href=\"https://github.com/manjunath5496/Oceanography-Books/blob/master/og(4).pdf\" target=\"_blank\" style=\"text-decoration:none\"> <font color=\"blue\"> <center> Download</center></font> </a></br>\n                \n5. Underworld: The Mysterious Origins of Civilization</br>\n                <a href=\"https://github.com/manjunath5496/Oceanography-Books/blob/master/og(5).pdf\" target=\"_blank\" style=\"text-decoration:none\"> <font color=\"blue\"> <center> Download</center></font> </a></br>\n                \n6. Oceanography And Marine Biology: An Annual Review</br>\n                <a href=\"https://github.com/manjunath5496/Oceanography-Books/blob/master/og(6).pdf\" target=\"_blank\" style=\"text-decoration:none\"> <font color=\"blue\"> <center> Download</center></font> </a></br>\n\n7. Oceanography in the Days of Sail</br>\n                <a href=\"https://github.com/manjunath5496/Oceanography-Books/blob/master/og(7).pdf\" target=\"_blank\" style=\"text-decoration:none\"> <font color=\"blue\"> <center> Download</center></font> </a></br>\n                \n8. Oceanography: An Illustrated Guide</br>\n                <a href=\"https://github.com/manjunath5496/Oceanography-Books/blob/master/og(8).pdf\" target=\"_blank\" style=\"text-decoration:none\"> <font color=\"blue\"> <center> Download</center></font> </a></br>\n                \n9. The Sea Around Us</br>\n                <a href=\"https://github.com/manjunath5496/Oceanography-Books/blob/master/og(9).pdf\" target=\"_blank\" style=\"text-decoration:none\"> <font color=\"blue\"> <center> Download</center></font> </a></br>\n                \n10. Tracers in the Sea</br>\n                <a href=\"https://github.com/manjunath5496/Oceanography-Books/blob/master/og(10).pdf\" target=\"_blank\" style=\"text-decoration:none\"> <font color=\"blue\"> <center> Download</center></font> </a></br> \n                \n"
 },
 {
  "repo": "ja754969/Satellite-Oceanography",
  "language": "MATLAB",
  "readme_contents": "---\ntitle: '\u885b\u661f\u6d77\u6d0b\u5b78-Curriculum mapping'\n---\n:::warning\n>>>[<<< \u56de\u5230 Curriculum mapping](/wYnQU277R3-kmL1UWGt8bA)\n:::\n# \u885b\u661f\u6d77\u6d0b\u5b78\n:::warning\n[Google Drive](https://drive.google.com/open?id=1zGa2fy-xz8wecCwZuDp4eq_RjD3tuCBq&authuser=00781035%40email.ntou.edu.tw&usp=drive_fs)\n:::\n- [name=Yu-Hao Tseng]\n- [time=Sun, Sep 26, 2021 9:29 PM]\n- [TOC]\n\n---\n## Week 2 : Introduction\n{%youtube 8BAH52z071c %}\n### Books\n- [1. Discovering the Ocean from Space (2010)](/4a1Nprp2Sz-SsNApfIwO1Q)\n- [2. Introduction to Satellite Remote Sensing (2017)](/b0EZa0-OTIqXqW1iUd_VWA)\n\n\n---\n## Week 3 : Chapter1 The methods of satellite oceanography\n\u885b\u661f\u6d77\u6d0b\u5b78 Week3 part1\n{%youtube t_M1PJVmH60 %}\n\u885b\u661f\u6d77\u6d0b\u5b78 Week3 part2\n{%youtube -KUZxv4j4bA %}\n\u885b\u661f\u6d77\u6d0b\u5b78 Week3 part3\n{%youtube Ayz9vxZ6i9M %}\n\n---\n### 2.2 THE UNIQUE SAMPLING CAPABILITIES OF SENSORS ON SATELLITES  \n\u53d6\u6a23\u80fd\u529b wide synoptic coverage  \nThe use of Earth-orbiting satellites as platforms for ocean-viewing sensors offers a number of unique advantages such as the opportunity to achieve wide synoptic coverage at fine spatial detail, and repeated regular sampling to produce time series several years long.  \n\n\u55ae\u9ede\u63a1\u6a23\u7684\u5f71\u50cf\uff0c\u5982\u9ad8\u5ea6\u8cc7\u6599\u662f\u4ee5\u591a\u500b\u55ae\u9ede\u5167\u63d2\u7684\u7d50\u679c  \n\n---\n### 2.2.1 Creating image-like data fields from point samples  \nThe sensor geometry restricts each individual observation to a\nparticular instantaneous field of view (IFOV).  \n![](https://i.imgur.com/3hA3fzf.png)  \n\n---\n### 2.2.2 Satellite orbits and how they constrain remote sensing  \n$GM=3.98603\\times10^{14}(m^3/s^2)$  \n\n### 2.2.3 The space-time sampling capabilities of satellite sensors\n![](https://i.imgur.com/SLanEzC.png)\n- Sensor revisit interval \u548c Repeat period \u7684\u5dee\u7570  \n    - Sensor revisit interval : \u4e00\u9846\u885b\u661f Swath \u91cd\u758a\u7684\u9031\u671f\u3002   \n    - Repeat period : \u8ecc\u9053 Exactly \u91cd\u758a\u7684\u9031\u671f  \n    - \u6240\u4ee5\uff0c\u901a\u5e38Sensor revisit interval\u7684\u9031\u671f\u8f03Repeat period\u77ed  \n\n---\n### 2.3 GENERIC DATA-PROCESSING TASKS\n![](https://i.imgur.com/AJ55Yr8.png)  \n![](https://i.imgur.com/W5lj5Cc.png)  \n\n\n## Week 4 : Section 2.3 Generic data-processing tasks\n### 2.3.2 Atmospheric correction\n\n\n### \u4f5c\u696d\nhttps://trello.com/c/rZsoHFWJ  \n\n\n## Week 18 : \u671f\u672b\u5831\u544a(\u5c0f\u8ad6\u6587)\n\u5f9e\u8ab2\u672c\u7ae0\u7bc0\u7576\u4e2d\u6311\u9078\u4e3b\u984c\n\n\n{%hackmd SybccZ6XD %}\n###### tags: `NTOU_1101` `courses` `Oceanography` `\u6d77\u6d0b\u5b78` `Research` `\u78a9\u73ed`\n \n"
 },
 {
  "repo": "NIVANorge/Oceanography-toolbox",
  "language": "HTML",
  "readme_contents": "# Oceanography-toolbox\nCommon toolbox for the oceanography section(s) \n"
 },
 {
  "repo": "ja754969/Dynamical-Oceanography",
  "language": "MATLAB",
  "readme_contents": "---\ntitle: '\u6d77\u6d0b\u52d5\u529b\u5b78_1091'\n---\n{%hackmd SybccZ6XD %}\n\n[\u96f2\u7aef\u786c\u789f](https://drive.google.com/drive/u/2/folders/1MPLsIrH_CHPmUOJzr2o92jMA1C_6NuL9)\n:star:\n###### tags: `NTOU_1091` `\u6d77\u6d0b\u5b78\u79d1` `Book`\n# Final-term_1091\n## 1081-Mid\n### 1. \u89e3\u91cb\u540d\u8a5e\n\n#### Richardson number (:star:CH7.4 Dynamic stability:star:)\n![](https://i.imgur.com/S79m3Xc.png)\n\n#### eddy viscosity (:star::star:)\n![](https://i.imgur.com/ysSqmG7.png)\n\n### 2. (:star:CH6.33 Gravitation and gravity:star:)\n- [x] (8%)\n\n#### (a) Please draw a sketch showing the relationship between the gravitational acceleration $g_f$ and the acceleration due to gravity g.\n$$\ng_f\\text{ : \u91cd\u529b\u52a0\u901f\u5ea6}\\\\\ng\\text{ : \u56e0\u91cd\u529b\u7522\u751f\u7684\u52a0\u901f\u5ea6}\\\\\n\\Omega\\times(\\Omega\\times R)\\text{ : \u5411\u5fc3\u52a0\u901f\u5ea6}\\\\\n-\\Omega\\times(\\Omega\\times R)\\text{ : \u96e2\u5fc3\u52a0\u901f\u5ea6}\\\\\n$$\n![](https://i.imgur.com/Tz7Qyck.png)\n\n#### (b) Where have the maximum and minimum values of g, respectively? And why?\n1. The maximum values of g at the poles.\n    \u5728\u6975\u9ede($90^o$N\u3001$90^o$S)\uff0c\u6240\u9700\u7684\u5411\u5fc3\u52a0\u901f\u5ea6$\\Omega\\times(\\Omega\\times R)$\u6d88\u5931\u4e86\uff0c\u4e26\u4e14\u56e0\u70ba\u5728\u6975\u9ede\u7684\u5730\u7403\u534a\u5f91\u662f\u6700\u5c0f\u7684\uff0c\u6240\u4ee5$g_f$\u7684\u503c\u4e5f\u5728\u6b64\u8655\u6700\u5927\uff0c\u56e0\u6b64 g \u7684\u6700\u5927\u503c\u51fa\u73fe\u5728\u6975\u9ede\u3002\n\n2. The minimum values of g at the equator.\n    \u5728\u8d64\u9053($0^o$)\uff0c\u6240\u9700\u7684\u5411\u5fc3\u52a0\u901f\u5ea6$\\Omega\\times(\\Omega\\times R)$\u70ba\u6700\u5927\u503c\uff0c\u4e26\u4e14\u56e0\u70ba\u5728\u8d64\u9053\u7684\u5730\u7403\u534a\u5f91\u662f\u6700\u5927\u7684\uff0c\u6240\u4ee5$g_f$\u7684\u503c\u4e5f\u5728\u6b64\u8655\u6700\u5c0f\uff0c\u56e0\u6b64 g \u7684\u6700\u5c0f\u503c\u51fa\u73fe\u5728\u8d64\u9053\u3002\n### 3. (:star:CH6.31 The pressure term:star:)\n- [x] done \n\n#### Please show how can we derive the pressure term in the equation of motion? Write down your answer clearly about every statement. In case, you can just show the pressure term in the x-component momentum equation.\n![](https://i.imgur.com/yLyJNXb.png)\n\n![](https://i.imgur.com/khCYIsJ.png)\n\n## 1081-Final\nIntroductory Dynamical Oceanography Final Exam (1/7/2020)\n\n### 1. (:star:CH9.3:star:)\n- [x] (15%)\n\n#### (a) What is the physical meaning of Ekman's theory? (explain what kind of forces balance).\n:::success\n$$\\begin{cases}\nfv+A_z{\\partial^2 u\\over\\partial z^2} = \\alpha {\\partial p\\over\\partial x} \\\\\n-fu+A_z{\\partial^2 v\\over\\partial z^2} = \\alpha {\\partial p\\over\\partial y}\n\\end{cases}\n (\u5f0f9.6)\n$$\n:::\n\u6839\u64daEkman\u7684\u7b2c5\u500b\u5047\u8aaa\uff0c\u82e5\u662f\u5747\u52fb\u7684\u6d77\u6c34(homogeneous water)\uff0c\u6b63\u58d3\u689d\u4ef6\u4e0b\uff0c\u6d77\u6c34\u5bc6\u5ea6\u7531\u58d3\u529b\u6c7a\u5b9a\uff0c\u9020\u6210 ${\\partial p\\over\\partial x}={\\partial p\\over\\partial y}=0$\uff0c\u56e0\u6b64\u6c92\u6709\u5730\u8f49\u6d41\u7522\u751f\u3002\n:::success\n$$\\begin{cases}\nfv+A_z{\\partial^2 u\\over\\partial z^2} = 0 \\\\\n-fu+A_z{\\partial^2 v\\over\\partial z^2} = 0\n\\end{cases}\n (\u5f0f9.8)\n$$\n:::\n\u56e0\u6b64\u5728\u7269\u7406\u610f\u7fa9\u4e0a\uff0cEkman\u7684\u98a8\u5439\u6d41\u7406\u8ad6\u662f\u79d1\u6c0f\u529b(Coriolis force)\u8207\u98a8\u526a\u529b(net frictional stress)\u5e73\u8861\u7684\u7d50\u679c\u3002\n#### (b) Based on what concepts, Ekman simplified the above governing equations as the balance between two forces? Please show the detail how to obtain the simplified Ekman's equations.\n\u6839\u64daEkman\u7684\u7b2c5\u500b\u5047\u8aaa\uff0c\u82e5\u662f\u5747\u52fb\u7684\u6d77\u6c34(homogeneous water)\uff0c\u6b63\u58d3\u689d\u4ef6\u4e0b\uff0c\u6d77\u6c34\u5bc6\u5ea6\u7531\u58d3\u529b\u6c7a\u5b9a\uff0c\u9020\u6210 ${\\partial p\\over\\partial x}={\\partial p\\over\\partial y}=0$\uff0c\u56e0\u6b64\u6c92\u6709\u5730\u8f49\u6d41\u7522\u751f\u3002\n:::success\n$$\\begin{cases}\nfv+A_z{\\partial^2 u\\over\\partial z^2} = 0 \\\\\n-fu+A_z{\\partial^2 v\\over\\partial z^2} = 0\n\\end{cases}\n (\u5f0f9.8)\n$$\n:::\n#### ( c ) What is the Ekman depth?\n\u98a8\u5439\u6d41\u7522\u751f\u4f5c\u7528\u7684\u6700\u6df1\u6df1\u5ea6\uff0c\u5728\u9019\u500b\u6df1\u5ea6\u7684\u98a8\u5439\u6d41\u65b9\u5411\u548c\u6d77\u8868\u9762\u98a8\u5439\u6d41\u6d41\u5411\u6b63\u597d\u76f8\u53cd\u3002\n#### (d) What is the Ekman Spiral?\n\u4ee5\u5317\u534a\u7403\u70ba\u4f8b\uff0c\u98a8\u5439\u6d41\u5728\u6d77\u6d0b\u8868\u9762\u7684\u6d41\u5411\u70ba\u98a8\u5411\u5411\u53f3\u504f\u8f4945\u5ea6\uff0c\u96a8\u8457\u6df1\u5ea6\u8d8a\u4f86\u8d8a\u6df1\uff0c\u4e0a\u5c64\u6c34\u6d41\u52d5\u6642\u6703\u62d6\u66f3\u4e0b\u65b9\u6d41\u9ad4\u904b\u52d5\uff0c\u98a8\u5439\u6d41\u6d41\u901f\u96a8\u6df1\u5ea6\u589e\u52a0\u800c\u4e0b\u964d\uff0c\u4e14\u6d77\u6d41\u7684\u504f\u8f49\u89d2\u5ea6\u4e5f\u96a8\u8457\u6df1\u5ea6\u589e\u52a0\uff0c\u5230\u9054Ekman\u6df1\u5ea6\u6642\uff0c\u98a8\u5439\u6d41\u65b9\u5411\u548c\u6d77\u8868\u9762\u98a8\u5439\u6d41\u6d41\u5411\u6b63\u597d\u76f8\u53cd\uff0c\u6c34\u6d41\u6d41\u5411\u7531\u6c34\u9762\u5411\u4e0b\u5448\u73fe\u87ba\u7dda\u578b\u614b\u7684\u65cb\u8f49\u69cb\u9020\u3002\n\n#### (e) What is the empirical formula to calculate the wind stress magnitude, $\\tau_\\eta$? Please explain each term.\n:::success\n$$\n\\bbox[5px,border:2px solid red]\n{\n\\tau_\\eta=\\rho_a \\times C_D\\times W^2\n}\n$$\n$\\rho_a$ = \u7a7a\u6c23\u5bc6\u5ea6(the density of air)\n$C_D$ = \u62d6\u66f3\u4fc2\u6578(the drag coefficient)\n$W$ = \u98a8\u901f(the wind speed in m/s)\n:::\n### 2. (:star:CH8:star:)\n- [x] (6%)\n\n#### (a) Please explain what is the barotropic and baroclinic condition, respectively? (CH8 p.68)\n1. barotropic condition\n\u7576\u6d41\u9ad4\u7684\u5bc6\u5ea6\u662f\u58d3\u529b\u7684\u51fd\u6578($\\rho=\\rho(p)$)\u6642\uff0c\u5982\u540c\u5728\u4f4d\u6eab(potential temperature)\u5747\u52fb\u7684\u6de1\u6c34\u4e2d\uff0c\u5c0e\u81f4\u6d41\u5834\u7684\u6240\u6709\u7b49\u58d3\u9762\u548c\u7b49\u5bc6\u9762\u7686\u4e92\u76f8\u5e73\u884c\u3002\n2. baroclinic condition\n\u7576\u6d41\u9ad4\u7684\u5bc6\u5ea6\u4e0d\u53ea\u662f\u662f\u58d3\u529b\u7684\u51fd\u6578\u4e14\u5728\u6c34\u5e73\u9762\u4e0a\u96a8\u8457\u5176\u53c3\u6578\u6539\u8b8a\u6642\uff0c\u53ef\u80fd\u662f\u6d77\u6c34\u6eab\u5ea6\u3001\u9e7d\u5ea6\u53ca\u58d3\u529b\u7684\u6539\u8b8a($\\rho=\\rho(s,t,p)$)\uff0c\u5c0e\u81f4\u6d41\u5834\u7684\u7b49\u58d3\u9762\u548c\u7b49\u5bc6\u9762\u4e26\u975e\u5e73\u884c\u65bc\u5f7c\u6b64\uff0c\u6703\u7522\u751f\u4ea4\u89d2\u3002\n#### (b) Show the \"thermal wind\" equations from the geostrophic equations and the hydrostatic equation.\n$$\n\u5730\u8f49\u6d41\u65b9\u7a0b\u5f0f\n\\begin{cases}\nfv - \\alpha {\\partial p\\over\\partial x}=0 \\\\\n-fu-\\alpha {\\partial p\\over\\partial y}=0\n\\end{cases}\n (\u5f0f8.10)\n$$\n1. from the geostrophic equations\n$$\n\\text{geostrophic equations}\n\\begin{cases}\nfv =\\alpha {\\partial p\\over\\partial x} \\\\\n-fu=\\alpha {\\partial p\\over\\partial y}\n\\end{cases}\n (\u5f0f8.10)\\\\\n\\implies\n\\begin{cases}\n{\\partial\\over\\partial z}(\\rho fv) ={\\partial\\over\\partial z}( {\\partial p\\over\\partial x})={\\partial\\over\\partial x}( {\\partial p\\over\\partial z}) \\\\\n{\\partial\\over\\partial z}(\\rho fu) ={\\partial\\over\\partial z}( -{\\partial p\\over\\partial y})=-{\\partial\\over\\partial y}( {\\partial p\\over\\partial z})\n\\end{cases}\n$$\n2. from the hydrostatic equation\n$$\n\\text{\"thermal wind\" equations}\n\\begin{cases}\nfv =\\alpha {\\partial p\\over\\partial x} \\\\\n-fu=\\alpha {\\partial p\\over\\partial y}\n\\end{cases}\n (\u5f0f8.10)\\\\\n\\implies\n\\text{Using hydrostatic equation,  }\n\\bbox[5px,border:2px solid red]\n{\n{\\partial p\\over\\partial z} = -\\rho g\n}\\\\\n\\implies\n\\begin{cases}\n{\\partial\\over\\partial z}(\\rho fv)=-g{\\partial \\rho\\over\\partial x} \\\\\n{\\partial\\over\\partial z}(\\rho fu)=g{\\partial \\rho\\over\\partial y} \\\\\n\\end{cases}\n$$\n### 3. (:star:CH9.14.2:star:)\n- [x] (12%) \n\nStommel assumes a two-layer model. The upper layer has density $\\rho_1$ and is moving\uff1bthe lower layer has density $\\rho_2$ and is at rest. If $\\eta$ is the surface elevation and d is the level of the interface between the layers, please answer the following questions:\n\n#### (a)What are the pressures in the upper and lower layers, respectively? \n#### (b)From (a), what is the horizontal pressure gradient in the upper and lower layers, respectively? You can just show the x-component for example.\n\n#### ( c ) What is \"reduced gravity\"? (:open_book:CH9.14.2)\nNote: The hydrostatic equation is dp = -pgdz.\n:::success\n$$\ng' = {(\\rho_2-\\rho_1)\\over\\rho_2}\\times g\\\\\n$$\n:::\n### 4. (:star:CH9.5 : Sverdrup's solution for the wind-driven circulation:star:)\n- [x] (10%) \n\nFrom the governing equations\n:::success\n$$\\begin{cases}\n\\alpha {\\partial p\\over\\partial x} =fv+\\alpha{\\partial \\tau_x\\over\\partial z}  \\\\\n\\alpha {\\partial p\\over\\partial y}= -fu+\\alpha{\\partial \\tau_y\\over\\partial z}\n\\end{cases}\n (\u5f0f9.6')\n$$\n:::\n\nPlease show how to obtain the Sverdrup equation, $\\beta M_y= curl_z \\overrightarrow\\tau_\\eta$ (\u5f0f9.21)?\n\n### 5. (:star:9.13 The equatorial current system:star:)\n- [x] (10%) \n#### Please describe the equatorial current systems in the Pacific, Atlantic and Indian Oceans, respectively. \n(Note: surface and sub-surface if possible)\n(:star:9.52 Application of the Sverdrup equations:star:)\n![](https://i.imgur.com/8tI85vl.png)\n![](https://i.imgur.com/k7gtLei.png)\n\n![](https://i.imgur.com/7DpqDNR.png)\n\n![](https://i.imgur.com/xZ0IV2y.png)\n1. ==The surface equatorial currents==\nThe equatorial current system is: zonal character with westward- following North and South Equatorial Currents and, between them, the eastward North Equatorial Countercurrent.\n2. ==The Equatorial Undercurrent==\nBeneath the surface and embedded in the westward-following Pacific ==South Equatorial Current== there exists a most remarkable current : the Equatorial Undercurrent.\n### 6. (:star:CH9.14.2:star:)\n- [x] (9%) \n\n#### (a) Please explain what is the Rossby radius of deformation and its physical meaning? \nNote: external and internal, respectively. (CH9.14.2 p.120)\n![](https://i.imgur.com/q4SMnRt.png)\n\n#### (b) If in coastal waters the scales are h = 10 m, please estimate roughly what is the scale of the internal Rossby radius of deformation?\n:::success\n$$\n{(\\rho_2-\\rho_1)\\over\\rho_2}\\approx2\\times10^{-3}\\\\\n{\\lambda_i={\\sqrt{g'D_0}\\over f}= {\\sqrt{9.8\\times 2\\times10^{-3}\\times 10}\\over10^{-4}}}-=4.4272e+03\\approx 4.4 km\\\\\n$$\n:::\n### 7. :black_nib:(:star:CH9.12:star:)\n- [x] (10%) \n\n#### (a) Stommel was the first one to present an explanation for the westward intensification. Please describe what physical mechanisms are responsible for the westward intensification? \n\u5047\u8a2d\u6d77\u6d0b\u5e95\u90e8\u662f\u5e73\u5766\u7684\uff0c\u4e14\u98a8\u61c9\u529b\u96a8\u7def\u5ea6\u8b8a\u5316\uff1b\u5728\u65cb\u8f49\u4e2d\u7684\u5730\u7403\u4e0a\uff0c\u79d1\u6c0f\u53c3\u6578 $f$ \u96a8\u7def\u5ea6\u8b8a\u5316\uff0c\u800c\u79d1\u6c0f\u53c3\u6578\u662f\u5927\u6d0b\u74b0\u6d41\u7684\u52d5\u529b\u7279\u5fb5\uff0c\u9020\u6210\u5927\u6d0b\u897f\u908a\u6d77\u6d41\u5f37\u5316\u7684\u7d50\u679c\u3002\n#### (b) Please use the conservation of vorticity to interpret the western intensification of the ocean currents. [CH9.12]\n:::success\n$$\n{\\xi+f\\over D}={constant} \\\\\n$$\n:::\n\u4ee5\u5317\u534a\u7403\u7684\u592a\u5e73\u6d0b\u70ba\u4f8b\uff0c\u9760\u8fd1\u8d64\u9053\u4f4d\u65bc\u5357\u65b9\u7684\u98a8\u5411\u70ba\u6771\u98a8(\u5439\u5411\u897f)\uff0c\u9060\u96e2\u8d64\u9053\u4f4d\u65bc\u5317\u65b9\u7684\u98a8\u5411\u70ba\u897f\u98a8(\u5439\u5411\u6771)\u3002\u5728\u6771\u592a\u5e73\u6d0b\uff0c\u56e0\u98a8\u61c9\u529b\u9020\u6210\u7684\u6e26\u5ea6\u70ba\u8ca0\uff0c\u6d77\u6d41\u6d41\u5411\u5357\uff0c\u56e0\u6b64\u96a8\u8457\u6d77\u6d41\u8d8a\u4f86\u8d8a\u9760\u8fd1\u8d64\u9053\uff0c\u79d1\u6c0f\u53c3\u6578(f)\u6e1b\u5c11\uff0c\u76f8\u5c0d\u6e26\u5ea6\u7531\u65bc\u4f4d\u6e26\u5b88\u6046\u800c\u589e\u52a0\uff0c\u5f62\u6210\u6b63\u6e26\u5ea6\uff0c\u6b64\u6642\uff0c\u6771\u592a\u5e73\u6d0b\u7684\u5169\u7a2e\u76f8\u5c0d\u6e26\u5ea6\u4e00\u589e\u4e00\u6e1b\uff0c\u6b63\u597d\u5e73\u8861\u3002\u7136\u800c\u5728\u897f\u592a\u5e73\u6d0b\uff0c\u56e0\u98a8\u61c9\u529b\u9020\u6210\u7684\u6e26\u5ea6\u540c\u6a23\u70ba\u8ca0\uff0c\u6d77\u6d41\u6d41\u5411\u5317\uff0c\u56e0\u6b64\u96a8\u8457\u6d77\u6d41\u8d8a\u4f86\u8d8a\u9060\u96e2\u8d64\u9053\uff0c\u79d1\u6c0f\u53c3\u6578(f)\u589e\u5927\uff0c\u76f8\u5c0d\u6e26\u5ea6\u7531\u65bc\u4f4d\u6e26\u5b88\u6046\u800c\u6e1b\u5c11\uff0c\u4e5f\u5f62\u6210\u8ca0\u6e26\u5ea6\uff0c\u6b64\u6642\uff0c\u897f\u592a\u5e73\u6d0b\u7684\u5169\u7a2e\u5c0d\u76f8\u6e26\u5ea6\u7686\u70ba\u8ca0\uff0c\u7121\u6cd5\u5e73\u8861\uff0c\u82e5\u8981\u4f7f\u6574\u500b\u5317\u592a\u5e73\u6d0b\u74b0\u6d41\u7cfb\u7d71\u7684\u7e3d\u6e26\u5ea6\u5b88\u6046\uff0c\u65b9\u6cd5\u662f\u5728\u897f\u592a\u5e73\u6d0b\u589e\u52a0\u4e00\u9805\u6c34\u5e73\u6469\u64e6\u526a\u529b\u6e26\u5ea6\uff0c\u4e14\u70ba\u6b63\u6e26\u5ea6\uff0c\u6b64\u6e26\u5ea6\u70ba\u4e86\u8981\u5e73\u8861\u5176\u4ed6\u5169\u9805\u8ca0\u6e26\u5ea6\uff0c\u6e26\u5ea6\u5927\u5c0f\u52e2\u5fc5\u6703\u662f\u6574\u500b\u5317\u592a\u5e73\u6d0b\u4e2d\u6700\u5f37\u7684\uff0c\u56e0\u800c\u5f62\u6210\u4e86\u897f\u908a\u5f37\u5316\u6d41\uff0c\u4e5f\u5c31\u662f\u9ed1\u6f6e\u3002\n![](https://i.imgur.com/Np5w2RU.png)\n\n### 8.  (:star:CH9.11.4:star:)\n- [x] (10%) \n\n#### (a) Please write down the equation the conservation of the potential vorticity and explain its physical meaning. \n![](https://i.imgur.com/kGNXT2j.png)\n:::success\n$$\n\\text{the conservation of the potential vorticity:}\\\\\n{d\\over dt}({\\xi+f\\over D})=0 \\\\\n$$\n:::\n$$\n\\text{explain its physical meaning:}\\\\\n\\text{\u4f4d\u6e26\u5b88\u6046\u662f\u5c0d\u65bc\u6d41\u9ad4\u7684\u89d2\u52d5\u91cf\u5b88\u6046\uff0c}(mr^2\\omega)\\\\\n\\text{\u7576\u6c34\u9ad4\u7684\u6df1\u5ea6\u589e\u52a0(\u6c34\u67f1\u88ab\u62c9\u9577)\uff0c\u65cb\u8f49\u534a\u5f91\u6e1b\u5c0f\uff0c\u89d2\u901f\u5ea6}(\\omega)\\text{\u589e\u52a0\uff0c\u7d55\u5c0d\u6e26\u5ea6\u6703\u96a8\u4e4b\u589e\u52a0\uff1b}\\\\\n\\text{\u53cd\u4e4b\uff0c\u7576\u6c34\u9ad4\u7684\u6df1\u5ea6\u6e1b\u5c0f\uff0c\u65cb\u8f49\u534a\u5f91\u8b8a\u5927\uff0c\u89d2\u901f\u5ea6}(\\omega)\\text{\u964d\u4f4e\uff0c\u7d55\u5c0d\u6e26\u5ea6\u6703\u96a8\u4e4b\u6e1b\u5c11}\\\\\n$$\n![](https://i.imgur.com/BQ98vhO.png)\n$$\n\\text{\u6839\u64da\u8cea\u91cf\u5b88\u6046\u5b9a\u7406\uff0c\u63a8\u5c0e\u51fa\u9023\u7e8c\u9ad4\u7a4d\u65b9\u7a0b\u5f0f}\\\\\n{\\partial u\\over \\partial x}+{\\partial v\\over \\partial y}=0 \\\\\n\\text{(\u5047\u8a2d\u7531\u6d77\u8868\u9762\u81f3\u6d77\u5e95\u7684\u5bc6\u5ea6\u7686\u76f8\u540c\uff0c\u6240\u4ee5\u6c34\u5e73\u6d41\u901f\u5206\u91cf\u548c\u6df1\u5ea6\u4e92\u76f8\u7368\u7acb)}\n$$\n#### (b) Please discuss the following situations, we can make some predictions about vorticity changes when a parcel of water moves from one place to another. (CH9.11.4)\n\n##### :one: if D remains constant\n![](https://i.imgur.com/4kRCDP5.png)\n\n###### :a: If a column of water moves zonally.\n\n###### :b: If a column of water moves meridionally toward the north pole.\n\n##### :two: If D increases,\n![](https://i.imgur.com/05QrecR.png)\n\n###### :a: If the water moves zonally.\n\n###### :b: If the water moves meridionally toward the south pole.\n\n### 9. (:x:Chap 10 Thermohaline Effects:x:)\n- [x] (6%) \n\nPlease interpret in your words what is the Stommel's model for the thermohaline (deep) circulation?\n\n### 10. (:star:CH8.2 Inertial motion:star:)\n- [x] (16%)\n\n#### (a) Under what assumptions we can obtain the inertial equations?\n:::success\n$$\\begin{cases}\n{\\partial p\\over\\partial x}={\\partial p\\over\\partial y}=0  \\\\\n\\overrightarrow F=0\\\\\nw=0\n\\end{cases}\n (\u5f0f8.3)\n$$\n:::\n#### (b) Derive the solution of u and v. (:star:CH8.2:star:)\n![](https://i.imgur.com/WU5n85R.png)\n![](https://i.imgur.com/VLjthwa.png)\n![](https://i.imgur.com/nrkvbft.png)\n![](https://i.imgur.com/2rmXt5X.png)\n\n#### ( c ) What are the radius and the period of inertial oscillation?\n![](https://i.imgur.com/yJD3JSQ.png)\nthe radius of inertial oscillation$= {{\\sqrt {u^2+v^2}}\\over f }={V_H\\over f}$\n![](https://i.imgur.com/xxNxkCB.png)\nthe period of inertial oscillation$={2\\pi\\over f}$\n#### (d) What is the traveling direction of inertial circle in the northern hemi-sphere?\n![](https://i.imgur.com/FMYJXYP.png)\n\n## 1071-Final\n### Introductory Dynamical Oceanography Final Exam (12/25/2018)\n\n### 1. (:star:CH8.2:star:) \n- [ ] (15%)\n\n(a) What is the physical meaning of Ekman's theory? (explain what kind of forces\n\nbalance).\n\nfv+ A, a'u \u0434\u0445\n\n- fut A = \u0434\u0440\n\n(b) Based on what concepts, Ekman simplified the above governing equations as the balance between two forces? Please show the detail how to obtain the\n\nsimplified Ekman's equations.\n\n(c) What is the Ekman depth?\n\n(d) What is the Ekman Spiral?\n\n(e) What is the empirical formula to calculate the wind stress magnitude, T,?\n\nPlease explain each term.\n\n### 2. \n- [ ] (6%)\n\nZ=Paca w\n\n(a) Please explain what is the barotropic and baroclinic condition, respectively? (b) Show the thermal wind\" equations from the geostrophic equations and the hydrostatic equation.\n\n### 3. \n- [ ] (12%) \n\nStommel assumes a two-layer model. The upper layer has density pi and is moving; the lower layer has density p and is at rest. If 7 is the surface elevation and d is the level of the interface between the layers, please answer the following questions: (a)What are the pressures in the upper and lower layers, respectively?\n\n(b)From (a), what is the horizontal pressure gradient in the upper and lower layers, respectively? You can just show the x-component for example. (c)What is \"reduced gravity\"?\n\nNote: The hydrostatic equation is dp =-pgdz.\n\n### 4. \n- [ ] (10%) \n\nFrom the governing equations\n\n\u0434\u0440 =fv+a\n\n\u0434\u0445\n\nOp =-fu+a\n\nPlease show how to obtain the Sverdrup equation. M = curl. 7,?\n\n### 5. \n- [ ] (10%) \n\nPlease describe the equatorial current systems in the Pacific, Atlantic and Indian Oceans, respectively. (Note: surface and sub-surface if possible) \n### 6. \n- [ ] (6%) \n\nPlease explain what is the Rossby radius of deformation and its physical\n\nmeaning? Note: barotropic and baroclinic, respectively.\n\n### 7. \n- [ ] (10%) \n\n(a) Stommel was the first one to present an explanation for the westward intensification. Please describe what physical mechanisms are responsible for the westward intensification? (6) Please use the conservation of vorticity to interpret the western intensification of the ocean currents.\n\n### 8. \n- [ ] (10%) \n\n(a) Please write down the equation the conservation of the potential vorticity and explain its physical meaning. \n(b) Please discuss the following situations, we can make some predictions about vorticity changes when a parcel of water moves from one place to another.\n\n#### (1) if D remains constant \n##### (a) If a column of water moves zonally.\n\n##### (b) If a column of water moves meridionally toward the north pole.\n\n#### (2) If D increases,\n\n##### (a) If the water moves zonally.\n\n##### (b) If the water moves meridionally toward the south pole.\n\n### 9. (:x:Chap 10 Thermohaline Effects:x:)\n- [ ] (6%) \n\nPlease interpret in your words what is the Stommel's model for the thermohaline (deep) circulation?\n\n### 10.\n- [ ] (5%) \n\n[CH9.9]\nPlease show the main features of the general pattern of the global winds.\n\n\u6ca1\u6709\u9678\u5730\u7684\u884c\u661f\u98a8\u5834\n![](https://i.imgur.com/GHFRS6h.png)\n\n### 11. \n- [ ] (10%)\n\nWhat is the definition of the following technical words used in oceanography? \n#### (a) Ekman pumping [CH9.6 (7)]\n[\u827e\u514b\u66fc\u6c72\u5438](http://terms.naer.edu.tw/detail/3359170/):\n\n![](https://i.imgur.com/vfQI0MW.png)\n![](https://i.imgur.com/2wRnc68.png)\n\n![](https://i.imgur.com/Rf5lIw4.png)\n\n![](https://i.imgur.com/gaHS6gc.png)\n\n![](https://i.imgur.com/zmbsEkO.png)\n\n#### (b) isopycnal surface [CH8.7]\n[\u7b49\u5bc6\u9762](http://terms.naer.edu.tw/detail/3360647/)\n![](https://i.imgur.com/BZISHLg.png)\n\n#### ( c ) cyclonic\n[\u6c23\u65cb](https://terms.naer.edu.tw/detail/2924733/)\n![](https://i.imgur.com/KYtrDsM.png)\n\n#### (d) geopotential [CH8.3]\n[\u91cd\u529b\u4f4d](http://terms.naer.edu.tw/detail/3395933/)\n![](https://i.imgur.com/dbp2ZWE.png)\n\n#### (e) level of no motion (:star:CH8.5:star:)\n[\u4e0d\u52d5\u6c34\u5c64](http://terms.naer.edu.tw/detail/366332/)\n\u6d41\u901f\u5728\u67d0\u4e00\u6df1\u5ea6\u88ab\u6307\u5b9a\u70ba0\u7684\u6d77\u6c34\u9762\u3002\nAn ocean surface at a depth where the current speed is assumed to be zero.\n# Ch1_\n\n# Ch2_\n\n# Ch3_\n\n# Ch4_\n\n# Ch5_\n\n\n# Ch6_The Equation of Motion in Oceanography\n\n# Ch7_The role of the non-linear terms and the magnitudes of terms in the equations of motion\n\n\n# Ch8_Currents without friction: Geostrophic flow\n\n# Ch9_Currents with Friction; Wind-driven Circulation\n[ocng608-9-1.pdf](https://drive.google.com/file/d/1Miucr_dQa53gGBjPDYOEnRMK6cp-HqE1/view?usp=sharing)\n\n## W15 \u4f5c\u696d\n[bottom_Ekman.pdf](https://drive.google.com/file/d/1cVwTWePwpikqwg9CkNrsC4f6YJDkXHW3/view?usp=sharing)\nApplying Ekman\u2019s theory to the bottom friction layer.\n\n"
 },
 {
  "repo": "dado3212/oceanography-final",
  "language": "JavaScript",
  "readme_contents": "# Oceanography Final Project\n\nThis was my final project for EARS 3: Oceanography.\n\nThe task was anything that encapsulated what we had learned throughout the term.  I'd been messing around with some visualization software in JS, so I decided on a dynamic visualization of the oceans and currents using Three.js.\n\n<p align=\"center\">\n  <img src=\"/assets/preview.gif\" height=\"300\">\n</p>\n\nThe five visualizations included are 'currents', 'surfaceflow', 'velocity', 'salinity', and 'tectonics'.  All data is courtesy of NASA, though it was post-processed for the display.\n\n### Setup\nSome of the visualization files could not be hosted on GitHub, as they are too large.  To allow all visualizations, download the video files from the mega.nz links, name them according to their paranthesized name, and save them in the \"assets/videos/\" folder.\n\n* [Ocean Currents (currents.mp4)](https://mega.nz/#!9R5EGDzD!ioChXH6gVIC37R3Lz_lnvJ2tXxiTOHAAOyk8u9a-CUg) (330 mB)\n* [Ocean Velocity (velocity.mp4)](https://mega.nz/#!8UYCUabT!ixAL9LjO3WKbkFYakp4apO2AJA7orju_Fh19PN8DtD8) (120 mB)\n* [Ocean Surface Flow (surfaceflow.mp4)](https://mega.nz/#!xV5QHZDQ!-on9A9sv5XZqtlxcpEb6cN2rXn48Vt9NX5U2aHg9FFk) (245 mB)\n\nAlternatively, if you want to download the entire project complete with videos, you can get it from [this link](https://mega.nz/#!UVRm0ByI!4AucdjKRSHXximJqGSp3SgLT96BGpgCmwoG9mNhnqZk).\n\nTo run it, open the `index.html` file in a browser that allows local file access.  I used Chrome with the file access flag by running `open -a Google\\ Chrome --args --allow-file-access-from-files`.\n"
 },
 {
  "repo": "florianboergel/theoretical_oceanography",
  "language": "Prolog",
  "readme_contents": "# theoretical_oceanography\n\nContent of presentation is: Turbulence closure models\n"
 },
 {
  "repo": "mannyOceans/oceanography_tools",
  "language": null,
  "readme_contents": "# oceanography_tools\nGeneral tools for oceanography\n\n\nThis toolbox contains general tools for oceanographic analysis including standard plotting for CTD casts, transect, T-S diagrams, etc...\n\n"
 },
 {
  "repo": "reint-fischer/autodidact-oceanography",
  "language": null,
  "readme_contents": "# autodidact-oceanography\na collection of found and written models and programs for oceanographic studying\nHello\n\nReint Pieter Brons Fischer is my name and this repository will serve as a collection of ideas during my gap year.\nSuper excited!\nI like to talk and walk in and about water :)\n"
 },
 {
  "repo": "lipfpp/oceanography_dataprocessing",
  "language": "Jupyter Notebook",
  "readme_contents": "### Importante: para rodar a rotina *api_ERA5.ipynb* \u00e9 necess\u00e1rio ter o CDS API instalado\n\n*PASSOS PARA INSTALAR O CDS API*\n\n>**1. Cadastro**\n\nFazer o cadastro no site da [Climate Data Store](https://cds.climate.copernicus.eu/#!/home)\n\n>**2. Adicionar a chave e instalar o pacote CDS API no Python**\n\nEste [LINK](https://cds.climate.copernicus.eu/api-how-to#install-the-cds-api-key) mostra como adicionar a chave e como realizar a instala\u00e7\u00e3o da biblioteca CDS API no Python.\n\nA instala\u00e7\u00e3o do pacote pode ser feito pelo gerenciador de pacotes PIP ou CONDA\\\n*ex: pip install cdsapi\\\nou\\\nconda install cdsapi*\n\n***Caso encontrem dificuldades em instalar o CDS API, o v\u00eddeo no link a seguir mostra o passo a passo detalhado.***\n[V\u00cdDEO](https://www.youtube.com/watch?v=NHbvfggMC10&ab_channel=CopernicusECMWF)\n\n<hr style=\"border:1px solid lightblue\"> </hr>\n\nOs dados adquiridos do ERA5 s\u00e3o do [ERA5 hourly data on single levels from 1979 to present](https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels?tab=overview). Para adquirir dados com n\u00edveis de press\u00e3o espec\u00edficos [(ERA5 hourly data on pressure levels from 1979 to present)](https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-pressure-levels?tab=overview) \u00e9 preciso inserir a vari\u00e1vel e os n\u00edveis de press\u00e3o de interesse ``'pressure_level': ['825','850',....]`` e trocar a vari\u00e1vel ``'reanalysis-era5-single-levels'`` pela ``'reanalysis-era5-pressure-levels'``\n\n* *A rotina estar configurada para realizar aquisi\u00e7\u00e3o dos dados hor\u00e1rios (resolu\u00e7\u00e3o temporal de 1h) do ERA5 para todos os dias e meses para o intervalo de tempo desejado. O arquivo de sa\u00edda da rotina ser\u00e1 um .nc com m\u00e9dias di\u00e1rias.* \n\n* *A \u00fanica altera\u00e7\u00e3o que deve ser feita pelo usu\u00e1rio \u00e9 determinar os diret\u00f3rios para os arquivos de sa\u00edda.*\n\n* *Os par\u00e2metros **latitude, longitude e intervalo de tempo** ser\u00e3o pedidos pela pr\u00f3pria rotina quando executada.*  \n\n\n<hr style=\"border:1px solid lightblue\"> </hr>\n\n**Bibliotecas usadas**\n\n* CDO\n* numpy\n* pandas\n* xarray\n* cdsapi\n* os\n* glob\n"
 },
 {
  "repo": "yinleon/biological-oceanography",
  "language": null,
  "readme_contents": "# biological-oceanography\n"
 },
 {
  "repo": "PorterWang2020/Oceanography-data-visualization",
  "language": null,
  "readme_contents": "# Oceanography-data-visualization\nTo design a program similar to NCL which could be better and used in visualization of data collected from oceanography observation stations\n"
 },
 {
  "repo": "martinostrowski/R4Oceanography",
  "language": "HTML",
  "readme_contents": "# R4Oceanography\n\nAn introduction to R for marine science students\n\nThis practical assumes no previous knowledge and is designed to introduce the R language and showcase sources of marine data to marine students from earth and life science backgrounds.\n\n*getting started*\n\n* Use R to plot an annotated map of Australia\n- navigating the Rstudio environment\n- loadng and manipulating data\n- understanding basic data types: variables, vectors, dataframes, matrices etc.\n- basic plots\n- maps\n(45 min)\n\n- exercises: practice customising plots\n\n*extending*\n\n* Obtain Satellite Sea Surface Temperataure and Chlorophyll data in the form of netcdf files\n* Use R to plot SST on a map of Australia\n* Use R to plot Chlorophyll concentration on a map\n\n(30 min)\n"
 },
 {
  "repo": "oceanography-rookie/oceanography-rookie.github.io",
  "language": "JavaScript",
  "readme_contents": "# oceanography-rookie.github.io\nportfolio\n"
 },
 {
  "repo": "paige-arsene-cayt-nlp-project/oceanography-nlp",
  "language": "Jupyter Notebook",
  "readme_contents": "# Project Title\n*Audience: Target audience for my final report is*\n\n\n<hr style=\"background-color:silver;height:3px;\" />\n\n## Project Summary\n<hr style=\"background-color:silver;height:3px;\" />\n\n### Project Deliverables\n> - A final report notebook\n> - Python modules for automation and to facilitate project reproduction\n> - Notebooks that show:\n>  - Data acquisition and preparation \n>  - exploratory analysis not included in final report\n>  - model creation, refinement and evaluation\n\n### Initial questions on the data\n\n>  - Questions\n>  - Thoughts\n>  - etc\n\n### Project Plan \n\n- [ ] **Acquire** data from the Codeup SQL Database. \n- [ ] Clean and **prepare** data for the explore phase. \n- [ ] Create wrangle.py to store functions I created to automate the cleaning and preparation process. \n- [ ] Separate train, validate, test subsets and scaled data.\n- [ ] **Explore** the data through visualization and hypothesis testing.\n    - [ ] Clearly define at hypotheses and questions.\n    - [ ] Document findings and takeaways.\n- [ ] Perform **modeling**:\n   - [ ] Identify model evaluation criteria\n   - [ ] Create at least three different models.\n   - [ ] Evaluate models on appropriate data subsets.\n- [ ] Create **Final Report** notebook with a curtailed version of the above steps.\n- [ ] Create and review README. Ensure it contaions:\n   - [ ] Data dictionary\n   - [ ] Project summary and goals\n   - [ ] Initial Hypothesis\n   - [ ] Executive Summary\n---\n\n<hr style=\"background-color:silver;height:3px;\" />\n\n## Executive Summary\n<hr style=\"background-color:silver;height:3px;\" />\n\n**Project Goal:**\n\n**Discoveries and Recommendations**\n\n\n<hr style=\"background-color:silver;height:3px;\" />\n\n## Data Dictionary\n<hr style=\"background-color:silver;height:3px;\" />\n\n|Target|Definition|\n|:-------|:----------|\n| Target | Definition|\n\n|Feature|Definition|\n|:-------|:----------|\n| Feature       | Definition |\n| Feature        | Definition |\n| Feature       | Definition |\n| Feature        | Definition \n\n\n<hr style=\"background-color:silver;height:3px;\" />\n\n## Reproducing this project\n<hr style=\"background-color:silver;height:3px;\" />\n\n> In order to reproduce this project you will need your own environment file and access to the database. You can reproduce this project with the following steps:\n> - Read this README\n> - Clone the repository or download all files into your working directory\n> - Add your environment file to your working directory:\n>  - filename should be env.py\n>  - contains variables: username, password, host\n> - Run the Final_Report notebook or explore the other notebooks for greater insight into the project.\n\n"
 },
 {
  "repo": "npolar/oceanography-ruby",
  "language": "Ruby",
  "readme_contents": "[![Build Status](https://travis-ci.org/npolar/oceanography-ruby.svg?branch=master)](https://travis-ci.org/npolar/oceanography-ruby) [![Code Climate](https://codeclimate.com/github/npolar/oceanography-ruby/badges/gpa.svg)](https://codeclimate.com/github/npolar/oceanography-ruby) [![Test Coverage](https://codeclimate.com/github/npolar/oceanography-ruby/badges/coverage.svg)](https://codeclimate.com/github/npolar/oceanography-ruby)\n\n# oceanography-ruby\n\n## Features\n* Scan for netCDF files recursivly\n* Parse each measurement to a json document\n* Key mapping to standardize naming\n* Complies with Climate and Forecast netCDF conventions where possible\n* Validate json documents against schema\n* Write documents to file or POST to document database\n* Track parsed source files to source API\n* Track rejected files to STDOUT\n\n## Usage\nYou need netcdf c lib installed.\n`sudo apt-get install libnetcdf-dev`\n\n    Usage: ./ncdocs.sh [options] FILE|PATH\n    -m, --mappers LIST               List of mappers to use, Default MissingValuesMapper,KeyValueCorrectionsMapper,\n                                     CommentsMapper,CollectionMapper,ClimateForecastMapper\n    -o, --outpath PATH               Path to write json docs to\n    -p, --post URL                   URL to post json docs to\n    -s, --schema PATH                Path to json schema to validate docs against\n    -t, --threads #                  Number of threads. Default 4\n    -v, --verbose                    Log level debug. Default info\n    -h, --help                       Display this screen\n\n\n## Contribute\nClone, `bundle install`, `bundle exec rspec`  \nPlease use feature branches.\n"
 },
 {
  "repo": "hafez-ahmad/Computational_Oceanography",
  "language": "Python",
  "readme_contents": "## Welcome to Computational Oceanography\n[MyPersonal_webPage](https://hafez-ahmad.github.io/HafezAhmadOceanographer.github.io/#)\n\n\n### Code\n\npython\nimport numpy as np\nimport matplotlib.pyplot as plt\n# Set maximum iteration\nmaxIter = 500\n\n\n\n### python\npython\nimport numpy as np\nimport matplotlib.pyplot as plt\n# Set maximum iteration\nmaxIter = 500\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n# Set maximum iteration\nmaxIter = 500\n\n# Set Dimension and delta\nlenX = lenY = 20 #we set it rectangular\ndelta = 1\n\n# Boundary condition\nTtop = 0 #100,0\nTbottom = 100\nTleft = 30\nTright = 30 #0,30\n\n# Initial guess of interior grid\nTguess = 30\n# Set colour interpolation and colour map.\n# You can try set it to 10, or 100 to see the difference\n# You can also try: colourMap = plt.cm.coolwarm\ncolorinterpolation = 50\ncolourMap = plt.cm.jet\n\n#set mesh\n\nX,Y=np.meshgrid(np.arange(0,lenX),np.arange(0,lenY))\n\n# Set array size and set the interior value with Tguess\nT = np.empty((lenX, lenY))\nT.fill(Tguess)\n\n# Set Boundary condition\nT[(lenY-1):, :] = Ttop # bottom row 100\nT[:1, :] = Tbottom  #top row 0\nT[:, (lenX-1):] = Tright\nT[:, :1] = Tleft\n\n# Iteration (We assume that the iteration is convergence in maxIter = 500)\nprint(\"Please wait for a moment\")\nfor k in range(0,maxIter):\n    for i in range(1,lenX-1,delta):\n        for j in range(1,lenY-1,delta):\n            T[i,j]=0.25*(T[i+1][j]+T[i-1][j]+T[i][j+1]+T[i][j-1])\n        \nprint('finisted')\n#Configure the contour\nplt.title(\"Contour of Temperature\")\nplt.contourf(X,Y,T, colorinterpolation, cmap=colourMap)\n\n# Set Colorbar\nplt.colorbar()\n\n# Show the result in the plot window\nplt.show()\n\n#http://kitchingroup.cheme.cmu.edu/pycse/pycse.html\n\nimport numpy as np\nfrom scipy.integrate import odeint\n\nCa0 = 2     # Entering concentration\nvo = 2      # volumetric flow rate\nvolume = 20 # total volume of reactor, spacetime = 10\nk = 1       # reaction rate constant\n\nN = 100     # number of points to discretize the reactor volume on\n\ninit = np.zeros(N)    # Concentration in reactor at t = 0\ninit[0] = Ca0         # concentration at entrance\n\nV = np.linspace(0, volume, N) # discretized volume elements\ntspan = np.linspace(0, 25)    # time span to integrate over\n\ndef method_of_lines(C, t):\n    'coupled ODES at each node point'\n    D = -vo * np.diff(C) / np.diff(V) - k * C[1:]**2\n    return np.concatenate([[0],D])\n\nsol = odeint(method_of_lines, init, tspan)\n\n    \n# steady state solution\ndef pfr(C, V):\n    return 1.0 / vo * (-k * C**2)\n\nssol = odeint(pfr, Ca0, V)\n\nplt.plot(tspan,sol[:,1])\nplt.show()\nplt.plot(V, ssol, label='Steady state')\nplt.plot(V, sol[-1], label='t = {}'.format(tspan[-1]))\nplt.xlabel('Volume')\nplt.ylabel('$C_A$')\nplt.legend(loc='best')\n\n\nfrom matplotlib import animation \n\nfig=plt.figure()\nax=plt.axes()\nline,=ax.plot(V,init,lw=2)\nplt.show()\n\ndef anim(i):\n    line.set_xdata(V)\n    line.set_ydata(sol[i])\n    ax.set_title('t = {0}'.format(tspan[i]))\n    ax.figure.canvas.draw()\n    return line,\n\nanim=animation.FuncAnimation(fig,anim,frames=50,blit=True)\nanim\nplt.show()\n\nanim.save('transient_pfr.mp4', fps=10)\n```\n\n\n### Support or Contact\n\nDo you have any Query? Check out our [Email](hafezahmad100@gmail.com) or [contact support](hafezahmad100@gmail.com) and we\u2019ll help you sort it out.\n"
 },
 {
  "repo": "UWA-SCIE2204-Marine-Systems/Biological-oceanography",
  "language": null,
  "readme_contents": "# Biological-oceanography"
 },
 {
  "repo": "wrf/oceanography_scripts",
  "language": "R",
  "readme_contents": "# oceanography_scripts\noceanography code\n\n## replot of Russell 1928 vertical migration of plankton ##\nData from Russell (1928) [The Vertical Distribution of Marine Macroplankton. VI. Further Observations on Diurnal Changes](https://doi.org/10.1017/S0025315400055545), presumably in the English channel not far from Plymouth. Original figures had violin plots, but the X-axis was not uniform. Some errors appear to be in the original data from miscounting for the totals in the table.\n\n![russell1928_vertical_migration_data.example.png](https://github.com/wrf/oceanography_scripts/blob/master/images/russell1928_vertical_migration_data.example.png)\n\n## WOD OSD bottle data ##\nData for OSD (Ocean Station Data) were downloaded from [NOAA WOD](https://www.ncei.noaa.gov/access/world-ocean-database-select/dbsearch.html), as of March 2021, containing data for 3069708 casts/stations for 28987253 total stops.\n\nThis is in a completely useless .csv format that is NOT a table, broken down into 13 files (`ocldb1616358314.25649.OSD.csv.gz` up to `ocldb1616358314.25649.OSD13.csv.gz`. Here, a python parser converts it into a giant table. The process took 16 minutes on my laptop and requires 22Gb RAM.\n\n`compile_wod_csv_to_real_table.py -c *.csv.gz > ocldb1616358314.25649.OSD_all.all_vars.tab`\n\nThe table headers are listed below, and most are self explanatory. `stop` refers to the bottle order in a single cast, with 0 being the first. Note that the variable names mostly keep format of the OSD data, including the 10 character limit (e.g. `Temperatur`).\n\n`cast_id\tcruise_id\torig_station_id\torig_cruise_id\tlatitude\tlongitude\tyear\tmonth\tday\tcountry\tcountry_acc_number\tstop\tdepth\tCFC11\tDeltaC13\tDeltaC14\tNitrate\tpH\tCFC12\tChlorophyl\tAlkalinity\tPressure\tArgon\tTemperatur\tCFC113\ttCO2\tSilicate\tOxygen\tSalinity\tOxy18\tTritium\tNeon\tDeltaHe3\tPhosphate\tpCO2\tHelium\tAmmonia`\n\nUnits should be:\n\n```\nDepth         m\nPressure      dbar\nTemperatur    degrees C\nSalinity      PSS\nOxygen        umol/kg\nPhosphate     umol/kg\nNitrate       umol/kg\nSilicate      umol/kg\nAmmonia       umol/l\nChlorophyl    ug/l\ntCO2          mM\nDeltaC14      per mille\nDeltaC13      per mille\nOxy18         per mille\nAlkalinity    meq/l\nCFC11         pmol/kg\nCFC12         pmol/kg\nCFC113        pmol/kg\nHelium        nmol/kg\nDeltaHe3      percent\nTritium       TU\nNeon          nmol/kg\nArgon         nmol/kg\n```\n\nLoading the entire table into R then requires 14Gb RAM.\n\n```\nwod_data_file = \"~/project/WOD_select/ocldb1616358314.25649.OSD_all.all_vars.tab\"\nwod_data = read.table(wod_data_file, header=TRUE, sep=\"\\t\")\nwod_summary = summary(wod_data)\nwod_summary\n```\n\n![WOD_OSD_samples_per_year.png](https://github.com/wrf/oceanography_scripts/blob/master/images/WOD_OSD_samples_per_year.png)\n\n![WOD_OSD_max_depth_by_year.png](https://github.com/wrf/oceanography_scripts/blob/master/images/WOD_OSD_max_depth_by_year.png)\n\nSome basic filtering can be applied to simplify the dataset. To take only the first, or shallowest bottle, set `stop==0`. This would be looking at surface values of nearly all measurements, of a total of 3069708 observations.\n\n`first_stop_only = filter(wod_data, stop == 0)`\n\nTaking all shallow water measurements is a larger set, since there are many 10m or 20m samples, this leaves 12134025 samples.\n\n`surface_data_only = filter(wod_data, abs(depth) < 50)`\n\nIn general, the data are messy and need substantial post-processing. For example, most casts have temperature. However, this column contains a few negative values (below -2, which would be the temperature of [brine-excluded polar water](https://nsidc.org/cryosphere/seaice/index.html)), and values between 50 and 100, which are likely Fahrenheit, instead of [hydrothermal vents](https://oceanservice.noaa.gov/facts/vents.html).\n\n```\n> table( round(wod_data[[\"Temperatur\"]]) )\n\n   -100     -60     -48     -44     -39     -34     -33     -22     -16     -15     -12     -11     -10      -8 \n      7       1       1       1       2       1       1       1       1       1       4       2       1       1 \n     -7      -5      -4      -3      -2      -1       0       1       2       3       4       5       6       7 \n      1       2       4      23  258491  718538 1000463 1075674 1522455 1735250 1978899 1625205 1600309 1521408 \n      8       9      10      11      12      13      14      15      16      17      18      19      20      21 \n1386334 1192817 1029452  914095  899285  972681  996685  848622  844351  759436  741297  599957  539063  455230 \n     22      23      24      25      26      27      28      29      30      31      32      33      34      35 \n 443248  374590  369995  329949  332909  319404  300566  214482   64114    5055     948     337     149      86 \n     36      37      38      39      40      42      43      45      47      48      49      50      51      52 \n     19      21      90       8       5       4       1       1       3       2       4       5       7       2 \n     53      55      56      57      58      59      60      61      62      63      64      65      66      67 \n      2       2      14       9       8       1       2       1       9       1       7       1       8       2 \n     68      70      72      74      76      77      78      79      80      81      82      83      87      88 \n      5       6       4       3       9       1       9       1       7       2      11       2       2       2 \n     90      93      94      96      98      99     100     105     107     110     116     117     118     127 \n      1       1       1       1       1       1      12       2       1       2       1       2       1       1 \n    131     138     266     270     311    1000 \n      1       1       1       1       1       1 \n```\n\n![WOD_OSD_samples_by_temp.png](https://github.com/wrf/oceanography_scripts/blob/master/images/WOD_OSD_samples_by_temp.png)\n\nThis code makes a map of global surface nitrate. The highest values appear to be due to river inputs. The southern ocean is also noticeably darker than much of the rest of the world, as a well known [HNCL region](https://en.wikipedia.org/wiki/High-nutrient,_low-chlorophyll_regions).\n\n```\nlibrary(ggplot2)\nlibrary(dplyr)\nworldpolygons = map_data(\"world\")\nfirst_stop_w_nitrate = filter(wod_data, stop==0, !is.na(Nitrate))\nwnit_gg = ggplot(worldpolygons) +\n    coord_cartesian(expand = c(0,0)) +\n    labs(x=NULL, y=NULL) +\n    theme(axis.text = element_blank(),\n          axis.ticks = element_blank(),\n          legend.position=c(0.75,0.75)  ) +\n    geom_polygon( aes(x=long, y = lat, group = group), fill=\"#aaaaaa\", colour=\"#ffffff\") +\n    scale_colour_gradient(low = \"#e7e1ef\", high = \"#8e1236\", trans=\"log10\", na.value=\"#f7f4f9\" ) +\n    geom_point(data=first_stop_w_nitrate, aes( x=longitude, y=latitude, colour=Nitrate), size=0.5 )\nggsave(file=\"~/git/oceanography_scripts/images/WOD_OSD_surface_nitrate.png\", wnit_gg, device=\"png\", width=12, height=6, dpi=90)\n```\n\n![WOD_OSD_surface_nitrate.png](https://github.com/wrf/oceanography_scripts/blob/master/images/WOD_OSD_surface_nitrate.png)\n\n## secchi disk plot ##\nPlot of [Secchi disk](https://en.wikipedia.org/wiki/Secchi_disk) data from [NOAA National Centers for Environmental Information](https://www.ncei.noaa.gov/data/oceans/woa/WOD/DATA_SUBSETS/). This was in the format of a .csv file containing 463875 casts, and required little post-processing.\n\n![WOD13_secchi_forel.png](https://github.com/wrf/oceanography_scripts/blob/master/images/WOD13_secchi_forel.png)\n\n## mbari CTD plot ##\nPlot of the CTD from [MBARI](https://www.mbari.org/products/data-repository/) ROVs\n\n`Rscript ../mbari_ctd_plotter.R mbari_dive_d420_ctd.txt`\n\n![mbari_dive_d420_ctd.png](https://github.com/wrf/oceanography_scripts/blob/master/images/mbari_dive_d420_ctd.png)\n\n## phanerozoic oxygen ##\nPlot of Phanerozoic oxygen level, based on various models ( [Bergman 2004 COPSE](https://doi.org/10.2475/ajs.304.5.397) and [Berner 2006 GEOCARBSULF](https://doi.org/10.1016/j.gca.2005.11.032) ). These data were hacked out of the paper, though an updated version of the model code is [here](https://github.com/sjdaines/COPSE) by [Lenton 2018](https://doi.org/10.1016/j.earscirev.2017.12.004).\n\n![o2_models_phanerozoic_v1.png](https://github.com/wrf/oceanography_scripts/blob/master/images/o2_models_phanerozoic_v1.png)\n\n## ts_diagram_example ##\nPlot of Temperature-Salinity diagram from [WOCE](https://en.wikipedia.org/wiki/World_Ocean_Circulation_Experiment) Station P17N in the North Pacific on 1-June-1993\n\n`Rscript ts_diagram_example.R`\n\n![woce_p17n_t-s_diagram_v1.png](https://github.com/wrf/oceanography_scripts/blob/master/images/woce_p17n_t-s_diagram_v1.png)\n"
 },
 {
  "repo": "mannyOceans/Oceanography-Tools",
  "language": "Python",
  "readme_contents": "# Oceanography-Tools\n\nThis is a toolbox of oceanography calculations\n"
 },
 {
  "repo": "kathy9980/Physical-Oceanography",
  "language": "Jupyter Notebook",
  "readme_contents": "# Physical-Oceanography\nPhysical Oceanography Homework\n"
 },
 {
  "repo": "albertcodes/albertcodes",
  "language": "Common Workflow Language",
  "readme_contents": "<p>\n  <img align=\"left\" alt=\"contact\" src=\"assets/images/welcome-rounded.png\" width=\"141\" height=\"125\" />\n  <img align=\"center\" alt=\"streak\" \n       src=\"https://github-readme-streak-stats.herokuapp.com/?user=albertcodes&hide_border=true&stroke=ffffff00&background=ffffff&ring=fe3f40&fire=fe3f40&currStreakLabel=fe3f40\" \n       width=\"390\" height=\"125\" />\n  <img align=\"center\" alt=\"gif\" src=\"assets/images/i-love-rounded.gif\" width=\"265\" height=\"100\" />\n</p>\n<img align=\"right\" alt=\"contact\" src=\"assets/images/contact-decoration.png\" />\n<br>\n<h3 align=\"left\">Toolbox:</h3>\n<p align=\"left\">\n  <a href=\"https://www.typescriptlang.org/\" target=\"_blank\">\n    <img src=\"https://raw.githubusercontent.com/devicons/devicon/master/icons/typescript/typescript-original.svg\"\n      alt=\"typescript\" width=\"40\" height=\"40\" />\n  </a>\n  <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript\" target=\"_blank\">\n    <img src=\"https://raw.githubusercontent.com/devicons/devicon/master/icons/javascript/javascript-original.svg\"\n      alt=\"javascript\" width=\"40\" height=\"40\" />\n  </a>\n  <a href=\"https://kotlinlang.org/\" target=\"_blank\">\n    <img src=\"https://raw.githubusercontent.com/devicons/devicon/master/icons/kotlin/kotlin-original.svg\" alt=\"kotlin\"\n      width=\"40\" height=\"40\" />\n  </a>\n  <a href=\"https://www.figma.com/\" target=\"_blank\">\n    <img src=\"https://raw.githubusercontent.com/devicons/devicon/master/icons/figma/figma-original.svg\" alt=\"figma\"\n      width=\"40\" height=\"40\" />\n  </a>\n  <a href=\"https://www.python.org/\" target=\"_blank\">\n    <img src=\"https://raw.githubusercontent.com/devicons/devicon/master/icons/python/python-original.svg\" alt=\"python\"\n      width=\"40\" height=\"40\" />\n  </a>\n  <a href=\"https://dart.dev\" target=\"_blank\">\n    <img src=\"https://www.vectorlogo.zone/logos/dartlang/dartlang-icon.svg\" alt=\"dart\" width=\"40\" height=\"40\" />\n  </a>\n  <a href=\"https://git-scm.com/\" target=\"_blank\">\n    <img src=\"https://www.vectorlogo.zone/logos/git-scm/git-scm-icon.svg\" alt=\"git\" width=\"40\" height=\"40\" />\n  </a>\n  <a href=\"https://firebase.google.com/\" target=\"_blank\"> \n    <img src=\"https://www.vectorlogo.zone/logos/firebase/firebase-icon.svg\" \n      alt=\"firebase\" width=\"40\" height=\"40\"/>\n  </a>\n  <a href=\"https://couchdb.apache.org/\" target=\"_blank\">\n    <img src=\"https://raw.githubusercontent.com/devicons/devicon/master/icons/couchdb/couchdb-original.svg\"\n      alt=\"couchdb\" width=\"40\" height=\"40\" />\n  </a>\n    <a href=\"https://www.meteor.com/\" target=\"_blank\">\n    <img src=\"https://raw.githubusercontent.com/devicons/devicon/master/icons/meteor/meteor-original.svg\" alt=\"meteor\" width=\"40\" height=\"40\" />\n  </a>\n  <a href=\"https://appwrite.io/\" target=\"_blank\">\n    <img src=\"https://raw.githubusercontent.com/devicons/devicon/master/icons/appwrite/appwrite-original.svg\"\n      alt=\"appwrite\" width=\"40\" height=\"40\" />\n  </a><br>\n  <a href=\"https://angular.io/\" target=\"_blank\">\n    <img src=\"https://raw.githubusercontent.com/devicons/devicon/master/icons/angularjs/angularjs-original.svg\"\n      alt=\"angularjs\" width=\"40\" height=\"40\" />\n  </a>\n  <a href=\"https://www.cplusplus.com/\" target=\"_blank\">\n    <img src=\"https://raw.githubusercontent.com/devicons/devicon/master/icons/cplusplus/cplusplus-original.svg\" alt=\"c++\"\n      width=\"40\" height=\"40\" />\n  </a>\n  <a href=\"https://www.w3.org/html/\" target=\"_blank\">\n    <img src=\"https://raw.githubusercontent.com/devicons/devicon/master/icons/html5/html5-original-wordmark.svg\"\n      alt=\"html5\" width=\"40\" height=\"40\" />\n  </a>\n  <a href=\"https://www.w3schools.com/css/\" target=\"_blank\">\n    <img src=\"https://raw.githubusercontent.com/devicons/devicon/master/icons/css3/css3-original-wordmark.svg\"\n      alt=\"css3\" width=\"40\" height=\"40\" />\n  </a>\n  <a href=\"https://flutter.dev/\" target=\"_blank\">\n    <img src=\"https://raw.githubusercontent.com/devicons/devicon/master/icons/flutter/flutter-original.svg\" alt=\"flutter\"\n      width=\"40\" height=\"40\" />\n  </a>\n  <a href=\"https://vuejs.org/\" target=\"_blank\">\n    <img src=\"https://raw.githubusercontent.com/devicons/devicon/master/icons/vuejs/vuejs-original.svg\" alt=\"vue\"\n      width=\"40\" height=\"40\" />\n  </a>\n  <a href=\"https://sass-lang.com\" target=\"_blank\">\n    <img src=\"https://raw.githubusercontent.com/devicons/devicon/master/icons/sass/sass-original.svg\" alt=\"sass\"\n      width=\"40\" height=\"40\" />\n  </a>\n  <a href=\"https://aws.amazon.com/\" target=\"_blank\">\n    <img\n      src=\"https://raw.githubusercontent.com/devicons/devicon/master/icons/amazonwebservices/amazonwebservices-original.svg\"\n      alt=\"aws\" width=\"40\" height=\"40\" />\n  </a>\n  <a href=\"https://www.mongodb.com/\" target=\"_blank\"> \n    <img src=\"https://raw.githubusercontent.com/devicons/devicon/master/icons/mongodb/mongodb-original-wordmark.svg\" \n      alt=\"mongodb\" width=\"40\" height=\"40\"/>\n  </a>\n  <a href=\"https://reactjs.org/\" target=\"_blank\">\n    <img src=\"https://raw.githubusercontent.com/devicons/devicon/master/icons/react/react-original-wordmark.svg\"\n      alt=\"react\" width=\"40\" height=\"40\" />\n  </a>\n  <a href=\"https://coffeescript.org/\" target=\"_blank\">\n    <img src=\"https://raw.githubusercontent.com/devicons/devicon/master/icons/coffeescript/coffeescript-original.svg\"\n      alt=\"coffeescript\" width=\"40\" height=\"40\" />\n  </a>\n</p><br>\n<h3 align=\"left\">Social:</h3>\n<p align=\"left\">\n  <a href=\"https://twitter.com/albertcodes_dev\" target=\"blank\">\n    <img src=\"https://raw.githubusercontent.com/devicons/devicon/master/icons/twitter/twitter-original.svg\"\n      alt=\"twitter\" width=\"40\" height=\"40\" />\n  </a>\n  <a href=\"https://albertcodes.dev\" target=\"blank\">\n    <img src=\"assets/images/world-vector.png\" alt=\"webpage\" width=\"40\" height=\"40\" />\n  </a>\n  <a href=\"https://dribbble.com/albertcodes\" target=\"blank\">\n    <img src=\"assets/images/dribbble.svg\" alt=\"dribble\" width=\"40\" height=\"40\" />\n  </a>\n</p><br>\n<p>\n  <img align=\"left\" height=\"180em\"\n    src=\"https://github-readme-stats.vercel.app/api?username=albertcodes&custom_title=GitHub Stats:&bg_color=ffffff&show_icons=true&title_color=fe3f40&icon_color=000000&hide_border=true&&count_private=true&include_all_commits=true\" />\n  <img align=\"center\" height=\"180em\"\n    src=\"https://github-readme-stats.vercel.app/api/top-langs/?username=albertcodes&custom_title=Codebase:&bg_color=ffffff&show_icons=true&title_color=fe3f40&hide_border=true&layout=compact&exclude_repo=sociee&langs_count=10&hide=Common Workflow Language\" width=\"350\" />\n</p>\n"
 },
 {
  "repo": "NOAA-CHAMP/EISES",
  "language": "BlitzBasic",
  "readme_contents": "# About EISES\nThe EISES coding project has been separated into four subsections of the software to be implemented. These sections of the program are, in procedural order: data feed processing, fact generation, rule evaluation , and alert distribution (see below).\n[insert image here]\n<!--- [EISES graphic]()-->\n## Current Functionality\n\n## Future Applications\n\n\n***\n# EISES Development Checklist:\n( todo: :white_large_square:, in progress: :hammer:, completed :heavy_check_mark:)\n##### Last Updated: 10/08/2019\n##### Questions: madison.soden@noaa.gov\n<!---\n:white_large_square:\n:heavy_check_mark:\n:hammer:\n-->\n#### Data Processing:\n:heavy_check_mark: 1. Compile a testing data set to use when adapting EISES to be used in PE project.  \n  ><b>Testing data: </b>\n  >:heavy_check_mark: OBS data  \n  >:heavy_check_mark: ABS data   \n  >:heavy_check_mark: Light attenuation (surface light and underwater light if possible)  \n  >:heavy_check_mark: Deposition  \n  >:heavy_check_mark: Waves  \n  >:heavy_check_mark: Ocean Current information \n\n:hammer: 2. Adapt EISES's data loading scripts, and fact class definitions to accept information from these types on sensors. \n\n:hammer: 3. Adapt EISES's parsers to get data from thredds server and/or parse csv file types. \n  >:hammer: waiting to get copy of csv file format.\n \n#### Fact Generation:\n#### Rule Evaluation:\n#### Alert Distribution:\n:hammer: 1. Create use-case/design document based on testing Alert request list (X/J).  \n:white_large_square: 2. Review possible design/development of alert distribution and web-based archives.\n"
 },
 {
  "repo": "saltyPhysics/manatee",
  "language": null,
  "readme_contents": "# Manatee\nOceanography tools which cover standard calculations and plots with \nas much freedom to customize plots as possible. \n\nXarray is used as the primary tool for handling large datasets but H5PY is also \nused to handle HDF files which are common for some large datasets\n\n\n"
 },
 {
  "repo": "cywhale/ODB",
  "language": "HTML",
  "readme_contents": "# ODB\nMy R works in Ocean Data Bank (ODB), Institute of Oceanography, NTU, Taiwan\n\nBioQuery: an integrated Open API front-end with Shiny App for bio-query (BioQuery) <a href=\"https://bio.odb.ntu.edu.tw/query\" target=\"_blank\">https://bio.odb.ntu.edu.tw/query</a>\n\n    - ?map https://bio.odb.ntu.edu.tw/query?map directly link to map query interface for bio-data, ODB\n    \n    - ?api(=1 - 7) directly open API document \n    \n    - ?help(=1 - 9)&lang=(EN, TW, which can be omitted if EN: English is selected; only TW: traditional Chinese need to be specified) directly open Help and tips manual\n\n#### Links\n1. Shiny R Dashboard for bio-database of ODB, linking PostgreSQL\n\n    - <a href=\"http://www.odb.ntu.edu.tw/biology/?page_id=925\">Current bio-records and status in Bio-database of ODB</a> \n\n2. Shiny R integrated query web-application for bio-database of ODB\n\n    - <a href=\"http://bio.odb.ntu.edu.tw/query/\">Query application for Bio-database of ODB</a>. Update: Jan, 2018\n    \n    - Remarks: <a href=\"http://bio.odb.ntu.edu.tw/index_tech_citations.html\">Technical citations</a> and [notes](Techniques_ref01.md)\n\n3. Manual for usage:\n    - <a href=\"http://bio.odb.ntu.edu.tw/index.html\">Chinese version</a>\n    \n    - <a href=\"http://bio.odb.ntu.edu.tw/index_en.html\">English version</a>\n\n\n_Update: 2018.06.05_\n![twitter notes](https://pbs.twimg.com/media/De6PWyLUYAA5rdQ.jpg)\n\n_Update: 2019.01.15_\n[![Step by step (in Chinese)](https://github.com/cywhale/ODB/blob/master/docs/web_poly_regionalBio02_chinese.png)](https://github.com/cywhale/ODB/blob/master/docs/web_poly_regionalBio02_chinese.png)\n\n"
 },
 {
  "repo": "hackforthesea/awesome-marine-hacking",
  "language": null,
  "readme_contents": "# Resources for Ocean Hacking\n\n[![Donate on Patreon](https://camo.githubusercontent.com/6446a7907a4d4f8de024ec85750feb07d7914658/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f70617472656f6e2d646f6e6174652d79656c6c6f772e737667)](https://www.patreon.com/user?u=4619046)\n\n## Blogs\n- [UNder the C](https://underthecblog.org/) - An Ocean Science Blog by Grad Students in Marine Science at UNC- Chapel Hill\n- [oceanbites](https://oceanbites.org/) - The latest oceanography literature, explained\n- [Saltwater Science](https://www.nature.com/scitable/blog/saltwater-science) - Marine science is fascinating, diverse, and relevant to us all. From tiny plankton to enormous mammals, currents, climate and corals, three scientists break down the science of the sea.\n- [The Drop-In Blog @ Moss Landing Marine Labs](https://mlmlblog.wordpress.com/)\n- [Wood's Hole Oceanographic Institution](http://www.whoi.edu/main/blogs-expeditions) - Blogs and Expeditions\n- [Scripp's Institute of Oceanography](https://scripps.ucsd.edu/news/blogs-and-expeditions) - Blogs and Expeditions\n- [Ocean Alliance](http://whale.org) - Ocean Alliance Blog\n- [Notey Blogs on Oceanography](http://www.notey.com/blogs/oceanography) - Aggregated list of interesting marine blogs and blog posts.\n- [Southern Fried Science](http://www.southernfriedscience.com/)\n- [oceans4ever](http://oceans4ever.com/)\n- [Skidaway Institutes](https://oceanscience.wordpress.com/) - A blog for the Skidaway Institute of Oceanography faculty and staff\n- [Ocean Portal](http://ocean.si.edu/blog) - Smithsonian Museum of Natural History Ocean Blog\n- [V & the Sea](http://vdives.blogspot.com/)\n- [Words in Mocean](https://wordsinmocean.com/)\n- [Ya like Dags?](http://yalikedags.southernfriedscience.com/) - Appreciating the Unappreciated Sharks\n- [Oceans of Opportunity](http://oceanopportunity.com/) - This site is the web portal into the undersea work and world of Michael Lombardi.\n- [Oceana Blog](http://usa.oceana.org/blog)\n- [British Oceanographic Data Center Blog](http://blog.bodc.ac.uk/)\n- [Carl Safina's Blog](http://carlsafina.org/)\n- [Deep Blue Home](http://deepbluehome.blogspot.com/)\n- [Deep Sea News](http://www.deepseanews.com/)\n- [Freaque Waves](http://freaquewaves.blogspot.com/)\n- [Coral Notes from the Field](http://coralnotesfromthefield.blogspot.com/)]\n- [Marine Conservation Institute Blog](https://blog.marine-conservation.org/)\n- [Monkey Face News](http://www.monkeyfacenews.com/my-blog/)\n- [Ocean Doctor](http://oceandoctor.org/)\n- [Oceanographer's Choice](http://www.oceanographerschoice.com/)\n\nHave more to add? [Create an issue!](https://github.com/hackforthesea/resources/issues/new)\n\n## Datasets\n\n### Cornell Lab's Macaulay Library of Natural Sounds\n- https://www.macaulaylibrary.org/\n\n## NOAA Digital Coast\n- https://coast.noaa.gov/digitalcoast/\n\n## Pangaea - Data Publisher for Earth & Environmental Science\n- https://www.pangaea.de/\n\n## Global Biodiversity Information Facility\n- https://www.gbif.org/\n\n#### World Register of Marine Species \n- http://marinespecies.org/aphia.php?p=webservice\n\n#### The International Council for Exploration of the Seas \n- http://www.ices.dk/marine-data/dataset-collections/Pages/default.aspx\n\n#### NOAA.gov Dataset search \n- https://data.noaa.gov/dataset\n\n#### NOAA Sea Turtles Data\n- http://seamap.env.duke.edu/\n- http://www.seaturtlestatus.org/learn/maps/all\n- https://www.sefsc.noaa.gov/species/turtles/strandings.htm\n\n#### Fish Watch - Provides the database on seafood counts \n- http://fishwatch.gov\n\n#### AEA Research\n- https://www.advancingecoag.com/aea-research\n\n#### Economics: National Ocean Watch \n- https://coast.noaa.gov/dataregistry/search/collection/info/enow \n- https://coast.noaa.gov/digitalcoast/training/diving-into-the-ocean-economy-with-economics.html\n\n#### Massachusetts Ocean Resource Information System \n- http://www.mass.gov/eea/agencies/czm/program-areas/mapping-and-data-management/moris/\n\n#### National \n- http://marinecadastre.gov/ \n- https://ioos.noaa.gov/data/\n- https://ioos.us/comt\n\n#### Global: \n\n- http://www.esri.com/industries/oceans \n- http://resources.arcgis.com/en/communities/oceans/ \n- http://shipmaps.exactearth.com/\n\n#### The Ocean Data Portal seems to be sound for quasi-real-time feeds: \n- http://www.oceandataportal.net/portal/\n\n#### Global Temperature and Salinity Profile Programme (GTSPP) dataset: \n- http://coastwatch.pfeg.noaa.gov/erddap/tabledap/erdGtsppBest.html?trajectory,org,type,platform,cruise&distinct()\n\n#### Bottom Sediments of Georges Bank dataset: \n\n- http://woodshole.er.usgs.gov/openfile/of03-001/data/seddata/wigley61/wigley61.zip\n\n#### Portland Fish Exchange's \"landing tool\"\n\n- http://www.pfex.org/info/\n\n#### British Oceanographic Data Center\n\n- https://www.bodc.ac.uk/data/published_data_library/\n\n#### Marine Data Systems Tools\n\n- http://www.searoutes.com/routing?speed=13&panama=true&suez=true&kiel=true\n- http://www.trusteddocks.com/\n- http://www2.searoutes.com/wp-includes/swagger-ui/#/\n\n#### Bureau of Ocean Energy Management \n\n- [Gulf of Mexico Deepwater Bathymetry Map](https://www.boem.gov/Gulf-of-Mexico-Deepwater-Bathymetry/)\n- [Oil and Gas Energy Programs Maps and GIS Data](https://www.boem.gov/Maps-and-GIS-Data/)\n- [Renewable Energy Program Maps and GIS Data](https://www.boem.gov/Renewable-Energy-Program-Mapping-and-Data/)\n- [Statistics and Facts](https://www.boem.gov/Statistics-and-Facts/)\n- [Research and Studies](https://www.boem.gov/Marine-Minerals-Research-and-Studies/)\\\n\n#### Data Dryad Data Sets\n\n- [Starts with Ocean](https://datadryad.org/search-filter)\n\n## Hardware\n- http://ocean-innovations.net/products/\n- http://ocean-innovations.net/companies/global-ocean-design/products/10-polystyrene-spheres/instrumentation-sphere-g-141/\n- http://ocean-innovations.net/companies/linkquest/tracklink-models/tracklink-1500-series/\n- http://www.teledynemarine.com/920-series-atm-926?ProductLineID=8\n- http://www.dupont.com/products-and-services/plastics-polymers-resins/thermoplastics/brands/delrin-acetal-resin.html\n- http://plasticworld.ca/store/index.php?main_page=product_info&products_id=41\n- http://www.novabraid.com/rope/spectec-12/\n\n#### Communications\n- https://www.metocean.com/shop/telematics/satellite-phones/\n- https://www.metocean.com/product/shout-ts/\n- https://www.iridium.com/products/iridium-go/\n- https://www.iridium.com/products/iridium-go/\n\n#### Acoustic Monitoring\n- https://www.metocean.com/shop/defence-security/acoustic-monitoring/ \n\n#### EM/ER\n- http://www.archipelago.ca/fisheries-monitoring/electronic-monitoring/\n- https://www.immervisionenables.com/\n\n## Research\n#### The tides are not what you think - the 29.52 day Spring Neap Cycle\n- http://www.sciencedirect.com/science/article/pii/S0025322706002544\n\n## Previous Hackathon Submissions\n- https://public.tableau.com/profile/cuong.nguyen1823#!/vizhome/Hack_for_the_Sea/\n- https://public.tableau.com/profile/bill.ostaski#!/vizhome/ClimateChangeSourcesandEffects/ClimateChangeSourcesandEffects\n- https://github.com/Feneric/LobstaListen\n- https://github.com/morganrstewart/hackforthesea16\n\n## Other Awesome Repositories\n\n- [FishAnnotator](https://github.com/BGWoodward/FishAnnotator) is an application that facilitates annotation of videos and images of fish. Eventually it will be expanded to automatically create annotations using computer vision algorithms.\n"
 },
 {
  "repo": "codewithpassion/oceanographyforeveryone.com",
  "language": "HTML",
  "readme_contents": "# oceanographyforeveryone.com\n\n[Link] (http://oceanographyforeveryone.com/): Oceanography for Everyone\n\n###Background\n\nOceanography for Everyone is the community portal for our DIY oceanography community. \n\n###Overview\n\nThe ocean belongs to us all, but the tools necessary to study, explore, and understand the ocean are rarely accessible to the vast majority of ocean users. By nurturing a community of low-cost, open-source hardware developers, scientists, and ocean stakeholders, we want to change that.\n\nWhether you're a researcher looking for alternatives to expensive scientific gear, a citizen scientist interested in building a monitoring program, a fisherman exploring new tools to understand their catch, or an ocean enthusiast seeking new ways to interact with the sea, this community is for you.\n\nUnder each project, you will find links to resources, build guides, 3D-printer files, code repositories, and, eventually, databases. Each section project will also point you towards ways that you can help contribute to the project. We have assembled a Parts Depot with links to the commercial products used to build each tool.\n\nThe ocean belongs to everyone. Let\u2019s ensure that we have access to the tools needed to understand it.\n\n###How you can help\n\nIn keeping with the spirit of Oceanography for Everyone, we've made the source-code for the community portal open to anyone. If you see a problem, bug, or accessibility issue, please feel free to open an issue or make a pull request.\n\n###Acknowledgements\n\nCode by Andrew Thaler with help from Russell Neches. \n"
 },
 {
  "repo": "tienhsiangkao/Phyiscal-Oceanography",
  "language": null,
  "readme_contents": "# Phyiscal-Oceanography\nAn on-line library for physical oceanography and related areas. !Private\n"
 },
 {
  "repo": "dario-domi/Oceanographic_temperature_reconstruction",
  "language": "MATLAB",
  "readme_contents": "# The project in short\nThe aim of the project is to combine: \n- ocean temperature outputs of the climate simulator HadCM3; and  \n- proxy data for ocean temperature (kindly provided by Prof Harry Dowsett - U.S. Geological Survey);\n\nto carry out a data-model comparison of sea-surface temperature during the mid Pliocene Warm Period, by accounting for the variability induced by orbital parameter changes.\nA PCA approach in combination with Gaussian process emulation is used to tackle the task. \n\n# Code\nThe uploaded pieces of code represent a subsample of the Matlab functions I wrote to undertake the project during my PhD. \nA description of the different functions can be found within each script.\n\n# Also present\nSome plots of emulated mean and variance of annual average temperature, compared with proxy data.\n"
 },
 {
  "repo": "GeoffCowles/IntroOceanography-Notebooks",
  "language": "Jupyter Notebook",
  "readme_contents": "# IntroOceanography-Notebooks \n\n\n"
 },
 {
  "repo": "hainegroup/Computational-Oceanography-Commentary",
  "language": "MATLAB",
  "readme_contents": "Computational-Oceanography-Commentary\n==============================\n[![Build Status](https://travis-ci.com/hainegroup/computational_oceanography_commentary.svg?branch=master)](https://travis-ci.com/hainegroup/computational_oceanography_commentary)\n[![codecov](https://codecov.io/gh/hainegroup/computational_oceanography_commentary/branch/master/graph/badge.svg)](https://codecov.io/gh/hainegroup/computational_oceanography_commentary)\n[![License:MIT](https://img.shields.io/badge/License-MIT-lightgray.svg?style=flt-square)](https://opensource.org/licenses/MIT)\n\nMATLAB code to make figures for computational oceanography commentary.\n\nBuild three figures from the MATLAB subdirectories\n\n* `Turing_test.pdf` from running `plot_DSO_overflow.m`, `plot_hydrography.m`, `plot_float_trajectories.m` then annotating in keynote. Data for this figure comes from [Haine (2010)](https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2010GL043272) and [Saberi et al. (2020)](https://doi.org/10.1175/JPO-D-19-0210.1).\n* `data_plot.pdf` from running `make_data_plot.m` then annotating in keynote. This code uses [World Ocean Atlas](https://www.ncei.noaa.gov/products/world-ocean-database) data.\n* `scales_plot.pdf` from running `scales_diagram.m` then annotating in keynote. This code is adapted from [Klinger & Haine (2019) Fig. 1.10.](https://www.cambridge.org/core/books/ocean-circulation-in-three-dimensions/BA67744EF2B76C3FCB239BCBF9D18271).\n\nThis commentary is resubmitted to the Bulletin of the American Meteorological Society. See the preprint [here](???).\n\n--------\n\n<p><small>Project based on the <a target=\"_blank\" href=\"https://github.com/jbusecke/cookiecutter-science-project\">cookiecutter science project template</a>.</small></p>\n"
 },
 {
  "repo": "gaelforget/PhysicalOceanography.jl",
  "language": "Julia",
  "readme_contents": "# PhysicalOceanography\n\n[![Stable](https://img.shields.io/badge/docs-stable-blue.svg)](https://gaelforget.github.io/PhysicalOceanography.jl/stable)\n[![Dev](https://img.shields.io/badge/docs-dev-blue.svg)](https://gaelforget.github.io/PhysicalOceanography.jl/dev)\n[![DOI](https://zenodo.org/badge/277150554.svg)](https://zenodo.org/badge/latestdoi/277150554)\n\n[![Build Status](https://github.com/gaelforget/PhysicalOceanography.jl/workflows/CI/badge.svg)](https://github.com/gaelforget/PhysicalOceanography.jl/actions)\n[![Build Status](https://travis-ci.org/gaelforget/PhysicalOceanography.jl.svg?branch=master)](https://travis-ci.org/gaelforget/PhysicalOceanography.jl)\n[![Coverage](https://codecov.io/gh/gaelforget/PhysicalOceanography.jl/branch/master/graph/badge.svg)](https://codecov.io/gh/gaelforget/PhysicalOceanography.jl)\n[![Coverage](https://coveralls.io/repos/github/gaelforget/PhysicalOceanography.jl/badge.svg?branch=master)](https://coveralls.io/github/gaelforget/PhysicalOceanography.jl?branch=master)\n\n_This package is in early development stage when breaking changes are to be expected._\n"
 },
 {
  "repo": "hgjun1026/DnCNN_seismic_oceanography",
  "language": "Python",
  "readme_contents": "# Random Noise Attenuation of Sparker Seismic Oceanography Data with Machine Learning\n\nHyunggu Jun, Hyeong-Tae Jou, Chung-Ho Kim, Sang Hoon Lee, Han-Joon Kim\n\nThis repository includes the codes and sample data for the paper\n\"Random Noise Attenuation of Sparker Seismic Oceanography Data with Machine Learning\" in Ocean Science. \n \n\nThe programs are based on the \"Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising (https://github.com/cszn/DnCNN)\"\n\n### Requirements:\nThe version numbers that were used for the program, and newer version would work fine. \n\nKeras==2.2.4   \nKeras-Applications==1.0.8   \nKeras-Preprocessing==1.1.0   \nnumpy==1.16.2   \nopencv-python==4.1.1.26   \nscikit-learn==0.20.3   \nscipy==1.2.1   \ntensorboard==1.13.1   \ntensorflow==1.13.1   \ntensorflow-estimator==1.13.0   \ntqdm==4.32.1   \n\n\n### Data:\n0.data/0.train: synthetic training data \n0.data/1.noise: field noise data   \n0.data/2.test/noise_added: synthetic data with noise for test   \n0.data/2.test/original_denoise: synthetic data without noise and synthetic denoised data for comparison   \n\n### Code:\n1.program/cube.txt: size of each data    \n1.program/train.py: program for training DnCNN model   \n1.program/data_generator.py: training data generation code   \n1.program/test.py: program for testing the trained model   \n\n### Training: \nTo train the DnCNN model, run with augmentations:\n\n```bash\npython train.py --model 'DnCNN' --batch_size 128 --train_data '../0.data/0.train/' --noise_data '../0.data/1.noise/' --epoch 20\n```\n\nUnless you specify the options, the default options will be used.\n\n### Test:\nTo test the trained DnCNN model, run with augmentations:\n\n```bash\npython test.py --set_dir '../0.data/2.test/' --set_name 'noise_added/' --model_dir './models/DnCNN/' --model_name 'model_020.hdf5' --result_dir 'results'\n```\n\nUnless you specify the options, the default options will be used.\n\n\n### Citation:\nJun, H., Jou, H.-T., Kim, C.-H., Lee, S. H., and Kim, H.-J.: Random Noise Attenuation of Sparker Seismic Oceanography Data with Machine Learning, Ocean Sci. Discuss., https://doi.org/10.5194/os-2020-13, in review, 2020.\n\n"
 },
 {
  "repo": "jerryli1019/Oceanography-Data-Analysis",
  "language": "Jupyter Notebook",
  "readme_contents": "# Oceanography-Data-Analysis\n\nAuthors: Cameron Wang, Yi Li, Weiyue Li, and Tonoya Ahmed.\n\nTopic: [CalCOFI](https://calcofi.org/) Data Visualization Experience: Physical Oceanography\n"
 },
 {
  "repo": "rgaiacs/smc-oceanography-ufsc",
  "language": null,
  "readme_contents": "SageMath Could Research Experiment with Oceanography at UFSC\n============================================================\n\nOn [Jupyter Workshop](http://opendreamkit.org/meetings/2017-01-16-ICMS/programme/),\nMark Quinn, from the [University of Sheffield](http://www.sheffield.ac.uk/), UK,\npresented \"Using SageMathCloud for teaching undergraduate physics\".\nAt the same workshop,\n[Mike Croucher](https://github.com/mikecroucher/),\nalso from the University of Sheffield,\nmentioned the [\"Using Jupyter Notebook in Undergraduate teaching\" Case Studie](https://www.sheffield.ac.uk/bms/teaching/stories/bms353).\n\nBased on conversations during the [SciPy Latin America 2016](scipyla.org/conf/2016/),\nhosted at the Federal University of Santa Catarina, Brazil,\nthere was a hole to be fill with computation training\nfor the Oceanography department who could achieve better\nresults using SageMath Could.\n\n# Timeline\n\n- February/2018\n\n  One week workshop in Florian\u00f3polis for tailor existing content\n  to be used on the workshop.\n\n- March-June/2018\n\n  Delivery of the course for the first group.\n\n- July/2018\n\n  One week workshop in Sheffield for assessment of the course outcomes.\n\n- August-November/2018\n\n  Delivery of the course for the second group.\n\n- December/2018\n\n  One week workshop in Florian\u00f3polis for assessment of the course outcomes.\n\n# Deliverables\n\n- Semester long programming course for oceanography notes\n  in portuguese.\n- Two semester long programming course for ocenanography offering\n  at the Federal University of Santa Catarina.\n\n  40 students trainned in total.\n- Case studie publish in one journal\n\n# Costs\n\n- SageMath Could license.\n- Travel and accommodations.\n"
 },
 {
  "repo": "project-hermes/hermes-firebase",
  "language": "Vue",
  "readme_contents": "# hermes-firebase\n\n[![Join the chat at https://gitter.im/hermes-firebase/Lobby](https://badges.gitter.im/hermes-firebase/Lobby.svg)](https://gitter.im/hermes-firebase/Lobby?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n[![Build Status](https://travis-ci.org/sonyccd/hermes-firebase.svg?branch=master)](https://travis-ci.org/sonyccd/hermes-firebase)\n[![Codacy Badge](https://api.codacy.com/project/badge/Grade/dd6fcab8566444d486ccb79b8ec91494)](https://app.codacy.com/app/sonyccd/hermes-firebase?utm_source=github.com&utm_medium=referral&utm_content=sonyccd/hermes-firebase&utm_campaign=badger)\n[![Dependencies](https://david-dm.org/sonyccd/hermes-firebase.svg)](https://david-dm.org/sonyccd/hermes-firebase)\n\nFirebase code for project hermes\n\n## To devel\n\n```\n$ npm install -g firebase-tools\n$ firebase serve   # Start development server\n$ firebase deploy  # Deploy new version of everything\n```\n\n## To deploy only the functions\n```\n$ firebase deploy -P staging --only=functions\n```\n\n## Databse\n\nAll data is layed out in Google Firestore document databse\n```\n{\n    \"Sensor\": {\n        \"s2d4t6rdw46yew3f\": {\n            \"Name\": \"hermes1\",\n            \"Id\": 12334,\n            \"FirmwareV\": 0.05,\n            \"BuildV\": 0.01,\n            \"LastUpdate\": 1543742463,\n            \"Mode\": \"debug\",\n            \"Dive\": {\n                \"3d3r5fw32r45fgr56yne46ewg\": {\n                    \"Time\": 153453533,\n                    \"Start Lat\": 23.24323432,\n                    \"Start Long\": -31.2423424,\n                    \"End Lat\": 23.456435,\n                    \"End Long\": -31.564564,\n                    \"Duration\": 1234345,\n                    \"Name\": \"key largo\",\n                    \"Diver\": \"Brad Bazemore\",\n                    \"Data\": {\n                        \"2s23d43654f4yef53g43q\": {\n                            \"time\": 154575673,\n                            \"temp\": 89.34,\n                            \"depth\": 6547\n                        },\n                        \"4r43fr5tr44rfw34tfe4t\": {\n                            \"time\": 154575679,\n                            \"temp\": 89.42,\n                            \"depth\": 6553\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n```\n"
 },
 {
  "repo": "jlreeve/Matlab_for_Oceanography",
  "language": null,
  "readme_contents": "# matlab\nCode for Matlab (I'm currently running r2015a)\n"
 },
 {
  "repo": "so-wise/so-fronts",
  "language": "Python",
  "readme_contents": "# Defining Southern Ocean fronts using unsupervised classification\n\n<a href=https://opensource.org/licenses/MIT><img src='https://img.shields.io/badge/License-MIT-blue.svg' alt='License: MIT' /></a>\n<a href=https://www.python.org/downloads/release/python-388/><img src='https://img.shields.io/badge/python-3.8-blue.svg' alt='Python version 3.8.8 used' /></a>\n<a href=\"https://github.com/psf/black\"><img alt=\"Python code style: black\" src=\"https://img.shields.io/badge/code%20style-black-000000.svg\"></a>\n<img src='https://readthedocs.org/projects/so-fronts/badge/?version=latest' alt='Documentation Status / link to documentation' />\n<a href=https://zenodo.org/badge/latestdoi/318541083><img src='https://zenodo.org/badge/318541083.svg' alt='Most recent Zenodo release' /></a>\n\n## Paper: <https://doi.org/10.5194/os-17-1545-2021>\n\n## Preprint: <https://doi.org/10.5194/os-2021-40>\n\n## Presentation at AGU2021: <https://doi.org/10.1002/essoar.10507114.1>\n\n## Short description\n\nIn the Southern Ocean, fronts delineate water masses, which correspond to upwelling\nand downwelling branches of the overturning circulation. Classically, oceanographers\ndefine Southern Ocean fronts as a small number of continuous linear features that\nencircle Antarctica. However, modern observational and theoretical developments are\nchallenging this traditional framework to accommodate more localized views of fronts\n[Chapman et al. 2020].\n\nHere we present code for implementing two related methods for calculating fronts from\noceanographic data. The first method uses unsupervised classification (specifically,\nGaussian Mixture Modeling or GMM) and a novel interclass metric to define fronts.\nThis approach produces a discontinuous, probabilistic view of front location,\nemphasising the fact that the boundaries between water masses are not uniformly sharp\nacross the entire Southern Ocean.\n\nThe second method uses Sobel edge detection to highlight\nrapid changes [Hjelmervik & Hjelmervik, 2019].\nThis approach produces a more local view of fronts,\nwith the advantage that it can highlight the movement\nof individual eddy-like features (such as the Agulhas rings).\n\n1. Chapman, C. C., Lea, M.-A., Meyer, A., Sallee, J.-B. & Hindell, M.\n    Defining Southern Ocean fronts and their influence on biological and\n    physical processes in a changing climate. Nature Climate Change (2020).\n    https://doi.org/10.1038/s41558-020-0705-4\n\n2. Maze, G. et al. Coherent heat patterns revealed by unsupervised\n    classification of Argo temperature profiles in the North Atlantic Ocean.\n    Progress in Oceanography (2017).\n    https://doi.org/10.1016/j.pocean.2016.12.008,\n    https://doi.org/10.5281/zenodo.3906236\n\n3. Hjelmervik, K. B. & Hjelmervik, K. T. Detection of oceanographic fronts\n    on variable water depths using empirical orthogonal functions.\n    IEEE Journal of Oceanic Engineering (2019).\n    https://doi.org/10.1109/JOE.2019.2917456\n\n## I metric for K=5\n\n![I metric for K=5](gifs/boundaries-k5.gif)\n\n## Getting started\n\n- Make the environment:\n\n    ```bash\n    make env\n    ```\n\n- Activate the environment in conda:\n\n     ```bash\n     conda activate ./env\n     ```\n\n- Change the settings in `src.constants` to set download location etc.\n\n- Download data (`get_zip`: 1694.64639 s):\n\n   ```bash\n   python3 src/data_loading/bsose_download.py\n   ```\n\n- Make I-metric:\n\n   ```bash\n   python3 src/models/batch_i_metric.py\n   ```\n\n- Make figures:\n\n   ```bash\n   python3 main.py\n   ```\n\n## Project Organization\n\n```txt\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 Makefile             <- Makefile with commands like `make env` or `make `\n\u251c\u2500\u2500 README.md            <- The top-level README for developers using this project.\n\u251c\u2500\u2500 main.py              <- The main python script to run.\n|\n\u251c\u2500\u2500 figures              <- .png images with non-enumerated names.\n\u2502\n\u251c\u2500\u2500 requirements         <- Directory containing the requirement files.\n\u2502\n\u251c\u2500\u2500 setup.py             <- makes project pip installable (pip install -e .) so src can be imported from jupyter notebooks etc.\n|\n\u251c\u2500\u2500 src                  <- Source code for use in this project.\n|   |\n\u2502   \u251c\u2500\u2500 __init__.py      <- Makes src a Python module\n|   |\n\u2502   \u251c\u2500\u2500 data             <- KO fronts to plot, other data.\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 data_loading     <- Scripts to download and name data.\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 models           <- Make I-metric and Sobel edge detection directory.\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 plot             <- plotting functions directory\n|   |\n\u2502   \u251c\u2500\u2500 plot_utils       <- plotting utilities directory\n|   |\n\u2502   \u251c\u2500\u2500 preprocessing    <- preprocessing scripts (to transform to density etc.).\n|   |\n|   \u251c\u2500\u2500  tests           <- Scripts for unit tests of your functions\n|   | \n|   \u251c\u2500\u2500 animate.py       <- animate i-metric.\n|   \u251c\u2500\u2500 constants.py     <- contains majority of run parameters that can be changed.\n|   \u251c\u2500\u2500 make_figures.py  <- make all figures in one long script.\n|   \u251c\u2500\u2500 move_figures.py  <- Move figures script (now unnecessary). \n|   |                       Changes figure names to Figure-X.png etc.\n|   \u2514\u2500\u2500 time_wrapper.py  <- time wrapper to time parts of the program.\n\u2502\n\u2514\u2500\u2500 setup.cfg            <- setup configuration file for linting rules\n```\n\n## Requirements\n\n- Anaconda, with `conda` working in shell.\n- `make` in shell.\n- Python 3.6+ (final run for paper used `python==3.8.8`)\n\n--------------------------------------------------------\n\nProject template created by the\n[Cambridge AI4ER Cookiecutter](https://github.com/ai4er-cdt/ai4er-cookiecutter).\n"
 },
 {
  "repo": "olmozavala/particleviz",
  "language": "JavaScript",
  "readme_contents": "\n#  <img src=\"docs/logos/logo_sm.png\" width=\"200px\" style=\"border:none\"> Welcome to ParticleViz  \nParticleViz is an Open Source software that is used to animate large number of particles inside dynamic web maps.\nIt is designed mostly for Earth Science scientists that simulate different processes using Lagrangian models.\n\nThe objectives of this software are:\n* Provide efficient visualizations that can help analyze and understand research made through lagrangian modelling in the Earth Sciences, in a fast and easy way. \n* Make it easy to share this research with other colleagues with self-contained websites. \n\n**ParticleViz** currently reads NetCDF outputs from [OceanParcels](https://oceanparcels.org/) and [OpenDrift](https://opendrift.github.io/).\n\n## Status\n![GitHub Repo stars](https://img.shields.io/github/stars/olmozavala/particleviz?style=social)\n![GitHub](https://img.shields.io/github/license/olmozavala/particleviz)\n![GitHub all releases](https://img.shields.io/github/downloads/olmozavala/particleviz/total)\n![GitHub issues](https://img.shields.io/github/issues/olmozavala/particleviz)\n![GitHub commit activity](https://img.shields.io/github/commit-activity/m/olmozavala/particleviz)\n\n## Install\n\n1. Clone the repository.\n\n```shell\ngit clone https://github.com/olmozavala/particleviz.git\ncd particleviz\n```\n\n2. Create a conda environment with the proper dependencies. For this step, you first need to install Anaconda (or Miniconda), more details can be found [here](https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html).\n\n```shell\nconda env create -f particleviz.yml\nconda activate particleviz\n```\n\n3. Enjoy life\n\n## Quick Start\n\nThe simplest way to use **ParticleViz** is to run it specifying the input netcdf from the command line directly (NetCDF should follow [OceanParcels](https://oceanparcels.org/) or [OpenDrift](https://opendrift.github.io/) format).\n\n```shell\npython ParticleViz.py --input_file <path_to_netcdf> \n```\n\nThis will generate the *default* web interface and store the parameters into a configuration file, `Current_Config.json`. It can be edited to customize the interface. You need to be _patient_ the first time you run it because it will install all the Javascript dependencies.\n\nTest it with the *Global_Marine_Debris.nc* example file in the *ExampleData* folder:\n\n```shell\npython ParticleViz.py --input_file ExampleData/Global_Marine_Debris.nc\n```\n<img src=\"docs/media/quickstart.gif\" alt=\"example\" />\n\n## Intro video\nThis is a presentation made at OceanSciences meeting about ParticleViz in March 2022.\n\n[![ParticleViz at OSM](docs/media/video_tm.png)](https://youtu.be/7Xk0DxRMPjQ?t=289)\n\n## Docs\nPlease take a look at the complete docs at [https://olmozavala.github.io/particleviz/](https://olmozavala.github.io/particleviz/)"
 },
 {
  "repo": "princessmittens/ml_examples_oceanography",
  "language": "Python",
  "readme_contents": "# ml_examples_oceanography\nA unsupervised and supervised learning example in machine learning with mock data (accelerometry timeseries and temp/salinity data). \n\n\n## Setup Repo\n\n1. Ensure python3 is installed on your machine i.e. `python3 -v`\n\n2. Clone the repo: `git clone https://github.com/princessmittens/ml_examples_oceanography.git`\n\n3. Install the requirements: `pip3 install requirements.txt`\n\n4. `cd unsupervised_temp_salinity_example` or `cd supervised_timeseries_example`\n\n5. Run the program: `python3 main.py`\n"
 },
 {
  "repo": "fribalet/ssPopModel",
  "language": "Jupyter Notebook",
  "readme_contents": "ssPopModel\n==========\nssPopModel is a R package that uses size-structured matrix population model to estimate hourly division rates of microbial populations from SeaFlow data. These estimates are independent of variations in cell abundance and can be used to study physiologically-driven changes in population dynamics. \n\nThe first version of the model is described in:\n[Ribalet, F. et al. Light-driven synchrony of <i>Prochlorococcus</i> growth and mortality in the subtropical Pacific gyre. Proc. Natl. Acad. Sci. 112, 8008\u20138012 (2015)](http://www.pnas.org/lookup/doi/10.1073/pnas.1424279112)\n\n_________\nnew release ssPopModel v2.0.0\n(November 2019)\n\n- Added respiration to the size-strucutured matrix population model.\n- Added Huber loss as objective function and Covariance matrix adapting evolutionary strategy as optimization method (from cmaes package)\n- Modified delta function by prohibiting cells less than twice the size of the smallest size to divide\n\nCurrently testing the model of the various SeaFlow cruises...\n"
 },
 {
  "repo": "apaloczy/AntarcticaVorticityBudget",
  "language": "Jupyter Notebook",
  "readme_contents": "# AntarcticaVorticityBudget\n\n[![DOI](https://www.zenodo.org/badge/213719618.svg)](https://www.zenodo.org/badge/latestdoi/213719618)\n\nThis repository contains codes and processed datasets for a manuscript entitled **\"The large-scale vorticity balance of the Antarctic continental margin in a fine-resolution global simulation\"**, by A. Pal\u00f3czy, J. L. McClean, S. T. Gille and He Wang, [published on the Journal of Physical Oceanography](https://journals.ametsoc.org/jpo/article/doi/10.1175/JPO-D-19-0307.1/348316/The-large-scale-vorticity-balance-of-the-Antarctic). This [Jupyter notebook](https://nbviewer.jupyter.org/github/apaloczy/AntarcticaVorticityBudget/blob/master/index.ipynb) provides an overview of the contents.\n\nThe directory **plot_figs/** contains the Python codes used to produce the tables and figures in the manuscript (Figures 1-9) and the svg figure for the schematic (Figure 10). The codes depend on the data files in the **data_reproduce_figs/** directory. Some of these are too large to be included in this repository, but are available for download from the links listed on the accompanying README files. Please contact [Andr\u00e9 Pal\u00f3czy](mailto:apaloczy@ucsd.edu) if you have issues downloading the files.\n\n## Abstract\nThe depth-integrated vorticity budget of a global, eddy-permitting ocean/sea-ice simulation over the Antarctic Continental Margin (ACM) is diagnosed to understand the physical mehanisms implicated in meridional transport. The leading-order balance is between the torques due to lateral friction, nonlinear effects, and bottom vortex stretching, although details vary regionally. Maps of the time-averaged depth-integrated vorticity budget terms and time series of the spatially-averaged, depth-integrated vorticity budget terms reveal that the flow in the Amundsen, Bellingshausen and Weddell Seas and, to a lesser extent, in the western portion of East Antarctica, is closer to an approximate Topographic Sverdrup Balance (TSB) compared to other segments of the ACM. Correlation and coherence analyses further support these findings, and also show that inclusion of the vorticity tendency term in the response (the planetary vorticity advection and the bottom vortex stretching term) increases the correlation with the forcing (the vertical net stress curl), and also increases the coherence between forcing and response at high frequencies across the ACM, except for the West Antarctic Peninsula. These findings suggest that the surface-stress curl, imparted by the wind and the sea ice, has the potential to contribute to the meridional, approximately cross-slope, transport to a greater extent in the Amundsen, Bellingshausen, Weddell and part of the East Antarctic continental margin than elsewhere in the ACM.\n\n## Authors\n* [Andr\u00e9 Pal\u00f3czy](http://scrippsscholars.ucsd.edu/apaloczy) (<apaloczy@ucsd.edu>)\n* [Julie L. McClean](http://scrippsscholars.ucsd.edu/jmcclean)\n* [Sarah T. Gille](http://scrippsscholars.ucsd.edu/sgille)\n* He Wang\n\n## Acknowledgments\nA.P., J.L.M and S.T.G. gratefully acknowledge support from the US Department of Energy (DOE, grants DE-SC0014440 and DE-SC0020073) and high-performance computing support from Yellowstone (ark:/85065/d7wd3xhc) provided by NCAR's Climate Simulation Laboratory (CSL), sponsored by the National Science Foundation (NSF) and other agencies. J.L.M. was supported by an earlier U.S. DOE Office of Science grant entitled ``Ultra-High Resolution Global Climate Simulation\" via a Los Alamos National Laboratory subcontract to carry out the POP/CICE simulation; both Caroline Papadopoulos (SIO/UCSD) and Elena Yulaeva (UCSD) participated in its production. S.T.G. also acknowledges NSF awards PLR-1425989 and OCE 1658001. The analyses of the model output performed in this study were enabled by computing resources provided by Oak Ridge Leadership Computing Facility (OLCF). We are also grateful to Andrew Stewart (University of California Los Angeles) and Matt Mazloff (SIO/UCSD), for insightful exchanges and Steve Yeager (National Center for Atmospheric Research, NCAR) for the additional POP code needed to archive non-standard terms for the budget as well as an NCAR Command Language (NCL) vorticity budget code that provided a starting place for our expanded Fortran-based budget code used in this study. Thoughtful critiques and comments from two anonymous reviewers substantially improved the quality of the manuscript. Reduced datasets and code necessary to reproduce the results are available at [https://github.com/apaloczy/AntarcticaVorticityBudget](https://github.com/apaloczy/AntarcticaVorticityBudget).\n"
 },
 {
  "repo": "sckw/intro_physical_oceanography",
  "language": "Jupyter Notebook",
  "readme_contents": "# Intro to Physical Oceanography #\n\nThis repository contains course materials for EESC4925. The lecture notes are in the form of interactive [Jupyter Notebooks](https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/What%20is%20the%20Jupyter%20Notebook.html).\n\n## View the lecture notes online ##\n\nThe links below will render the notebooks via the [nbviewer](http://nbviewer.jupyter.org/) service, which allows some of the fancy interactive graphics to be viewed online. If you browse directly to the notebooks on github, they may not show up properly. So please use these links.\n\n* [1: Ocean Bathymetry](http://nbviewer.jupyter.org/github/rabernat/intro_to_physical_oceanography/blob/master/lectures/01_ocean_bathymetry.ipynb)\n* 2: Physical Properties of Seawater\n  * [2a: Thermodynamics of Seawater](http://nbviewer.jupyter.org/github/rabernat/intro_to_physical_oceanography/blob/master/lectures/02-a_thermodynamics_of_seawater.ipynb)\n  * [2b: The Water Column](http://nbviewer.jupyter.org/github/rabernat/intro_to_physical_oceanography/blob/master/lectures/02-b_the_water_column.ipynb)\n  * [2c: Global Temperature Salinity and Stratification](http://nbviewer.jupyter.org/github/rabernat/intro_to_physical_oceanography/blob/master/lectures/02-c_ocean_temperature_salinity_stratification.ipynb)\n* [3: Air-Sea Interaction](http://nbviewer.jupyter.org/github/rabernat/intro_to_physical_oceanography/blob/master/lectures/03_air_sea_exchange.ipynb)\n* [4: Advection and Diffusion](http://nbviewer.jupyter.org/github/rabernat/intro_to_physical_oceanography/blob/master/lectures/04_advection_diffusion_continuity.ipynb)\n* [Mesoscale Mixing Processes](http://nbviewer.jupyter.org/github/rabernat/intro_to_physical_oceanography/blob/master/lectures/mesoscale_mixing.ipynb)\n* [Equations of motion](http://nbviewer.jupyter.org/github/rabernat/intro_to_physical_oceanography/blob/master/lectures/equations_of_motion.ipynb)\n* [Hydrostatic, Geostrophic, Ekman](http://nbviewer.jupyter.org/github/rabernat/intro_to_physical_oceanography/blob/master/lectures/hydrostatic_geostrophic_ekman.ipynb)\n* [Vorticity and Sverdrup Transport]((http://nbviewer.jupyter.org/github/rabernat/intro_to_physical_oceanography/blob/master/lectures/vorticity_sverdrup_transport_and_gyres.ipynb)\n\n## Run the lecture notes interactively ##\n\nThe best way to get the materials (including homework) is the use [git](https://git-scm.com/) to [clone this repository](https://git-scm.com/book/en/v2/Git-Basics-Getting-a-Git-Repository). If you don't have git already on you computer, it is easy to install on all platforms following [these instructions](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git).\n\nFrom the command line, run the command\n\n```bash\ngit clone https://github.com/rabernat/intro_to_physical_oceanography\n```\n\nIf you are not a fan of the command line, there are plent of [graphical interfaces to git](https://git-scm.com/download/gui/linux) available.\n\nOnce you have the repository cloned, you can update it as new lectures come out by running\n\n```bash\ngit pull\n```\n\nIf for some reason you can't get git working, the alternative is to use the link to the right to \"Download Zip\". The disadvantage here is that you will have to re-download every time the repo is updated.\n\nIn order to actually execute the code in the notebooks, you need to have the necessary python packages installed.\nThe recommended way to do this is to install the [anaconda python distribution](https://www.anaconda.com/download/) together with the [conda package management utility](https://conda.io/docs/).\nFor more depth, you can read my [detailed intstructions for installing python](https://rabernat.github.io/research_computing/python.html).\n\nThis repository includes an [environment file](https://github.com/rabernat/intro_to_physical_oceanography/blob/master/phys_ocean_env.yml) which you can use to set up your python environment. To install this environment, type the following\n\n```bash\ncd intro_to_physical_oceanography\nconda env create -f phys_ocean_env.yml\n```\n\nThis will create a new environment called `phys_ocean`. To activate this environment, type\n\n```bash\nsource activate phys_ocean\n```\n\nThe notebooks can be viewed and run using the [jupyter notebook](https://jupyter-notebook.readthedocs.io/en/stable/notebook.html) application. To launch the notebook interface, just type\n\n```bash\njupyter notebook\n```\n\nWhen you are done working with the notebooks, close the notebook app and, if you wish, deactive the environment by typing\n\n```bash\nsource deactivate\n```\n\n\n## Why Python ##\n\nA great deal has been written on [this subject](http://cyrille.rossant.net/why-using-python-for-scientific-computing/).\nMy reasons are summarized as follows.\n\n1. __Python is open source__. [Open source](https://en.wikipedia.org/wiki/Open_source)\nmeans that the source code is available freely to the public and can be examined,\nmodified, and improved. The alternative to open source is closed, proprietary.\nProprietary tools, such as MATLAB, are ultimately controlled by corporations, and\nthose corporations decide what features they will include. I consider software\ntools as a central part of scientific research---if we want to have transparent,\nreproducible, scientific results, we should be using open source tools.\n[Nature](http://www.nature.com/nature/journal/v482/n7386/full/nature10836.html)\nagrees with me.\n\n1. __Python is free__. It does not cost money to use python. If your scientific\ncode is written in MATLAB, it can only be run by others with access to MATLAB.\nThat means people outside the university world (e.g. high school students), in\neconomically disadvantaged communities, or in developing countries will be\nunable to reproduce and build on your results.\n\n1. __Python is easy to read__. This may seem like a superficial point, but it is\ncrucial for effective sharing of code. Even if you are the only one reading\nyour code, python is easy on the eyes.\n\n1. __Python has a great library__. The [scipy ecosystem](http://scipy.org)\nprovides the tools to do almost anything you can imagine.\n\n1. __Python is constantly evolving__. If you find something you _can't_ do with\npython, chances are someone is working on it. The world is changing: data is\nexploding, computers architecture is evolving, and new forms of analysis and\nvisualization are being invented. Python is evolving too, and it evolves based\non what the community needs. The new tool I am most excited about is\n[xray](http://continuum.io/blog/xray-dask).\n\n1. __Python is at home on the web__. The [Jupyter project](https://jupyter.org/)\ngrew out of the python community and is revolutionizing the way we do science\nand communicate it with others. With Jupyter, I never have to leave my browser.\n[Nature agrees](http://www.nature.com/news/interactive-notebooks-sharing-the-code-1.16261)\nthat this is the future of scientific communication.\n"
 },
 {
  "repo": "morokhalid16/Big-Data-for-oceanography",
  "language": "Jupyter Notebook",
  "readme_contents": "# Big-Data-for-oceanography\nAtlantic multidecadal oscillation \n"
 },
 {
  "repo": "RodrigoTroncoso/SE-EE_PaleOceanography",
  "language": null,
  "readme_contents": "# (Under Construction)\n\n# BOT (Bottom water computation)\n\nTo determine the theoretical bottom water characteristics on top of the sediment cores sites, we propose a simple finder script. From an hydrological data base organized in ODV software is possible to estimate it.\n\n![GC_MUC_sediment_model](https://user-images.githubusercontent.com/57748370/113622334-231a8e80-965d-11eb-977d-943e565c5e71.png)\n\n## Prerequisites\n\n- Instal or use Ocean Data View software https://odv.awi.de/ or other program that allow you to obtained gridded interpolation plots (eg. surface, matlab, python, R). In this exercise we will use ODV. \n- Instal or use text file manager (eg. note ++, Excel). \n- Hydrological data set organize for be readed in ODV software \n- Create a special folder for your data \n\n\n## Getting started\n\n\n\n\n### First Step for OVD users \n\nDownload the latest version of ODV in https://odv.awi.de/software/download/ \n\n- Organize your data using the Ocean Data View (ODV) data template (excel file example). Excel file example (Please not delete the first row items, you can add new columns after the parameter \u201cdepth [m]\u201d) https://drive.google.com/file/d/1ZyONN1T7nXM2F6SS73UdqvW5-ooHM6aB/view?usp=drive_open \n- The excel file with your data needs to be exported as a text file (preference format cvs)\n- Import from ODV the txt file \n- Organize the data by meridional o zonal hydrological section \n- Use the tool **clipboard copy (right bottom, option Extra) type export grilled filled values with tabulations** were used. The full grilled data was a copy in an excel file, edited the first line taking care to avoid text space and saved in txt separated with tabulations or in comma-separated csv. For futher information use https://odv.awi.de/fileadmin/user_upload/odv/misc/odv4Guide.pdf \n\n### Second Step (data-sediments)...\n\n#### Determination of relative bottom water characteristics at the surface sediment sites\n\nWith the goal of finding the relative bottom water hydrological characteristics from the grilled hydrological interpolations (explained in point 1) in top of surface sediments collected in the South-East Pacific, a python script was developed. \n\n**Step 1.** Install Python (recommendable last edition) https://www.python.org/ \n\n**Step 2.** Create in your PC a single folder for each hydrological parameter (eg. one specific folder for oxygen, temperature, salinity). In that folder, add \n\ni) the hydrological parameter grilled text file obtained previously in Ocean Data View (example file format, https://drive.google.com/open?id=1DU3xOgopRsxX4oWs65YJ8nyRAoMPxk-9) \n\nii) the station\u2019s text file (example file format https://drive.google.com/open?id=1usasObWq-_4XA57hb3dHwZay6TwVpQc2) and the \n\niii) python script. Remember that the name of the files needs to be simple and without spaces.  \n\n\n### Third Step (Script-Python)...\n\n\n\n## Example ...\n\n\n\n\n\n\n## Authors\n\n* **Dharma Reyes** - *MARUM Research, University of Bremen, Germany* \n\n* **Pablo Santamarina** - [*Innovex Tecnolog\u00edas (Ltd.)*](www.innovex.cl)*, Chile*\n\n* **Rodrigo Troncoso** - *Bremen, Germany*\n\n\n\n### Collaborators\n\n> **Dharma Reyes** - *MARUM Institute, University of Bremen, Germany* - [dreyesmacaya@marum.de]\n\n> **Pablo Santamarina** - *Innovex (Ltd.), Chile*\n\n> **Rodrigo Troncoso** - *Bremen, Germany*\n\n\n\nSee also the list of [contributors](https://github.com/your/project/contributors) who participated in this project.\n"
 },
 {
  "repo": "ja754969/Field-Work-in-Oceanography",
  "language": "MATLAB",
  "readme_contents": "# \u6d77\u6d0b\u89c0\u6e2c\u5be6\u7fd2-\u7814\u7a76\u8239\u6d77\u7814\u4e09\u865f\u63a2\u6e2c\u7d00\u9304 Survey Log (SL) \n\u822a\u6b21\u4ee3\u865f : 1267  \n## CTD \u6e2c\u7ad9\u8cc7\u6599 (21\u00b0N 54', 120\u00b0E 46')\n\n2007/12/25 15:42 ~ 2007/12/26 20:49\n\n- `./CTD/CNV/C01.CNV` 2007/12/25 15:42 ~ 2007/12/25 15:53\n- `./CTD/CNV/C02.CNV` 2007/12/25 16:57 ~ 2007/12/25 17:03\n- `./CTD/CNV/C03.CNV`\n- `./CTD/CNV/C04.CNV`\n- `./CTD/CNV/C05.CNV`\n- `./CTD/CNV/C06.CNV`\n- `./CTD/CNV/C07.CNV`\n- `./CTD/CNV/C08.CNV`\n- `./CTD/CNV/C09.CNV`\n- `./CTD/CNV/C10.CNV`\n- `./CTD/CNV/C11.CNV`\n- `./CTD/CNV/C12.CNV`\n- `./CTD/CNV/C13.CNV`\n- `./CTD/CNV/C14.CNV`\n- `./CTD/CNV/C15.CNV`\n- `./CTD/CNV/C16.CNV`\n- `./CTD/CNV/C17.CNV`\n- `./CTD/CNV/C18.CNV`\n- `./CTD/CNV/C19.CNV`\n- `./CTD/CNV/C20.CNV`\n- `./CTD/CNV/C21.CNV`\n- `./CTD/CNV/C22.CNV`\n- `./CTD/CNV/C23.CNV`\n- `./CTD/CNV/C24.CNV`\n- `./CTD/CNV/C25.CNV`\n- `./CTD/CNV/C26.CNV`\n- `./CTD/CNV/C27.CNV`\n- `./CTD/CNV/C28.CNV`\n\nMore Information : https://github.com/ja754969/Field-Work-in-Oceanography/blob/115a3be97fff9b2b710b5c5f4e96f17a3ccebb0c/1267.doc\n\n## Time-depth contours of temperature (upper panel)\n![](https://github.com/ja754969/Field-Work-in-Oceanography/blob/master/results/profile_time_series.jpg?raw=true)\n## Time-depth contours of salinity (lower panel)\n![](https://github.com/ja754969/Field-Work-in-Oceanography/blob/master/results/profile_time_series.jpg?raw=true)\n## Sb-ADCP \u8ecc\u8de1\u8207\u6d41\u77e2  \n\n### WinADCP \u8cc7\u6599\u524d\u8655\u7406  \n![](https://i.imgur.com/QMapR1t.png)  \n- `./C1267/C1267000_000000.STA`  \n\n#### Step \n![](https://i.imgur.com/uGkw2Zb.png)  \n![](https://i.imgur.com/KJrtSQu.png)  \n![](https://i.imgur.com/QtSvLHF.png)  \n![](https://i.imgur.com/aLbemUs.png)  \n![](https://i.imgur.com/YD2OAet.png)  \n![](https://i.imgur.com/fZEUAf6.png)  \n![](https://i.imgur.com/GJwnHCU.png)  \n![](https://i.imgur.com/RHzK9p3.png)  \n![](https://i.imgur.com/MJoSjOQ.png)  \n![](https://i.imgur.com/ZYwSVjH.png)\n\n## Current vector figure of 7 rounds (From 2007/12/25 10:02:53 ~ 2007/12/28 03:10:54)\n\n### Number 1 (2007/12/25 10:02:53 ~ )\n\n\n### Number 2 ()\n\n### Number 3 ()\n\n\n### Number 4 ()\n\n\n### Number 5 ()\n\n### Number 6 ()\n\n### Number 7 ( ~ 2007/12/28 03:10:54)\n\n\n### All rounds\n![](https://github.com/ja754969/Field-Work-in-Oceanography/blob/master/results/ship_track_current.jpg?raw=true)  \n\n![](https://github.com/ja754969/Field-Work-in-Oceanography/blob/master/Nan-bay_tides_predict.png?raw=true)  "
 },
 {
  "repo": "apershing/2018-Oceanography-Paper",
  "language": "Matlab",
  "readme_contents": "# 2018 Oceanography Paper\n"
 },
 {
  "repo": "Cuiyingzhe/OUC-Physical-Oceanography-Notes",
  "language": "PostScript",
  "readme_contents": "# OUC Physical Oceanography Notes \n# \u4e2d\u56fd\u6d77\u6d0b\u5927\u5b66\u300a\u7269\u7406\u6d77\u6d0b\u5b66\u300b\u7b14\u8bb0\n\u66f4\u65b0\u4e2d...\n\n\u770bnotes.pdf\u5373\u53ef(\u8bf7\u4e0b\u8f7d\u540e\u67e5\u770b)\n\n20200815 version 2.0 \u7b2c\u4e00\u7248\u7b14\u8bb0(\u672a\u68c0\u9605)\n\n20200816 version 2.1\n\n20200828 version 3.0 \u8003\u5b8c\u8bd5\u4e86\uff0c\u65e0\u9650\u671f\u6682\u505c\u66f4\u65b0"
 },
 {
  "repo": "ChanJeunlam/Books-on-physical-oceanography",
  "language": null,
  "readme_contents": "# -\u8fd9\u91cc\u4f1a\u5206\u4eab\u4e00\u4e9b\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u79ef\u7d2f\u7684\u4e66\u7c4d\n* \u7edf\u8ba1\u65b9\u5411\n* \u52a8\u529b\u65b9\u5411\n* \u7f16\u7a0b\u65b9\u5411\n"
 },
 {
  "repo": "sophieclayton/data_science_oceanography",
  "language": "Jupyter Notebook",
  "readme_contents": "# OEAS805: Advanced Data Science Techniques in Environmental Sciences.\n\nOceanography is quickly moving from being a data-poor to a data-rich discipline. \n\nThis is an advanced computational course designed to introduce students to data management, reproducibility and analysis methods commonly used in data science applications. The data analysis portion of the course will be primarily based on machine learning methods. The course will also give an overview of a selection of scientific databases which host freely available oceanographic data and output from numerical model simulations. This course is not discipline specific and will be useful for any students who want to work with data efficiently and gain experience in applying machine learning techniques.\n\n## Learning objectives\n1. Understand FAIR data principles and how to apply them when generating, sharing and accessing data.\n2. Develop a working knowledge of existing ocean and earth science databases and how to efficiently access data from them.\n3. Students will develop their own data analysis toolbox using, but not limited to, python and shell scripts.  \n4. Understand and use version control (e.g. git) to manage and share code.\n5. Understand the underlying principles of machine learning techniques and apply them to a targeted research question.\n"
 },
 {
  "repo": "hafez-ahmad/Julia_for_Computational_Oceanography",
  "language": "Jupyter Notebook",
  "readme_contents": "# Julia_for_Computational_Oceanography"
 },
 {
  "repo": "juoceano/chemicaloceanography2",
  "language": "TeX",
  "readme_contents": "<!--pandoc -V geometry:margin=1in --from markdown_github README.md -o README.pdf \n--latex-engine=xelatex -V geometry:margin=1in --smart --normalize --standalone --webtex -->\n\n\n# Plano de Aula #\n# QUI155 Oceanografia Qu\u00edmica II - UFBA #\n\nDisciplina: QUI155 Oceanografia Qu\u00edmica II - Semestre 2015.1\n\nProfessora: Juliana Leonel \n\nE-mail: jleonel@ufba.br\n\nDia/Hor\u00e1rio das aulas:7:00 - 10:40 -  Quintas-feiras Sala 07/PAF VI (Turma 2) e Sextas-feiras Sala 03/PAF VI (Turma B)\n\nAtendimento: Sextas-feiras 13:00 - 14:00 - IGEO, 2 andar, sala 10\n\nHomepage: http://juoceano.github.io/chemicaloceanography2\n\n## 1. Ementa:\nOs processos biogeoqu\u00edmicos marinhos dos elementos nutrientes e seus ciclos. Os fen\u00f4menos que controlam a acumula\u00e7\u00e3o e preserva\u00e7\u00e3o nos sedimentos marinhos e a qu\u00edmica de alguns compostos s\u00e3o discutidos, incluindo a discuss\u00e3o das regras do oceano em produzir petr\u00f3leo. Impactos marinhos s\u00e3o vista de maneira geral sob a perspectiva da qu\u00edmica do estu\u00e1rio. \n\n## 2. Objetivos:\n\nNo final dessa disciplina os alunos:\n\na) entender\u00e3o o princ\u00edpio de alguns dos principiais m\u00e9todos de an\u00e1lise instrumental usadas na oceanografia;\n\nb) reconhecer\u00e3o vantagens e desvantagens de diferentes m\u00e9todos de an\u00e1lise instrumental;\n\nc) saber\u00e3o escolher o m\u00e9todo de an\u00e1lise instrumental mais adequado para cada tipo de an\u00e1lise.\n\n## 3. Metodologia das Aulas: \n\na) aulas expositivas; \n\nb) discuss\u00f5es em sala de aula; \n\nc) exerc\u00edcios;\n\nd) semin\u00e1rios.\n\nEu encorajo voc\u00eas perguntarem durante as aulas, especialmente se n\u00e3o antederem o assunto que est\u00e1 em discuss\u00e3o. Os principais t\u00f3picos ser\u00e3o apresentados/discutidas em sala de aula. As leituras requeridas ajudar\u00e3o no entendimento das aulas al\u00e9m de trazerem informa\u00e7\u00f5es e discuss\u00f5es complementares. Portanto, leia o material antes das aulas!!!\n\nOs slides de aulas estar\u00e3o dispon\u00edveis na homepage da disciplina. Como eles s\u00e3o apenas um guia para as aulas eu recomendo que voc\u00eas tamb\u00e9m fa\u00e7am suas pr\u00f3prias anota\u00e7\u00f5es. \n\n## 4. Avalia\u00e7\u00f5es\n\na) exerc\u00edcios (40%);\n\nb) semin\u00e1rio (60%);\n\n## 5. Conduta \n\n### Assiduidade: \nSer\u00e1 cobrada presen\u00e7a em sala de aula durante a aula pr\u00e1tica atrav\u00e9s da chamada ou assinatura de lista de presen\u00e7a. Alunos que estiverem ausentes n\u00e3o poder\u00e3o entregar os relat\u00f3rios e, se houver alguma atividade avaliada no dia, receber\u00e3o zero na atividade. \n\n### Atividades: \nCuidado com c\u00f3pias (pl\u00e1gio)! Trabalhos que forem c\u00f3pias (integral ou parcial) do trabalho de colegas ou de outras fontes sem referencia desta (livros, artigos, material da internet) ser\u00e3o desconsiderados na hora da corre\u00e7\u00e3o recebendo nota zero.\n\n## 6. Bibliografia recomendada:\n\nMillero, F. (2006) Chemical Oceanography. 3rd Edition. CRC, USA, 469p.\n\nLibes, S. (2009) Introduction to Marine Biogeochemistry. 2nd Edition. John Wiley & Sons, USA, 734p.\n\nChester, R. (2000) Marine Geochemistry. 2nd Edition. Blackwell Science, UK, 506p.\n\nSarmiento, J. L. & Gruber, N. (2006) Ocean Biochemical Dynamics.  Princeton University Press, USA, 528p.\n\nSchlesinger, W. H. (Editor) (2005) Biogeochemistry. Treatise on Geochemistry. Elsevier, UK, 702p.\n\nLaane, R. W. P. M. & Middelbeurg, J. J. (2011) Biogeochemistry. Treatise on Estuarine and Coastal Science, Vol. 5,  Elsevier, UK, 362p.\n\nSkoog, D. A.; West, D. M.; Holler, F. J. & Stanley, R. C. (2007) Fundamentos da Qu\u00edmica Anal\u00edtica, Tradu\u00e7\u00e3o da 8\u00aa edi\u00e7\u00e3o norte americana. Thomson, S\u00e3o Paulo. \n\nSkoog, D. A.; Holler, F. J. & Nieman, T.A. (2002) Princ\u00edpios de An\u00e1lise Instrumental, Tradu\u00e7\u00e3o da 5\u00aa edi\u00e7\u00e3o. Bookman, S\u00e3o Paulo. \n\n\n## 7. Cronograma\n\n| Aula | Data   | Conte\u00fado                             |Leitura | Atividade Avaliada   |\n|:----:|:-------|:-------------------------------------|:-------|---------------------:|\n| 01   |05 mar\u00e7o|Apresenta\u00e7\u00e3o da Disciplina            |[Texto 1](https://github.com/juoceano/chemicaloceanography2/raw/master/README.pdf) ||\n| 01   |06 mar\u00e7o|Apresenta\u00e7\u00e3o da Disciplina            |[Texto 1](https://github.com/juoceano/chemicaloceanography2/raw/master/README.pdf) ||\n| 02   |12 mar\u00e7o|[Revis\u00e3o de Conceitos](https://github.com/juoceano/chemicaloceanography2/raw/master/classes/OceanoQuimica_Semana02(CC).pdf)                  |[Texto 2](https://github.com/juoceano/chemicaloceanography2/raw/master/chapters/Texto2.pdf) ||\n| 02   |13 mar\u00e7o|[Revis\u00e3o de Conceitos](https://github.com/juoceano/chemicaloceanography2/raw/master/classes/OceanoQuimica_Semana02(CC).pdf)                   |[Texto 2](https://github.com/juoceano/chemicaloceanography2/raw/master/chapters/Texto2.pdf) ||\n| 03   |19 mar\u00e7o|[Revis\u00e3o Conceito (Cont.)](https://github.com/juoceano/chemicaloceanography2/raw/master/classes/OceanoQuimica_Semana03(CC).pdf) / Exerc\u00edcios 1|        ||\n| 03   |20 mar\u00e7o|[Revis\u00e3o Conceito (Cont.)](https://github.com/juoceano/chemicaloceanography2/raw/master/classes/OceanoQuimica_Semana03(CC).pdf) / Exerc\u00edcios 1|        ||\n| 04   |26 mar\u00e7o|[Prepara\u00e7\u00e3o de Amostras](https://github.com/juoceano/chemicaloceanography2/raw/master/classes/OceanoQuimica_Semana04(CC).pdf)                | |Exerc\u00edcio 1|\n| 04   |27 mar\u00e7o|[Prepara\u00e7\u00e3o de Amostras](https://github.com/juoceano/chemicaloceanography2/raw/master/classes/OceanoQuimica_Semana04(CC).pdf)                 | |Exerc\u00edcio 1|\n| 05   |02 abril|Feriado                               |        ||\n| 05   |03 abril|Feriado                               |        ||\n| 06   |09 abril|[Prepara\u00e7\u00e3o de Amostras (cont.)](https://github.com/juoceano/chemicaloceanography2/raw/master/classes/OceanoQuimica_Semana05(CC).pdf)       |        |Exerc\u00edcio 2|\n| 06   |10 abril|[Prepara\u00e7\u00e3o de Amostras (cont.)](https://github.com/juoceano/chemicaloceanography2/raw/master/classes/OceanoQuimica_Semana05(CC).pdf)         |        |Exerc\u00edcio 2|\n| 07   |16 abril|[Prepara\u00e7\u00e3o de Amostras - aplica\u00e7\u00f5es em estudos oceanogr\u00e1ficos](https://github.com/juoceano/chemicaloceanography2/raw/master/classes/OceanoQuimica_Semana06(CC).pdf)|||\n| 07   |17 abril|[Prepara\u00e7\u00e3o de Amostras - aplica\u00e7\u00f5es em estudos oceanogr\u00e1ficos](https://github.com/juoceano/chemicaloceanography2/raw/master/classes/OceanoQuimica_Semana06(CC).pdf)|||\n| 08   |23 abril|[Introdu\u00e7\u00e3o aos M\u00e9todos Instrumentais](https://github.com/juoceano/chemicaloceanography2/raw/master/classes/OceanoQuimica_Semana08(CC).pdf)                     | |[Exerc\u00edcio 3](https://github.com/juoceano/chemicaloceanography2/raw/master/evaluations/Exercicio3.pdf)|\n| 08   |24 abril|[Introdu\u00e7\u00e3o aos M\u00e9todos Instrumentais](https://github.com/juoceano/chemicaloceanography2/raw/master/classes/OceanoQuimica_Semana08(CC).pdf)                     | |[Exerc\u00edcio 3](https://github.com/juoceano/chemicaloceanography2/raw/master/evaluations/Exercicio3.pdf)|\n| 09   |30 abril|Exerc\u00edcios 2 (para casa)              |        ||\n| 09   |01 maio |FERIADO                               |        ||\n| 10   |07 maio |[Introdu\u00e7\u00e3o aos M\u00e9todos Instrumentais (cont.)](https://github.com/juoceano/chemicaloceanography2/raw/master/classes/OceanoQuimica_Semana10(CC).pdf)  |Texto 6 |[Exerc\u00edcios 4](https://github.com/juoceano/chemicaloceanography2/raw/master/evaluations/Exercicio4.pdf)|\n| 10   |08 maio |[Introdu\u00e7\u00e3o aos M\u00e9todos Instrumentais (cont.)](https://github.com/juoceano/chemicaloceanography2/raw/master/classes/OceanoQuimica_Semana10(CC).pdf)  |Texto 5 |[Exerc\u00edcios 4](https://github.com/juoceano/chemicaloceanography2/raw/master/evaluations/Exercicio4.pdf)|\n| 11   |14 maio |Espectrometria de Absor\u00e7\u00e3o At\u00f4mica (FAAS e GFAAS)||[Avalia\u00e7\u00e3o](https://github.com/juoceano/chemicaloceanography2/raw/master/evaluations/Seminarios.pdf)|\n| 11   |15 maio |Espectrometria de Absor\u00e7\u00e3o At\u00f4mica (FAAS e GFAAS)||[Avalia\u00e7\u00e3o](https://github.com/juoceano/chemicaloceanography2/raw/master/evaluations/Seminarios.pdf)|\n| 12   |21 maio |Espectrometria de Emiss\u00e3o At\u00f4mica (ICP-OES e ICP-MS)||[Avalia\u00e7\u00e3o](https://github.com/juoceano/chemicaloceanography2/raw/master/evaluations/Seminarios.pdf)|\n| 12   |22 maio |Espectrometria de Emiss\u00e3o At\u00f4mica (ICP-OES e ICP-MS)||[Avalia\u00e7\u00e3o](https://github.com/juoceano/chemicaloceanography2/raw/master/evaluations/Seminarios.pdf)|\n| 13   |28 maio |Cromatografia                         ||[Avalia\u00e7\u00e3o](https://github.com/juoceano/chemicaloceanography2/raw/master/evaluations/Seminarios.pdf)|\n| 13   |29 maio |Cromatografia                         ||[Avalia\u00e7\u00e3o](https://github.com/juoceano/chemicaloceanography2/raw/master/evaluations/Seminarios.pdf)|\n| 14   |04 junho|FERIADO                               |        ||\n| 14   |05 junho|-              |        ||\n| 15   |11 junho|Semana de Oceanografia     |        ||\n| 15   |12 junho|Semana de Oceanografia     |        ||\n| 16   |18 junho|Aplica\u00e7\u00e3o dos m\u00e9todos em oceanografia |        ||\n| 16   |19 junho|Aplica\u00e7\u00e3o dos m\u00e9todos em oceanografia |        ||\n| 17   |25 junho|Aplica\u00e7\u00e3o dos m\u00e9todos em oceanografia (cont.)| ||\n| 17   |26 junho|Aplica\u00e7\u00e3o dos m\u00e9todos em oceanografia (cont.)| ||\n| 18   |02 julho|FERIADO                               |        ||\n| 18   |03 julho|Exerc\u00edcios (para casa)                |        ||\n| 19   |09 julho|Encerramento da disciplina            |        ||\n| 19   |10 julho|Encerramento da disciplina            |        ||\n"
 },
 {
  "repo": "marieaudep/oceanpy",
  "language": "Python",
  "readme_contents": "oceanpy\n=======\n\nPhysical Oceanography for Python\n"
 },
 {
  "repo": "scausio/oceantools",
  "language": "Python",
  "readme_contents": "<h1>Oceantools</h1>\n\nCollection of tools for oceanography\n\n<h2>How to install</h2>\n\nThe tool is  written in Python3, the use of a virtual environment is suggested.\n\n\n<p>In the oceantools/install you can find the yaml file to build a new environment:</p>\n<ul>\n<li><em>cd oceantools/install</em></li>\n<li><em>conda env create -f environment.yml</em></li>\n</ul>\n\n<strong>Please be sure that anaconda is installed under your machine!  </strong>\n\n<p>Activate the virtual environment:</p>\n<ul>\n<li><em>conda activate oceantools</em></li>\n</ul>\n\n\n<h2>How to run spatial regridding for regular netcdf files</h2>\n<em>spatial_regrid.py</em> performs horizontal and vertical bilinear interpolation of regular netcdf file, from an input file to a target user-defined grid. The procedure includes also a Sea-Over-Land step before regridding.\n\n<p>The command is:</p>\n<ul>\n<li><em>cd oceantools/geometry</em></li>\n<li><em>python spatial_regrid.py -i <strong>input_file</strong> -o <strong>output_grid</strong> -n <strong>output filename</strong> </em></li>\n</ul>\n\n\n<strong>input_file</strong> is the absolute path to the file you want to regrid. The file needs to have 1D spatial coordinates (as in CF-compliant netcdf). The tool is not able to manage 2D coordinates (e.g NEMO based files).\n\n<strong>output_grid</strong> is the target grid which can (or not) have a vertical dimension. If the vertical dimension is not present, the interpolation will be performed only at the first layer of the input_file .\nIf a vertical dimension is present in the target grid, the data will be interpolated also in vertical.\n\n<strong>output filename</strong> name for the output netcdf\n\n\nThe user should provides also some information about the name for dimensions and variable using the <em>catalog.yaml</em> file.\n\nHere an example:\n\n\n> -------- catalog.yaml file --------\n> source:\n> <ul>\n>\n>  input_file:\n>  <ul>\n>  fillvalue: 1e20\n>\n>   coords:\n>    <ul>\n>\n>      latitude: lat\n>\n>      longitude: lon\n>\n>      depth: depth\n>\n>      time: time\n>\n>   </ul>\n>    variables:\n>    <ul>\n>\n>      temperature: thetao\n>      #salinity: so\n>    </ul>\n>    </ul>\n>  output_grid:\n>\n>  <ul>\n>    fillvalue: 9999\n>    sol_iterations: 5\n>    coords:  <ul>\n>\n>      latitude: lat\n>      longitude: lon\n>      depth: depth\n>    </ul>\n>    variables:\n>    <ul>\n>\n>        lsm: mask\n>\n>  </ul>\n>\n> </ul>\n>----------------------------------------\n\n>\n> **#** is the comment character in catalog.\n\n<p><strong>input_file block</strong> allows to define:</p>\n<ul>\n<li>  fill values of the input file</li>\n<li> in the <strong>coords block</strong>,  the latitude and longitude names.\n If the <em>input_file</em> has also time and/or depth dimensions these must be declared here. If not, the procedure raise a self-explaining error.\n If the  <em>input_file</em> has time dimension, the regridding will be applied at each time.\n<strong>!!!Only four names allowed in the coords block: latitude, longitude, time, depth!!!</strong></li>\n<li> in the <strong>variables block</strong>, the variable/s to regrid. Here the user can define one or more variables, and the keys used in the block (temperature and salinity in the example) will be used as variable name in the output file.\n</ul>\n\n<p><strong>output_grid block</strong> allows to define:</p>\n<ul>\n<li>  fill values to be used in the output file</li>\n<li>  sol_iterations: number of iterations for SeaOvearLand</li>\n<li>in the <strong>coords block</strong>, the latitude and longitude names. If the <em>output_grid</em> has also vertical coordinate, this has to be defined here.\n If the vertical coordinate is not defined here, (using depth key in catalog) the regrid will be applied only at the first level of the <em>input_file</em>.</li>\n<li>in the <strong>variables block</strong> the user defines the name of the land-sea mask variable. The  mask should has been defined as following: <em>ocean=1 (or True), land=0 (or False)</em></li>\n</ul>\n\n\n<h2>Test </h2>\nIn _<em>oceantools/geometry/tests</em> directory some example netcdf  are available, with/without time and depth dimensions, with one or two variables.\n\n<p>Examples of test commands: </p>\n <ul>\n<li>python ./spatial_regrid.py  -i ./tests/NWP_noTime.nc -o ./tests/mask.nc -n example_1  ---> for temperature horizontal and vertical regrid without time dimension  </li>\n<li>python ./spatial_regrid.py  -i ./tests/NWP_2vars.nc -o ./tests/mask.nc -n example_2 ---> for salinity and temperature horizontal and vertical regrid  </li>\n<li>python ./spatial_regrid.py  -i ./tests/NWP_complete.nc -o ./tests/mask.nc -n example_3 ---> temperature horizontal and vertical regrid with time dimension</li>\n\nEach of these examples requires a proper configuration of the catalog according to dimensions/variables in each input_file/output_file. If the configuration is not properly set, an error warns about the catalog check needed. The log will also show the dumping of files and catalogs, so the user can easily checks where is the error.\n\n<h2><em>Notes</em></h2>\nBefore interpolation on the new grid, the tool applies a Sea-Over-Land (SOL) procedure.\nHere SOL is a python application implemented by E.Jansen at CMCC from a G. Girardi's fortran development. SOL extrapolates sea information on the land to avoid the loosing of data near coast due to interpolation with NaN or FillValue. </em>. Please consider that SOL affects only data on land, and if, for example, the procedure fills completely an island, nothing occurs anymore even if the iterations continue.\n\n<h3> Author reference</h3>\nPlease contact Salvatore at <em>salvatore.causio@cmcc.it</em> for questions, bugs, suggestions."
 },
 {
  "repo": "edavishydro/gast-oceanography-database",
  "language": "JavaScript",
  "readme_contents": "# gast-oceanography-database\nCodebase for Gast Oceanography Database on TGAEC.com\n"
 },
 {
  "repo": "Omar-Meharab/oceanography-blog-site",
  "language": "HTML",
  "readme_contents": "# Getting Started with Create React App\n\nThis project was bootstrapped with [Create React App](https://github.com/facebook/create-react-app).\n\n## Available Scripts\n\nIn the project directory, you can run:\n\n### `yarn start`\n\nRuns the app in the development mode.\\\nOpen [http://localhost:3000](http://localhost:3000) to view it in the browser.\n\nThe page will reload if you make edits.\\\nYou will also see any lint errors in the console.\n\n### `yarn test`\n\nLaunches the test runner in the interactive watch mode.\\\nSee the section about [running tests](https://facebook.github.io/create-react-app/docs/running-tests) for more information.\n\n### `yarn build`\n\nBuilds the app for production to the `build` folder.\\\nIt correctly bundles React in production mode and optimizes the build for the best performance.\n\nThe build is minified and the filenames include the hashes.\\\nYour app is ready to be deployed!\n\nSee the section about [deployment](https://facebook.github.io/create-react-app/docs/deployment) for more information.\n\n### `yarn eject`\n\n**Note: this is a one-way operation. Once you `eject`, you can\u2019t go back!**\n\nIf you aren\u2019t satisfied with the build tool and configuration choices, you can `eject` at any time. This command will remove the single build dependency from your project.\n\nInstead, it will copy all the configuration files and the transitive dependencies (webpack, Babel, ESLint, etc) right into your project so you have full control over them. All of the commands except `eject` will still work, but they will point to the copied scripts so you can tweak them. At this point you\u2019re on your own.\n\nYou don\u2019t have to ever use `eject`. The curated feature set is suitable for small and middle deployments, and you shouldn\u2019t feel obligated to use this feature. However we understand that this tool wouldn\u2019t be useful if you couldn\u2019t customize it when you are ready for it.\n\n## Learn More\n\nYou can learn more in the [Create React App documentation](https://facebook.github.io/create-react-app/docs/getting-started).\n\nTo learn React, check out the [React documentation](https://reactjs.org/).\n\n### Code Splitting\n\nThis section has moved here: [https://facebook.github.io/create-react-app/docs/code-splitting](https://facebook.github.io/create-react-app/docs/code-splitting)\n\n### Analyzing the Bundle Size\n\nThis section has moved here: [https://facebook.github.io/create-react-app/docs/analyzing-the-bundle-size](https://facebook.github.io/create-react-app/docs/analyzing-the-bundle-size)\n\n### Making a Progressive Web App\n\nThis section has moved here: [https://facebook.github.io/create-react-app/docs/making-a-progressive-web-app](https://facebook.github.io/create-react-app/docs/making-a-progressive-web-app)\n\n### Advanced Configuration\n\nThis section has moved here: [https://facebook.github.io/create-react-app/docs/advanced-configuration](https://facebook.github.io/create-react-app/docs/advanced-configuration)\n\n### Deployment\n\nThis section has moved here: [https://facebook.github.io/create-react-app/docs/deployment](https://facebook.github.io/create-react-app/docs/deployment)\n\n### `yarn build` fails to minify\n\nThis section has moved here: [https://facebook.github.io/create-react-app/docs/troubleshooting#npm-run-build-fails-to-minify](https://facebook.github.io/create-react-app/docs/troubleshooting#npm-run-build-fails-to-minify)\n"
 },
 {
  "repo": "upadyandarux/ocean_modelling",
  "language": null,
  "readme_contents": "# ocean_modelling\nFor Modelling in Oceanography\n"
 },
 {
  "repo": "msdevana/oceanTools",
  "language": "Python",
  "readme_contents": "# oceanTools for python oceanographers\n- A collection of ocean tools for my PhD that may be useful for other people \nin ocean and earth science\n\n\n"
 },
 {
  "repo": "dankelley/ocesd",
  "language": null,
  "readme_contents": "This directory holds a latex class style (``ocesd.cls``) and a sample latex\nfile (``sd_example.tex``) for formatting a student dossier for use in the\nDepartment of Oceanography at Dalhousie University. \n"
 },
 {
  "repo": "rmholmes/rmholmes.github.io",
  "language": "JavaScript",
  "readme_contents": "A Github Pages template for academic websites.\n\nSee more info at https://academicpages.github.io/\n\nTo run locally to preview:\n```bundle exec jekyll liveserve --tracer```\n\n"
 }
]