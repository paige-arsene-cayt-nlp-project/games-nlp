[
 {
  "repo": "SciTools/iris",
  "language": "Python",
  "readme_contents": "<h1 align=\"center\">\n  <a href=\"https://scitools-iris.readthedocs.io/en/latest/\">\n   <img src=\"https://scitools-iris.readthedocs.io/en/latest/_static/iris-logo-title.svg\" alt=\"Iris\" width=\"300\"></a><br>\n</h1>\n\n\n<h4 align=\"center\">\n    Iris is a powerful, format-agnostic, community-driven Python package for\n    analysing and visualising Earth science data\n</h4>\n\n<p align=\"center\">\n<a href=\"https://github.com/SciTools/iris/actions/workflows/ci-tests.yml\">\n<img src=\"https://github.com/SciTools/iris/actions/workflows/ci-tests.yml/badge.svg?branch=main\"\n     alt=\"ci-tests\"></a>\n<a href=\"https://scitools-iris.readthedocs.io/en/latest/?badge=latest\">\n<img src=\"https://readthedocs.org/projects/scitools-iris/badge/?version=latest\"\n     alt=\"Documentation Status\"></a>\n<a href=\"https://results.pre-commit.ci/latest/github/SciTools/iris/main\">\n<img src=\"https://results.pre-commit.ci/badge/github/SciTools/iris/main.svg\"\n     alt=\"pre-commit.ci status\"></a>\n</p>\n\n<p align=\"center\">\n<a href=\"https://anaconda.org/conda-forge/iris\">\n<img src=\"https://img.shields.io/conda/v/conda-forge/iris?color=orange&label=conda-forge%7Ciris&logo=conda-forge&logoColor=white\"\n     alt=\"conda-forge\"></a>\n<a href=\"https://pypi.org/project/scitools-iris\">\n<img src=\"https://img.shields.io/pypi/v/scitools-iris?color=orange&label=pypi%7Cscitools-iris&logo=python&logoColor=white\"\n     alt=\"pypi\"></a>\n<a href=\"https://github.com/SciTools/iris/releases\">\n<img src=\"https://img.shields.io/github/v/release/scitools/iris\"\n     alt=\"latest release\"></a>\n<a href=\"https://github.com/SciTools/iris/commits/main\">\n<img src=\"https://img.shields.io/github/commits-since/SciTools/iris/latest.svg\"\n     alt=\"Commits since last release\"></a>\n<a href=\"https://zenodo.org/badge/latestdoi/5312648\">\n<img src=\"https://zenodo.org/badge/5312648.svg\"\n     alt=\"zenodo\"></a>\n<a href=\"https://github.com/psf/black\">\n<img src=\"https://img.shields.io/badge/code%20style-black-000000.svg\"\n     alt=\"black\"></a>\n<a href=\"https://github.com/SciTools/iris/discussions\">\n<img src=\"https://img.shields.io/badge/github-discussions-yellow?style=social&logo=github&style=plastic\"\n     alt=\"github discussions\"></a>\n<a href=\"https://twitter.com/scitools_iris\">\n<img src=\"https://img.shields.io/twitter/follow/scitools_iris?color=yellow&label=twitter%7Cscitools_iris&logo=twitter&style=plastic\"\n     alt=\"twitter scitools_iris\"></a>\n</p>\n\n<p align=\"center\">\nFor documentation see the \n<a href=\"https://scitools-iris.readthedocs.io/en/latest/\">latest</a>  \ndeveloper version or the most recent released\n<a href=\"https://scitools-iris.readthedocs.io/en/stable/\">stable</a> version.\n</p>\n"
 },
 {
  "repo": "team-ocean/veros",
  "language": "Python",
  "readme_contents": "<p align=\"center\">\n<img src=\"doc/_images/veros-logo-400px.png?raw=true\">\n</p>\n\n<p align=\"center\">\n<i>Versatile Ocean Simulation in Pure Python</i>\n</p>\n\n<p align=\"center\">\n  <a href=\"http://veros.readthedocs.io/?badge=latest\">\n    <img src=\"https://readthedocs.org/projects/veros/badge/?version=latest\" alt=\"Documentation status\">\n  </a>\n  <a href=\"https://github.com/team-ocean/veros/actions/workflows/test-all.yml\">\n    <img src=\"https://github.com/team-ocean/veros/actions/workflows/test-all.yml/badge.svg\" alt=\"Test status\">\n  </a>\n  <a href=\"https://codecov.io/gh/team-ocean/veros\">\n    <img src=\"https://codecov.io/gh/team-ocean/veros/branch/master/graph/badge.svg\" alt=\"Code Coverage\">\n  </a>\n  <a href=\"https://zenodo.org/badge/latestdoi/87419383\">\n    <img src=\"https://zenodo.org/badge/87419383.svg\" alt=\"DOI\">\n  </a>\n</p>\n\nVeros, *the versatile ocean simulator*, aims to be the swiss army knife of ocean modeling. It is a full-fledged primitive equation ocean model that supports anything between idealized toy models and [realistic, high-resolution, global ocean simulations](https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2021MS002717). And because Veros is written in pure Python, the days of struggling with complicated model setup workflows, ancient programming environments, and obscure legacy code are finally over.\n\n*In a nutshell, we want to enable high-performance ocean modelling with a clear focus on flexibility and usability.*\n\nVeros supports a NumPy backend for small-scale problems, and a\nhigh-performance [JAX](https://github.com/google/jax) backend\nwith CPU and GPU support. It is fully parallelized via MPI and supports\ndistributed execution on any number of nodes, including multi-GPU architectures (see also [our benchmarks](https://veros.readthedocs.io/en/latest/more/benchmarks.html)).\n\nThe dynamical core of Veros is based on [pyOM2](https://wiki.cen.uni-hamburg.de/ifm/TO/pyOM2), an ocean model with a Fortran backend and Fortran and Python frontends.\n\nTo learn more about Veros, make sure to [visit our documentation](https://veros.readthedocs.io/en/latest/).\n\n#### How about a demonstration?\n\n<p align=\"center\">\n  <a href=\"https://vimeo.com/391237951\">\n      <img src=\"doc/_images/veros-preview.gif?raw=true\" alt=\"0.25\u00d70.25\u00b0 high-resolution model spin-up\">\n  </a>\n</p>\n\n<p align=\"center\">\n(0.25\u00d70.25\u00b0 high-resolution model spin-up, click for better\nquality)\n</p>\n\n## Features\n\nVeros provides\n\n-   a fully staggered **3-D grid geometry** (*C-grid*)\n-   support for both **idealized and realistic configurations** in\n    Cartesian or pseudo-spherical coordinates\n-   several **friction and advection schemes**\n-   isoneutral mixing, eddy-kinetic energy, turbulent kinetic energy,\n    and internal wave energy **parameterizations**\n-   several **pre-implemented diagnostics** such as energy fluxes,\n    variable time averages, and a vertical overturning stream function\n    (written to netCDF4 output)\n-   **pre-configured idealized and realistic set-ups** that are ready to\n    run and easy to adapt\n-   **accessibility and extensibility** - thanks to the\n    power of Python!\n\n## Veros for the impatient\n\nA minimal example to install and run Veros:\n\n```bash\n$ pip install veros\n$ veros copy-setup acc --to /tmp/acc\n$ veros run /tmp/acc/acc.py\n```\n\nFor more detailed installation instructions, have a look at [our\ndocumentation](https://veros.readthedocs.io).\n\n## Basic usage\n\nTo run Veros, you need to set up a model --- i.e., specify which settings\nand model domain you want to use. This is done by subclassing the\n`VerosSetup` base class in a *setup script* that is written in Python. You\nshould use the `veros copy-setup` command to copy one into your current\nfolder. A good place to start is the\n[ACC model](https://github.com/team-ocean/veros/blob/master/veros/setups/acc/acc.py):\n\n```bash\n$ veros copy-setup acc\n```\n\nAfter setting up your model, all you need to do is call the `setup` and\n`run` methods on your setup class. The pre-implemented setups can all be\nexecuted via `veros run`:\n\n```bash\n$ veros run acc.py\n```\n\nFor more information on using Veros, have a look at [our\ndocumentation](http://veros.readthedocs.io).\n\n## Contributing\n\nContributions to Veros are always welcome, no matter if you spotted an\ninaccuracy in [the documentation](https://veros.readthedocs.io), wrote a\nnew setup, fixed a bug, or even extended Veros\\' core mechanics. There\nare 2 ways to contribute:\n\n1.  If you want to report a bug or request a missing feature, please\n    [open an issue](https://github.com/team-ocean/veros/issues). If you\n    are reporting a bug, make sure to include all relevant information\n    for reproducing it (ideally through a *minimal* code sample).\n2.  If you want to fix the issue yourself, or wrote an extension for\n    Veros - great! You are welcome to submit your code for review by\n    committing it to a repository and opening a [pull\n    request](https://github.com/team-ocean/veros/pulls). However,\n    before you do so, please check [the contribution\n    guide](http://veros.readthedocs.io/quickstart/get-started.html#enhancing-veros)\n    for some tips on testing and benchmarking, and to make sure that\n    your modifications adhere with our style policies. Most importantly,\n    please ensure that you follow the [PEP8\n    guidelines](https://www.python.org/dev/peps/pep-0008/), use\n    *meaningful* variable names, and document your code using\n    [Google-style\n    docstrings](http://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html).\n\n## How to cite\n\nIf you use Veros in scientific work, please consider citing [the following publication](https://gmd.copernicus.org/articles/11/3299/2018/):\n\n```bibtex\n@article{hafner_veros_2018,\n\ttitle = {Veros v0.1 \u2013 a fast and versatile ocean simulator in pure {Python}},\n\tvolume = {11},\n\tissn = {1991-959X},\n\turl = {https://gmd.copernicus.org/articles/11/3299/2018/},\n\tdoi = {10.5194/gmd-11-3299-2018},\n\tnumber = {8},\n\tjournal = {Geoscientific Model Development},\n\tauthor = {H\u00e4fner, Dion and Jacobsen, Ren\u00e9 L\u00f8we and Eden, Carsten and Kristensen, Mads R. B. and Jochum, Markus and Nuterman, Roman and Vinter, Brian},\n\tmonth = aug,\n\tyear = {2018},\n\tpages = {3299--3312},\n}\n```\n\nOr have a look at [our documentation](https://veros.readthedocs.io/en/latest/more/publications.html)\nfor more publications involving Veros.\n"
 },
 {
  "repo": "matplotlib/cmocean",
  "language": "Python",
  "readme_contents": "cmocean\n=======\n\nDocumentation available: [http://matplotlib.org/cmocean/](http://matplotlib.org/cmocean/).\n\nWe have a paper with guidelines to colormap selection for your application and a description of the `cmocean` colormaps:\nThyng, K. M., Greene, C. A., Hetland, R. D., Zimmerle, H. M., & DiMarco, S. F. (2016). True colors of oceanography. Oceanography, 29(3), 10.\nlink: [http://tos.org/oceanography/assets/docs/29-3_thyng.pdf](http://tos.org/oceanography/assets/docs/29-3_thyng.pdf)\n\nBesides Python, the cmocean colormaps are also available:\n\n* For [MATLAB](http://www.mathworks.com/matlabcentral/fileexchange/57773-cmocean-perceptually-uniform-colormaps) by [Chad Greene](http://www.chadagreene.com/)\n* For [R cmocean](https://github.com/aitap/cmocean), which includes [ggplot2](ggplot2.tidyverse.org/) compatible functions. Also included in [Oce](http://dankelley.github.io/oce/): an oceanographic analysis package by [Dan Kelley](http://www.dal.ca/faculty/science/oceanography/people/faculty/daniel-e-kelley.html) and [Clark Richards](http://clarkrichards.org/).\n* For Julia, included in [Plots.jl](https://github.com/JuliaPlots/Plots.jl) and [Makie.jl](https://github.com/JuliaPlots/Makie.jl)\n* For [Ocean Data Viewer](https://github.com/kthyng/cmocean-odv)\n* For Generic Mapping Tools (GMT)  at [cpt-city](http://soliton.vm.bytemark.co.uk/pub/cpt-city/cmocean/index.html) and on [github](https://github.com/kthyng/cmocean-gmt)\n* For [Paraview](https://github.com/kthyng/cmocean-paraview) inspired by [Phillip Wolfram](https://github.com/pwolfram)\n* In [Plotly](https://plot.ly/python/cmocean-colorscales/)\n* Chad Greene's [Antarctic Mapping Tools](http://www.mathworks.com/matlabcentral/fileexchange/47638-antarctic-mapping-tools) in Matlab uses `cmocean`\n* For [Tableau](https://www.tableau.com) as a preferences file on [github](https://github.com/shaunwbell/cmocean_tableau)\n* For [ImageJ](https://imagej.nih.gov/ij/) as [LUTs](https://github.com/mikeperrins/cmocean-LUT-ImageJ)\n* For [ncview](http://meteora.ucsd.edu/~pierce/ncview_home_page.html) via [ncmaps](https://github.com/TomLav/ncmaps).\n* For [SeaDAS](https://github.com/gertvd-stanford/cmocean_seadas), and should work with BEAM/SNAP as well.\n\n\nTo install:\n``pip install cmocean``\n\nTo install with Anaconda:\n``conda install -c conda-forge cmocean``\n\nIf you want to be able to use the `plots` submodule, you can instead install with:\n\n`pip install \"cmocean[plots]\"`\n\nwhich will also install `viscm` and `colorspacious`.\n"
 },
 {
  "repo": "VIAME/VIAME",
  "language": "Python",
  "readme_contents": "\n<img src=\"http://www.viametoolkit.org/wp-content/uploads/2016/08/viami_logo.png\" alt=\"VIAME Logo\" width=\"200\" height=\"78\">\n\nVIAME is a computer vision application designed for do-it-yourself artificial intelligence including\nobject detection, object tracking, image/video annotation, image/video search, image mosaicing,\nsize measurement, rapid model generation, and tools for the evaluation of different algorithms.\nBoth a desktop and web version exist for deployments in different types of environments, with\nan open annotation archive and example of the web platform available at\n[viame.kitware.com](https://viame.kitware.com). Originally targetting marine species analytics,\nVIAME now contains many common algorithms and libraries, and is also useful as a generic computer\nvision toolkit. It contains a number of standalone tools for accomplishing the above, a pipeline\nframework which can connect C/C++, python, and matlab nodes together in a multi-threaded fashion,\nand, lastly, multiple algorithms resting on top of the pipeline infrastructure.\n\n\nDocumentation\n-------------\n\nThe [User's Quick-Start Guide](https://data.kitware.com/api/v1/item/5fdaf1dd2fa25629b99843f8/download),\n[Tutorial Videos](https://www.youtube.com/channel/UCpfxPoR5cNyQFLmqlrxyKJw), \nand [Developer's Manual](http://viame.readthedocs.io/en/latest/) are more comprehensive,\nbut select entries are also listed below broken down by individual functionality:\n\n\n[Documentation Overview](https://viame.readthedocs.io/en/latest/section_links/documentation_overview.html) <>\n[Install or Build Instructions](examples/building_and_installing_viame) <>\n[All Examples](https://github.com/Kitware/VIAME/tree/master/examples) <>\n[DIVE Interface](https://kitware.github.io/dive) <>\n[VIEW Interface](examples/annotation_and_visualization) <>\n[Search and Rapid Model Generation](examples/search_and_rapid_model_generation) <>\n[Object Detector CLI](examples/object_detection) <>\n[Object Tracker CLI](examples/object_tracking) <>\n[Detector Training CLI](examples/object_detector_training) <>\n[Evaluation of Detectors](examples/scoring_and_roc_generation) <>\n[Detection File Formats](https://viame.readthedocs.io/en/latest/section_links/detection_file_conversions.html) <>\n[Calibration and Image Enhancement](examples/image_enhancement) <>\n[Registration and Mosaicing](examples/image_registration)  <>\n[Stereo Measurement and Depth Maps](examples/measurement_using_stereo) <>\n[Pipelining Overview](https://github.com/Kitware/kwiver) <>\n[Core Class and Pipeline Info](http://kwiver.readthedocs.io/en/latest/architecture.html) <>\n[Plugin Integration](examples/hello_world_pipeline) <>\n[Example Plugin Templates](plugins/templates) <>\n[Embedding Algorithms in C++](examples/using_detectors_in_cxx_code)\n\nInstallations\n-------------\n\nFor a full installation guide and description of the various flavors of VIAME, see the\nquick-start guide, above. The full desktop version is provided as either a .msi, .zip or\n.tar file. Alternatively, standalone annotators (without any processing algorithms)\nare available via smaller installers. Lastly, docker files are available for both VIAME\nDesktop and Web (below). For full desktop installs, extract the binaries and place them\nin a directory of your choosing, for example /opt/noaa/viame on Linux\nor C:\\Program Files\\VIAME on Windows. If using packages built with GPU support, make sure\nto have sufficient video drivers installed, version 465.19 or higher. The best way to\ninstall drivers depends on your operating system. This isn't required if just using\nmanual annotators (or frame classifiers only). The binaries are quite large,\nin terms of disk space, due to the inclusion of multiple default model files and\nprograms, but if just building your desired features from source (e.g. for embedded\napps) they are much smaller.\n\n**Installation Requirements:** <br>\n* Up to 8 Gb of Disk Space for the Full Installation <br>\n* Windows 7\\*, 8, 10, or 11 (64-Bit) or Linux (64-Bit, e.g. RHEL, CentOS, Ubuntu) <br>\n  * Windows 7 requires some updates and service packs installed, e.g. [KB2533623](https://www.microsoft.com/en-us/download/details.aspx?id=26764). <br>\n  * MacOS is currently only supported running standalone annotation tools, see below.\n\n**Installation Recommendations:** <br>\n* NVIDIA Drivers (Version 465.19 or above, \nWindows \n[\\[1\\]](https://www.nvidia.com/Download/index.aspx?lang=en-us)\n[\\[2\\]](https://developer.nvidia.com/cuda-downloads)\nUbuntu \n[\\[1\\]](https://linuxhint.com/ubuntu_nvidia_ppa/)\n[\\[2\\]](https://developer.nvidia.com/cuda-downloads)\nCentOS \n[\\[1\\]](https://developer.nvidia.com/cuda-downloads)\n[\\[2\\]](https://www.nvidia.com/Download/index.aspx?lang=en-us)) <br>\n* A [CUDA-enabled GPU](https://developer.nvidia.com/cuda-gpus) with 8 Gb or more VRAM <br>\n\n**Windows Full Desktop Binaries:** <br>\n* VIAME v0.19.4 Windows, GPU Enabled, Wizard (.msi) (Coming Soon...) <br>\n* [VIAME v0.19.4 Windows, GPU Enabled, Mirror1 (.zip)](https://drive.google.com/file/d/1XaKWnaL3K-j98Yvk3diUYEcOC3mOJglj/view?usp=sharing) <br>\n* [VIAME v0.19.4 Windows, GPU Enabled, Mirror2 (.zip)](https://data.kitware.com/api/v1/item/62d580febddec9d0c45bf62f/download) <br>\n* [VIAME v0.19.4 Windows, CPU Only, Mirror1 (.zip)](https://drive.google.com/file/d/106BCdISlTn7FOtd2H38wJ43B5pgAclKZ/view?usp=sharing) <br>\n* [VIAME v0.19.4 Windows, CPU Only, Mirror2 (.zip)](https://data.kitware.com/api/v1/item/62d58096bddec9d0c45bf506/download)\n\n**Linux Full Desktop Binaries:** <br>\n* [VIAME v0.19.4 Linux, GPU Enabled, Mirror1 (.tar.gz)](https://drive.google.com/file/d/1UWxH-m8VPxYMKYuTMMs_ICFwryM92Bhe/view?usp=sharing) <br>\n* [VIAME v0.19.4 Linux, GPU Enabled, Mirror2 (.tar.gz)](https://data.kitware.com/api/v1/item/62d6d727bddec9d0c471a305/download) <br>\n* [VIAME v0.19.4 Linux, CPU Only, Mirror1 (.tar.gz)](https://drive.google.com/file/d/1Ar5nKs9I1lmGSQ2Zd4xxMWu02cThreLD/view?usp=sharing) <br>\n* [VIAME v0.19.4 Linux, CPU Only, Mirror2 (.tar.gz)](https://data.kitware.com/api/v1/item/62d6d5f1bddec9d0c47147fb/download)\n\n**Web Applications**: <br>\n* [VIAME Online Web Annotator and Public Annotation Archive](https://viame.kitware.com/) <br>\n* [VIAME Web Local Installation Instructions](https://kitware.github.io/dive/Deployment-Overview/) <br>\n* [VIAME Web Source Repository](https://github.com/Kitware/dive)\n\n**DIVE Standalone Desktop Annotator:** <br>\n* [DIVE Installers (Linux, Mac, Windows)](https://github.com/Kitware/dive/releases/tag/1.8.0-beta.3)\n\n**SEAL Standalone Desktop Annotator:** <br>\n* [SEAL Windows 7/8/10, GPU Enabled (.zip)](https://data.kitware.com/api/v1/item/602296172fa25629b95482f6/download) <br>\n* [SEAL Windows 7/8/10, CPU Only (.zip)](https://data.kitware.com/api/v1/item/602295642fa25629b9548196/download) <br>\n* [SEAL CentOS 7, GPU Enabled (.tar.gz)](https://data.kitware.com/api/v1/item/6023362a2fa25629b957c365/download) <br>\n* [SEAL Generic Linux, GPU Enabled (.tar.gz)](https://data.kitware.com/api/v1/item/6023359c2fa25629b957c2f3/download)\n\n**Optional Add-Ons and Model Files:** <br>\n* [Arctic Seals Models, Windows](https://data.kitware.com/api/v1/item/5e30b8ffaf2e2eed3545bff6/download) <br>\n* [Arctic Seals Models, Linux](https://data.kitware.com/api/v1/item/5e30b283af2e2eed3545a888/download) <br>\n* [EM Tuna Detectors, All OS](https://viame.kitware.com/api/v1/item/627b326cc4da86e2cd3abb5b/download) <br>\n* [HabCam Models (Scallop, Skate, Flatfish), All OS](https://viame.kitware.com/api/v1/item/627b145487bad2e19a4c4697/download) <br>\n* [Motion Detector Model, All OS](https://viame.kitware.com/api/v1/item/627b326fea630db5587b577b/download) <br>\n* [MOUSS Deep 7 Bottomfish Models, All OS](https://viame.kitware.com/api/v1/item/627b3282c4da86e2cd3abb5d/download) <br>\n* [Penguin Head FF Models, All OS](https://viame.kitware.com/api/v1/item/627b3289ea630db5587b577d/download) <br>\n* [Sea Lion Models, All OS](https://viame.kitware.com/api/v1/item/62d5da95fd05facb6e178bdd/download) <br>\n* [SEFSC 100-200 Class Fish Models, All OS](https://viame.kitware.com/api/v1/item/627b32b1994809b024f207a7/download)\n\nNote: To install Add-Ons and Patches, copy them into an existing VIAME installation folder.\nFolders should match, for example, the Add-On packages contains a 'configs' folder, and the\nmain installation also contains a 'configs' folder so they should just be merged.\n\n\nDocker Images\n-------------\n\nDocker images are available on: https://hub.docker.com. For a default container with just core\nalgorithms, runnable via command-line, see:\n\nkitware/viame:gpu-algorithms-latest\n\nThis image is headless (ie, it contains no GUI) and contains a VIAME desktop (not web)\ninstallation in the folder /opt/noaa/viame. For links to the VIAME-Web docker containers see the\nabove section in the installation documentation. Most add-on models are not included in the\ninstance but can be downloaded via running the script download_viame_addons.sh in the bin folder.\n\nQuick Build Instructions\n------------------------\n\nThese instructions are intended for developers or those interested in building the latest master\nbranch. More in-depth build instructions can be found [here](examples/building_and_installing_viame),\nbut the software can be built either as a super-build, which builds most of its dependencies\nalongside itself, or standalone. To build VIAME requires, at a minimum, [Git](https://git-scm.com/),\n[CMake](https://cmake.org/), and a [C++ compiler](http://www.cplusplus.com/doc/tutorial/introduction/).\nInstalling Python and CUDA is also recommended. If using CUDA, versions 11.5, 11.3, or 10.2 are\npreferred, with CUDNN 8. Other CUDA or CUDNN versions may or may not work. On both Windows and Linux\nit is also recommended to use [Anaconda3 2021.05](https://repo.anaconda.com/archive/) for python,\nwhich is the most tested distribution used by developers. If using other python distributions,\nat a minimum Python3.7 or above, Numpy, and Cython is necessary.\n\nTo build on the command line in Linux, use the following commands, only replacing [source-directory]\nand [build-directory] with locations of your choice. While these directories can be the same,\nit's good practice to have a 'src' checkout then a seperate 'build' directory alongside it:\n\n\tgit clone https://github.com/VIAME/VIAME.git [source-directory]\n\n\tcd [source-directory] && git submodule update --init --recursive\n\nNext, create a build directory and run the following `cmake` command (or alternatively\nuse the cmake GUI if you are not using the command line interface):\n\n\tmkdir [build-directory] && cd [build-directory]\n\n\tcmake -DCMAKE_BUILD_TYPE:STRING=Release [source-directory]\n\nOnce your `cmake` command has completed, you can configure any build flags you want\nusing 'ccmake' or the cmake GUI, and then build with the following command on Linux:\n\n\tmake -j8\n\nOr alternatively by building it in Visual Studio or your compiler of choice on\nWindows. On Linux, '-j8' tells the build to run multi-threaded using 8 threads, this\nis useful for a faster build though if you get an error it can be difficult to see\nit, in which case running just 'make' might be more helpful. For Windows,\ncurrently VS2019 is the most tested compiler.\n\nThere are several optional arguments to viame which control which plugins get built,\nsuch as those listed below. If a plugin is enabled that depends on another dependency\nsuch as OpenCV) then the dependency flag will be forced to on. If uncertain what to turn\non, it's best to just leave the default enable and disable flags which will build most\n(though not all) functionalities. These are core components we recommend leaving turned on:\n\n\n<center>\n\n| Flag                         | Description                                                                    |\n|------------------------------|--------------------------------------------------------------------------------|\n| VIAME_ENABLE_OPENCV          | Builds OpenCV and basic OpenCV processes (video readers, simple GUIs)          |\n| VIAME_ENABLE_VXL             | Builds VXL and basic VXL processes (video readers, image filters)              |\n| VIAME_ENABLE_PYTHON          | Turns on support for using python processes (multiple algorithms)              |\n| VIAME_ENABLE_PYTORCH         | Installs all pytorch processes (detectors, trackers, classifiers)              |\n\n</center>\n\n\nAnd a number of flags which control which system utilities and optimizations are built, e.g.:\n\n\n<center>\n\n| Flag                         | Description                                                                    |\n|------------------------------|--------------------------------------------------------------------------------|\n| VIAME_ENABLE_CUDA            | Enables CUDA (GPU) optimizations across all packages                           |\n| VIAME_ENABLE_CUDNN           | Enables CUDNN (GPU) optimizations across all processes                         |\n| VIAME_ENABLE_DIVE            | Enables DIVE GUI (annotation and training on multiple sequences)               |\n| VIAME_ENABLE_VIVIA           | Builds VIVIA GUIs (VIEW and SEARCH for annotation and video search)            |\n| VIAME_ENABLE_KWANT           | Builds KWANT detection and track evaluation (scoring) tools                    |\n| VIAME_ENABLE_DOCS            | Builds Doxygen class-level documentation (puts in install tree)                |\n| VIAME_BUILD_DEPENDENCIES     | Build VIAME as a super-build, building all dependencies (default)              |\n| VIAME_INSTALL_EXAMPLES       | Installs examples for the above modules into install/examples tree             |\n| VIAME_DOWNLOAD_MODELS        | Downloads pre-trained models for use with the examples and interfaces          |\n\n</center>\n\n\nAnd lastly, a number of flags which build algorithms or interfaces with more specialized functionality:\n\n\n<center>\n\n| Flag                         | Description                                                                    |\n|------------------------------|--------------------------------------------------------------------------------|\n| VIAME_ENABLE_TENSORFLOW      | Builds TensorFlow object detector plugin                                       |\n| VIAME_ENABLE_DARKNET         | Builds Darknet (YOLO) object detector plugin                                   |\n| VIAME_ENABLE_TENSORRT        | Builds TensorRT object detector plugin                                         |\n| VIAME_ENABLE_BURNOUT         | Builds Burn-Out based pixel classifier plugin                                  |\n| VIAME_ENABLE_SMQTK           | Builds SMQTK plugins to support image/video indexing and search                |\n| VIAME_ENABLE_SCALLOP_TK      | Builds Scallop-TK based object detector plugin                                 |\n| VIAME_ENABLE_SEAL            | Builds Seal multi-modality GUI                                                 |\n| VIAME_ENABLE_ITK             | Builds ITK cross-modality image registration                                   |\n| VIAME_ENABLE_UW_CLASSIFIER   | Builds UW fish classifier plugin                                               |\n| VIAME_ENABLE_MATLAB          | Turns on support for and installs all matlab processes                         |\n| VIAME_ENABLE_LANL            | Builds an additional (Matlab) scallop detector                                 |\n\n</center>\n\n\nSource Code Layout\n------------------\n<pre>\n VIAME\n   \u251c\u2500\u2500 cmake               # CMake configuration files for subpackages\n   \u251c\u2500\u2500 docs                # Documentation files and manual (pre-compilation)\n   \u251c\u2500\u2500 configs             # All system-runnable config files and models\n   \u2502   \u251c\u2500\u2500 pipelines       # All processing pipeline configs\n   \u2502   \u2502   \u2514\u2500\u2500 models      # All models, which only get downloaded based on flags\n   \u2502   \u251c\u2500\u2500 prj-linux       # Default linux project files\n   \u2502   \u2514\u2500\u2500 prj-windows     # Default windows project files \n   \u251c\u2500\u2500 examples            # All runnable examples and example tutorials\n   \u251c\u2500\u2500 packages            # External projects used by the system\n   \u2502   \u251c\u2500\u2500 kwiver          # Processing backend infastructure\n   \u2502   \u251c\u2500\u2500 fletch          # Dependency builder for things which don't change often\n   \u2502   \u251c\u2500\u2500 kwant           # Scoring and detector evaluation tools\n   \u2502   \u251c\u2500\u2500 vivia           # Baseline desktop GUIs (v1.0)\n   \u2502   \u2514\u2500\u2500 ...             # Assorted other packages (typically for algorithms)\n   \u251c\u2500\u2500 plugins             # Integrated algorithms or wrappers around external projects\n   \u2502   \u2514\u2500\u2500 ...             # Assorted plugins (detectors, depth maps, filters, etc.)\n   \u251c\u2500\u2500 tools               # Standalone tools or scripts, often building on the above\n   \u2514\u2500\u2500 README.md           # Project introduction page that you are reading\n   \u2514\u2500\u2500 RELEASE_NOTES.md    # A list of the latest updates in the system per version\n</pre>\n\n\nUpdate Instructions\n-------------------\n\nIf you already have a checkout of VIAME and want to switch branches or\nupdate your code, it is important to re-run:\n\n\tgit submodule update --init --recursive\n\nAfter switching branches to ensure that you have on the correct hashes\nof sub-packages within the build. Very rarely you may also need to run:\n\n\tgit submodule sync\n\nJust in case the address of submodules has changed. You only need to\nrun this command if you get a \"cannot fetch hash #hashid\" error.\n\n\nLicense, Citations, and Acknowledgements\n----------------------------------------\n\nVIAME is released under a BSD-3 license.\n\nA non-exhaustive list of relevant papers used within the project alongside contributors\ncan be found [here](docs/citations.md).\n\nVIAME was developed with funding from multiple sources, with special thanks\nto those listed [here](docs/acknowledgements.md).\n"
 },
 {
  "repo": "dankelley/oce",
  "language": "R",
  "readme_contents": "\n\n\n# oce <img src=\"https://raw.githubusercontent.com/dankelley/oce/develop/oce-logo-3.png\" align=\"right\" height=\"95\" />\n\n<!-- badges: start -->\n\n[![Project Status: Active \u2013 The project has reached a stable, usable\nstate and is being actively\ndeveloped.](http://www.repostatus.org/badges/latest/active.svg)](https://www.repostatus.org/)\n[![status](https://joss.theoj.org/papers/10.21105/joss.03594/status.svg)](https://joss.theoj.org/papers/10.21105/joss.03594)\n[![R-CMD-check](https://github.com/dankelley/oce/workflows/R-CMD-check/badge.svg)](https://github.com/dankelley/oce/actions)\n[![Codecov test\ncoverage](https://codecov.io/gh/dankelley/oce/branch/develop/graph/badge.svg)](https://app.codecov.io/gh/dankelley/oce?branch=develop)\n[![CRAN\\_Status\\_Badge](https://www.r-pkg.org/badges/version/oce)](https://cran.r-project.org/package=oce)\n![RStudio CRAN mirror\ndownloads](https://cranlogs.r-pkg.org/badges/last-month/oce) ![RStudio\nCRAN mirror downloads](https://cranlogs.r-pkg.org/badges/last-week/oce)\n![RStudio CRAN mirror\ndownloads](https://cranlogs.r-pkg.org/badges/last-day/oce)\n<!-- badges: end -->\n\n## Why use R for oceanographic analysis?\n\nThe R language is popular in many branches of science, and Oceanography is no\nexception. With its broad statistical support, R is a natural choice for\noceanographers in the biological, chemical and geological sub-disciplines.\nHowever, some physical oceanographers have remained attached to Matlab, which\nwas widely adopted during the 1990s. Lately, this has been changing, as\noceanographers turn to open-source systems such as Python and R. A particular\nstrength of R is its provision of many powerful and well-vetted packages for\nhandling specialized calculations. The oce package is a prime example.\n\n## About oce\n\nThe oce package handles a wide variety of tasks that come up in the analysis of\nOceanographic data. In addition to the present README file, a brief sketch of\nthe package has been written by the core developers (Kelley Dan E., Clark\nRichards and Chantelle Layton, 2022. [oce: an R package for Oceanographic\nAnalysis](https://doi.org/10.21105/joss.03594). Journal of Open Source\nSoftware, 7(71), 3594), and the primary developer uses the package extensively\nin his book about the place of R in oceanographic analysis\n(Kelley, Dan E., 2018.\n[Oceanographic Analysis with R](https://link.springer.com/book/10.1007/978-1-4939-8844-0).\nNew York. Springer-Verlag ISBN 978-1-4939-8844-0).  Details of oce functions\nare provided within the R help system, and in the package\n[webpage](https://dankelley.github.io/oce/).\n\n## Installing oce\n\nStable versions of oce are available from CRAN, and may be installed\nfrom within R, in the same way as other packages. However, the CRAN\nversion is only updated a few times a year (pursuant to policy), so many\nusers install the `\"develop\"` branch instead. This branch may be updated\nseveral times per day, as the authors fix bugs or add features that are\nmotivated by day-to-day usage. This is the branch favoured by users who\nneed new features or who would wish to contribute to Oce development.\n\nThe easy way to install the `\"develop\"` branch is to execute the\nfollowing commands in R.\n\n    remotes::install_github(\"dankelley/oce\", ref=\"develop\")\n\nand most readers should also install Ocedata, with\n\n    remotes::install_github(\"dankelley/ocedata\", ref=\"main\")\n\n## Evolution of oce\n\nOce is emphatically an open-source system, and so the participation of\nusers is very important. This is why Git is used for version control of\nthe Oce source code, and why GitHub is the host for that code. Users are\ninvited to take part in the development process, by suggesting features,\nby reporting bugs, or just by watching as others do such things.\nOceanography is a collaborative discipline, so it makes sense that the\nevolution of Oce be similarly collaborative.\n\n## Examples using built-in datasets\n\n### CTD\n\n    library(oce)\n    data(ctd)\n    plot(ctd, which=c(1,2,3,5), type=\"l\", span=150)\n\n![Sample CTD plot.](https://raw.githubusercontent.com/dankelley/oce/develop/oce-demo-1.png)\n\n\n### Acoustic Doppler profiler\n\n    library(oce)\n    data(adp)\n    plot(adp)\n\n![Sample adp plot.](https://raw.githubusercontent.com/dankelley/oce/develop/oce-demo-2.png)\n\n### Sealevel and tides\n\n    library(oce)\n    data(sealevel)\n    m <- tidem(sealevel)\n    par(mfrow=c(2, 1))\n    plot(sealevel, which=1)\n    plot(m)\n\n![Sample sealevel plot.](https://raw.githubusercontent.com/dankelley/oce/develop/oce-demo-3.png)\n\n### Echosounder\n\n    library(oce)\n    data(echosounder)\n    plot(echosounder, which=2, drawTimeRange=TRUE, drawBottom=TRUE)\n\n![Sample echosounder plot.](https://raw.githubusercontent.com/dankelley/oce/develop/oce-demo-4.png)\n\n### Map\n\n    library(oce)\n    par(mar=rep(0.5, 4))\n    data(endeavour, package=\"ocedata\")\n    data(coastlineWorld, package=\"oce\")\n    mapPlot(coastlineWorld, col=\"gray\")\n    mapPoints(endeavour$longitude, endeavour$latitude, pch=20, col=\"red\")\n\n![Sample map plot.](https://raw.githubusercontent.com/dankelley/oce/develop/oce-demo-5.png)\n\n### Landsat image\n\n    library(ocedata)\n    library(oce)\n    data(landsat)\n    plot(landsat)\n\n![Sample landsat image plot.](https://raw.githubusercontent.com/dankelley/oce/develop/oce-demo-6.png)\n\n"
 },
 {
  "repo": "euroargodev/argopy",
  "language": "Python",
  "readme_contents": "|<img src=\"https://raw.githubusercontent.com/euroargodev/argopy/master/docs/_static/argopy_logo_long.png\" alt=\"argopy logo\" width=\"200\"/><br>``argopy`` is a python library dedicated to Argo data access, visualisation and manipulation for regular users as well as Argo experts and operators|\n|:---------:|\n|[![JOSS](https://img.shields.io/badge/DOI-10.21105%2Fjoss.02425-brightgreen)](//dx.doi.org/10.21105/joss.02425) ![CI](https://github.com/euroargodev/argopy/actions/workflows/pytests.yml/badge.svg) [![codecov](https://codecov.io/gh/euroargodev/argopy/branch/master/graph/badge.svg)](https://codecov.io/gh/euroargodev/argopy) [![Documentation Status](https://img.shields.io/readthedocs/argopy?logo=readthedocs)](https://argopy.readthedocs.io/en/latest/?badge=latest) [![PyPI](https://img.shields.io/pypi/v/argopy)](//pypi.org/project/argopy/)|\n\n### Documentation\n\nThe official documentation is hosted on ReadTheDocs.org: https://argopy.readthedocs.io\n\n### Install\n\nBinary installers for the latest released version are available at the [Python Package Index (PyPI)](https://pypi.org/project/argopy/) and on [Conda](https://anaconda.org/conda-forge/argopy).\n\n```bash\n# conda\nconda install -c conda-forge argopy\n````\n```bash\n# or PyPI\npip install argopy\n````\n\n``argopy`` is continuously tested to work under most OS (Linux, Mac, Windows) and with python versions 3.7 and 3.8.\n\n### Usage\n\n[![badge](https://img.shields.io/static/v1.svg?logo=Jupyter&label=Binder&message=Click+here+to+try+argopy+online+!&color=blue&style=for-the-badge)](https://mybinder.org/v2/gh/euroargodev/binder-sandbox/main?urlpath=git-pull%3Frepo%3Dhttps%253A%252F%252Fgithub.com%252Feuroargodev%252Fargopy%26urlpath%3Dlab%252Ftree%252Fargopy%252Fdocs%252Ftryit.ipynb%26branch%3Dmaster)\n\n```python\n# Import the main fetcher:\nfrom argopy import DataFetcher as ArgoDataFetcher\n```\n```python\n# Define what you want to fetch... \n# a region:\nArgoSet = ArgoDataFetcher().region([-85,-45,10.,20.,0,10.])\n# floats:\nArgoSet = ArgoDataFetcher().float([6902746, 6902747, 6902757, 6902766])\n# or specific profiles:\nArgoSet = ArgoDataFetcher().profile(6902746, 34)\n```\n```python\n# then fetch and get data as xarray datasets:\nds = ArgoSet.load().data\n# or\nds = ArgoSet.to_xarray()\n```\n```python\n# you can even plot some information:\nArgoSet.plot('trajectory')    \n```\n\nThey are many more usages and fine-tuning to allow you to access and manipulate Argo data:\n- [filters at fetch time](https://argopy.readthedocs.io/en/latest/user_mode.html) (standard vs expert users, automatically select QC flags or data mode, ...)\n- [select data sources](https://argopy.readthedocs.io/en/latest/data_sources.html) (erddap, ftp, local, ...)\n- [manipulate data](https://argopy.readthedocs.io/en/latest/data_manipulation.html) (points, profiles, interpolations, binning, ...)\n- [visualisation](https://argopy.readthedocs.io/en/latest/visualisation.html) (trajectories, topography, histograms, ...)\n- [tools for Quality Control](https://argopy.readthedocs.io/en/latest/data_quality_control.html) (OWC, figures, ...)\n- [improve performances](https://argopy.readthedocs.io/en/latest/performances.html) (caching, parallel data fetching)\n\nJust check out [the documentation for more](https://argopy.readthedocs.io) ! \n\n## Development and contributions \n\nSee our development roadmap here: https://github.com/euroargodev/argopy/milestone/3\n\nCheckout [the contribution page](https://argopy.readthedocs.io/en/latest/contributing.html) if you want to get involved and help maintain or develop ``argopy``.\n"
 },
 {
  "repo": "Alexander-Barth/NCDatasets.jl",
  "language": "Julia",
  "readme_contents": "# NCDatasets\n\n[![Build Status](https://github.com/Alexander-Barth/NCDatasets.jl/workflows/CI/badge.svg)](https://github.com/Alexander-Barth/NCDatasets.jl/actions)\n[![codecov.io](http://codecov.io/github/Alexander-Barth/NCDatasets.jl/coverage.svg?branch=master)](http://codecov.io/github/Alexander-Barth/NCDatasets.jl?branch=master)\n[![documentation stable](https://img.shields.io/badge/docs-stable-blue.svg)](https://alexander-barth.github.io/NCDatasets.jl/stable/)\n[![documentation dev](https://img.shields.io/badge/docs-dev-blue.svg)](https://alexander-barth.github.io/NCDatasets.jl/dev/)\n\n\n`NCDatasets` allows one to read and create netCDF files.\nNetCDF data set and attribute list behave like Julia dictionaries and variables like Julia arrays.\n\n\nThe module `NCDatasets` provides support for the following [netCDF CF conventions](http://cfconventions.org/):\n* `_FillValue` will be returned as `missing` ([more information](https://docs.julialang.org/en/v1/manual/missing/))\n* `scale_factor` and `add_offset` are applied if present\n* time variables (recognized by the `units` attribute) are returned as `DateTime` objects.\n* Support of the [CF calendars](http://cfconventions.org/Data/cf-conventions/cf-conventions-1.7/cf-conventions.html#calendar) (standard, gregorian, proleptic gregorian, julian, all leap, no leap, 360 day)\n* The raw data can also be accessed (without the transformations above).\n* [Contiguous ragged array representation](http://cfconventions.org/Data/cf-conventions/cf-conventions-1.7/cf-conventions.html#_contiguous_ragged_array_representation)\n\nOther features include:\n* Support for NetCDF 4 compression and variable-length arrays (i.e. arrays of vectors where each vector can have potentailly a different length)\n* The module also includes an utility function [`ncgen`](https://alexander-barth.github.io/NCDatasets.jl/stable/dataset/#NCDatasets.ncgen) which generates the Julia code that would produce a netCDF file with the same metadata as a template netCDF file.\n\n## Installation\n\nInside the Julia shell, you can download and install the package by issuing:\n\n```julia\nusing Pkg\nPkg.add(\"NCDatasets\")\n```\n\nWindows users are required to pin the version of NetCDF_jll until this [issue](https://github.com/JuliaPackaging/Yggdrasil/issues/4511) is resolved (help is more than welcome).\n\n```julia\nusing Pkg\nPkg.add(\"NetCDF_jll\")\nPkg.pin(name=\"NetCDF_jll\", version=\"400.702.400\")\n```\n\n\n# Manual\n\nThis Manual is a quick introduction in using NCDatasets.jl. For more details you can read the [stable](https://alexander-barth.github.io/NCDatasets.jl/stable/) or [latest](https://alexander-barth.github.io/NCDatasets.jl/latest/) documentation.\n\n* [Explore the content of a netCDF file](#explore-the-content-of-a-netcdf-file)\n* [Load a netCDF file](#load-a-netcdf-file)\n* [Create a netCDF file](#create-a-netcdf-file)\n* [Edit an existing netCDF file](#edit-an-existing-netcdf-file)\n\n## Explore the content of a netCDF file\n\nBefore reading the data from a netCDF file, it is often useful to explore the list of variables and attributes defined in it.\n\nFor interactive use, the following commands (without ending semicolon) display the content of the file similarly to `ncdump -h file.nc`:\n\n```julia\nusing NCDatasets\nds = Dataset(\"file.nc\")\n```\n\nThis creates the central structure of NCDatasets.jl, `Dataset`, which represents the contents of the netCDF file (without immediatelly loading everything in memory). `NCDataset` is an alias for `Dataset`.\n\nThe following displays the information just for the variable `varname`:\n\n```julia\nds[\"varname\"]\n```\n\nwhile to get the global attributes you can do:\n```julia\nds.attrib\n```\nwhich produces a listing like:\n\n```\nDataset: file.nc\nGroup: /\n\nDimensions\n   time = 115\n\nVariables\n  time   (115)\n    Datatype:    Float64\n    Dimensions:  time\n    Attributes:\n     calendar             = gregorian\n     standard_name        = time\n     units                = days since 1950-01-01 00:00:00\n[...]\n```\n\n## Load a netCDF file\n\nLoading a variable with known structure can be achieved by accessing the variables and attributes directly by their name.\n\n```julia\n# The mode \"r\" stands for read-only. The mode \"r\" is the default mode and the parameter can be omitted.\nds = Dataset(\"/tmp/test.nc\",\"r\")\nv = ds[\"temperature\"]\n\n# load a subset\nsubdata = v[10:30,30:5:end]\n\n# load all data\ndata = v[:,:]\n\n# load all data ignoring attributes like scale_factor, add_offset, _FillValue and time units\ndata2 = v.var[:,:]\n\n\n# load an attribute\nunit = v.attrib[\"units\"]\nclose(ds)\n```\n\nIn the example above, the subset can also be loaded with:\n\n```julia\nsubdata = Dataset(\"/tmp/test.nc\")[\"temperature\"][10:30,30:5:end]\n```\n\nThis might be useful in an interactive session. However, the file `test.nc` is not directly closed (closing the file will be triggered by Julia's garbage collector), which can be a problem if you open many files. On Linux the number of opened files is often limited to 1024 (soft limit). If you write to a file, you should also always close the file to make sure that the data is properly written to the disk.\n\nAn alternative way to ensure the file has been closed is to use a `do` block: the file will be closed automatically when leaving the block.\n\n```julia\ndata =\nDataset(filename,\"r\") do ds\n    ds[\"temperature\"][:,:]\nend # ds is closed\n```\n\n## Create a netCDF file\n\nThe following gives an example of how to create a netCDF file by defining dimensions, variables and attributes.\n\n```julia\nusing NCDatasets\nusing DataStructures\n# This creates a new NetCDF file /tmp/test.nc.\n# The mode \"c\" stands for creating a new file (clobber)\nds = Dataset(\"/tmp/test.nc\",\"c\")\n\n# Define the dimension \"lon\" and \"lat\" with the size 100 and 110 resp.\ndefDim(ds,\"lon\",100)\ndefDim(ds,\"lat\",110)\n\n# Define a global attribute\nds.attrib[\"title\"] = \"this is a test file\"\n\n# Define the variables temperature with the attribute units\nv = defVar(ds,\"temperature\",Float32,(\"lon\",\"lat\"), attrib = OrderedDict(\n    \"units\" => \"degree Celsius\"))\n\n# add additional attributes\nv.attrib[\"comments\"] = \"this is a string attribute with Unicode \u03a9 \u2208 \u2211 \u222b f(x) dx\"\n\n# Generate some example data\ndata = [Float32(i+j) for i = 1:100, j = 1:110]\n\n# write a single column\nv[:,1] = data[:,1]\n\n# write a the complete data set\nv[:,:] = data\n\nclose(ds)\n```\n\n\n## Edit an existing netCDF file\n\nWhen you need to modify variables or attributes in a netCDF file, you have\nto open it with the `\"a\"` option. Here, for example, we add a global attribute *creator* to the\nfile created in the previous step.\n\n```julia\nds = Dataset(\"/tmp/test.nc\",\"a\")\nds.attrib[\"creator\"] = \"your name\"\nclose(ds);\n```\n\n\n# Benchmark\n\nThe benchmark loads a variable of the size 1000x500x100 in slices of 1000x500\n(applying the scaling of the CF conventions)\nand computes the maximum of each slice and the average of each maximum over all slices.\nThis operation is repeated 100 times.\nThe code is available at https://github.com/Alexander-Barth/NCDatasets.jl/tree/master/test/perf .\n\n\n| Module           | median | minimum |  mean | std. dev. |\n|:---------------- | ------:| -------:| -----:| ---------:|\n| R-ncdf4          |  0.572 |   0.550 | 0.575 |     0.023 |\n| python-netCDF4   |  0.504 |   0.498 | 0.505 |     0.003 |\n| julia-NCDatasets |  0.228 |   0.212 | 0.226 |     0.005 |\n\nAll runtimes are in seconds.\nJulia 1.6.0 (with NCDatasets b953bf5), R 3.4.4 (with ncdf4 1.17) and Python 3.6.9 (with netCDF4 1.5.4).\nThis CPU is a i7-7700.\n\n\n# Filing an issue\n\nWhen you file an issue, please include sufficient information that would _allow somebody else to reproduce the issue_, in particular:\n1. Provide the code that generates the issue.\n2. If necessary to run your code, provide the used netCDF file(s).\n3. Make your code and netCDF file(s) as simple as possible (while still showing the error and being runnable). A big thank you for the 5-star-premium-gold users who do not forget this point! \ud83d\udc4d\ud83c\udfc5\ud83c\udfc6\n4. The full error message that you are seeing (in particular file names and line numbers of the stack-trace).\n5. Which version of Julia and `NCDatasets` are you using? Please include the output of:\n```\nversioninfo()\nusing Pkg\nPkg.installed()[\"NCDatasets\"]\n```\n6. Does `NCDatasets` pass its test suite? Please include the output of:\n\n```julia\nusing Pkg\nPkg.test(\"NCDatasets\")\n```\n\n# Alternative\n\nThe package [NetCDF.jl](https://github.com/JuliaGeo/NetCDF.jl) from Fabian Gans and contributors is an alternative to this package which supports a more Matlab/Octave-like interface for reading and writing NetCDF files.\n\n# Credits\n\n`netcdf_c.jl`, `build.jl` and the error handling code of the NetCDF C API are from NetCDF.jl by Fabian Gans (Max-Planck-Institut f\u00fcr Biogeochemie, Jena, Germany) released under the MIT license.\n\n<!--  LocalWords:  NCDatasets codecov io NetCDF FillValue DataArrays\n -->\n<!--  LocalWords:  DateTime ncdump nc julia ds Dataset varname attrib\n -->\n<!--  LocalWords:  lon defDim defVar dx subdata println attname jl\n -->\n<!--  LocalWords:  attval filename netcdf API Gans Institut f\u00fcr Jena\n -->\n<!--  LocalWords:  Biogeochemie macOS haskey runnable versioninfo\n -->\n"
 },
 {
  "repo": "chadagreene/CDT",
  "language": "HTML",
  "readme_contents": "[![DOI](https://zenodo.org/badge/171331090.svg)](https://zenodo.org/badge/latestdoi/171331090) [![View Climate Data Toolbox for MATLAB on File Exchange](https://www.mathworks.com/matlabcentral/images/matlab-file-exchange.svg)](https://www.mathworks.com/matlabcentral/fileexchange/70338-climate-data-toolbox-for-matlab)\n\n![Climate Data Toolbox for Matlab](CDT_reduced.jpg)\n\n# CDT Contents and Documentation\n[**Click here**](https://www.chadagreene.com/CDT/CDT_Contents.html) to view the CDT documentation online.\n\nA version of the documentation in Chinese can be found [here](https://github.com/chadagreene/CDT/files/6985814/CDT.pdf), translated by Shi Weiheng. \u4e2d\u6587\u6587\u6863\u7531Shi Weiheng\u7ffb\u8bd1\u3002[CDT \u4e2d\u6587\u7ffb\u8bd1.pdf](https://github.com/chadagreene/CDT/files/6985814/CDT.pdf)\n\n# Video Tutorials\n### Analyzing Trends and Global Warming:\n[![How to analyze trends in sea surface temperature using CDT](https://img.youtube.com/vi/t46dTVp7NHY/0.jpg)](https://www.youtube.com/watch?v=t46dTVp7NHY \"The Climate Data Toolbox for MATLAB - Analyzing Trends & Global Warming\")\n\n### El Ni\u00f1o and Empirical Orthogonal Functions:\n[![How to apply EOFs to identify ENSO](https://img.youtube.com/vi/A5UjLO-67GQ/0.jpg)](https://www.youtube.com/watch?v=A5UjLO-67GQ \"The Climate Data Toolbox for MATLAB - El Ni\u00f1o and Empirical Orthogonal Functions\")\n\n# Installing the toolbox:\nThere are a few different ways to install this toolbox. Pick your favorite among the following:\n\n### ...from the Add-On Explorer in MATLAB: \nIn the Home menu of MATLAB, click Add-Ons, and search for Climate Data Toolbox. Click \"Add from GitHub\" and that's all you need to do. Installing this way is easy and will provide the most up-to-date version available. \n\n### ...or as individual files and folders:\nThe files in this GitHub repository will always be the most up to date. So if you want to be on the bleeding edge of innovation, get the cdt folder, put it somewhere Matlab can find it, and then right-click on it from within Matlab and select \"Add to Path--Selected folder and subfolders.\"\n\n### ...as a .mltbx toolbox: \nInstalling as an .mltbx is perhaps the easiest option, but I don't update the .mltbx very often, so you might not get the latest features and bug fixes. \n\nFirst, download the ~100 MB .mltbx file [here](https://chadagreene.com/ClimateDataToolbox.mltbx). After downloading the .mltbx file, installation should be as easy as double clicking on the zip file and clicking \"install\". Or you can navigate to it in the Matlab file explorer, right click on the .mltbx, and click \"Install.\" \n\nThe installation process puts the files in a folder called something like:\n\n```~MATLAB/Add-Ons/Toolboxes/Climate Data Toolbox/```\n\nIf that's not correct, find the CDT folder by typing this into the Matlab Command Window: \n\n```which cdt -all```\n\nIf the which hunt still turns up nothing, that suggests the toolbox hasn't been properly installed. \n\n\n# After installation:\nType \n\n```cdt```\n\ninto the command line to check out the documentation.\n\n# Citing CDT: \nPlease cite our paper! \n\nChad A. Greene, Kaustubh Thirumalai, Kelly A. Kearney, Jos\u00e9 Miguel Delgado, Wolfgang Schwanghart, Natalie S. Wolfenbarger, Kristen M. Thyng, David E. Gwyther, Alex S. Gardner, and Donald D. Blankenship (2019). The Climate Data Toolbox for MATLAB. _Geochemistry, Geophysics, Geosystems,_ 20, 3774-3781. [doi:10.1029/2019GC008392](https://doi.org/10.1029/2019GC008392)\n\nThe Climate Data Toolbox is also mirrored on the MathWorks File Exchange site [here](https://www.mathworks.com/matlabcentral/fileexchange/70338).\n"
 },
 {
  "repo": "rabernat/intro_to_physical_oceanography",
  "language": "Jupyter Notebook",
  "readme_contents": "# Intro to Physical Oceanography\n\nThis repository contains course materials for EESC4925. The lecture notes are in the form of interactive Jupyter Notebooks.\n\n**New** The class notes are now compiled as a Jupyter Book: http://rabernat.github.io/intro_to_physical_oceanography\n\nGithub source: https://github.com/rabernat/intro_to_physical_oceanography\n"
 },
 {
  "repo": "wafo-project/pywafo",
  "language": "Jupyter Notebook",
  "readme_contents": "\n|wafo_logo|\n==========================================\nWave Analysis for Fatigue and Oceanography\n==========================================\n\n|pkg_img| |tests_img| |docs_img| |health_img| |coverage_img| |versions_img| |downloads_img|\n\n\nDescription\n===========\n\nWAFO is a toolbox Python routines for statistical analysis and simulation of\nrandom waves and random loads. WAFO is freely redistributable software, see WAFO\nicence, cf. the GNU General Public License (GPL) and contain tools for:\n\nFatigue Analysis\n----------------\n\n- Fatigue life prediction for random loads\n- Theoretical density of rainflow cycles\n\nSea modelling\n-------------\n\n- Simulation of linear and non-linear Gaussian waves\n- Estimation of seamodels (spectrums)\n- Joint wave height, wave steepness, wave period distributions\n\nStatistics\n------------\n\n- Extreme value analysis\n- Kernel density estimation\n- Hidden markov models\n\nClasses\n-------\nA short description of the main classes found in WAFO:\n\n\n* TimeSeries:\n    Data analysis of time series. Example: extraction of turning points,\n    estimation of spectrum and covariance function. Estimation transformation\n    used in transformed Gaussian model.\n\n* CovData:\n    Computation of spectral functions, linear and non-linear time series\n    simulation.\n\n* SpecData:\n    Computation of spectral moments and covariance functions, linear and\n    non-linear time series simulation. Ex: common spectra implemented,\n    directional spectra, bandwidth measures, exact distributions for wave\n    characteristics.\n\n* CyclePairs:\n    Cycle counting, discretization, and crossings, calculation of damage.\n    Simulation of discrete Markov chains, switching Markov chains,\n    harmonic oscillator. Ex:  Rainflow cycles and matrix, discretization of\n    loads. Damage of a rainflow count or matrix, damage matrix, S-N plot.\n\n\nSubpackages\n-----------\nA short descriptions the subpackages of WAFO:\n\n* TRANSFORM\n    Modelling with linear or transformed Gaussian waves.\n* STATS\n    Statistical tools and extreme-value distributions. Ex: generation of random\n    numbers, estimation of parameters, evaluation of pdf and cdf\n* KDETOOLS\n    Kernel-density estimation.\n* MISC\n    Miscellaneous routines.\n* DOCS\n    Documentation of toolbox, definitions. An overview is given in the routine\n    wafomenu.\n* DATA\n    Measurements from marine applications.\n\nWAFO homepage: <http://www.maths.lth.se/matstat/wafo/>\nOn the WAFO home page you will find:\n- The WAFO Tutorial\n- List of publications related to WAFO.\n\nInstallation\n============\n\nWAFO contains some Fortran and C extensions that require a properly configured\ncompiler and NumPy/f2py.\n\nCreate a binary wheel package and place it in the dist folder as follows::\n\n    python setup.py bdist_wheel -d dist\n\nAnd install the wheel package with::\n\n    pip install dist/wafo-X.Y.Z+abcd123-os_platform.whl\n\nGetting started\n===============\n\nA quick introduction to some of the many features of wafo can be found in the Tutorial IPython notebooks in the\n`tutorial scripts folder`_:\n\n* Chapter 1 - `Some applications of WAFO`_\n\n* Chapter 2 - `Modelling random loads and stochastic waves`_\n\n* Chapter 3 - `Demonstrates distributions of wave characteristics`_\n\n* Chapter 4 - `Fatigue load analysis and rain-flow cycles`_\n\n* Chapter 5 - `Extreme value analysis`_\n\n-- _tutorial scripts folder: http://nbviewer.jupyter.org/github/wafo-project/pywafo/tree/master/src/wafo/doc/tutorial_scripts/\n\n.. _Some applications of WAFO: http://nbviewer.jupyter.org/github/wafo-project/pywafo/blob/master/src/wafo/doc/tutorial_scripts/WAFO%20Chapter%201.ipynb\n\n.. _Modelling random loads and stochastic waves: http://nbviewer.jupyter.org/github/wafo-project/pywafo/blob/master/src/wafo/doc/tutorial_scripts/WAFO%20Chapter%202.ipynb\n\n.. _Demonstrates distributions of wave characteristics: http://nbviewer.jupyter.org/github/wafo-project/pywafo/blob/master/src/wafo/doc/tutorial_scripts/WAFO%20Chapter%203.ipynb\n\n.. _Fatigue load analysis and rain-flow cycles: http://nbviewer.jupyter.org/github/wafo-project/pywafo/blob/master/src/wafo/doc/tutorial_scripts/WAFO%20Chapter%204.ipynb\n\n.. _Extreme value analysis: http://nbviewer.jupyter.org/github/wafo-project/pywafo/blob/master/src/wafo/doc/tutorial_scripts/WAFO%20Chapter%205.ipynb\n\n\nUnit tests\n==========\n\nTo test if the toolbox is working paste the following in an interactive\npython session::\n\n   import wafo as wf\n   wf.test(coverage=True, doctests=True)\n\n\n.. |wafo_logo| image:: https://github.com/wafo-project/pywafo/blob/master/src/wafo/data/wafoLogoNewWithoutBorder.png\n    :target: https://github.com/wafo-project/pywafo\n\n\n.. |pkg_img| image:: https://badge.fury.io/py/wafo.png\n    :target: https://pypi.python.org/pypi/wafo/\n\n.. |tests_img| image:: https://travis-ci.org/wafo-project/pywafo.svg?branch=master\n    :target: https://travis-ci.org/wafo-project/pywafo\n\n.. |docs_img| image:: https://readthedocs.org/projects/pip/badge/?version=latest\n    :target: http://pywafo.readthedocs.org/en/latest/\n\n.. |health_img| image:: https://codeclimate.com/github/wafo-project/pywafo/badges/gpa.svg\n   :target: https://codeclimate.com/github/wafo-project/pywafo\n   :alt: Code Climate\n\n.. |coverage_img| image:: https://coveralls.io/repos/wafo-project/pywafo/badge.svg?branch=master\n   :target: https://coveralls.io/github/wafo-project/pywafo?branch=master\n\n.. |versions_img| image:: https://img.shields.io/pypi/pyversions/wafo.svg\n   :target: https://github.com/wafo-project/pywafo\n\n\n.. |downloads_img| image:: https://img.shields.io/pypi/dm/wafo.svg\n   :alt: PyPI - Downloads\n\n"
 },
 {
  "repo": "hainegroup/oceanspy",
  "language": "Jupyter Notebook",
  "readme_contents": ".. _readme:\n\n======================================================================================\nOceanSpy - A Python package to facilitate ocean model data analysis and visualization.\n======================================================================================\n\n|OceanSpy|\n\n|version| |conda forge| |docs| |CI| |pre-commit| |codecov| |black| |license| |doi| |JOSS| |binder|\n\n.. admonition:: Interactive Demo\n\n   Check out the interactive demonstration of OceanSpy at `www.bndr.it/nakt7 <https://bndr.it/nakt7>`_\n\nFor publications, please cite the following paper:  \n\nAlmansi, M., R. Gelderloos, T. W. N. Haine, A. Saberi, and A. H. Siddiqui (2019). OceanSpy: A Python package to facilitate ocean model data analysis and visualization. *Journal of Open Source Software*, 4(39), 1506, doi: https://doi.org/10.21105/joss.01506 .\n\nThis material is based upon work supported by the National Science Foundation under Grant Numbers 1835640, 124330, 118123, and 1756863. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.\n\nWhat is OceanSpy?\n-----------------\n**OceanSpy** is an open-source and user-friendly Python package that enables scientists and interested amateurs to analyze and visualize ocean model datasets. \nOceanSpy builds on software packages developed by the Pangeo_ community, in particular xarray_, dask_, and xgcm_. \nThe integration of dask facilitates scalability, which is important for the petabyte-scale simulations that are becoming available. \n\nWhy OceanSpy?\n-------------\nSimulations of ocean currents using numerical circulation models are becoming increasingly realistic.\nAt the same time, these models generate increasingly large volumes of model output data, making the analysis of model data harder.\nUsing OceanSpy, model data can be easily analyzed in the way observational oceanographers analyze field measurements.\n\nHow to use OceanSpy?\n--------------------\nOceanSpy can be used as a standalone package for analysis of local circulation model output, or it can be run on a remote data-analysis cluster, such as the Johns Hopkins University SciServer_ system, which hosts several simulations and is publicly available (see `SciServer Access`_, and `Datasets`_).\n\n.. note::\n\n   OceanSpy has been developed and tested using MITgcm output. However, it is designed to work with any (structured grid) ocean general circulation model. OceanSpy's architecture allows to easily implement model-specific features, such as different grids, numerical schemes for vector calculus, budget closures, and equations of state. We actively seek input and contributions from users of other ocean models (`feedback submission`_).\n\n\n\n\n.. _Pangeo: http://pangeo-data.github.io\n.. _xarray: http://xarray.pydata.org\n.. _dask: https://dask.org\n.. _xgcm: https://xgcm.readthedocs.io\n.. _SciServer: http://www.sciserver.org\n.. _`SciServer Access`: https://oceanspy.readthedocs.io/en/latest/sciserver.html\n.. _Datasets: https://oceanspy.readthedocs.io/en/latest/datasets.html\n.. _`feedback submission`: https://github.com/hainegroup/oceanspy/issues\n\n.. |OceanSpy| image:: https://github.com/hainegroup/oceanspy/raw/main/docs/_static/oceanspy_logo_blue.png\n   :alt: OceanSpy image\n   :target: https://oceanspy.readthedocs.io\n\n.. |version| image:: https://img.shields.io/pypi/v/oceanspy.svg?style=flat\n    :alt: PyPI\n    :target: https://pypi.python.org/pypi/oceanspy\n\n.. |conda forge| image:: https://anaconda.org/conda-forge/oceanspy/badges/version.svg\n   :alt: conda-forge\n   :target: https://anaconda.org/conda-forge/oceanspy\n\n.. |docs| image:: http://readthedocs.org/projects/oceanspy/badge/?version=latest\n    :alt: Documentation\n    :target: http://oceanspy.readthedocs.io/en/latest/?badge=latest\n\n.. |CI| image:: https://img.shields.io/github/workflow/status/hainegroup/oceanspy/CI?logo=github\n    :alt: CI\n    :target: https://github.com/hainegroup/oceanspy/actions\n    \n.. |codecov| image:: https://codecov.io/github/hainegroup/oceanspy/coverage.svg?branch=main\n    :alt: Coverage\n    :target: https://codecov.io/github/hainegroup/oceanspy?branch=main\n\n.. |black| image:: https://img.shields.io/badge/code%20style-black-000000.svg\n    :alt: black\n    :target: https://github.com/psf/black\n\n.. |license| image:: https://img.shields.io/github/license/mashape/apistatus.svg\n   :alt: License\n   :target: https://github.com/hainegroup/oceanspy\n   \n.. |doi| image:: https://zenodo.org/badge/DOI/10.5281/zenodo.3270646.svg\n   :alt: doi\n   :target: https://doi.org/10.5281/zenodo.3270646\n   \n.. |JOSS| image:: http://joss.theoj.org/papers/10.21105/joss.01506/status.svg\n   :alt: JOSS\n   :target: https://doi.org/10.21105/joss.01506\n\n.. |binder| image:: https://mybinder.org/badge_logo.svg\n   :alt: binder\n   :target: https://mybinder.org/v2/gh/hainegroup/oceanspy.git/main?filepath=binder\n\n.. |pre-commit| image:: https://results.pre-commit.ci/badge/github/hainegroup/oceanspy/main.svg\n   :target: https://results.pre-commit.ci/latest/github/hainegroup/oceanspy/main\n   :alt: pre-commit.ci status\n"
 },
 {
  "repo": "nasa/podaacpy",
  "language": "Python",
  "readme_contents": "podaacpy\n========\n\n|DOI| |license| |PyPI| |documentation| |Travis| |Coveralls| |Requirements Status| |Anaconda-Server Version| |Anaconda-Server Downloads| \n\n|DeepSource|\n\n|image7|\n\nA python utility library for interacting with NASA JPL's\n`PO.DAAC <https://podaac.jpl.nasa.gov>`__\n\n\nSoftware DOI\n------------\n\nIf you are using Podaacpy in your research, please consider citing the software |DOI|. This DOI represents all versions, and will always resolve to the latest one. If you wish to reference actual versions, then please find the appropriate DOI's over at Zenodo.\n\n\nWhat is PO.DAAC?\n----------------\n\n| The Physical Oceanography Distributed Active Archive Center (PO.DAAC)\n  is an element of the\n| Earth Observing System Data and Information System\n  (`EOSDIS <https://earthdata.nasa.gov/>`__).\n| The EOSDIS provides science data to a wide community of users for\n  NASA's Science Mission Directorate.\n\nWhat does podaacpy offer?\n-------------------------\n\nThe library provides a Python toolkit for interacting with all\n`PO.DAAC Web Services v3.2.2 APIs <https://podaac.jpl.nasa.gov/ws>`__, namely\n\n-  `PO.DAAC Web Services <https://podaac.jpl.nasa.gov/ws/>`__: services\n   include\n-  `Dataset\n   Metadata <https://podaac.jpl.nasa.gov/ws/search/dataset/index.html>`__\n   - retrieves the metadata of a dataset\n-  `Granule\n   Metadata <https://podaac.jpl.nasa.gov/ws/metadata/granule/index.html>`__\n   - retrieves the metadata of a granule\n-  `Search\n   Dataset <https://podaac.jpl.nasa.gov/ws/search/dataset/index.html>`__\n   - searches PO.DAAC's dataset catalog, over Level 2, Level 3, and\n   Level 4 datasets\n-  `Search\n   Granule <https://podaac.jpl.nasa.gov/ws/search/granule/index.html>`__\n   - does granule searching on PO.DAAC level 2 swath datasets\n   (individual orbits of a satellite), and level 3 & 4 gridded datasets\n   (time averaged to span the globe)\n-  `Image\n   Granule <https://podaac.jpl.nasa.gov/ws/image/granule/index.html>`__ -\n   renders granules in the PO.DAAC's catalog to images such as jpeg\n   and/or png\n-  `Extract\n   Granule <https://podaac.jpl.nasa.gov/ws/extract/granule/index.html>`__\n   - subsets a granule in PO.DAAC catalog and produces either netcdf3 or\n   hdf4 files\n\n-  | `Metadata Compliance\n     Checker <https://podaac-uat.jpl.nasa.gov/mcc>`__: an online tool and\n     web\n   | service designed to check and validate the contents of netCDF and\n     HDF granules for the\n   | Climate and Forecast (CF) and Attribute Convention for Dataset\n     Discovery (ACDD) metadata conventions.\n\n-  | `Level 2 Subsetting \n      <https://podaac-tools.jpl.nasa.gov/hitide/>`__: allows users to subset \n      and download popular PO.DAAC level 2 (swath) datasets.\n\n-  | `PO.DAAC Drive <https://podaac-tools.jpl.nasa.gov/drive/>`__: an HTTP based \n      data access service. PO.DAAC Drive replicates much of the functionality \n      of FTP while addressing many of its issues.\n\nAdditionally, Podaacpy provides the following ocean-related data services \n\n- `NASA OceanColor Web <https://oceancolor.gsfc.nasa.gov>`_:\n\n- `File Search <https://oceandata.sci.gsfc.nasa.gov/api/file_search>`_ -  locate publically available files within the NASA Ocean Data Processing System (ODPS)\n- `Bulk data downloads via HTTP <https://oceancolor.gsfc.nasa.gov/forum/oceancolor/topic_show.pl?pid=12520>`_ - mimic FTP bulk data downloads using the `HTTP-based data distribution server <https://oceandata.sci.gsfc.nasa.gov>`_.\n\nInstallation\n------------\n\nFrom the cheeseshop\n\n::\n\n    pip3 install podaacpy\n    \nor from conda\n\n::\n\n    conda install -c conda-forge podaacpy    \n\nor from source\n\n::\n\n    git clone https://github.com/nasa/podaacpy.git && cd podaacpy\n    python3 setup.py install\n\nQuickstart\n----------\nCheck out the **examples** directory for our Jupyter notebook examples.\n\nTests\n-----\n\n| podaacpy uses the popular\n  `nose <http://nose.readthedocs.org/en/latest/>`__ testing suite for\n  unit tests.\n| You can run the podaacpy tests simply by running\n\n::\n\n    nosetests\n\nAdditonally, click on the build sticker at the top of this readme to be\ndirected to the most recent build on\n`travis-ci <https://travis-ci.org/nasa/podaacpy>`__.\n\nDocumentation\n-------------\n\nYou can view the documentation online at\n\nhttp://podaacpy.readthedocs.org/en/latest/\n\nAlternatively, you can build the documentation manually as follows\n\n::\n\n    cd docs && make html\n\nDocumentation is then available in docs/build/html/\n\nCommunity, Support and Development\n----------------------------------\n\n| Please open a ticket in the `issue\n  tracker <https://github.com/nasa/podaacpy/issues>`__.\n| Please use\n  `labels <https://help.github.com/articles/applying-labels-to-issues-and-pull-requests/>`__\n  to\n| classify your issue.\n\nLicense\n-------\n\n| podaacpy is licensed permissively under the `Apache License\n  v2.0 <http://www.apache.org/licenses/LICENSE-2.0>`__.\n| A copy of that license is distributed with this software.\n\nCopyright and Export Classification\n-----------------------------------\n\n::\n\n    Copyright 2016-2019, by the California Institute of Technology. ALL RIGHTS RESERVED. \n    United States Government Sponsorship acknowledged. Any commercial use must be \n    negotiated with the Office of Technology Transfer at the California Institute \n    of Technology.\n    This software may be subject to U.S. export control laws. By accepting this software, \n    the user agrees to comply with all applicable U.S. export laws and regulations. \n    User has the responsibility to obtain export licenses, or other export authority \n    as may be required before exporting such information to foreign countries or \n    providing access to foreign persons.\n\n.. |DOI| image:: https://zenodo.org/badge/DOI/10.5281/zenodo.1751972.svg\n   :target: https://doi.org/10.5281/zenodo.1751972\n.. |license| image:: https://img.shields.io/github/license/nasa/podaacpy.svg?maxAge=2592000\n   :target: http://www.apache.org/licenses/LICENSE-2.0\n.. |PyPI| image:: https://img.shields.io/pypi/v/podaacpy.svg?maxAge=2592000?style=plastic\n   :target: https://pypi.python.org/pypi/podaacpy\n.. |documentation| image:: https://readthedocs.org/projects/podaacpy/badge/?version=latest\n   :target: http://podaacpy.readthedocs.org/en/latest/\n.. |Travis| image:: https://img.shields.io/travis/nasa/podaacpy.svg?maxAge=2592000?style=plastic\n   :target: https://travis-ci.org/nasa/podaacpy\n.. |Coveralls| image:: https://coveralls.io/repos/github/nasa/podaacpy/badge.svg?branch=master\n   :target: https://coveralls.io/github/nasa/podaacpy?branch=master\n.. |Requirements Status| image:: https://requires.io/github/nasa/podaacpy/requirements.svg?branch=master\n   :target: https://requires.io/github/nasa/podaacpy/requirements/?branch=master\n.. |Anaconda-Server Version| image:: https://anaconda.org/conda-forge/podaacpy/badges/version.svg\n   :target: https://anaconda.org/conda-forge/podaacpy\n.. |Anaconda-Server Downloads| image:: https://anaconda.org/conda-forge/podaacpy/badges/downloads.svg\n   :target: https://anaconda.org/conda-forge/podaacpy\n.. |image7| image:: https://podaac.jpl.nasa.gov/sites/default/files/image/custom_thumbs/podaac_logo.png\n.. |DeepSource| image:: https://static.deepsource.io/deepsource-badge-light.svg\n    :target: https://deepsource.io/gh/nasa/podaacpy/?ref=repository-badge\n\n"
 },
 {
  "repo": "miniufo/xgrads",
  "language": "Jupyter Notebook",
  "readme_contents": "# xgrads\n\n[![DOI](https://zenodo.org/badge/244529165.svg)](https://zenodo.org/badge/latestdoi/244529165)\n![GitHub](https://img.shields.io/github/license/miniufo/xgrads)\n[![Documentation Status](https://readthedocs.org/projects/xgrads/badge/?version=latest)](https://xgrads.readthedocs.io/en/latest/?badge=latest)\n\n\n![3D plot](https://raw.githubusercontent.com/miniufo/xgrads/master/pics/3D.png)\n\n\n## 1. Introduction\nThe Grid Analysis and Display System ([GrADS](http://cola.gmu.edu/grads/) or [OpenGrADS](http://www.opengrads.org/)) is a widely used software for easy access, manipulation, and visualization of earth science data.  It uses a [descriptor (or control) file with a suffix `.ctl`](http://cola.gmu.edu/grads/gadoc/descriptorfile.html) to  describe a raw binary 4D dataset.  The `ctl` file is similar to the header information of a [NetCDF](https://www.unidata.ucar.edu/software/netcdf/docs/file_structure_and_performance.html) file, containing all the information about dimensions, attributes, and variables except for the variable data.\n\nThis python package [`xgrads`](https://github.com/miniufo/xgrads) is designed for parse and read the `.ctl` file commonly used by [GrADS](http://cola.gmu.edu/grads/).  Right now it can parse various kinds of `.ctl` files.  However, only the commonly used raw binary 4D datasets can be read using [`dask`](https://dask.org/) and return as a [`xarray.Dataset`](http://xarray.pydata.org/en/stable/)  Other types of binary data, like `dtype` is `station` or`grib`, may be supported in the future.\n\n---\n## 2. How to install\n**Requirements**\n`xgrads` is developed under the environment with `xarray` (=version 0.15.0), `dask` (=version 2.11.0), `numpy` (=version 1.15.4), `cartopy` (=version 0.17.0), and `pyproj` (=version 1.9.6).  Older versions of these packages are not well tested.\n\n**Install via pip**\n```\npip install xgrads\n```\n\n**Install from github**\n```\ngit clone https://github.com/miniufo/xgrads.git\ncd xgrads\npython setup.py install\n```\n\n\n---\n## 3. Examples\n### 3.1 Parse a `.ctl` file\nParsing a `.ctl` file is pretty simple using the following code:\n```python\nfrom xgrads import CtlDescriptor\n\nctl = CtlDescriptor(file='test.ctl')\n\n# print all the info in ctl file\nprint(ctl)\n```\n\nIf you have already load the ASCII content in the `.ctl` file, you can do it as:\n```python\ncontent = \\\n    \"dset ^binary.dat\\n\" \\\n    \"* this is a comment line\\n\" \\\n    \"title 10-deg resolution model\\n\" \\\n    \"undef -9.99e8\\n\" \\\n    \"xdef 36 linear   0 10\\n\" \\\n    \"ydef 19 linear -90 10\\n\" \\\n    \"zdef  1 linear   0  1\\n\" \\\n    \"tdef  1 linear 00z01Jan2000 1dy\\n\" \\\n    \"vars  1\\n\" \\\n    \"test  1 99 test variable\\n\" \\\n    \"endvars\\n\"\n\nctl = CtlDescriptor(content=content)\n\n# print all the info\nprint(ctl)\n```\n---\n\n### 3.2 Read binary data into a `xarray.Dataset`\nReading a `.ctl` related binary data file is also pretty simple using the following code:\n```python\nfrom xgrads import open_CtlDataset\n\ndset = open_CtlDataset('test.ctl')\n\n# print all the info in ctl file\nprint(dset)\n```\n\nThen you have the `dset` as a `xarray.Dataset`.  This is similar to [`xarray.open_dataset`](http://xarray.pydata.org/en/stable/generated/xarray.open_dataset.html) that use [`dask`](https://dask.org/) to chunk (buffer) parts of the whole dataset in physical memory if the whole dataset is too large to fit in.\n\nIf there are many `.ctl` files in a folder, we can also open all of them in a single call of `open_mfdataset` as:\n```python\nfrom xgrads import open_mfDataset\n\ndset = open_mfDataset('./folder/*.ctl')\n\n# print all the info in ctl file\nprint(dset)\n```\nassuming that every `.ctl` file has similar data structure except the time step is different.  This is similar to [`xarray.open_mfdataset`](http://xarray.pydata.org/en/v0.12.3/generated/xarray.open_mfdataset.html).\n\n---\n\n### 3.3 Convert a GrADS dataset to a NetCDF dataset\nWith the above functionality, it is easy to convert a `.ctl` ([GrADS](http://cola.gmu.edu/grads/)) dataset to a `.nc` ([NetCDF](https://www.unidata.ucar.edu/software/netcdf/docs/file_structure_and_performance.html)) dataset:\n```python\nfrom xgrads import open_CtlDataset\n\nopen_CtlDataset('input.ctl').to_netcdf('output.nc')\n```\n"
 },
 {
  "repo": "pyoceans/sea-py",
  "language": "CSS",
  "readme_contents": "# sea-py\n\n[![Travis-CI](https://travis-ci.org/pyoceans/sea-py.svg?branch=master)](https://travis-ci.org/pyoceans/sea-py)\n\nA collaborative effort to organize and distribute Python tools for the Oceanographic Community\n\nSee the rendered page here:\nhttp://pyoceans.github.io/sea-py/\n"
 },
 {
  "repo": "gher-ulg/DIVAnd.jl",
  "language": "Julia",
  "readme_contents": "# DIVAnd\n<div align=\"center\"> <img src=\"docs/src/assets/logo.png\"></img></div>\n\n---\n\n[![Project Status: Active \u2013 The project has reached a stable, usable state and is being actively developed.](https://www.repostatus.org/badges/latest/active.svg)](https://www.repostatus.org/#active)\n[![Build Status](https://github.com/gher-ulg/DIVAnd.jl/workflows/CI/badge.svg)](https://github.com/gher-ulg/DIVAnd.jl/actions)\n[![codecov.io](http://codecov.io/github/gher-ulg/DIVAnd.jl/coverage.svg?branch=master)](http://codecov.io/github/gher-ulg/DIVAnd.jl?branch=master)\n[![documentation stable](https://img.shields.io/badge/docs-stable-blue.svg)](https://gher-ulg.github.io/DIVAnd.jl/stable/)\n[![documentation latest](https://img.shields.io/badge/docs-latest-blue.svg)](https://gher-ulg.github.io/DIVAnd.jl/latest/)\n[![DOI](https://zenodo.org/badge/79277337.svg)](https://zenodo.org/badge/latestdoi/79277337)\n\n\n\n`DIVAnd` (Data-Interpolating Variational Analysis in n dimensions) performs an n-dimensional variational analysis/gridding of arbitrarily located observations. Observations will be interpolated/analyzed on a curvilinear grid in 1, 2, 3 or more dimensions. In this sense it is a generalization of the original two-dimensional DIVA version (still available here https://github.com/gher-ulg/DIVA but not further developed anymore).\n\nThe method bears some similarities and equivalences with Optimal Interpolation or Krigging in that it allows to create a smooth and continous field from a collection of observations, observations which can be affected by errors. The analysis method is however different in practise, allowing to take into account topological features, physical constraints etc in a natural way. The method was initially developped with ocean data in mind, but it can be applied to any field where localized observations have to be used to produce gridded fields which are \"smooth\".\n\nSee also https://gher-ulg.github.io/DIVAnd-presentation/#1\n\nPlease cite this paper as follows if you use `DIVAnd` in a publication:\n\nBarth, A., Beckers, J.-M., Troupin, C., Alvera-Azc\u00e1rate, A., and Vandenbulcke, L.: DIVAnd-1.0: n-dimensional variational data analysis for ocean observations, Geosci. Model Dev., 7, 225-241, doi:[10.5194/gmd-7-225-2014](https://doi.org/10.5194/gmd-7-225-2014), 2014.\n\n(click [here](./data/DIVAnd.bib) for the BibTeX entry).\n\n\n# Installing\n\nYou need [Julia](http://julialang.org) (version 1.6 or 1.7) to run `DIVAnd`. The command line version is sufficient for `DIVAnd`.\nInside a Julia terminal, you can download and install the package by issuing:\n\n```julia\nusing Pkg\nPkg.add(\"DIVAnd\")\n```\n\nIt is not recommended to download the source of `DIVAnd.jl` directly (using the green *Clone or Download* button above) because this by-passes Julia's package manager and you would need to install the dependencies of `DIVAnd.jl` manually.\n\n\nWindows users are required to pin the version of NetCDF_jll until this [issue](https://github.com/JuliaPackaging/Yggdrasil/issues/4511) is resolved (help is more than welcome).\n\n```julia\nusing Pkg\nPkg.add(\"NetCDF_jll\")\nPkg.pin(name=\"NetCDF_jll\", version=\"400.702.400\")\n```\n\n\n# Updating DIVAnd\n\nTo update DIVAnd, run the following command and restart Julia (or restart the jupyter notebook kernel using `Kernel` -> `Restart`):\n\n```julia\nusing Pkg\nPkg.update(\"DIVAnd\")\n```\n\nNote that Julia does not directly delete the previous installed version.\nTo check if you have the latest version run the following command:\n\n```julia\nusing Pkg\nPkg.status()\n```\n\nThe latest version number is available from [here](https://github.com/gher-ulg/DIVAnd.jl/releases).\n\nTo explicitly install a given version `X.Y.Z` you can also use:\n\n```julia\nusing Pkg\nPkg.add(name=\"DIVAnd\", version=\"X.Y.Z\")\n```\nOr the master version:\n\n```julia\nusing Pkg\nPkg.add(name=\"DIVAnd\", rev=\"master\")\n```\n\n# Testing\n\nA test script is included to verify the correct functioning of the toolbox.\nThe script should be run in a Julia session.\nMake sure to be in a directory with write-access (for example your home directory).\nYou can change the directory to your home directory with the `cd(homedir())` command.\n\n```julia\nusing Pkg\nPkg.test(\"DIVAnd\")\n```\n\nAll tests should pass without error (it can take several minutes).\n\n```\nINFO: Testing DIVAnd\nTest Summary: | Pass  Total\n  DIVAnd      |  461    461\nINFO: DIVAnd tests passed\n```\n\nThe test suite will download some sample data.\nYou need to have Internet access and run the test function from a directory with write access.\n\n# Documentation\n\nThe main routine of this toolbox is called `DIVAnd` which performs an n-dimensional variational analysis of arbitrarily located observations. Type the following in Julia to view a list of parameters:\n\n```julia\nusing DIVAnd\n?DIVAndrun\n```\n\nsee also https://gher-ulg.github.io/DIVAnd.jl/latest/index.html\n\n## Example\n\n[DIVAnd_simple_example_4D.jl](https://github.com/gher-ulg/DIVAnd.jl/blob/master/examples/DIVAnd_simple_example_4D.jl) is a basic example in fours dimensions. The call to `DIVAndrun` looks like this:\n\n```julia\nfi,s = DIVAndrun(mask,(pm,pn,po,pq),(xi,yi,zi,ti),(x,y,z,t),f,len,epsilon2);\n\n```\nwhere\n`mask` is the land-sea mask, usually obtained from the bathymetry/topography,\n`(pm,pn,po,pq)` is a *n*-element tuple (4 in this case) containing the scale factors of the grid,\n`(xi,yi,zi,ti)` is a *n*-element tuple containing the coordinates of the final grid,\n`(x,y,z,t)` is a *n*-element tuple containing the coordinates of the observations,\n`f` is the data anomalies (with respect to a background field),\n`len` is the correlation length and\n`epsilon2` is the error variance of the observations.\n\nThe call returns `fi`, the analyzed field on the grid `(xi,yi,zi,ti)`.\n\nMore examples are available in the notebooks from the [Diva Workshop](https://github.com/gher-ulg/Diva-Workshops).\n\n## Note on which analysis function to use\n\n`DIVAndrun` is the core analysis function in n dimensions. It does not know anything about the physical parameters or units you work with. Coordinates can also be very general. The only constraint is that the metrics `(pm,pn,po,...)` when multiplied by the corresponding length scales `len` lead to non-dimensional parameters. Furthermore the coordinates of the output grid `(xi,yi,zi,...)` need to have the same units as the observation coordinates `(x,y,z,...)`.\n\n`DIVAndfun` is a version with a minimal set of parameters (the coordinates and values of observations)  `(x,f)` and provides and interpolation function rather than an already gridded field. \n\n`diva3D` is a higher-level function specifically designed for climatological analysis of data on Earth, using longitude/latitude/depth/time coordinates and correlations length in meters. It makes the necessary preparation of metrics, parameter optimizations etc you normally would program yourself before calling the analysis function `DIVAndrun`.\n\n`DIVAnd_heatmap` can be used for additive data and produces Kernel Density Estimations.\n\n\n`DIVAndgo` is only needed for very large problems when a call to `DIVAndrun` leads to memory or CPU time problems. This function tries to decide which solver (direct or iterative) to use and how to make an automatic domain decomposition. Not all options from `DIVAndrun` are available.\n\n## Note about the background field\n\nIf zero is not a valid first guess for your variable (as it is the case for e.g. ocean temperature), you have to subtract the first guess from the observations before calling `DIVAnd` and then add the first guess back in.\n\n## Determining the analysis parameters\n\nThe parameter `epsilon2` and parameter `len` are crucial for the analysis.\n\n`epsilon2` corresponds to the inverse of the [signal-to-noise ratio](https://en.wikipedia.org/wiki/Signal-to-noise_ratio). `epsilon2` is the normalized variance of observation error (i.e. divided by the background error variance). Therefore, its value depends on how accurate and how representative the observations are.\n`len` corresponds to the correlation length and the value of `len` can sometimes be determined by physical arguments. Note that there should be one correlation length per dimension of the analysis.\n\nOne statistical way to determine the parameter(s) is to do a [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_%28statistics%29).\n\n1. choose, at random, a relatively small subset of observations (about 5%). This is the validation data set.\n2. make the analysis without your validation data set\n3. compare the analysis to your validation data set and compute the RMS difference\n4. repeat steps 2 and 3 with different values of the parameters and try to minimize the RMS difference.\n\nYou can repeat all steps with a different validation data set to ensure that the optimal parameter values are robust.\nTools to help you are included in  ([DIVAnd_cv.jl](https://github.com/gher-ulg/DIVAnd.jl/blob/master/src/DIVAnd_cv.jl)).\n\n\n## Note about the error fields\n\n`DIVAnd` allows the calculation of the analysis error variance, scaled by the background error variance. Though it can be calculated \"exactly\" using the diagonal of the error covariance matrix s.P, it is too costly and approximations are provided. Two version are recommended, `DIVAnd_cpme` for a quick estimate and `DIVAnd_aexerr` for a version closer the theoretical estimate (see [Beckers et al 2014](https://doi.org/10.1175/JTECH-D-13-00130.1) )\n\n## Advanced usage\n\n### Additional constraint\n\nAn arbitrary number of additional constraints can be included to the cost function which should have the following form:\n\n*J*(**x**) = \u2211<sub>*i*</sub> (**C**<sub>*i*</sub> **x**  - **z**<sub>*i*</sub>)\u1d40 **Q**<sub>*i*</sub><sup>-1</sup> (**C**<sub>*i*</sub> **x** - **z**<sub>*i*</sub>)\n\nFor every constrain, a structure with the following fields is passed to `DIVAnd`:\n\n* `yo`: the vector **z**<sub>*i*</sub>\n* `H`: the matrix **C**<sub>*i*</sub>\n* `R`: the matrix **Q**<sub>*i*</sub> (symmetric and positive defined)\n\nInternally the observations are also implemented as constraint defined in this way.\n\n## Run notebooks on a server which has no graphical interface\n\nOn the server, launch the notebook with:\n```bash\njupyter-notebook --no-browser --ip='0.0.0.0' --port=8888\n```\nwhere the path to `jupyter-notebook` might have to be adapted, depending on your installation. The `ip` and `port` parameters can also be modified.\n\nThen from the local machine it is possible to connect to the server through the browser.\n\nThanks to Lennert and Bart (VLIZ) for this trick.\n\n# Example data\n\nSome examples in `DIVAnd.jl` use a quite large data set which cannot be efficiently distributed through `git`. This data can be downloaded from the URL https://dox.ulg.ac.be/index.php/s/Bo01EicxnMgP9E3/download. The zip file should be decompressed and the directory `DIVAnd-example-data` should be placed on the same level than the directory `DIVAnd.jl`.\n\n# Reporting issues\n\nPlease include the following information when reporting an issue:\n\n* Version of Julia\n* Version of DIVAnd\n* Operating system\n* Full screen output preferably obtained by setting `ENV[\"JULIA_DEBUG\"] = \"DIVAnd\"`.\n* Full stack strace with error message\n* A short description of the problem\n* The command and their arguments which produced the error\n\n# Fun\n\nAn [educational web application](http://data-assimilation.net/Tools/divand_demo/html/) has been developed to reconstruct a field based on point \"observations\". The user must choose in an optimal way the location of 10 observations such that the analysed field obtained by `DIVAnd` based on these observations is as close as possible to the original field.\n"
 },
 {
  "repo": "MITgcm/xmitgcm",
  "language": "Python",
  "readme_contents": "xmitgcm: Read MITgcm mds binary files into xarray\n=================================================\n\n|pypi| |Build Status| |codecov| |docs| |DOI|\n\nxmitgcm is a python package for reading MITgcm_ binary MDS files into\nxarray_ data structures. By storing data in dask_ arrays, xmitgcm enables\nparallel, out-of-core_ analysis of MITgcm output data.\n\nLinks\n-----\n\n-  HTML documentation: https://xmitgcm.readthedocs.org\n-  Issue tracker: https://github.com/MITgcm/xmitgcm/issues\n-  Source code: https://github.com/MITgcm/xmitgcm\n\nInstallation\n------------\n\nRequirements\n^^^^^^^^^^^^\n\nxmitgcm is compatible with python >=3.7. It requires xarray_\n(>= version 0.14.1) and dask_ (>= version 1.0).\nThese packages are most reliably installed via the\n`conda <https://conda.pydata.org/docs/>`_ environment management\nsystem, which is part of the Anaconda_ python distribution. Assuming you have\nconda available on your system, the dependencies can be installed with the\ncommand::\n\n    conda install xarray dask\n\nIf you are using earlier versions of these packages, you should update before\ninstalling xmitgcm.\n\nInstallation via pip\n^^^^^^^^^^^^^^^^^^^^\n\nIf you just want to use xmitgcm, the easiest way is to install via pip::\n\n    pip install xmitgcm\n\nThis will automatically install the latest release from\n`pypi <https://pypi.python.org/pypi>`_.\n\nInstallation from github\n^^^^^^^^^^^^^^^^^^^^^^^^\n\nxmitgcm is under active development. To obtain the latest development version,\nyou may clone the `source repository <https://github.com/MITgcm/xmitgcm>`_\nand install it::\n\n    git clone https://github.com/MITgcm/xmitgcm.git\n    cd xmitgcm\n    python setup.py install\n\nUsers are encouraged to `fork <https://help.github.com/articles/fork-a-repo/>`_\nxmitgcm and submit issues_ and `pull requests`_.\n\nQuick Start\n-----------\n\nFirst make sure you understand what an xarray_ Dataset object is. Then find\nsome MITgcm MDS data. If you don't have any data of your own, you can download\nthe xmitgcm\n`test repositories <https://figshare.com/articles/xmitgcm_test_datasets/4033530>`_\nTo download the some test data, run the shell commands::\n\n    $ curl -L -J -O https://ndownloader.figshare.com/files/6494718\n    $ tar -xvzf global_oce_latlon.tar.gz\n\nThis will create a directory called ``global_oce_latlon`` which we will use\nfor the rest of these examples. If you have your own data, replace this with\nthe path to your mitgcm files.\n\nTo open MITgcm MDS data as an xarray.Dataset, do the following in python::\n\n    from xmitgcm import open_mdsdataset\n    data_dir = './global_oce_latlon'\n    ds = open_mdsdataset(data_dir)\n\n``data_dir``, should be the path (absolute or relative) to an\nMITgcm run directory. xmitgcm will automatically scan this directory and\ntry to determine the file prefixes and iteration numbers to read. In some\nconfigurations, the ``open_mdsdataset`` function may work without further\nkeyword arguments. In most cases, you will have to specify further details.\n\nConsult the `online documentation <https://xmitgcm.readthedocs.org>`_ for\nmore details.\n\n.. |DOI| image:: https://zenodo.org/badge/70649781.svg\n   :target: https://zenodo.org/badge/latestdoi/70649781\n.. |Build Status| image:: https://travis-ci.org/MITgcm/xmitgcm.svg?branch=master\n   :target: https://travis-ci.org/MITgcm/xmitgcm\n   :alt: travis-ci build status\n.. |codecov| image:: https://codecov.io/github/MITgcm/xmitgcm/coverage.svg?branch=master\n   :target: https://codecov.io/github/MITgcm/xmitgcm?branch=master\n   :alt: code coverage\n.. |pypi| image:: https://badge.fury.io/py/xmitgcm.svg\n   :target: https://badge.fury.io/py/xmitgcm\n   :alt: pypi package\n.. |docs| image:: https://readthedocs.org/projects/xmitgcm/badge/?version=stable\n   :target: https://xmitgcm.readthedocs.org/en/stable/?badge=stable\n   :alt: documentation status\n\n.. _dask: https://dask.pydata.org\n.. _xarray: https://xarray.pydata.org\n.. _Comodo: https://pycomodo.forge.imag.fr/norm.html\n.. _issues: https://github.com/MITgcm/xmitgcm/issues\n.. _`pull requests`: https://github.com/MITgcm/xmitgcm/pulls\n.. _MITgcm: http://mitgcm.org/public/r2_manual/latest/online_documents/node277.html\n.. _out-of-core: https://en.wikipedia.org/wiki/Out-of-core_algorithm\n.. _Anaconda: https://www.continuum.io/downloads\n"
 },
 {
  "repo": "kthyng/python4geosciences",
  "language": "Roff",
  "readme_contents": "# python4geosciences\nPython for the Geosciences, a class offered at Texas A&M University in the Oceanography department.\n\n**Course Topics, Calendar of Activities, Major Assignment Dates (subject to change)**  \nHomework is typically due every Friday night at midnight\n\n\nWeek 0\u20132 (Aug 27/29, Sep 3/5/10/12):  Course intro; Python basics \u2014 Core language  \n*Homework 00 due Aug 30, hw01 due Sep 6, hw02 due Sep 13*  \nUsing Jupyter notebooks and JupyterHub.  Overview of the standard Python programming language, standard data containers (lists, tuples, dictionaries, etc), importing packages, for/while loops, functions, and object oriented programming (objects as containers for data and associated functions).  \nMaterials: \n- [Intro](https://redfish.geos.tamu.edu:8000/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fkthyng%2Fpython4geosciences&subPath=materials%2F0_intro.ipynb&app=notebook)\n- [Core](https://redfish.geos.tamu.edu:8000/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fkthyng%2Fpython4geosciences&subPath=materials%2F1_core.ipynb&app=notebook)   \n\nWeek 3\u20134 (Sep 17/19/24/26):  Numerical Python   \n*hw03 due Sep 20, hw04 due Sep 27*  \nNumpy, vector operations, data types, and array broadcasting.  \nMaterials: \n- [numpy](https://redfish.geos.tamu.edu:8000/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fkthyng%2Fpython4geosciences&subPath=materials%2F2_numpy.ipynb&app=notebook)\n\nWeek 5 (Oct 1): Review  \n*hw05 due Oct 4*  \nReview core language and numpy.\n\nWeek 5\u20137 (Oct 3/8/10/15):  Basic plotting in Python with matplotlib  \n*hw06 due Oct 11, hw07 due Oct 18, Email project plan by Oct 18*  \nOverview of the matplotlib plotting package: 1D (line plots, histograms), 2D (contours, pcolor).  \nMaterials: \n- [matplotlib](https://redfish.geos.tamu.edu:8000/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fkthyng%2Fpython4geosciences&subPath=materials%2F3_matplotlib.ipynb&app=notebook)\n- [interpolation](https://redfish.geos.tamu.edu:8000/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fkthyng%2Fpython4geosciences&subPath=materials%2FST_interpolation.ipynb&app=notebook)\n\nWeek 7\u20139 (Oct 17/22/24/29):  1D time series analysis  \n*hw08 due Oct 25, hw09 due Nov 1*  \npandas, indexing, averaging.  \nMaterials: \n- [pandas, 1d](https://redfish.geos.tamu.edu:8000/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fkthyng%2Fpython4geosciences&subPath=materials%2F4_pandas.ipynb&app=notebook)\n\nWeek 9\u201310 (Oct 31, Nov 5):  Review  \n*hw10 due Nov 8*  \nReview core, numpy, matplotlib, and pandas.\n\nWeek 10-12 (Nov 7/12/14/19): 2D geospatial plotting  \n*hw11 due Nov 15*  \nCartopy mapping package and shapefiles. xarray: reading and writing NetCDF files locally and over the internet.  \nMaterials: \n- [maps](https://redfish.geos.tamu.edu:8000/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fkthyng%2Fpython4geosciences&subPath=materials%2F5_maps.ipynb&app=notebook)\n- [shapefiles on maps](https://redfish.geos.tamu.edu:8000/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fkthyng%2Fpython4geosciences&subPath=materials%2F7_shapefiles.ipynb&app=notebook)\n- [netCDF and xarray](https://redfish.geos.tamu.edu:8000/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fkthyng%2Fpython4geosciences&subPath=materials%2F6_xarray.ipynb&app=notebook)\n\nWeek 12-13 (Nov 21/26, No class Nov 28): Python beyond the notebook  \nAnaconda package installer, iPython for terminal window, writing scripts, Jupyterlab; debugging, unit testing.  \nMaterials: \n- [beyond the notebook](https://redfish.geos.tamu.edu:8000/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fkthyng%2Fpython4geosciences&subPath=materials%2FST_beyond_notebook.ipynb&app=notebook)\n\nWeek 14 (Dec 3): Share projects in groups  \n*hw12/project due Dec 3 (undergrads and grads)*  \nShare projects in groups and get feedback. Attendance is required.\n"
 },
 {
  "repo": "ocefpaf/python4oceanographers",
  "language": "Jupyter Notebook",
  "readme_contents": "python4oceanographers\n=====================\n\nLearn python with examples applied to oceanography.\n"
 },
 {
  "repo": "lnferris/ocean_data_tools",
  "language": "MATLAB",
  "readme_contents": "# ocean_data_tools: a MATLAB toolbox for interacting with bulk freely-available oceanographic data\n\n<img src=\"https://user-images.githubusercontent.com/24570061/85356569-4664ba80-b4dd-11ea-9ec7-8ec26df76dcf.png\" width=\"500\">\n\n[![GitHub license](https://img.shields.io/github/license/lnferris/ocean_data_tools)](https://github.com/lnferris/ocean_data_tools/blob/master/LICENSE) [![GitHub stars](https://img.shields.io/github/stars/lnferris/ocean_data_tools)](https://github.com/lnferris/ocean_data_tools/stargazers) [![GitHub forks](https://img.shields.io/github/forks/lnferris/ocean_data_tools)](https://github.com/lnferris/ocean_data_tools/network) \n[![View ocean_data_tools on File Exchange](https://www.mathworks.com/matlabcentral/images/matlab-file-exchange.svg)](https://www.mathworks.com/matlabcentral/fileexchange/80047-ocean_data_tools) [![DOI](https://joss.theoj.org/papers/10.21105/joss.02497/status.svg)](https://doi.org/10.21105/joss.02497)\n\n**Copyright (c) 2020 lnferris** \n\nocean_data_tools simplifies the process of extracting, formatting, and visualizing freely-available oceanographic data. While a wealth of oceanographic data is accessible online, some end-users may be dissuaded from utilizing this data due to the overhead associated with obtaining and formatting it into usable data structures. ocean_data_tools solves this problem by allowing the user to transform common oceanographic data sources into uniform structs, call generalized functions on these structs, easily perform custom calculations, and make graphics.\n\nFind a bug, have a question, or want to chat about contributing? Open an issue or email lnferris@alum.mit.edu.\n\n### [Getting Started](#getting-started-1)\n\n### [Dependencies](#dependencies-1)\n\n### [Accessing Help](#accessing-help-1)\n\n### [How to Contribute](#how-to-contribute-1)\n\n### [Contents](#contents-1)\n\n### [Finding Data](#finding-data-1)\n\n### [Citing ODT](#citing-odt-1)\n\n## Getting Started\n\n1. Download [bathymetry](#bathymetry).\n2. Download [nctoolbox](https://github.com/nctoolbox/nctoolbox). You will need to run the command ``setup_nctoolbox`` at the beginning of each MATLAB session.\n3. Add ocean_data_tools and nctoolbox to the path. Specifically, the following folders must be added to the [path](https://www.mathworks.com/help/matlab/matlab_env/what-is-the-matlab-search-path.html):\n\n- ocean_data_tools/ocean_data_tools\n- ocean_data_tools/ocean_data_tools/utilities\n- nctoolbox/\n\n4. Run each demonstration in **demos/demos.m**, which contains example usages for all functions. All required test data is included in **data/**.\n\nFunctions are named using a two-part system. The prefix (``argo_``, ``bathymetry_``, ``general_``, etc.) indicates the appropriate data source, while the suffix (``\\_build``, ``\\_profiles``, ``\\_section``, etc.) indicates the action performed. Functions with the ``\\_build`` suffix load raw data into uniform structs (e.g. ``argo``, ``cruise``, ``hycom``, ``mercator``, ``woa``, ``wod``). Uniform structs created by ``\\_build`` functions are compatable with any ``general_`` function.\n\nData sources currently supported:\n| Data Source | DOI, Product Code, or Link    |\n|:--  |:--|\n| Argo floats | [doi:10.17882/42182](https://doi.org/10.17882/42182) |\n| Smith & Sandwell bathymetry | [doi:10.1126/science.277.5334.1956](https://doi.org/10.1126/science.277.5334.1956) |\n| IOOS Glider DAC | https://gliders.ioos.us/ |\n| MOCHA Climatology | [doi:10.7282/T3XW4N4M](https://doi.org/10.7282/T3XW4N4M) |\n| HYbrid Coordinate Ocean Model | https://hycom.org |\n| CMEMS Global Ocean 1/12\u00b0 Physics Analysis and Forecast | GLOBAL_ANALYSIS_FORECAST_ PHY_001_024 |\n| CMEMS Global Ocean Waves Multi Year | GLOBAL_REANALYSIS_WAV_001_032 |\n| GO-SHIP hydrographic cruises | https://www.go-ship.org/ |\n| World Ocean Atlas 2018 | https://www.ncei.noaa.gov/products/world-ocean-atlas |\n| World Ocean Database | https://www.ncei.noaa.gov/products/world-ocean-database |\n\nMain functions are located in **ocean_data_tools/**. Demonstrations are located in **demos/**. Test datas are located in **data/**. Shell scripts for batch downloading data are located in **shell_scripts/**. While shell scripts can be run directly in a macOS Terminal, running them in Windows requires [Cygwin](https://www.cygwin.com/) (and perhaps slight modification of commands). Python syntax examples are located in **python/**, which may be grow to become a module in the future.\n\n## Dependencies\n\nThe only true dependency is [nctoolbox](https://github.com/nctoolbox/nctoolbox).\n\nIt is recommended to also download [Gibbs-SeaWater (GSW) Oceanographic Toolbox](http://www.teos-10.org/software.htm#1). A benefit of ocean_data_tools is that neatly packs data into uniform structs; at which point a user can easily apply custom calculations or functions from other toolboxes such as GSW. See an [example](docs/gsw_example.md).\n\n## Accessing Help\n\nTo access help, run the command ``doc ocean_data_tools``.\n\n## How to Contribute\n\n* Want to make changes or add a new function? (1) Fork the repository (make your own separate copy), (2) make changes, and (3) open a 'pull request'. Once approved, it can be merged into the master branch. If you wish to chat beforehand about your contribution, open an issue or email lnferris@alum.mit.edu. \n* Don't use git often and don't want to remember all the terminal commands? Download [GitHub Desktop](https://desktop.github.com/).\n* Find a bug in the code? Open an 'issue' to notify contributors and create an official record.\n\nBefore contributing, please see [Contents](#contents-1) and consider how your function fits into ocean_data_tools and its ethos of structure arrays. At a minimum, functions must be well-documented and address a specific freely-available oceanographic data source which can be accessed by anyone online.\n\nAdding a new function isn't the only way to contribute. Python, Julia, etc. translations of existing Matlab functions are also welcomed!\n\nIf you are interested in becoming a formal collaborator (e.g. have direct access and co-manage this repository), please reach out.\n\n## Contents\n\n#### [Building uniform structs from data sources](#building-uniform-structs-from-data-sources-1)\n\n#### [General functions for subsetting and plotting uniform structs](#general-functions-for-subsetting-and-plotting-uniform-structs-1)\n\n#### [Plotting gridded data without building structs](#plotting-gridded-data-without-building-structs-1)\n\n#### [Adding bathymetry to existing plots](#adding-bathymetry-to-existing-plots-1)\n\n#### [Additional functions for inspecting Argo data](#additional-functions-for-inspecting-argo-data-1)\n\n#### [Miscellaneous utilities](#miscellaneous-utilities-1)\n\n\n### Building uniform structs from data sources\n\n**[argo_build](docs/argo_build.md)** searches the locally-stored Argo profiles matching the specified region & time period and builds a uniform struct\n\n**[glider_build](docs/glider_build.md)** loads an archived glider survey (downloaded from gliders.ioos.us/erddap) and builds a uniform struct\n\n**[mocha_build_profiles](docs/mocha_build_profiles.md)** builds a uniform struct of profiles from the MOCHA Mid-Atlantic Bight climatology\n\n**[model_build_profiles](docs/model_build_profiles.md)**  builds a uniform struct of profiles from HYCOM or Operational Mercator CMEMS GLOBAL_ANALYSIS_FORECAST_PHY_001_024\n\n<img src=\"https://user-images.githubusercontent.com/24570061/88250150-ac776580-cc74-11ea-8c72-cea7cc50b4d9.png\" width=\"700\">\n\n**waves_build** builds a uniform struct of timeseries from CMEMS Global Ocean Waves Multi Year product GLOBAL_REANALYSIS_WAV_001_032\n\n**[whp_cruise_build](docs/whp_cruise_build.md)** builds a uniform struct of profiles from GO-SHIP cruise data in WHP-Exchange Format\n\n**[woa_build_profiles](docs/woa_build_profiles.md)** builds a uniform struct of profiles from World Ocean Atlas 2018 Statistical Mean for All Decades, Objectively Analyzed Mean Fields\n\n**[wod_build](docs/wod_build.md)** builds a uniform struct of profiles from World Ocean Database data\n\n*Don't see a function yet for your preferred data source? Email lnferris@alum.mit.edu to request or [contribute](#how-to-contribute-1).*\n\n### General functions for subsetting and plotting uniform structs\n\n**[general_depth_subset](docs/general_depth_subset.md)** subsets a uniform struct by depth\n\n**[general_map](docs/general_map.md)** plots coordinate locations in a uniform struct, with optional bathymetry contours\n\n**[general_profiles](docs/general_profiles.md)** plots vertical profiles in a uniform struct\n\n**[general_region_subset](docs/general_region_subset.md)** subsets a uniform struct by polygon region\n\n<img src=\"https://user-images.githubusercontent.com/24570061/88250944-358f9c00-cc77-11ea-9b0d-2d582ad186dd.png\" width=\"700\">\n\n**[general_remove_duplicates](docs/general_remove_duplicates.md)** removes spatially (or spatially and temporally) non-unique profiles from a uniform struct\n\n**[general_section](docs/general_section.md)** plots a data section from a uniform struct\n\n\n### Plotting gridded data without building structs\n\n**[mocha_domain_plot](docs/mocha_domain_plot.md)** plots a 3-D domain from the MOCHA Mid-Atlantic Bight climatology\n\n**[mocha_simple_plot](docs/mocha_simple_plot.md)** plots a 2-D layer from the MOCHA Mid-Atlantic Bight climatology\n\n**[model_domain_plot](docs/model_domain_plot.md)** plots a 3-D domain from HYCOM or Operational Mercator CMEMS GLOBAL_ANALYSIS_FORECAST_PHY_001_024\n\n**[model_simple_plot](docs/model_simple_plot.md)** plots a 2-D layer from HYCOM or Operational Mercator CMEMS GLOBAL_ANALYSIS_FORECAST_PHY_001_024\n\n<img src=\"https://user-images.githubusercontent.com/24570061/88250403-8900ea80-cc75-11ea-8a5d-8a474d2e5c3f.png\" width=\"700\">\n\n**[woa_domain_plot](docs/woa_domain_plot.md)** plots a 3-D domain from World Ocean Atlas 2018 Statistical Mean for All Decades, Objectively Analyzed Mean Fields\n\n**[woa_simple_plot](docs/woa_simple_plot.md)** plots a 2-D layer from World Ocean Atlas 2018 Statistical Mean for All Decades, Objectively Analyzed Mean Fields\n\n### Adding bathymetry to existing plots\n\n**[bathymetry_extract](docs/bathymetry_extract.md)** extracts a region of Smith & Sandwell Global Topography and outputs as arrays\n\n**[bathymetry_plot](docs/bathymetry_plot.md)** adds bathymetry to 2-D (latitude vs. longitude) or 3-D (latitude vs. longitude vs. depth) data plots\n\n<img src=\"https://user-images.githubusercontent.com/24570061/88251161-ed24ae00-cc77-11ea-87d6-0e3b4484764d.jpg\" width=\"700\">\n\n**[bounding_region](docs/bounding_region.md)** finds the rectangular region around a uniform struct and/or list of coordinates to pass as an argument for other bathymetry functions\n\n**[bathymetry_section](docs/bathymetry_section.md)** adds Smith & Sandwell Global Topography to a section from plot using bathymetry data nearest to specified coordinates\n\n<img src=\"https://user-images.githubusercontent.com/24570061/88250660-3d027580-cc76-11ea-808c-f51d5105e420.png\" width=\"700\">\n\n### Additional functions for inspecting Argo data\n\n**[argo_platform_map](docs/argo_platform_map.md)** plots locations of Argo profiles in a uniform struct, coloring markers by platform (individual Argo float)\n\n<img src=\"https://user-images.githubusercontent.com/24570061/88250439-a2099b80-cc75-11ea-9516-ad3d1f65fdf9.jpg\" width=\"700\">\n\n**[argo_platform_subset](docs/argo_platform_subset.md)** subsets a uniform struct of Argo data to one platform (individual Argo float)\n\n**[argo_profiles_map](docs/argo_profiles_map.md)** plots coordinate locations of Argo profiles in uniform struct argo, using colors corresponding to argo_profiles called on the same struct\n\n**[argo_profiles](docs/argo_profiles.md)** plots vertical Argo profiles in uniform struct argo, using colors corresponding to argo_profiles_map called on the same struct\n\n\n### Miscellaneous utilities\n\n**[region_select](docs/region_select.md)** creates coordinate list (which represents vertices of a polygon region) by clicking stations on a plot\n\n**[transect_select](docs/transect_select.md)** creates a coordinate list (which represents a virtual transect) by clicking stations on a plot\n\n<img src=\"https://user-images.githubusercontent.com/24570061/88250639-2b20d280-cc76-11ea-9c94-3ce16300f735.png\" width=\"700\">\n\n\n## Finding Data\n\nThere two types of datasets: those that need to be downloaded manually<sup>1</sup> and those that can be accessed remotely<sup>2</sup> through OpenDAP (e.g. the data can be accessed directly on the the internet using a url). \n\n#### argo<sup>1</sup>\n\nDownload [Argo data](https://argo.ucsd.edu/) directly from GDAC FTP servers using either the [Coriolis selection tool](http://www.argodatamgt.org/Access-to-data/Argo-data-selection), or the [US GDAC](https://nrlgodae1.nrlmry.navy.mil/cgi-bin/argo_select.pl). See the [Argo User's Manual](http://www.argodatamgt.org/Documentation) for more information.\n\nAlternatively run **shell_scripts/download_argo** to download data via File Transfer Protocol.\n\n#### bathymetry<sup>1</sup>\n\nTo get bathymetry data (for ``bathymetry_dir``), download Smith & Sandwell under [Global Topography V19.1](https://topex.ucsd.edu/marine_topo/) in netcdf form (topo_20.1.nc).\n\n#### glider<sup>1</sup>\n\nVist [gliders.ioos.us/erddap](https://gliders.ioos.us/erddap/index.html). Click \"View a List of All 779 Datasets\" or use the \"Advanced Search\". After choosing a dataset, navigate to the [Data Access Form](https://gliders.ioos.us/erddap/tabledap/ce_311-20170725T1930-delayed.html). To get started, select these variables:\n\n<img src=\"https://user-images.githubusercontent.com/24570061/94058620-419af580-fdaf-11ea-859a-616c8b5b1433.png\" width=\"700\">\n\nScroll to \"File type:\". In the drop-down menu, select \".nc\". Click \"Submit\".\n\n#### mocha<sup>2</sup>\n\nThe url for MOCHA Mid-Atlantic Bight climatology is embedded. See [Rutgers Marine catalog](http://tds.marine.rutgers.edu/thredds/catalog.html).\n\n#### model<sup>1,2</sup>\n\nHYCOM data may be accessed remotely using OpenDAP. Get the data url by visiting the [HYCOM website](https://www.hycom.org/dataserver/gofs-3pt1/analysis). For example, click Access Data Here -> GLBv0.08/expt_57.7 (Jun-01-2017 to Sep-30-2017)/ -> Hindcast Data: Jun-01-2017 to Sep-30-2017. Click on the\u00a0OpenDAP link. Copy the url as and use this as the ``source`` in ``model_build_profiles``.\n\nAlteratively, download subsetted HYCOM data using NCSS. Get the data url by visiting the [HYCOM website](https://www.hycom.org/dataserver/gofs-3pt1/analysis). For example, click Access Data Here -> GLBv0.08/expt_57.7 (Jun-01-2017 to Sep-30-2017)/ -> Hindcast Data: Jun-01-2017 to Sep-30-2017. Click on the\u00a0NetcdfSubset\u00a0link. Set constraints and copy the NCSS Request URL at the bottom of the page. Run **shell_scripts/download_hycom_lite**. To download multiple months or years, run **shell_scripts/download_hycom_bulk_daily** (partition files by day) or **shell_scripts/download_hycom_bulk_monthly** (partition files by month). Please use responsibly.\n\nFor Mercator, download Copernicus Marine data directly from FTP servers. First make a [Copernicus account](http://marine.copernicus.eu/services-portfolio/access-to-products/). Use the selection tool to download GLOBAL_ANALYSIS_FORECAST_PHY_001_024. Alternatively run **shell_scripts/download_mercator**. Before running the script, follow the instructions for modifying your ~/.netrc file in the comments of the script.\n\n#### waves<sup>1</sup>\n\nFirst make a [Copernicus account](http://marine.copernicus.eu/services-portfolio/access-to-products/). Use the selection tool to download CMEMS Global Ocean Waves Multi Year product GLOBAL_REANALYSIS_WAV_001_032.\n\n#### whp_cruise<sup>1</sup>\n\nFor [GO-SHIP data](https://usgoship.ucsd.edu/hydromap/), get CTD data (for ``ctdo_dir``) by choosing a [GO-SHIP cruise](https://cchdo.ucsd.edu/search?q=GO-SHIP) and downloading the\u00a0CTD\u00a0data in\u00a0whp_netcdf\u00a0format. More information about whp_netcdf parameters is available [here](https://exchange-format.readthedocs.io/en/latest/index.html#). Get LADCP data (for ``uv_dir``, ``wke_dir``) [here](https://currents.soest.hawaii.edu/go-ship/ladcp/). There is information about LACDP processing [here](https://www.ldeo.columbia.edu/~ant/LADCP.html).\n\n#### woa<sup>2</sup>\n\nFunctions build the World Ocean Atlas url at maximum resolution based on arguments, but coarser resolutions and seasonal climatologies are available at the [NODC website](https://www.nodc.noaa.gov/OC5/woa18/woa18data.html). Note NCEI is scheduled to update data urls in the near future. Functions will be updated as such.\n\n#### wod<sup>1</sup>\n\nSearch the [World Ocean Database](https://www.nodc.noaa.gov/OC5/SELECT/dbsearch/dbsearch.html) and select products.\n\n## Citing ODT\n\nFerris, L., (2020).  ocean_data_tools:  A MATLAB toolbox for interacting with bulk freely-available oceanographic data. Journal of Open Source Software, 5(54), 2497. https://doi.org/10.21105/joss.02497\n\n"
 },
 {
  "repo": "clstoulouse/motu-client-python",
  "language": "Python",
  "readme_contents": "# Motu Client Python Project \n@author Product owner <tjolibois@cls.fr>  \n@author Scrum master, software architect <smarty@cls.fr>  \n@author Quality assurance, continuous integration manager <smarty@cls.fr>  \n\n>How to read this file? \nUse a markdown reader: \nplugins [chrome](https://chrome.google.com/webstore/detail/markdown-preview/jmchmkecamhbiokiopfpnfgbidieafmd?utm_source=chrome-app-launcher-info-dialog) exists (Once installed in Chrome, open URL chrome://extensions/, and check \"Markdown Preview\"/Authorise access to file URL.), \nor for [firefox](https://addons.mozilla.org/fr/firefox/addon/markdown-viewer/)  (anchor tags do not work)\nand also plugin for [notepadd++](https://github.com/Edditoria/markdown_npp_zenburn).\n\n>Be careful: Markdown format has issue while rendering underscore \"\\_\" character which can lead to bad variable name or path.\n\n\n# Summary\n* [Overview](#Overview)\n* [Build](#Build)\n* [Installation](#Installation)\n    * [Prerequisites](#InstallationPre)\n    * [Using PIP](#InstallationPIP)\n    * [From tar.gz file](#InstallationTGZ)\n* [Configuration](#Configuration)\n* [Usage and options](#Usage)\n    * [Usage from PIP installation](#UsagePIP)\n    * [Usage from tar.gz installation](#UsageTGZ)\n* [Usage examples](#UsageExamples)\n    * [Download](#UsageExamplesDownload)\n    * [GetSize](#UsageExamplesGetSize)\t\n    * [DescribeProduct](#UsageExamplesDescribeProduct)\n* [Licence](#Licence)\n* [Troubleshooting](#Troubleshooting)\n    * [Unable to download the latest version watched on GitHub from PIP](#Troubleshooting)  \n    * [From Windows, Parameter error](#TroubleshootingWinArgErr)\n\n# <a name=\"Overview\">Overview</a>\nMotu client \"motuclient-python\" is a python script used to connect to Motu HTTP server in order to:  \n\n* __extract__ the data of a dataset, with geospatial, temporal and variable criterias (default option)   \n* __get the size__ of an extraction with geospatial, temporal and variable criterias  \n* __get information__ about a dataset  \n\nThis program can be integrated into a processing chain in order to automate the downloading of products via the Motu.  \n  \n  \n# <a name=\"Build\">Build</a>  \nFrom the root folder runs the command:  \n  \n```\n./patchPOMtoBuild.sh  \nmvn clean install -Dmaven.test.skip=true\n[...]\n[INFO] BUILD SUCCESS\n[...]\n```  \n\nThis creates two archives in the target folder:\n\n* motuclient-python-$version-$buildTimestamp-src.tar.gz: Archive containing all the source code\n* motuclient-python-$version-$buildTimestamp-bin.tar.gz: Archive ready to be installed\n\n\n\n# <a name=\"Installation\">Installation</a> \n\n## <a name=\"InstallationPre\">Prerequisites</a>\nSince motuclient release version 3.X.Y, you must use python version 3.7.10 or later.  \n__/!\\__ motuclient does not work with the OpenSSL library release 1.1.1.e. Either use an older version such as the 1.1.1.d or jump to the 1.1.1.f release.  \nThere are two methods to install the client, by using PIP or from a tar.gz file.  \n [setuptools](#InstallationSetuptools) python package has be installed in order to display the motuclient version successfully.    \n  \n## <a name=\"InstallationPIP\">Using PIP</a>\nPython Package Index is used to ease installation.  \nIf your host needs a PROXY set it, for example:  \n```\nexport HTTPS_PROXY=http://myCompanyProxy:8080  \n```  \n\nThen run:  \n  \n```\npip install motuclient --upgrade  \n```\n  \nNow \"motuclient\" is installed, you can [configured it](#Configuration) and [use it](#UsagePIP).\n  \n  \n## <a name=\"InstallationTGZ\">From tar.gz file</a>\nDeploy the archive (file motuclient-python-$version-bin.tar.gz available from [GitHub release](https://github.com/clstoulouse/motu-client-python/releases)) in the directory of your choice.  \n```  \ntar xvzf motuclient-python-$version-$buildTimestamp-bin.tar.gz\n```  \n\nCreate a [configuration file](#Configuration) and set the user and password to use to connect to the CAS server.   \n\n## <a name=\"InstallationSetuptools\">Install setuptools python package</a>\n\"[Setuptools](https://pypi.python.org/pypi/setuptools)\" python package has to be installed in order to display the version with option --version, here is how to install it:    \n \nIf your host needs a PROXY set it, for example:  \n```\nexport HTTPS_PROXY=http://myCompanyProxy:8080  \n```  \n\nThen run:  \n\n```  \nsudo apt install python-pip  \npip install --upgrade setuptools  \n```  \n\n# <a name=\"Configuration\">Configuration file</a>  \n\nAll parameters can be defined as command line options or can be written in a configuration file.  \nThe configuration file is a .ini file, encoded in UTF-8 without BOM. This file is located in the following directory:  \n\n* on __Unix__ platforms: $HOME/motuclient/motuclient-python.ini\n* on __Windows__ platforms: %USERPROFILE%\\motuclient\\motuclient-python.ini\n  \nThe expected structure of file is:  \n``` \n[Main]  \n# Motu credentials  \nuser=john  \npwd=secret  \n\nmotu=http://motu-ip-server:port/motu-web/Motu  \nservice_id=GLOBAL_ANALYSIS_FORECAST_PHY_001_024-TDS   \nproduct_id=global-analysis-forecast-phy-001-024-hourly-t-u-v-ssh  \ndate_min=2019-03-27  \ndate_max=2019-03-27  \nlatitude_min=-30  \nlatitude_max=40.0  \nlongitude_min=-10  \nlongitude_max=179.9166717529297    \ndepth_min=0.493    \ndepth_max=0.4942  \n# Empty or non set means all variables  \n# 1 or more variables separated by a coma and identified by their standard name  \nvariable=sea_water_potential_temperature,sea_surface_height_above_geoid \n# Accept relative or absolute path. The dot character \".\" is the current folder  \nout_dir=./out_dir  \nout_name=test.nc  \n\n# Logging\n# https://docs.python.org/3/library/logging.html#logging-levels  \n# log_level=X {CRITICAL:50, ERROR:40, WARNING:30, INFO:20, DEBUG:10, TRACE:0}   \nlog_level=0   \n\n# block_size block used to download file (integer expressing bytes) default=65535\n# block_size=65535  \nsocket_timeout=120000  \n\n# Http proxy to connect to Motu server\n# proxy_server=proxy.domain.net:8080  \n# proxy_user=john  \n# proxy_pwd=secret  \n``` \n\nA configuration file in another location can be specified by the `--config-file` option. It is even possible to split the configuration into two or more files. This is useful, for example, to keep server configuration in one file and dataset configuration in another:\n```  \n./motuclient.py --config-file ~/server.ini --config-file ~/mercator.ini\n``` \nIf by chance there is a parameter listed in both configuration files, the value in the last file (e.g. `mercator.ini`) is the one actually used.\n\nNote that the password must be encoded in UTF-8.  \nIf it contains UTF-8 special characters, on Windows host only, you only have to double the \"percent\" character. If password is CMS2017@%! then enter   \n\n```  \npwd = CMS2017@%%! \n```  \n\nExample of server.ini on Windows host only, with user password is   \n__Password__:  \n```  \nloginForTesting2 &~#\"'{([-|`_\\^@)]=}\u00a8^\u00a3$ \u00b5*\u00a7!/:.;?,%<>  \n```  \n\n__server.ini__:  \n```  \n[Main]\nuser = loginForTesting2@groupcls.com\npwd = loginForTesting2 &~#\"'{([-|`_\\^@)]=}\u00a8^\u00a3$ \u00b5*\u00a7!/:.;?,%%<>\nauth-mode = cas\nmotu = http://motuURL:80/motu-web/Motu\nout_dir = J:/dev/CMEMS-CIS-MOTU/git/motu-validkit/output/04-python-client/MOTU-208\n```  \n\nExample of server.ini on Linux host only, with user password is   \n__Password__:\n```  \nloginForTesting2 &~#\"'{([-|`_\\^@)]=}\u00a8^\u00a3$ \u00b5*\u00a7!/:.;?,%<>  \n```  \n\n__server.ini__:\n```   \n[Main]\nuser = loginForTesting2@groupcls.com\npwd = loginForTesting2 &~#\"'{([-|`_\\^@)]=}\u00a8^\u00a3$ \u00b5*\u00a7!/:.;?,%<>\nauth-mode = cas\nmotu = http://motuURL:80/motu-web/Motu\nout_dir = J:/dev/CMEMS-CIS-MOTU/git/motu-validkit/output/04-python-client/MOTU-208\n```  \n\n# <a name=\"Usage\">Usage</a>  \nStarts the motu python client.  \n\n## <a name=\"UsagePIP\">Usage from PIP installation</a>  \nSince version 1.8.0:  \n```  \nmotuclient -h  \nmotuclient [options]\n```  \nBefore version 1.8.0:  \n```  \npython -m motu-client -h  \npython -m motu-client [options]\n```  \n  \n[Options](#UsageOptions) are listed below.  \nMethod to used when it has been installed with [PIP method](#InstallationPIP).  \n\n\n## <a name=\"UsageTGZ\">Usage from tar.gz installation</a>  \n```  \n./motuclient.py  -h  \nmotuclient.py [options]\n```  \nMethod to used when it has been installed with [tar.gz method](#InstallationTGZ).  \nUsefull if host is offline and has no Internet access.\n\n### <a name=\"UsageOptions\">__Options:__</a>  \n\n\n* __-h, --help__            show this help message and exit  \n* __-q, --quiet__           print logs with level WARN in stdout, used to prevent any output in stdout  \n* __--noisy__               print logs with level TRACE in stdout  \n* __--verbose__             print logs with level DEBUG in stdout  \n* __--version__             show program's version number and exit, [setuptools](#InstallationSetuptools) python package has be installed to run it successfully    \n\n* __--proxy-server=PROXY_SERVER__ Proxy server (url) used to contact Motu \n* __--proxy-user=PROXY_USER__ Proxy user name (string)\n* __--proxy-pwd=PROXY_PWD__ Proxy password (string)  \n\n* __--auth-mode=AUTH_MODE__  the authentication mode: [default: cas]  \n  * __none__ for no authentication\n  * __basic__ for basic authentication\n  * __cas__ for Central Authentication Service  \n* __-u USER, --user=USER__  User name (string) for the specified authentication mode\n* __-p PWD, --pwd=PWD__ User password (string) for the specified authentication mode. UTF8 special characters can be used but contraints depending of your operating system must be applyied:\n\n  * __Windows__ users, be careful if your password contain once of the following characters:\n    * __percent__: From a Windows batch command, if your password contains a percent character, double the percent character: If password is CMS2017@%! then enter   \n    \n    ```\n    -u username-p CMS2017@%%! \n    ```  \n\n    * __space__: From a Windows batch command, if your password contains a space character, set password between double quotes: If password is CMS2017 @%! then enter  \n    \n    ```\n    -u username-p \"CMS2017 @%%!\"\n    ```  \n  \n  \n    * __double quotes__: From a Windows batch command, if your password contains a double quotes character, double the double quotes character: If password is CMS2017\"@%! then enter  \n    \n    ```\n    -u username-p \"CMS2017\"\"@%%!\"\n    ```  \n  \n  * __Linux__ users, be careful if your password contain once of the following characters:\n    * __space__: From a Linux bash shell command, if your password contains a space character, set password between double quotes: If password is CMS2017 @% then enter  \n    \n    ```\n    -u username-p \"CMS2017 @%\"\n    ```  \n  \n    * __exclamation point__: From a Linux bash shell command, if your password contains an exclamation point character, cut the password in two double quotes strings, and append the exclamation point between simple quote: If password is CMS!2017@% then enter  \n    \n    ```\n    -u username-p \"CMS\"'!'\"2017@%\" \n    ```  \n  \n    * __double quotes__: From a Linux bash shell command, if your password contains a double quotes character, escape the double quotes character by prefixing it with backslash: If password is CMS2017\"@% then enter  \n    \n    ```\n    -u username-p \"CMS2017\\\"@%\"\n    ```  \n\n    * __grave accent__: From a Linux bash shell command, if your password contains a grave accent character, escape the grave accent character by prefixing it with backslash: If password is CMS2017`@% then enter  \n    \n    ```\n    -u username-p \"CMS2017\\`@%\"\n    ``` \n  \n* __-m MOTU, --motu=MOTU__ Motu server url, e.g. \"-m http://localhost:8080/motu-web/Motu\"  \n* __-s SERVICE_ID, --service-id=SERVICE_ID__ The service identifier (string), e.g. -s Mercator_Ocean_Model_Global-TDS  \n* __-d PRODUCT_ID, --product-id=PRODUCT_ID__ The product (data set) to download (string), e.g. -d dataset-mercator-psy4v3-gl12-bestestimate-uv  \n* __-t DATE_MIN, --date-min=DATE_MIN__ The min date with optional hour resolution (string following format YYYY-MM-DD [HH:MM:SS]), e.g. -t \"2016-06-10\" or -t \"2016-06-10 12:00:00\". Be careful to not forget double quotes around the date.     \n* __-T DATE_MAX, --date-max=DATE_MAX__ The max date with optional hour resolution (string following format YYYY-MM-DD  [HH:MM:SS ]), e.g. -T \"2016-06-11\" or -T \"2016-06-10 12:00:00\".  Be careful to not forget double quotes around the date.      \n* __-y LATITUDE_MIN, --latitude-min=LATITUDE_MIN__ The min latitude (float in the interval  [-90 ; 90 ]), e.g. -y -80.5  \n* __-Y LATITUDE_MAX, --latitude-max=LATITUDE_MAX__ The max latitude (float in the interval  [-90 ; 90 ]), e.g. -Y 80.5   \n* __-x LONGITUDE_MIN, --longitude-min=LONGITUDE_MIN__ The min longitude (float), e.g. -x -180      \n* __-X LONGITUDE_MAX, --longitude-max=LONGITUDE_MAX__ The max longitude (float), e.g. -X 355.5      \n* __-z DEPTH_MIN, --depth-min=DEPTH_MIN__ The min depth (float in the interval  [0 ; 2e31 ] or string 'Surface'), e.g. -z 0.49  \n* __-Z DEPTH_MAX, --depth-max=DEPTH_MAX__ The max depth (float in the interval  [0 ; 2e31 ] or string 'Surface'), e.g. -Z 0.50\n* __-v VARIABLE, --variable=VARIABLE__ The variable (list of strings), e.g. -v salinity -v sst  \n* __-S, --sync-mode__ Sets the download mode to synchronous (not recommended). If this parameter is set, Motu server is called with parameter [console](https://github.com/clstoulouse/motu#download-product). Otherwise\n, Motu server is called with parameter [status](https://github.com/clstoulouse/motu#download-product).   \n\n\n* __-o OUT_DIR, --out-dir=OUT_DIR__ The output dir where result (download file) is written (string). If it starts with \"console\", behaviour is the same as with --console-mode.       \n* __-f OUT_NAME, --out-name=OUT_NAME__ The output file name (string)  \n* __--console-mode__ Write result on stdout. In case of an extraction, write the nc file http URL where extraction result can be downloaded. In case of a getSize or a describeProduct request, display the XML result.\n\n* __-D, --describe-product__ Get all updated information on a dataset. Output is in XML format, [API details](https://github.com/clstoulouse/motu#describe-product)  \n* __--size__ Get the size of an extraction. Output is in XML format, [API details](https://github.com/clstoulouse/motu#get-size)\n\n* __--block-size=BLOCK_SIZE__ The block used to download file (integer expressing bytes), default=65535 bytes  \n* __--socket-timeout=SOCKET_TIMEOUT__ Set a timeout on blocking socket operations (float expressing seconds)  \n* __--user-agent=USER_AGENT__ Set the identification string (user-agent) for HTTP requests. By default this value is 'Python-urllib/x.x' (where x.x is the version of the python interpreter)  \n* __--outputWritten=OUTPUT_FORMAT__ Set the output format (file type) of the returned file for download requests. By default this value is 'netcdf' and no other value is supported.  \n  \n  \n# <a name=\"UsageExamples\">Usage examples</a>   \n\nIn the following examples, variable ${MOTU\\_USER} and ${MOTU\\_PASSWORD} are user name and user password used to connect to the CAS server for single sign on.  \n${MOTU\\_SERVER\\_URL} is the URL on the MOTU HTTP(s) server, for example http://localhost:8080/motu-web/Motu.  \nCommands \"./motuclient.py\" has to be replaced by \"python -m motuclient\" if it has been installed with [PIP method](#UsagePIP).  \n\n\n## <a name=\"UsageExamplesDownload\">Download</a>  \n\n### Download and save extracted file on the local machine\nThis command writes the extraction result data in file: /data/test.nc  \n\n```  \n./motuclient.py --verbose --auth-mode=none -m ${MOTU_SERVER_URL} -s HR_MOD_NCSS-TDS -d HR_MOD -z 0.49 -Z 0.50 -x -70 -X 25 -y -75 -Y 10 -t \"2016-06-10\" -T \"2016-06-11\" -v salinity -o /data -f test.nc\n``` \n\n### Display on stdout the HTTP(s) URL of the NC file available on the Motu server\nThe HTTP(s) URL is displayed on stdout. This URL is a direct link to the file which is available to be downloaded.  \n\n```  \n./motuclient.py --quiet --auth-mode=cas -u ${MOTU_USER} -p ${MOTU_PASSWORD}  -m ${MOTU_SERVER_URL} -s HR_MOD_NCSS-TDS -d HR_MOD -z 0.49 -Z 0.50 -x -70 -X 25 -y -75 -Y 10 -t \"2016-06-10\" -T \"2016-06-11\" -v salinity -o console\n``` \n\n## <a name=\"UsageExamplesGetSize\">GetSize</a>  \nSee [https://github.com/clstoulouse/motu#ClientAPI_GetSize](https://github.com/clstoulouse/motu#ClientAPI_GetSize) for more details about XML result.  \n\n### Get the XML file which contains the extraction size on the local machine\n```  \n./motuclient.py --size --auth-mode=cas -u ${MOTU_USER} -p ${MOTU_PASSWORD}  -m ${MOTU_SERVER_URL} -s HR_MOD_NCSS-TDS -d HR_MOD -z 0.49 -Z 0.50 -x -70 -X 25 -y -75 -Y 10 -t \"2016-06-10\" -T \"2016-06-11\" -v salinity -o /data -f getSizeResult.xml\n``` \n\n### Display the extraction size as XML on stdout\n```  \n./motuclient.py --quiet --size --auth-mode=cas -u ${MOTU_USER} -p ${MOTU_PASSWORD}  -m ${MOTU_SERVER_URL} -s HR_MOD_NCSS-TDS -d HR_MOD -z 0.49 -Z 0.50 -x -70 -X 25 -y -75 -Y 10 -t \"2016-06-10\" -T \"2016-06-11\" -v salinity -o console\n``` \n\n\n## <a name=\"UsageExamplesDescribeProduct\">DescribeProduct</a>  \nSee [https://github.com/clstoulouse/motu#describe-product](https://github.com/clstoulouse/motu#describe-product) for more details about XML result.  \n\n### Get the XML file which contains the dataset description on the local machine\n```  \n./motuclient.py -D --auth-mode=cas -u ${MOTU_USER} -p ${MOTU_PASSWORD}  -m ${MOTU_SERVER_URL} -s HR_MOD_NCSS-TDS -d HR_MOD -o /data -f describeProductResult.xml\n``` \n\n### Display the dataset description XML result on stdout\n```  \n./motuclient.py --quiet -D --auth-mode=cas -u ${MOTU_USER} -p ${MOTU_PASSWORD}  -m ${MOTU_SERVER_URL} -s HR_MOD_NCSS-TDS -d HR_MOD -o console\n``` \n\n\n\n\n# <a name=\"Licence\">Licence</a> \nThis library is free software; you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation; either version 2.1 of the License, or (at your option) any later version.  \n  \nThis library is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.  \n  \nYou should have received a copy of the GNU Lesser General Public License along with this library; if not, write to the Free Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.  \n\n# <a name=\"Troubleshooting\">Troubleshooting</a>  \n# <a name=\"TroubleshootingPIPCache\">Unable to download the latest version watched on GitHub from PIP</a>\nExample:  \n```  \npip install motuclient  \nCollecting motuclient  \n  Using cached https://test-files.pythonhosted.org/packages/4a/7d/41c3bdd973baf119371493c193248349c9b7107477ebf343f3889cabbf48/motuclient-X.Y.Z.zip  \nInstalling collected packages: motuclient  \n  Running setup.py install for motuclient ... done  \nSuccessfully installed motuclient-X.Y.Z  \n```  \n  \nClear your PIP cache: On Windows, delete the folder %HOMEPATH%/pip. On archlinux pip cache is located at ~/.cache/pip.\nAfter re run the command:  \n```  \npip install motuclient  \nCollecting motuclient  \n  Using https://test-files.pythonhosted.org/packages/4a/7d/41c3bdd973baf119371493c193248349c9b7107477ebf343f3889cabbf48/motuclient-X.Y.Z.zip  \nInstalling collected packages: motuclient  \n  Running setup.py install for motuclient ... done  \nSuccessfully installed motuclient-X.Y.Z  \n``` \n\n# <a name=\"TroubleshootingWinArgErr\">From Windows, Parameter error</a>\nFrom Windows, the command \"motuclient.py --version\" returns an error.  \n10:44:24 [ERROR] Execution failed: [Excp 13] User (option 'user') is mandatory when 'cas' authentication is set. Please provide it.\n\n__Analyse:__  \nThis issue comes from the fact that Windows command line does not pass parameters to python command.  \n  \n__Solution:__  \n``` \nEdit the Windows Registry Key \"HKEY_CLASSES_ROOT\\py_auto_file\\shell\\open\\command\" and append at the end of the value %*  \nExemple: \"C:\\dvltSoftware\\python\\Python27\\python.exe\" \"%1\" %*  \n``` \n\n# <a name=\"TroubleshootingPythonVersionErr\">Error on all motuclient commands</a>\nFor example the command \"motuclient.py --version\" returns this kind of error:  \n``` \nTraceback (most recent call last):\n  File \"C:\\dvlt\\python\\python2.7.18\\Scripts\\motuclient-script.py\", line 11, in <module>\n    load_entry_point('motuclient==3.0.0.post1', 'console_scripts', 'motuclient')()\n  File \"c:\\dvlt\\python\\python2.7.18\\lib\\site-packages\\motuclient\\motuclient.py\", line 352, in main\n    initLogger()\n  File \"c:\\dvlt\\python\\python2.7.18\\lib\\site-packages\\motuclient\\motuclient.py\", line 336, in initLogger\n    logging.addLevelName(utils_log.TRACE_LEVEL, 'TRACE')\nAttributeError: 'module' object has no attribute 'TRACE_LEVEL'\n``` \n\n__Analyse:__  \nThis issue comes from a too old python installation version.  You must use Python 3.7.10 or higher.  \n  \n__Solution:__  \nFind and install the Python 3 distribution for your operating system.  "
 },
 {
  "repo": "introocean/introocean-en",
  "language": "PostScript",
  "readme_contents": "[![License: CC BY-NC-SA 4.0](https://img.shields.io/badge/License-CC%20BY--NC--SA%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by-nc-sa/4.0/)\n[![Latest Release](https://img.shields.io/badge/-Latest%20Release-lightgrey.svg)](https://github.com/introocean/introocean-en/releases/latest)\n\n\n# Introduction to Physical Oceanography, open-source edition\n**Based on the work of Robert Stewart**\n\nRobert Stewart's \"Introduction to Physical Oceanography\" was \nthe first oceanography textbook for many students. \nIt was always the free high-quality source of basic \noceanographic knowledge. Robert Stewart retired in \n2009 and did not update the book after that. \nWith his permission, we now publish the latest \nversion of the book's LaTeX source code on GitHub. \nDr. Stewart gives his permission to update the book\nby fixing errors and adding new information. He has \nonly two conditions:\n\n> Don\u2019t let the book grow into an encyclopedia. \nIt is an INTRODUCTION, leaving details to more\nexpert and focused publications (which could also\nbe on the web server). Keep the book focused on the\nmost important issues students need to know about.\nToday, satellite oceanography needs to be emphasized\nas most of our knowledge comes from satellites and \ndrifters. \n\n> My only request is that the book not be offered\nfor sale. It should be open source and available \nto everyone at no cost.\n\nWe are going to update the book through the usual\nGitHub procedure with PRs and code review. Any\nhelp and contributions from the oceanographic \ncommunity will be greatly appreciated.\n"
 },
 {
  "repo": "aodn/imos-toolbox",
  "language": "MATLAB",
  "readme_contents": "# IMOS Toolbox\n\nThe IMOS Toolbox aims at **converting oceanographic instrument files into quality-controlled IMOS compliant NetCDF files**. \n\nThe toolbox process instruments deployed on moorings (**time series**) or during casts (**profiles**). The processing, including quality control (**QC**) of several oceanographic variables, can be done in **batch mode** or **interactive**.\n\nFinally, the package allows deployment metadata to be ingested into the files from any JDBC supported database (including MS-access), CSV files, or added manually through a graphical user interface (GUI). Manual **QC** is also possible.\n\nSee our [wiki page](https://github.com/aodn/imos-toolbox/wiki) page for more details.\n\n# Distribution\n\nThe **stable** releases may be obtained [here](https://github.com/aodn/imos-toolbox/releases). The releases contain both the source code and binary applications (executables).\n\n## Licensing\nThis project is licensed under the terms of the GNU GPLv3 license.\n# Requirements\n\nWe support Windows and Linux. The toolbox may be used as a **Matlab stand-alone library** or **stand-alone application** (**No Matlab is required**).\n\nFor a **stand-alone library** usage, **Matlab R2018b** or newer is required ( since version 2.6.1).\n\nFor a **stand-alone application**, you will need the Matlab Component Runtime (v95).\n\nSee [installation instructions](https://github.com/aodn/imos-toolbox/wiki/ToolboxInstallation) for further information.\n\n# Usage\n\nThe toolbox can connect to a deployment database to collect the relevant metadata attached to each dataset. We provide an MS-Access database file template and an underlying schema, but several types of databases can be used. The Java code interface can use any JDBC API to query the deployment database. By default, [UCanAccess](http://ucanaccess.sourceforge.net/site.html) is used to query the MS-Access file.\n\nPlease read the [wiki](https://github.com/aodn/imos-toolbox/wiki) for more information on how to **use** the toolbox. \n\nThis project is designed and maintained by [ANMN](http://imos.org.au/facilities/nationalmooringnetwork/) and [AODN](http://imos.org.au/facilities/aodn/).\n\n# License\n\nThe toolbox is copyrighted and licensed under the terms of the GNU GPLv3. For more details, click [here](https://raw.githubusercontent.com/aodn/imos-toolbox/master/license.txt).\n"
 },
 {
  "repo": "nasa/podaac_tools_and_services",
  "language": "Python",
  "readme_contents": "podaac_tools_and_services\n=========================\nThis is a meta-repository which lists locations of code related to all tools and services software for `NASA JPL's Physical Oceanography Distributed Active Archive Center (PO.DAAC) <https://podaac.jpl.nasa.gov>`__.\n\n|image7|\n\nWhat is PO.DAAC?\n----------------\nThe `PO.DAAC <https://podaac.jpl.nasa.gov>`__ is an element of the Earth Observing System Data and Information System (`EOSDIS <https://earthdata.nasa.gov/>`__). The EOSDIS provides science data to a wide community of users for NASA's Science Mission Directorate. `PO.DAAC <https://podaac.jpl.nasa.gov>`__ has become the premier data center for measurements focused on ocean surface topography (OST), sea surface temperature (SST), ocean winds, sea surface salinity (SSS), gravity, ocean circulation and sea ice.\n\nWhat's in this repository?\n--------------------------\nThis repository reflects an active catalog of all tools and services software pertaining to `PO.DAAC data access <https://podaac.jpl.nasa.gov/dataaccess>`__. If you have a suggestion for a new tool or would like to update the content here, please `open an issue <https://github.com/nasa/podaac_tools_and_services/issues>`__ or `send a pull request <https://github.com/nasa/podaac_tools_and_services/pulls>`__.\n\nWhere do I find detailed information on tools and services included in this repository?\n---------------------------------------------------------------------------------------\nEach repository has it's own README file e.g. `data_animation/README.rst <https://github.com/nasa/podaac_tools_and_services/blob/master/data_animation/README.rst>`__\n\nKeeping Git submodules up-to-date\n---------------------------------\nIn order to keep the submodules as defined in [.gitmodules](https://github.com/nasa/podaac_tools_and_services/blob/master/.gitmodules) up-to-date it is necessary to periodically push updates. You can safely execute this command to do so::\n\n\n    $ git submodule foreach git pull origin master\n    $ git status //you will then see the changes which have been mode\n    $ git add -A\n    $ git commit -m \"Update submodules\"\n    $ git push origin master\n\n\nLicense\n-------\n| Unless noted explicitly, all code in this repository is licensed permissively under the `Apache License\n  v2.0 <http://www.apache.org/licenses/LICENSE-2.0>`__.\n| A copy of that license is distributed with each software project.\n\nCopyright and Export Classification\n-----------------------------------\n\n::\n\n    Copyright 2019, by the California Institute of Technology. ALL RIGHTS RESERVED. \n    United States Government Sponsorship acknowledged. Any commercial use must be \n    negotiated with the Office of Technology Transfer at the California Institute \n    of Technology.\n    This software may be subject to U.S. export control laws. By accepting this software, \n    the user agrees to comply with all applicable U.S. export laws and regulations. \n    User has the responsibility to obtain export licenses, or other export authority \n    as may be required before exporting such information to foreign countries or \n    providing access to foreign persons.\n\n.. |image7| image:: https://podaac.jpl.nasa.gov/sites/default/files/image/custom_thumbs/podaac_logo.png\n"
 },
 {
  "repo": "DFO-Ocean-Navigator/Ocean-Data-Map-Project",
  "language": "Python",
  "readme_contents": "# Ocean Navigator\n\n[![CodeFactor](https://www.codefactor.io/repository/github/dfo-ocean-navigator/ocean-data-map-project/badge)](https://www.codefactor.io/repository/github/dfo-ocean-navigator/ocean-data-map-project)\n[![Lint Python](https://github.com/DFO-Ocean-Navigator/Ocean-Data-Map-Project/actions/workflows/lint_python.yml/badge.svg)](https://github.com/DFO-Ocean-Navigator/Ocean-Data-Map-Project/actions/workflows/lint_python.yml)\n[![Python tests](https://github.com/DFO-Ocean-Navigator/Ocean-Data-Map-Project/actions/workflows/python-tests.yml/badge.svg)](https://github.com/DFO-Ocean-Navigator/Ocean-Data-Map-Project/actions/workflows/python-tests.yml)\n\n## Contents\n* Overview\n* Development\n* Automate CLASS4 pickle generation\n\n---\n\n## Overview\n\nOcean Navigator is a Data Visualization tool that enables users to discover and view 3D ocean model output quickly and easily.\n\nThe model outputs are stored as [NetCDF4](https://en.wikipedia.org/wiki/NetCDF) files. Our file management is now handled by an SQLite3 process that incrementally scans the files for a dataset, and updates a corresponding table so that the Python layer can only open the exact files required to perform computations; as opposed to the THREDDS aggregation approach which serves all the files in a dataset as a single netcdf file. The THREDDS approach was unable to scale to the sheer size of the datasets we deal with.\n\nThe server-side component of the Ocean Navigator is written in Python 3, using the Flask web API. Conceptually, it is broken down into three components:\n\n-\tQuery Server\n\n\tThis portion returns metadata about the selected dataset in JSON format. These queries include things like the list of variables in the dataset, the times covered, the list of depths for that dataset, etc.\n\n\tThe other queries include things such as predefined areas (NAFO divisions, EBSAs, etc), and ocean drifter paths. The drifter paths are loaded from NetCDF files, but all the other queries are loaded from KML files.\n\n-\tPlotting\n\n\tThis portion generates an image plot, which could be a map with surface fields (or fields at a particular depth), a transect through a defined part of the ocean, depth profiles of one or more points, etc. We use the matplotlib python module to generate the plots.\n\n\tBecause the model grid rarely lines up with the map projection, and profiles and transects don't necessarily fall on model grid points, we employ some regridding and interpolation to generate these plots. For example, for a map plot, we select all the model points that fall within the area, plus some extra around the edges and regrid to a 500x500 grid that is evenly spaced over the projection area. An added benefit of this regridding is that we can directly compare across models with different grids. This allows us to calculate anomalies on the fly by comparing the model to a climatology. In theory, this would also allow for computing derived outputs from variables in different datasets with different native grids.\n\n-\tTile Server\n\n\tThis portion is really a special case of the plotting component. The tile server serves 256x256 pixel tiles at different resolutions and projections that can be used by the OpenLayers web mapping API. This portion doesn't use matplotlib, as the tiles don't have axis labels, titles, legends, etc. The same style of interpolation/regridding is done to generate the data for the images.\n\n\tThe generated tiles are cached to disk after they are generated the first time, this allows the user request to bypass accessing the NetCDF files entirely on subsequent requests.\n\nThe user interface is written in Javascript using the React framework. This allows for a single-page, responsive application that offloads as much processing from the server onto the user's browser as possible. For example, if the user chooses to load points from a CSV file, the file is parsed in the browser and only necessary parts of the result are sent back to the server for plotting.\n\nThe main display uses the OpenLayers mapping API to allow the user to pan around the globe to find the area of interest. It also allows the user to pick an individual point to get more information about, draw a transect on the map, or draw a polygon to extract a map or statistics for an area.\n\n---\n\n## Development\n\n### Local Installation\nThe instructions for performing a local installation of the Ocean Data Map Project are available at:\n[https://github.com/DFO-Ocean-Navigator/Navigator-Installer/blob/master/README.md](https://github.com/DFO-Ocean-Navigator/Navigator-Installer/blob/master/README.md)\n\n* While altering Javascript code, it can be actively transpiled using:\n\t* `cd oceannavigator/frontend`\n\t* `yarn run dev`\n* There's also a linter available: `yarn run lint`.\n* For production use the command: \n\t* `rm -r oceannavigator/frontend`\n\t* `cd oceannavigator/frontend`\n\t* `yarn run build`\n\n### SQLite3 backend\nSince we're now using a home-grown indexing solution, as such there is now no \"server\" to host the files through a URL (at the moment). You also need to install the dependencies for the [netcdf indexing tool](https://github.com/DFO-Ocean-Navigator/netcdf-timestamp-mapper). Then, download a released binary for Linux systems [here](https://github.com/DFO-Ocean-Navigator/netcdf-timestamp-mapper/releases). You should go through the README for basic setup and usage details.\n\nThe workflow to import new datasets into the Navigator has also changed:\n1. Run the indexing tool linked above.\n2. Modify `datasetconfig.json` so that the `url` attribute points to the absolute path of the generated `.sqlite3` database.\n3. Restart web server.\n\n### Running the webserver for development\nAssuming the above installation script succeeded, your PATH should be set to point towards `${HOME}/miniconda/3/amd64/bin`, and the `navigator` conda environment has been activated.\n* Debug server (single-threaded):\n\t* `python ./bin/runserver.py`\n* Multi-threaded (via gUnicorn):\n\t* `./bin/runserver.sh`\n\n### Running the webserver for production\nUsing the launch-web-service.sh script will automatically determine how many processors are available, determine the platform's IP address, what port above 5000 can be used, print out the IP and port information. The IP:PORT information can then be copied to a web browser to access the Ocean Navigator web service either locally or shared with others. This script will also copy all information bring written to stdout and place the information in the ${HOME}/launch-on-web-service.log file.\n* Multi-threaded (via gUnicorn):\n        * `./bin/launch-web-service.sh`\n\n### Coding Style (Javascript)\nJavascript is a dynamically-typed language so it's super important to have clear and concise code, that demonstrates it's exact purpose.\n\n* Comment any code whose intention may not be self-evident (safer to have more comments than none at all).\n* Use `var`, `let`, and `const` when identifying variables appropriately:\n\t* `var`: scoped to the nearest function block. Modern ES6/Javascript doesn't really use this anymore because it usually leads to scoping conflicts. However, `var` allows re-declaration of a variable.\n\t* `let`: new keyword introduced to ES6 standard which is scoped to the *nearest block*. It's very useful when using `for()` loops (and similar), so don't predefine loop variable:\n\n\t\t* Bad:\n\t\t\t```\n\t\t\t\tmyfunc() {\n\t\t\t\t\tvar i;\n\t\t\t\t\t...\n\t\t\t\t\t// Some code\n\t\t\t\t\t...\n\t\t\t\t\tfor (i = 0; i < something; ++i) {\n\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t```\n\t\t* Good:\n\t\t\t```\n\t\t\t\tmyFunc() {\n\t\t\t\t\t...\n\t\t\t\t\t// Some code\n\t\t\t\t\t...\n\t\t\t\t\tfor (let i = 0; i < something; ++i) {\n\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t```\n\t\t\n\t\tKeep in mind that `let` *does not* allow re-declaration of a variable.\n\n\t* `const`: functionally identical to the `let` keyword, however disallows variable re-assignment. Just like const-correctness in C++, `const` is a great candidate for most variable declarations, as it immediately states that \"I am not being changed\". This leads to the next rule.\n* Use `const` when declaring l-values with `require()`. Example:\n\t```\n\t\tconst LOADING_IMAGE = require(\"../images/bar_loader.gif\");\n\t```\n* Unless using `for` loops, *DO NOT* use single-letter variables! It's an extreme nuisance for other programmers to understand the intention of the code if functions are littered with variables like: `s`, `t`, etc. Slightly more verbose code that is extremely clear will result in a much lower risk for bugs.\n\t* Bad:\n\t\t```\n\t\t\t$.when(var_promise, time_promise).done(function(v, t) {\n\t\t\t\t// Some code\n\t\t\t\t...\n\t\t\t}\n\t\t```\n\t* Good:\n\t\t```\n\t\t\t$.when(var_promise, time_promise).done(function(variable, time) {\n\t\t\t\t// Some code\n\t\t\t\t...\n\t\t\t}\n\n\t\t```\n* Try to avoid massive `if` chains. Obviously the most important thing is to get a feature/bugfix working. However if it results in a whole bunch of nested `if` statements, or `if`-`for`-`if`-`else`, etc., try to take that working result and incorporate perhaps a `switch`, or hashtable to make your solution cleaner, and more performant. If it's unavoidable, a well-placed comment would reduce the likelihood of a fellow developer trying to optimize it.\n\n### Coding Style (Python)\nComing soon...\n\n## Automate CLASS4 pickle generation\n\nIn order to generate the class4.pickle file daily. You should create a crontab entry for the user account hosting the Ocean Navigator instance. Use the command `crontab -e` to add the string `@daily ${HOME}/Ocean-Data-Map-Project/bin/launch-pickle.sh`. Then once a day at midnight the script launch-pickle.sh will index all the CLASS4 files.\n\n## Proper handling of the datasetconfig.json and oceannavigator.cfg configuration files\n\nIn order to provide a production ready and off-site configuration files. We have implemented a new configurations repository. When people clone the Ocean-Data-Map-Project repository they will need to perform an additional step of updating any defined submodules. The following command changes your working directory to your local Ocean-Data-Map-Project directory and then updates the submodules recursively.\n\n* cd ${HOME}/Ocean-Data-Map-Project ; git submodule update --init --recursive\n"
 },
 {
  "repo": "amnh/HackTheDeep",
  "language": "Matlab",
  "readme_contents": "# Hack The Deep\nRepository for AMNH's Fourth Annual BridgeUP: STEM Hackathon, Hack the Deep!\n\nThe root directory of this repository contains data, documents, and images that are general purpose and could pertain to multiple challenges. Challenge specific data, documents, and images can be found under the challenges directory in a subdirectory with the same name as that challenge. Cloning this repository will give you local copies of all source files involved with all challenges that are not available on hard drives at the event (see organizers at the event for details).\n\nEveryone attending Hack the Deep must read, agree to, and sign the [Code of Conduct](https://github.com/amnh/HackTheDeep/blob/master/CODE_OF_CONDUCT.md).\n\nDon't forget to check social media for hashtag [#HackTheDeep](https://twitter.com/search?f=tweets&vertical=default&q=hackthedeep&src=typd)!\n\nSee the [**wiki**](https://github.com/amnh/HackTheDeep/wiki) for all the details! PLEASE NOTE: The wiki is still a work in progress, but we are working to supply you with all the challenge details and data we can give you prior to the event.\n\nHackathon project repositories will eventually appear on the [HackTheDeep organization page](https://github.com/HackTheDeep)\n"
 },
 {
  "repo": "castelao/CoTeDe",
  "language": "Python",
  "readme_contents": "======\nCoTeDe\n======\n\n.. image:: https://joss.theoj.org/papers/10.21105/joss.02063/status.svg\n   :target: https://doi.org/10.21105/joss.02063\n\n.. image:: https://zenodo.org/badge/10284681.svg\n   :target: https://zenodo.org/badge/latestdoi/10284681\n\n.. image:: https://readthedocs.org/projects/cotede/badge/?version=latest\n   :target: https://cotede.readthedocs.io/en/latest/?badge=latest\n   :alt: Documentation Status\n\n.. image:: https://img.shields.io/travis/castelao/CoTeDe.svg\n   :target: https://travis-ci.org/castelao/CoTeDe\n\n.. image:: https://codecov.io/gh/castelao/CoTeDe/branch/master/graph/badge.svg\n   :target: https://codecov.io/gh/castelao/CoTeDe\n\n.. image:: https://img.shields.io/pypi/v/cotede.svg\n   :target: https://pypi.python.org/pypi/cotede\n\n.. image:: https://mybinder.org/badge_logo.svg\n   :target: https://mybinder.org/v2/gh/castelao/CoTeDe/master?filepath=docs%2Fnotebooks\n\n\n`CoTeDe <http://cotede.castelao.net>`_ is an Open Source Python package to quality control (QC) oceanographic data such as temperature and salinity.\nIt was designed to attend individual scientists as well as real-time operations on large data centers.\nTo achieve that, CoTeDe is highly customizable, giving the user full control to compose the desired set of tests including the specific parameters of each test, or choose from a list of preset QC procedures.\n\nI believe that we can do better than we have been doing with more flexible classification techniques, which includes machine learning. My goal is to minimize the burden on manual expert QC improving the consistency, performance, and reliability of the QC procedure for oceanographic data, especially for real-time operations.\n\nCoTeDe is the result from several generations of quality control systems that started in 2006 with real-time QC of TSGs and were later expanded for other platforms including CTDs, XBTs, gliders, and others.\n\n\n----------\nWhy CoTeDe\n----------\n\nCoTeDe contains several QC procedures that can be easily combined in different ways:\n\n- Pre-set standard tests according to the recommendations by GTSPP, EGOOS, XBT, Argo or QARTOD;\n- Custom set of tests, including user defined thresholds;\n- Two different fuzzy logic approaches: as proposed by Timms et. al 2011 & Morello et. al. 2014, and using usual defuzification by the bisector;\n- A novel approach based on Anomaly Detection, described by `Castelao 2021 <https://doi.org/10.1016/j.cageo.2021.104803>`_ (available since 2014 `<http://arxiv.org/abs/1503.02714>`_).\n\nEach measuring platform is a different realm with its own procedures, metadata, and meaningful visualization. \nSo CoTeDe focuses on providing a robust framework with the procedures and lets each application, and the user, to decide how to drive the QC.\nFor instance, the `pySeabird package <http://seabird.castelao.net>`_ is another package that understands CTD and uses CoTeDe as a plugin to QC.\n\n-------------\nDocumentation\n-------------\n\nA detailed documentation is available at http://cotede.readthedocs.org, while a collection of notebooks with examples is available at\nhttp://nbviewer.ipython.org/github/castelao/CoTeDe/tree/master/docs/notebooks/\n\n--------\nCitation\n--------\n\nIf you use CoTeDe, or replicate part of it, in your work/package, please consider including the reference:\n\nCastel\u00e3o, G. P., (2020). A Framework to Quality Control Oceanographic Data. Journal of Open Source Software, 5(48), 2063, https://doi.org/10.21105/joss.02063\n\n::\n\n  @article{Castelao2020,\n    doi = {10.21105/joss.02063},\n    url = {https://doi.org/10.21105/joss.02063},\n    year = {2020},\n    publisher = {The Open Journal},\n    volume = {5},\n    number = {48},\n    pages = {2063},\n    author = {Guilherme P. Castelao},\n    title = {A Framework to Quality Control Oceanographic Data},\n    journal = {Journal of Open Source Software}\n  }\n\nFor the Anomaly Detection techinique specifically, which was implemented in CoTeDe, please include the reference:\n\nCastel\u00e3o, G. P. (2021). A Machine Learning Approach to Quality Control Oceanographic Data. Computers & Geosciences, https://doi.org/10.1016/j.cageo.2021.104803\n\n::\n\n  @article{Castelao2021,\n    doi = {10.1016/j.cageo.2021.104803},\n    url = {https://doi.org/10.1016/j.cageo.2021.104803},\n    year = {2021},\n    publisher = {Elsevier},\n    author = {Guilherme P. Castelao},\n    title = {A Machine Learning Approach to Quality Control Oceanographic Data},\n    journal = {Computers and Geosciences}\n  }\n\nIf you are concerned about reproducibility, please include the DOI provided by Zenodo on the top of this page, which is associated with a specific release (version).\n"
 },
 {
  "repo": "stoqs/stoqs",
  "language": "Jupyter Notebook",
  "readme_contents": "Spatial Temporal Oceanographic Query System\n-------------------------------------------\n\n[![Build Status](https://travis-ci.org/stoqs/stoqs.svg)](https://travis-ci.org/stoqs/stoqs/branches)\n[![Requirements Status](https://requires.io/github/stoqs/stoqs/requirements.svg?branch=master)](https://requires.io/github/stoqs/stoqs/requirements/?branch=master)\n[![PyUp](https://pyup.io/repos/github/MBARIMike/stoqs/shield.svg)](https://pyup.io/account/repos/github/MBARIMike/stoqs/)\n[![DOI](https://zenodo.org/badge/20654/stoqs/stoqs.svg)](https://zenodo.org/badge/latestdoi/20654/stoqs/stoqs)\n \nSTOQS is a geospatial database and web application designed to give oceanographers\nefficient integrated access to *in situ* measurement and *ex situ* sample data.\nSee http://www.stoqs.org.\n\n#### Getting started with a STOQS development system \n\nFirst, install [Vagrant](https://www.vagrantup.com/) and and [VirtualBox](https://www.virtualbox.org/)\n&mdash; there are standard installers for Mac, Windows, and Linux. (You will also need \n[X Windows System](doc/instructions/XWINDOWS.md) sofware on your computer.) Then create an empty folder off your \nhome directory such as `Vagrants/stoqsvm`, open a command prompt window, cd to that folder, and enter these \ncommands:\n\n```bash\ncurl \"https://raw.githubusercontent.com/stoqs/stoqs/master/Vagrantfile\" -o Vagrantfile\ncurl \"https://raw.githubusercontent.com/stoqs/stoqs/master/provision.sh\" -o provision.sh\nvagrant plugin install vagrant-vbguest\nvagrant up --provider virtualbox\n```\nThe Vagrantfile and provision.sh will provision a development system with an NFS mounted\ndirectory from your host operating system. If your host doesn't support serving files via\nNFS (most Windows hosts don't support NFS file serving) then you'll need to edit these files \nbefore executing `vagrant up`. Look for the `support NFS file serving` comments in these \nfiles for the lines you need to change.\n\nThe `vagrant up` command takes an hour or so to provision and setup a complete CentOS 7 \nSTOQS Virtual Machine that also includes MB-System, InstantReality, and all the Python data science \ntools bundled in packages such as Anaconda.  You will be prompted for your admin password\nfor configuring a shared folder from the VM (unless you've disabled the NFS mount).  All connections to this VM are \nperformed from the the directory you installed it in; you must cd to it (e.g. `cd\n~/Vagrants/stoqsvm`) before logging in with the `vagrant ssh -- -X` command.  After \ninstallation finishes log into your new VM and test it:\n\n```bash\nvagrant ssh -- -X                        # Wait for [vagrant@localhost ~]$ prompt\nexport STOQS_HOME=/vagrant/dev/stoqsgit  # Use STOQS_HOME=/home/vagrant/dev/stoqsgit if not using NFS mount\ncd $STOQS_HOME && source venv-stoqs/bin/activate\nexport DATABASE_URL=postgis://stoqsadm:CHANGEME@127.0.0.1:5438/stoqs\n./test.sh CHANGEME load noextraload\n```\n\nIn another terminal window start the development server (after a `cd ~/Vagrants/stoqsvm`):\n\n```bash\nvagrant ssh -- -X                        # Wait for [vagrant@localhost ~]$ prompt\nexport STOQS_HOME=/vagrant/dev/stoqsgit  # Use STOQS_HOME=/home/vagrant/dev/stoqsgit if not using NFS mount\ncd $STOQS_HOME && source venv-stoqs/bin/activate\nexport DATABASE_URL=postgis://stoqsadm:CHANGEME@127.0.0.1:5438/stoqs\nstoqs/manage.py runserver 0.0.0.0:8000 --settings=config.settings.local\n```\n\nVisit your server's STOQS User Interface using your host computer's browser:\n\n    http://localhost:8008\n\nMore instructions are in the doc/instructions directory &mdash; see [LOADING](doc/instructions/LOADING.md) \nfor specifics on loading your own data. For example, you may create your own database of an archived MBARI campaign:\n\n    cd stoqs\n    ln -s mbari_campaigns.py campaigns.py\n    loaders/load.py --db stoqs_cce2015\n\nYou are encouraged to contribute to the STOQS project! Please see [CONTRIBUTING](CONTRIBUTING.md)\nfor how to share your work.  Also, see example \n[Jupyter Notebooks](http://nbviewer.jupyter.org/github/stoqs/stoqs/blob/master/stoqs/contrib/notebooks)\nthat demonstrate specific analyses and visualizations that go beyond the capabilities of the STOQS User Interface.\nVisit the [STOQS Wiki pages](https://github.com/stoqs/stoqs/wiki) for updates and links to presentations.\nThe [stoqs-discuss](https://groups.google.com/forum/#!forum/stoqs-discuss) list in Google Groups is also \na good place to ask questions and engage in discussion with the STOQS user and developer communities.\n\nSupported by the David and Lucile Packard Foundation, STOQS undergoes continual development\nto help support the mission of the Monterey Bay Aquarium Research Institue.  If you have your\nown server you will occasionally want to get new features with:\n\n```bash\ngit pull\n```\n\n#### Production Deployment with Docker\n\nFirst, install [Docker](https://www.docker.com/) and [docker-compose](https://docs.docker.com/compose/install/)\non your system.  Then clone the repository; in the docker directory copy the `template.env` file to `.env` \nand edit it for your specific installation, then execute `docker-compose up`:\n\n```bash\ngit clone https://github.com/stoqs/stoqs.git stoqsgit\ncd stoqsgit/docker\ncp template.env .env\nchmod 600 .env      # You must then edit .env and change settings for your environment\ndocker-compose up\n```\nIf the directory set to the STOQS_VOLS_DIR variable in your .env file doesn't exist then the \nexecution of `docker-compose up` will create the postgresql database cluster, load a default \nstoqs database, and execute the unit and functional tests of the stoqs application.  If you\ndon't see these tests being executed (they will take several minutes) then check for error\nmessages.\n\nOnce you see `... [emperor] vassal /etc/uwsgi/django-uwsgi.ini is ready to accept requests`\nyou can visit the site at https://localhost &mdash; it uses a self-signed certificate, so your\nbrowser will complain and you will need to add an exception. (The nginx service also delivers \nthe same app at http://localhost:8000 without the certificate issue.)\n\nThe default settings in `template.env` will run a production nginx/uwsgi/stoqs server configured\nfor https://localhost in a Vagrant virtual machine. To configure a server for intranet or public serving of\nyour data follow the instructions provided in the comments for the settings in your `.env` file.\nAfter editing your `.env` file you will need to rebuild the images and restart the Docker \nservices, this time with the `-d` option to run the containers in the background:\n\n```bash\ndocker-compose build\ndocker-compose up -d\n```\n\nThe above commands should also be done following a `git pull` in order to deploy updated\nsoftware on your server.\n\nOne thing that's good to do is monitor logs and check for error messages, this can be done with:\n\n```\ndocker-compose logs -f\n```\n\n#### Using STOQS in Docker\n\nYou can execute Python code in the stoqs server from your host by prefacing it with `docker-compose exec stoqs`\n(Use `docker-compose run stoqs` to launch another container for long-running processes), for \nexample to load some existing MBARI campaign data:\n\n```bash\ndocker-compose run stoqs stoqs/loaders/load.py --db stoqs_simz_aug2013\n```\n\n(To load MBARI Campaigns you will need to have uncommented the `CAMPAIGNS_MODULE=stoqs/mbari_campaigns.py` \nline in your .env file. Make sure that you do not have a symbolic link named `campaigns.py` in the stoqs \ndirectory. This is needed only for a Vagrant development machine &mdash; it's best to keep the directory used\nfor a Docker deployment separate from one used for Vagrant.)\n\nIn another window monitor its output:\n\n```bash\ndocker-compose run stoqs tail -f /srv/stoqs/loaders/MolecularEcology/loadSIMZ_aug2013.out\n# Or (The stoqs code is bound as a volume in the container from the GitHub cloned location)\ntail -f stoqsgit/stoqs/loaders/MolecularEcology/loadSIMZ_aug2013.out\n```\n\nYou may also use `pg_restore` to more quickly load an existing Campaign database on your system.\nFor instructions click on the Campaign name in the top bar of a Campaign on another STOQS server, \nfor example on [MBARI's Public STOQS Server](https://stoqs.mbari.org).\n\n\n\nIf you use STOQS for your research please cite this publication:\n\n> McCann, M.; Schramm, R.; Cline, D.; Michisaki, R.; Harvey, J.; Ryan, J., \"Using STOQS (The spatial \n> temporal oceanographic query system) to manage, visualize, and understand AUV, glider, and mooring data,\" \n> in *Autonomous Underwater Vehicles (AUV), 2014 IEEE/OES*, pp.1-10, 6-9 Oct. 2014\n> doi: 10.1109/AUV.2014.7054414\n\n![STOQS logo](stoqs/static/images/STOQS_logo_gray1_689.png)\n\n"
 },
 {
  "repo": "JuliaOcean/AIBECS.jl",
  "language": "Julia",
  "readme_contents": "<a href=\"https://github.com/JuliaOcean/AIBECS.jl\">\n  <img src=\"https://user-images.githubusercontent.com/4486578/60554111-8fc27400-9d79-11e9-9ca7-6d78ee89ea70.png\" alt=\"logo\" title=\"The AIBECS logo: It represents three global marine biogeochemical cycles, where each element affects the others\" align=\"center\" width=\"50%\"/>\n</a>\n\n# AIBECS.jl\n\n*The ideal tool for exploring global marine biogeochemical cycles.*\n\n<p>\n  <a href=\"https://JuliaOcean.github.io/AIBECS.jl/stable/\">\n    <img src=\"https://img.shields.io/github/workflow/status/JuliaOcean/AIBECS.jl/Documentation?style=for-the-badge&label=Documentation&logo=Read%20the%20Docs&logoColor=white\">\n  </a>\n  <a href=\"https://doi.org/10.21105/joss.03814\">\n    <img src=\"https://img.shields.io/static/v1?label=JOSS&message=10.21105/joss.03814&color=9cf&style=flat-square\" alt=\"DOI badge\">\n  </a>\n  <a href=\"https://www.bpasquier.com/talk/osm_sandiego_2020/OSM_SanDiego_2020.pdf\">\n    <img src=https://img.shields.io/static/v1?label=Poster&message=OSM2020&color=9cf&style=flat-square>\n  </a>\n</p>\n\n<p>\n  <a href=\"https://doi.org/10.5281/zenodo.2864051\">\n    <img src=\"http://img.shields.io/badge/DOI-10.5281%20%2F%20zenodo.2864051-blue.svg?&style=flat-square\">\n  </a>\n  <a href=\"https://github.com/JuliaOcean/AIBECS.jl/blob/master/LICENSE\">\n    <img alt=\"License: MIT\" src=\"https://img.shields.io/badge/License-MIT-blue.svg?&style=flat-square\">\n  </a>\n</p>\n\n<p>\n  <a href=\"https://github.com/JuliaOcean/AIBECS.jl/actions\">\n    <img src=\"https://img.shields.io/github/workflow/status/JuliaOcean/AIBECS.jl/Mac%20OS%20X?label=OSX&logo=Apple&logoColor=white&style=flat-square\">\n  </a>\n  <a href=\"https://github.com/JuliaOcean/AIBECS.jl/actions\">\n    <img src=\"https://img.shields.io/github/workflow/status/JuliaOcean/AIBECS.jl/Linux?label=Linux&logo=Linux&logoColor=white&style=flat-square\">\n  </a>\n  <a href=\"https://github.com/JuliaOcean/AIBECS.jl/actions\">\n    <img src=\"https://img.shields.io/github/workflow/status/JuliaOcean/AIBECS.jl/Windows?label=Windows&logo=Windows&logoColor=white&style=flat-square\">\n  </a>\n  <a href=\"https://codecov.io/gh/JuliaOcean/AIBECS.jl\">\n    <img src=\"https://img.shields.io/codecov/c/github/JuliaOcean/AIBECS.jl/master?label=Codecov&logo=codecov&logoColor=white&style=flat-square\">\n  </a>\n</p>\n\n\n\n\n**AIBECS** (for **A**lgebraic **I**mplicit **B**iogeochemical **E**lemental **C**ycling **S**ystem, pronounced like the cool [ibex](https://en.wikipedia.org/wiki/Ibex)) is a Julia package that provides ocean biogeochemistry modellers with an easy-to-use interface for creating and running models of the ocean system.\n\nAIBECS is a system because it allows you to choose some biogeochemical tracers, define their interactions, select an ocean circulation and *Voil\u00e0!* \u2014 your model is ready to run.\n\n## Getting started\n\nIf you are new to AIBECS, head over to the [documentation](https://JuliaOcean.github.io/AIBECS.jl/stable/) and look for the tutorials.\n(You can also click on the big \"Documentation\" badge above.)\n\n## Concept\n\nThis package was developed to exploit linear-algebra tools and algorithms in Julia to efficiently simulate marine tracers.\nAIBECS represents global biogeochemical cycles with a discretized system of nonlinear ordinary differential equations that takes the generic form\n\n$$\\frac{\u2202\\boldsymbol{x}}{\u2202t} + \\mathbf{T} \\boldsymbol{x} = \\boldsymbol{G}(\\boldsymbol{x})$$\n\nwhere $\\boldsymbol{x}$ represents the model state variables, i.e., the marine tracer(s) concentration.\nFor a single tracer, $\\boldsymbol{x}$ can be interpreted as the 3D field of its concentration.\nIn AIBECS, $\\boldsymbol{x}$ is represented as a column vector (that's why it's **bold** and *italic*).\n\nThe operator $\\mathbf{T}$ is a spatial differential operator that represents the transport of tracers.\nFor example, for a single tracer transported by ocean circulation,\n\n$$\\mathbf{T} = \\nabla \\cdot(\\boldsymbol{u} - \\mathbf{K}\\nabla)$$\n\nrepresents the effects of advection and eddy-diffusion.\n($\\boldsymbol{u}$ is the 3D vector of the marine currents and $\\mathbf{K}$ is a 3\u00d73 eddy-diffusivity matrix.)\nThus, $\\mathbf{T}$ \"acts\" on $\\boldsymbol{x}$ such that $\\mathbf{T}\\boldsymbol{x}$ is the flux divergence of that tracer.\nIn AIBECS, $\\mathbf{T}$ is represented by a matrix (that's why it's **bold** and upstraight).\n\nLastly, the right-hand-side, $\\boldsymbol{G}(\\boldsymbol{x}$), represents the local sources minus sinks of each tracer, which must be provided as functions of the tracer(s) $\\boldsymbol{x}$.\n\nTo simulate tracers using the AIBECS, you just need to define the transport operators $\\mathbf{T}$ and the net sources and sinks $\\boldsymbol{G}$.\nThat's pretty much the whole concept!\n\n## References\n\nIf you use this package, please cite it.\n\nIf you use data provided by this package (like the ocean circulation from the OCIM), please cite them as well.\n\nFor convenience, all the references are available in [BibTeX](https://en.wikipedia.org/wiki/BibTeX) format in the [CITATION.bib](./CITATION.bib) file.\n\nAlso, if you want to do research using the AIBECS, and you think I could help, do not hesitate to contact me directly (contacts on my [website](www.bpasquier.com)), I would be happy to contribute and collaborate!\n\n<img src=\"https://www.nsf.gov/images/logos/NSF_4-Color_bitmap_Logo.png\" alt=\"NSF\" title=\"NSF_logo\" align=\"right\" height=\"50\"/>\n\nThe authors acknowledge funding from the Department of Energy grant DE-SC0016539 and from the National Science Foundation grant 1658380.\n"
 },
 {
  "repo": "gher-ulg/DIVA",
  "language": "Fortran",
  "readme_contents": "[![Project Status: Inactive \u2013 The project has reached a stable, usable state but is no longer being actively developed; support/maintenance will be provided as time allows.](https://www.repostatus.org/badges/latest/inactive.svg)](https://www.repostatus.org/#inactive)\n[![Build Status](https://travis-ci.org/gher-ulg/DIVA.svg?branch=master)](https://travis-ci.org/gher-ulg/DIVA)\n[![DOI](https://zenodo.org/badge/80114691.svg)](https://zenodo.org/badge/latestdoi/80114691)\n\n![made-with-bash](https://img.shields.io/badge/Made%20with-Bash-1f425f.svg) \n\n\nIMPORTANT: this original `DIVA` tool will remain available, but will not be further developped. For new features, users are invited to switch to the generalization in N-dimensions [`DIVAnd`](https://github.com/gher-ulg/DIVAnd.jl) using a very modern progamming language: [`Julia`](julialang.org/).\n\n\n# DIVA (Data-Interpolating Variational Analysis)\n\nDIVA allows the spatial interpolation of data (*analysis*) in an optimal way, comparable to *optimal interpolation* (OI). In comparison to OI, it takes into account coastlines, sub-basins and advection. Calculations are highly optimized and rely on a [*finite element*](https://en.wikipedia.org/wiki/Finite_element_method) resolution. \n\nTools to generate the finite element mesh are provided as well as tools to optimize the parameters of the analysis. Quality control of data can be performed and error fields can be calculated. In addition, *detrending* of data is possible. Finally 3D and 4D extensions are included with emphasis on direct computations of climatologies from [Ocean Data View](https://odv.awi.de/) (ODV) spreadsheet files.\n\n![Diva logo](https://cloud.githubusercontent.com/assets/11868914/24106959/c6d8fb44-0d89-11e7-921b-a36fcccf5a21.png)\n\n## Getting started\n\n### Prerequisites\n\n* A fortran compiler: [gfortran](https://gcc.gnu.org/wiki/GFortran), ifort, pgf, ...\n* The [NetCDF](https://www.unidata.ucar.edu/software/netcdf/netcdf-4/newdocs/netcdf-f90.html) library for Fortran for the output writing.\n* [gnuplot](http://www.gnuplot.info/) for the creation of graphics [optional].\n\nFor Debia architectures, you can run:\n```bash\nsudo apt-get install -y software-properties-common\t# needed to use add-apt-repository\nsudo add-apt-repository universe  \t                # needed to get netCDF\nsudo apt-get install -y git make                   # needed for the compilation\nsudo apt-get install -y gfortran netcdf-bin libnetcdf-dev libnetcdff-dev\n```\n\n### Installing\n\n1. Download the latest stable [release](https://github.com/gher-ulg/DIVA/releases) and extract the archive:\n```bash\ntar xvf DIVA-4.7.2.tar.gz\n```\nor clone the project and checkout the last version.\n2. Go in the source directory\n```bash\ncd DIVA-4.7.2/DIVA3D/src/Fortran/\n```\n3. Run the compilation script:\n```bash\nmake\n```\nNotes: \n- the compiler (by default `gfortran`) and its flags can be modified by editing `Makefile`\n- the netCDF _library_ and _include_ flags are deduced from `nf-config` command, which provides the options with which netCDF was build.     \nThe values can be specified differently (if for example you use a non-standard path) by editing the lines\n```bash\nexport nclib=$(shell nf-config --flibs)\nexport ncinc=$(shell nf-config --fflags)\n```\n\n### Testing\n\nGo in the main execution directory (*divastripped*) and run the tests:\n```bash\ncd ../../divastripped/\ndivatest\ndivatest0\n...\n```\n## How does it work?\n\nDIVA is a software tool developed for gridding in situ data.\nIt uses a finite-element method to solve a variational principle which takes into account:\n 1. the distance between analysis and data (observation constraint),\n 2. the regularity of the analysis (*smoothness* constraint),\n 3. physical laws (behaviour constraint). \n \n ![800px-diva_gridding_canary](https://cloud.githubusercontent.com/assets/11868914/24946939/09c918fc-1f65-11e7-9974-06264c70ec1e.png)\n\nThe advantage of the method over classic interpolation methods is multiple:\n* the coastline are taken into account during the analysis, since the variational principle is solved only in the region covered by the sea. This prevents the information from traveling across boundaries (e.g., peninsula, islands, etc) and then produce artificial mixing between water masses.\n* the numerical cost is not dependent on the number of data, but on the number of degrees of freedom, itself related to the size of the finite-element mesh. \n\n##  How to try DIVA without installing?\n\nIf you are familiar with Ocean Data View [ODV](http://odv.awi.de/) software tool, you can perfom DIVA gridding when plotting vertical or horizontal sections, as the other 25000 scientists using ODV.\n\nYou can also use basic DIVA features in a web application [Diva on Web](https://ec.oceanbrowser.net/emodnet/diva.html) if you have your data ready in a simple three-column ascii file or ODV ascii spreadsheet format. \n\n![divaonweb](https://cloud.githubusercontent.com/assets/11868914/24947093/a980dd26-1f65-11e7-8715-f1e50bd69a83.png)\n\n## Related tools \n\n* [DIVAnd.jl](https://github.com/gher-ulg/divand.jl) performs n-dimensional variational analysis of arbitrarily located observations (written in Julia language).\n* [divand.py](https://github.com/gher-ulg/divand.py) is the Python interface to [DIVAnd.jl](https://github.com/gher-ulg/divand.jl).\n* [DivaPythonTools](https://github.com/gher-ulg/DivaPythonTools) is a set of utilies to read, write and plot the content of input or output files used in Diva.\n\n## Publications & documents \n\nCheck the [GHER publications]([http://orbi.ulg.ac.be/ulg-report?query=%28%28affil%3A%22GeoHydrodynamics+and+Environment+Research%22%29+OR+%28affil%3A%22Oc%C3%A9anographie+physique%22%29%29&model=a&format=apa&sort_by0=1&order0=DESC&sort_by1=3&order1=ASC&sort_by2=2&order2=ASC&output=html&language=en&title=GHER+publications]) for the most recent updates.\n\n### Articles\n\n#### Theory\n\nBarth, A., Beckers, J.-M., Troupin, C., Alvera-Azc\u00e1rate, A., and Vandenbulcke, L.: divand-1.0: n-dimensional variational data analysis for ocean observations, Geosci. Model Dev., 7, 225-241, [doi:10.5194/gmd-7-225-2014](https://doi.org/10.5194/gmd-7-225-2014), 2014.\n\nTroupin, C.; Sirjacobs, D.; Rixen, M.; Brasseur, P.; Brankart, J.-M.; Barth, A.; Alvera-Azc\u00e1rate, A.; Capet, A.; Ouberdous, M.; Lenartz, F.; Toussaint, M.-E. & Beckers, J.-M. (2012) Generation of analysis and consistent error fields using the Data Interpolating Variational Analysis (Diva). *Ocean Modelling*, **52-53**: 90-101. doi:[10.1016/j.ocemod.2012.05.002](https://doi.org/10.1016/j.ocemod.2012.05.002)\n\nBeckers, J.-M.; Barth, A.; Troupin, C. & Alvera-Azc\u00e1rate, A. Some approximate and efficient methods to assess error fields in spatial gridding with DIVA (Data Interpolating Variational Analysis) (2014). *Journal of Atmospheric and Oceanic Technology*,  **31**: 515-530. doi:[10.1175/JTECH-D-13-00130.1](https://doi.org/10.1175/JTECH-D-13-00130.1)\n\n#### Applications \n\nCapet, A.; Troupin, C.; Carstensen, J.; Gr\u00e9goire, M. & Beckers, J.-M. Untangling spatial and temporal trends in the variability of the Black Sea Cold Intermediate Layer and mixed Layer Depth using the DIVA detrending procedure (2014). *Ocean Dynamics*, **64**: 315-324. doi:[10.1007/s10236-013-0683-4](https://doi.org/10.1007/s10236-013-0683-4)\n\nTroupin, C.; Mach\u00edn, F.; Ouberdous, M.; Sirjacobs, D.; Barth, A. & Beckers, J.-M. High-resolution Climatology of the North-East Atlantic using Data-Interpolating Variational Analysis (Diva) (2010). *Journal of Geophysical Research*, **115**: C08005. doi:[10.1029/2009JC005512](https://doi.org/10.1029/2009JC005512)\n\n### User guide\n\nThe most recent version is available in [PDF](https://github.com/gher-ulg/Diva-User-Guide/raw/master/DivaUserGuide.pdf).\n\n### Posters and presentations\n\nCheck the complete list of documents hosted through the [ULiege Orbi catalogue](http://orbi.ulg.ac.be/orbi-report?query=%28%28affil%3A%22GeoHydrodynamics+and+Environment+Research%22%29+OR+%28affil%3A%22Oc%C3%A9anographie+physique%22%29%29&model=a&format=apa&sort_by0=1&order0=DESC&sort_by1=3&order1=ASC&sort_by2=2&order2=ASC&output=html&language=en&title=GHER+publications).\n\n## Acknowledgments\n\nThe DIVA development has received funding from:\n- the European Union Sixth Framework Programme (FP6/2002-2006) under grant agreement n\u00b0 026212, [SeaDataNet](http://www.seadatanet.org/), \n- the Seventh Framework Programme (FP7/2007-2013) under grant agreement n\u00b0 283607, SeaDataNet II, \n- SeaDataCloud and \n- [EMODNet](http://www.emodnet.eu/) (MARE/2008/03 - Lot 3 Chemistry - SI2.531432) from the [Directorate-General for Maritime Affairs and Fisheries](http://ec.europa.eu/dgs/maritimeaffairs_fisheries/index_en.htm).\n"
 },
 {
  "repo": "lkilcher/dolfyn",
  "language": "Python",
  "readme_contents": "<img src=\"img/logo.png\" width=\"70\"> DOLfYN\n=======================\n![Build](https://github.com/lkilcher/dolfyn/actions/workflows/build.yml/badge.svg)\n[![Coverage Status](https://coveralls.io/repos/github/lkilcher/dolfyn/badge.svg?branch=master)](https://coveralls.io/github/lkilcher/dolfyn?branch=master)\n[![Documentation Status](https://readthedocs.org/projects/dolfyn/badge/?version=latest)](https://dolfyn.readthedocs.io/en/latest/?badge=latest)\n\nBIG NEWS!!!\n------\n\nHello everyone! Just so that you know, we have released\ndolfyn 1.0! This is a MAJOR REFACTOR of the code so that DOLfYN is\nnow built on xarray, rather than the somewhat contrived and\npurpose-built `pyDictH5` package.\n\nThis means that DOLfYN 1.0 is _not_ backwards compatible with\nearlier version. This, in turn, means two things:\n\n1. The data files (`.h5` files) you created with earlier versions\nof DOLfYN will no longer load with DOLfYN 1.0.0.\n2. The syntax of DOLfYN 1.0 is completely different from earlier version.\n\nBecause of this, it's probably easiest to continue using earlier\nversions of DOLfYN for your old data. If you want to bring some data\ninto DOLfYN 1.0, you will need to\n`dolfyn.read(binary_source_file.ext)`, and then refactor your code to\nwork properly with DOLfYN's new syntax. I may be providing some\nupdates to dolfyn 0.12 via the v0.12-backports branch (and associated\nreleases), but I doubt that will last long. If you are\nusing 0.13, we recommend switching to 1.0.\n\nVery sorry that we didn't communicate the plan for this change, but\nthe truth is that we simply don't know who our users are. The good\nnews is that I think in the long run this will make DOLfYN a much more\nrobust, powerful, and compatible tool -- especially because we now\nwrite/load xarray-formatted netcdf4 files, which is becoming a\nstandard.\n\nA **HUGE THANK YOU** to @jmcvey3 who did the vast majority of the work\nto make this happen.\n\n\nSummary\n------\n\nDOLfYN is the Doppler Oceanography Library for pYthoN.\n\nIt is designed to read and work with Acoustic Doppler Velocimeter\n(ADV) and Acoustic Doppler Profiler (ADP/ADCP) data. DOLfYN includes\nlibraries for reading binary Nortek(tm) and Teledyne RDI(tm) data\nfiles.\n* Read in binary data files from acoustic Doppler instruments\n* Clean data\n* Rotate vector data through coordinate systems (i.e. beam - instrument - Earth frames of reference)\n* Motion correction for buoy-mounted ADV velocity measurements (via onboard IMU data)\n* Bin/ensemble averaging\n* Calculate turbulence statistics\n\nDocumentation\n-------------\n\nFor details visit the \n[DOLfYN homepage](https://dolfyn.readthedocs.io/en/latest/).  \n\nInstallation\n------------\n\nDOLfYN requires Python 3.7 or later and a number of dependencies. See the \n[install page](https://dolfyn.readthedocs.io/en/latest/install.html)\nfor greater details.\n\nLicense\n-------\n\nDOLfYN is copyright through the National Renewable Energy Laboratory, \nPacific Northwest National Laboratory, and Sandia National Laboratories. \nThe software is distributed under the Revised BSD License.\nSee the [license](LICENSE.txt) for more information.\n\n"
 },
 {
  "repo": "OceanLabPy/OceanLab",
  "language": "Python",
  "readme_contents": "# OceanLab\n\nPackage of Python scripts for Oceanography  (Python +3.6)\n\n## Code Example\n\nCheck `examples` folder in our [github repository](../../tree/master/examples).\n\n## Installation\n\n`pip install OceanLab`\n\n## Modules\n\n- **OA**\n  - *vectoa()*: Objective analysis for vectorial fields;\n  - *scaloa()*: Objective analysis for scalar fields;\n- **DYN**\n  - *dyn_amp()*: Makes the projection of every dynamical mode to velocity to obtain its amplitude;\n  - *zeta()*: Calculates the vorticity field by velocity field;\n  - *psi2uv()*: Calculates the velocity field by stream function scalar field;\n  - *vmodes()*: Calculates the QG pressure modes from N2 profile;\n  - *eqmodes()*: Calculates the equatorial pressure and vertical velocity modes from N2 profile;\n- **EOF**\n  - *eoft()*: Calculates the Empirical Orthogonal Functions;\n  - *my_eof_interp()*: Fillgaps on matrix based on EOFs (translated from Cesar Rocha Matlab version);\n  - *ceof()*: Performs the Complex (or Hilbert) Empirical Orthogonal Functions decomposition;\n  - *reconstruct_ceof()*: Reconstructs the CEOF modes individually;\n- **UTILS**\n  - *argdistnear()*: Searchs the position of the closest points in an array to a reference point;\n  - *meaneddy()*: Performs an eddy-mean decomposition with a low-pass filter;\n\n\n## Contributors\n\nEveryone can contribute to this code. Some of functions were based on Filipe Fernandes or Cesar Rocha functions and some of them were created with help of Dante C. Napolitano, H\u00e9lio M. R. Almeida and Wandrey Watanabe at Ocean Dynamics Lab of University of S\u00e3o Paulo (USP).\n"
 },
 {
  "repo": "MikkoVihtakari/ggOceanMaps",
  "language": "R",
  "readme_contents": "---\noutput: github_document\n---\n\n```{r, echo = FALSE}\nknitr::opts_chunk$set(\n  collapse = TRUE,\n  message = FALSE, \n  warning = FALSE,\n  comment = \"#>\",\n  fig.path = \"man/figures/README-\"\n)\n```\n\n```{r echo=FALSE, message=FALSE, warning=FALSE}\n#library(knitr)\n#knitr::opts_chunk$set(message = FALSE, warning = FALSE, comment = \"#>\")\n#knit_hooks$set(optipng = hook_optipng)\n#knit_hooks$set(pngquant = hook_pngquant)\n```\n\n\n# ggOceanMaps\n**Plot data on oceanographic maps using ggplot2. R package version `r packageVersion(\"ggOceanMaps\")`**\n\n<!-- badges: start -->\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.4554714.svg)](https://doi.org/10.5281/zenodo.4554714)\n[![R-CMD-check](https://github.com/MikkoVihtakari/ggOceanMaps/workflows/R-CMD-check/badge.svg)](https://github.com/MikkoVihtakari/ggOceanMaps/actions)\n[![CRAN_Status_Badge](https://www.r-pkg.org/badges/version/ggOceanMaps)](https://CRAN.R-project.org/package=ggOceanMaps)\n<!-- badges: end -->\n\n<!-- [![R-CMD-check](https://github.com/MikkoVihtakari/ggOceanMaps/workflows/R-CMD-check/badge.svg)](https://github.com/MikkoVihtakari/ggOceanMaps/actions/workflows/R-CMD-check.yaml) -->\n\n## Overview\n\nThe ggOceanMaps package for [R](https://www.r-project.org/) allows plotting data on bathymetric maps using [ggplot2](https://ggplot2.tidyverse.org/reference). The package is designed for ocean sciences and greatly simplifies bathymetric map plotting anywhere around the globe. ggOceanMaps uses openly available geographic data. Citing the particular data sources is advised by the CC-BY licenses whenever maps from the package are published (see the [*Citations and data sources*](#citations-and-data-sources) section). \n\nThe ggOceanMaps package has been developed by the [Institute of Marine Research](https://www.hi.no/en). Note that the package comes with absolutely no warranty and that maps generated by the package are meant for plotting scientific data only. The maps are coarse generalizations of third-party data and therefore inaccurate. Any [bug reports and code fixes](https://github.com/MikkoVihtakari/ggOceanMaps/issues) are warmly welcomed. See [*Contributions*](#contributions) for further details.\n\n## Installation\n\nThe package is available on [CRAN](https://CRAN.R-project.org/package=ggOceanMaps) and as a [GitHub version](https://github.com/MikkoVihtakari/ggOceanMaps), which is updated more frequently than the CRAN version. \n\nInstallation of the CRAN version:\n\n```{r eval = FALSE}\ninstall.packages(\"ggOceanMaps\")\n```\n\nDue to the package size limitations, ggOceanMaps requires the [ggOceanMapsData](https://github.com/MikkoVihtakari/ggOceanMapsData) package which stores shapefiles used in low-resolution maps.\n\nThe GitHub version of ggOceanMaps can be installed using the [**devtools**](https://cran.r-project.org/web/packages/devtools/index.html) package. \n\n```{r eval = FALSE}\ndevtools::install_github(\"MikkoVihtakari/ggOceanMapsData\") # required by ggOceanMaps\ndevtools::install_github(\"MikkoVihtakari/ggOceanMaps\")\n```\n\n## Usage\n\n**ggOceanMaps** extends on [**ggplot2**](http://ggplot2.tidyverse.org/reference/). The package uses spatial shapefiles, [GIS packages for R](https://cran.r-project.org/web/views/Spatial.html) to manipulate, and the [**ggspatial**](https://cran.r-project.org/web/packages/ggspatial/index.html) package to help to plot these shapefiles. The shapefile plotting is conducted internally in the `basemap` function and uses [ggplot's sf object plotting capabilities](https://ggplot2.tidyverse.org/reference/ggsf.html). Maps are plotted using the `basemap()` or `qmap()` functions that work almost similarly to [`ggplot()` as a base](https://ggplot2.tidyverse.org/reference/index.html) for adding further layers to the plot using the `+` operator. The maps generated this way already contain multiple ggplot layers. Consequently, the [`data` argument](https://ggplot2.tidyverse.org/reference/ggplot.html) needs to be explicitly specified inside `geom_*` functions when adding `ggplot2` layers. Depending on the location of the map, the underlying coordinates may be projected. Decimal degree coordinates need to be transformed to the projected coordinates using the `transform_coord`, [ggspatial](https://paleolimbot.github.io/ggspatial/), or [`geom_sf` functions.](https://ggplot2.tidyverse.org/reference/ggsf.html)\n\n```{r}\nlibrary(ggOceanMaps)\n\ndt <- data.frame(lon = c(-30, -30, 30, 30), lat = c(50, 80, 80, 50))\n\nbasemap(data = dt, bathymetry = TRUE) + \n  geom_polygon(data = transform_coord(dt), aes(x = lon, y = lat), color = \"red\", fill = NA)\n```\n\nSee the [ggOceanMaps website](https://mikkovihtakari.github.io/ggOceanMaps/index.html), [function reference](https://mikkovihtakari.github.io/ggOceanMaps/reference/index.html), and the [user manual](https://mikkovihtakari.github.io/ggOceanMaps/articles/ggOceanMaps.html) for how to use and modify the maps plotted by the package. You may also find [these slides about the package](https://aen-r-workshop.github.io/4-ggOceanMaps/ggOceanMaps_workshop.html#1) useful. \n\n## Data path\n\nWhile ggOceanMaps allows plotting any custom-made shapefiles, the package contains a shortcut to plot higher resolution maps for [certain areas needed by the author](https://github.com/MikkoVihtakari/ggOceanMapsLargeData/tree/master/data) without the need of generating the shapefiles manually. These high-resolution shapefiles are downloaded from the [ggOceanMapsLargeData](https://github.com/MikkoVihtakari/ggOceanMapsLargeData) repository. As a default, the shapefiles are downloaded into a temporary directory meaning that the user would need to download the large shapefiles every time they restart R. This limitation is set by [CRAN policies](https://cran.r-project.org/web/packages/policies.html). You can define a custom folder for high-resolution shapefiles on your computer by modifying your .Rprofile file (e.g. `usethis::edit_r_profile()`). Add the following lines to the file: \n\n```{r eval = FALSE}\n.ggOceanMapsenv <- new.env()\n.ggOceanMapsenv$datapath <- 'YourCustomPath'\n```\n\nIt is smart to use a directory R has writing access to. For example `\"~/Documents/ggOceanMapsLargeData\"` would work for most operating systems.\n\nYou will need to set up the data path to your .Rprofile file only once and ggOceanMaps will find the path even though you updated your R or packages. ggOceanMaps will inform you about your data path when you load the package. \n\n## Citations and data sources\n\nThe data used by the package are not the property of the Institute of Marine Research nor the author of the package. It is, therefore, important that you cite the data sources used in a map you generate with the package. The spatial data used by this package have been acquired from the following sources:\n\n- **ggOceanMapsData land polygons.** [Natural Earth Data](https://www.naturalearthdata.com/downloads/10m-physical-vectors/) 1:10m Physical Vectors with the Land and Minor Island datasets combined. Distributed under the [CC Public Domain license](https://creativecommons.org/publicdomain/) ([terms of use](https://www.naturalearthdata.com/about/terms-of-use/)).\n- **ggOceanMapsData glacier polygons.** [Natural Earth Data](https://www.naturalearthdata.com/downloads/10m-physical-vectors/) 1:10m Physical Vectors with the Glaciated Areas and Antarctic Ice Shelves datasets combined. Distributed under the [CC Public Domain license](https://creativecommons.org/publicdomain/) ([terms of use](https://www.naturalearthdata.com/about/terms-of-use/)).\n- **ggOceanMapsData bathymetry.** [Amante, C. and B.W. Eakins, 2009. ETOPO1 1 Arc-Minute Global Relief Model: Procedures, Data Sources and Analysis. NOAA Technical Memorandum NESDIS NGDC-24. National Geophysical Data Center, NOAA](https://www.ngdc.noaa.gov/mgg/global/relief/ETOPO1/docs/ETOPO1.pdf). Distributed under the [U.S. Government Work license](https://www.usa.gov/government-works).\n- **Detailed shapefiles of Svalbard and Norwegian coast in [ggOceanMapsLargeData](https://github.com/MikkoVihtakari/ggOceanMapsLargeData)** are from [Geonorge.no](https://www.geonorge.no/). Distributed under the [CC BY 4.0 license](https://creativecommons.org/licenses/by/4.0/).\n- **Detailed bathymetry of the Barents Sea in [ggOceanMapsLargeData](https://github.com/MikkoVihtakari/ggOceanMapsLargeData)** is vectorized from the [General Bathymetric Chart of the Oceans](https://www.gebco.net/data_and_products/gridded_bathymetry_data/) 15-arcsecond 2020 grid. [Terms of use](https://www.gebco.net/data_and_products/gridded_bathymetry_data/gebco_2019/grid_terms_of_use.html)\n\nFurther, please cite the package whenever maps generated by the package are published. For up-to-date citation information, please use:\n\n```{r}\ncitation(\"ggOceanMaps\")\n```\n\n## Getting help\n\nIf your problem does not involve bugs in ggOceanMaps, the quickest way of getting help is posting your problem to [Stack Overflow](https://stackoverflow.com/questions/tagged/r). Please remember to include a reproducible example that illustrates your problem.\n\n## Contributions\n\nAny contributions to the package are more than welcome. Please contact the package maintainer Mikko Vihtakari (<mikko.vihtakari@hi.no>) to discuss your ideas on improving the package. Bug reports and corrections should be submitted directly to [the GitHub site](https://github.com/MikkoVihtakari/ggOceanMaps/issues). Please include a [minimal reproducible example](https://en.wikipedia.org/wiki/Minimal_working_example). Considerable contributions to the package development will be credited with authorship. \n\n## Debugging installation\n\nAfter a successful installation, the following code should return a plot shown under\n\n```{r}\nlibrary(ggOceanMaps)\nbasemap(60)\n```\n\nIf the `basemap()` function complains about ggOceanMapsData package not being available, the drat repository may have issues (assuming you followed the installation instructions above). Try installing the ggOceanMapsData package using the devtools/remotes package. The data package does not contain any C++ code and should compile easily. \n\nIf you encounter problems during the devtools installation, you may set the `upgrade` argument to `\"never\"` and try the following steps: \n\n1. Manually update all R packages you have installed (Packages -> Update -> Select all -> Install updates in R Studio). If an update of a package fails, try installing that package again using the `install.packages` function or the R Studio menu. \n1. Run `devtools::install_github(\"MikkoVihtakari/ggOceanMaps\", upgrade = \"never\")`.\n1. If the installation of a dependency fails, try installing that package manually and repeat step 2.\n1. Since R has lately been updated to 4.0, you may have to update your R to the latest major version for all dependencies to work (`stars`, `rgdal` and `sf` have been reported to cause trouble during the installation).\n"
 },
 {
  "repo": "powellb/seapy",
  "language": "Python",
  "readme_contents": "# State Estimation and Analysis in PYthon (SEAPY)\n\nTools for working with ocean models and data.\n\nSEAPY requires: basemap, h5py, joblib, netcdf4, numpy, numpy_groupies, rich and scipy.\n\n\n## Installation\n\n### Install from Conda-Forge\n\nInstall from [conda-forge](https://conda-forge.org/) with the Conda package manager:\n```\n$ conda install -c conda-forge seapy\n```\n\nYou should also consider making conda-forge your default channel. See the [conda-forge tips and tricks page](https://conda-forge.org/docs/user/tipsandtricks.html).\n\nThe Conda-Forge [SEAPY feedstock](https://github.com/conda-forge/seapy-feedstock) is maintained by Filipe Fernandes, [ocefpaf](https://github.com/ocefpaf/). As of February 2021 there are binary packages on all the platforms that Conda-Forge supports: Python 3.6 through 3.9 on Linux, Windows and Mac OSX (all 64-bit).\n\nTo remove seapy:\n```\n$ conda remove seapy\n```\n\n## Install from PyPI\n\nInstall from [PyPI](https://pypi.org/) with PIP:\n```\n$ pip install seapy-ocean\n```\n\nNote that on PyPI (but nowhere else) the package name is seapy-ocean to avoid a name clash with another package. The module name is still seapy.\n\nSEAPY packages on PyPI have been built and uploaded by Mark Hadfield, [hadfieldnz](https://pypi.org/user/hadfieldnz/). There is a source distribution that should build with no problems on Linux (and Mac OSX, but we haven't tested it). In the pst there have been binary distributions for Windows (64-bit), but these have now been deleted as binary builds with PIP are no longer supported.\n\nIn a Conda environment, it is quite possible to install with PIP, but dependency handling and updating will be cleaner if you use the Conda package.\n\nTo remove seapy-ocean\n```\n$ pip uninstall seapy-ocean\n```\n\n## Install from source code on GitHub.com\n\nThe SEAPY source code is maintained by Brian Powell, (powellb)[https://github.com/powellb]. Releases are made on the [master branch](https://github.com/powellb/seapy/tree/master)\n\nInstall from [GitHub.com](https://github.com/) with PIP:\n```\n$ pip install git+git://github.com/powellb/seapy@master\n```\n\nOR clone a copy of the source and install in editable mode, eg:\n```\n$ git clone https://github.com/powellb/seapy.git\n$ pip install -e seapy\n```\n\nWith an [editable-mode](https://pip.pypa.io/en/stable/cli/pip_install/#editable-installs) installation, changes you make to your copy of the source code will take effect when you import the module.\n\nIn principle it is possible to build from source on Windows--and success with this has been achieved in the past--but the process tends to break with changes in the environment or Python version, so we don't recommend it or support it.\n\n## Contributing\n\n\nIf you've installed from source in editable mode, then you should definitely consider forking your own copy of the repository. This allows you to keep your changes under revision control on GitHub.com and potentially contribute them to the main project. You should follow the procedures described in this [Git Workflow](https://www.asmeurer.com/git-workflow/) document.\n\n[Forking on GitHub.com](https://help.github.com/en/articles/fork-a-repo) is a lightweight process that won't complicate your workflow and keeps the relationship between your work and the original project clear, so it is strongly advised to do it early. However the immutable and unique nature of Git commits means that you can create and populate a fork later if you want to, as long as you have saved your work somewhere in Git format. To create a fork you will need a [GitHub.com user account](https://help.github.com/en/articles/signing-up-for-a-new-github-account).\n\n\nAll your changes should be committed to a branch other than \"master\", which is reserved for the master branch in Brian Powell's repository (or copies thereof). A common practice in the [existing SEAPY forks](https://github.com/powellb/seapy/network/members) is to use a branch name matching your user name for your own work. However if you are developing a specific feature or bug fix to be pulled into master, it may be sensible to name the branch after that feature or bug fix.\n\n\n## Examples\n\nMany of the time-saving features are in generating fields for running the ROMS model.\n\n1. To load the meta information about a model (ROMS, HYCOM, MITgcm, POM, SODA), load an output file (history, average, climatology, grid, etc.) via:\n\n        >> mygrid = seapy.model.asgrid(filename)\n\n        >> mygrid\n        C-Grid: 32x194x294\n\n        >> print(mygrid)\n        filename\n        32x194x294: C-Grid with S-level\n        Available: I,J,_isroms,_nc,angle,cgrid,cs_r,depth_rho,depth_u,depth_v,dm,dn,eta_rho,eta_u,eta_v,f,filename,h,hc,lat_rho,lat_u,lat_v,lm,ln,lon_rho,lon_u,lon_v,mask_rho,mask_u,mask_v,n,name,pm,pn,s_rho,shape,spatial_dims,tcline,theta_b,theta_s,thick_rho,thick_u,thick_v,vstretching,vtransform,xi_rho,xi_u,xi_v\n\n\n2. Most methods available in SEAPY require a grid, which can be specified as a \"filename\" or as a grid object.\n\n3. Find out how to download global HYCOM data that will span my grid from 1/1/2015 through 5/1/2015:\n\n\n        >> seapy.model.hycom.load_history(\"hycom_file.nc\", start_time=datetime(2015,1,1),\n                                         end_time=datetime(2015,5,1),\n                                         grid=mygrid, load_data=False)\n        ncks -v water_temp,salinity,surf_el,water_v,water_u -d time,352,352 -d lat,1204,1309 -d lon,2438,2603 http://tds.hycom.org/thredds/dodsC/GLBu0.08/expt_91.1 hycom_file.nc\n\nThis will display the 'ncks' command necessary to download the data. If you want to have SEAPY download it (not recommended due to server-speed), use `'load_data=True'`.\n\n4. Once you have HYCOM data, interpolate it to your grid\n\n        >> seapy.roms.interp.to_clim(\"hycom_file.nc\", \"my_clim.nc\",\n                          dest_grid=mygrid, nx=1/6, ny=1/6,\n                          vmap={\"surf_el\":\"zeta\", \"water_temp\":\"temp\",\n                          \"water_u\":\"u\", \"water_v\":\"v\", \"salinity\":\"salt\"})\n\n5. Generate boundary conditions from the climatology\n\n        >> seapy.roms.boundary.from_roms(\"my_clim.nc\", \"my_bry.nc\")\n\n6. Generate initial conditions from the climatology\n\n        >> seapy.roms.initial.from_roms(\"my_clim.nc\", \"my_ini.nc\")\n\n7. You now have what you need to run your model\n\n8. To set up data assimilation, download the raw observations (e.g., `aviso_map_day1.nc`, `aviso_map_day2.nc`, `argo_day1.nc` ). You can then process the data:\n\n        >> dt = 400/86400       # time-step of the model in days\n        >> aviso_gen = seapy.roms.obsgen.aviso_sla_map(mygrid, dt)\n        >> aviso_gen.batch_files(seapy.list_files('.','aviso.*nc'), 'aviso_roms_#.nc')\n        >> argo_gen = seapy.roms.obsgen.argo_ctd(mygrid, dt)\n        >> obs = argo_gen.convert_file(\"argo_day1.nc\")\n        >> obs.to_netcdf(\"argo_roms_1.nc\")\n\n9. Put all of the processed observations files together into a file for a given assimilation window\n\n        >> seapy.roms.obs.merge_files(seapy.list_files('.*roms_[0-9]+.nc'), 'roms_obs_#.nc', np.arange([0, 10.1, 5]))\n\nThere are many more things that can be done, but these show some of the power available via simple commands.\n"
 },
 {
  "repo": "gher-ulg/DINCAE",
  "language": "Python",
  "readme_contents": "[![documentation latest](https://img.shields.io/badge/docs-latest-blue.svg)](https://gher-ulg.github.io/DINCAE/)\n[![DOI](https://zenodo.org/badge/193079989.svg)](https://zenodo.org/badge/latestdoi/193079989)\n[![Build Status](https://travis-ci.org/gher-ulg/DINCAE.svg?branch=master)](https://travis-ci.org/gher-ulg/DINCAE)\n[![codecov.io](http://codecov.io/github/gher-ulg/DINCAE/coverage.svg?branch=master)](http://codecov.io/github/gher-ulg/DINCAE?branch=master)\n\n# DINCAE\n\n\nDINCAE (Data-Interpolating Convolutional Auto-Encoder) is a neural network to\nreconstruct missing data in satellite observations which is described in the following open access paper:\nhttps://doi.org/10.5194/gmd-13-1609-2020\n\n\n*Note that this code is no longer maintained and has been superseeded by https://github.com/gher-ulg/DINCAE.jl*\n\n\n\n## Installation\n\nPython 3.6 or 3.7 with the modules:\n* numpy (https://docs.scipy.org/doc/numpy/user/install.html)\n* netCDF4 (https://unidata.github.io/netcdf4-python/netCDF4/index.html)\n* TensorFlow 1.15 with GPU support (https://www.tensorflow.org/install)\n\nTested versions:\n\n* Python 3.6.8\n* netcdf4 1.4.2\n* numpy 1.15.4\n* Tensorflow version 1.15 (DINCAE does not work with TensforFlow 2.0; TensorFlow 1.5 does not work on python 3.8)\n\nYou can install those packages either with `pip3` or with `conda`.\n\n\n## Documentation\n\nThe document is available at https://gher-ulg.github.io/DINCAE/.\n\n## Input format\n\nThe input data should be in netCDF with the variables:\n* `lon`: longitude (degrees East)\n* `lat`: latitude (degrees North)\n* `time`: time (days since 1900-01-01 00:00:00)\n* `mask`: boolean mask where true means the data location is valid\n* `SST` (or any other varbiable name): the data\n\n\nThis is the example output from `ncdump -h`:\n\n```\nnetcdf avhrr_sub_add_clouds {\ndimensions:\n\ttime = UNLIMITED ; // (5266 currently)\n\tlat = 112 ;\n\tlon = 112 ;\nvariables:\n\tdouble lon(lon) ;\n\tdouble lat(lat) ;\n\tdouble time(time) ;\n\t\ttime:units = \"days since 1900-01-01 00:00:00\" ;\n\tint mask(lat, lon) ;\n\tfloat SST(time, lat, lon) ;\n\t\tSST:_FillValue = -9999.f ;\n}\n```\n\nAn example for how to create this file in the examples folder:\n* [python example](https://github.com/gher-ulg/DINCAE/blob/master/examples/create\\_input\\_file.py)\n* [julia example](https://github.com/gher-ulg/DINCAE/blob/master/examples/create\\_input\\_file.jl)\n\n\n## Running DINCAE\n\nCopy the template file `run_DINCAE.py` and adapt the filename, variable name and the output directory and possibly optional arguments for the reconstruction method as mentioned in the [documentation](https://gher-ulg.github.io/DINCAE/).\nThe code can be run as follows:\n\n```bash\npython3 run_DINCAE.py\n```\n## Reducing GPU memory\n\nConvolutional neural networks can require \"a lot\" of GPU memory. These parameters can affect GPU memory utilisation:\n\n* reduce the mini-batch size\n* use fewer layers (e.g. `enc_nfilter_internal` = [16,24,36] or [16,24])\n* use less filters (reduce the values of the optional parameter enc_nfilter_internal)\n* reduce `frac_dense_layer`, a parameter controlling the width of the dense layer in the bottleneck\n* use a smaller domain or lower resolution\n\n\n## Example results\n\n[Link to animation](http://data-assimilation.net/upload/Alex/DINCAE/data-avg-DINCAE-AVHRR.gif)\n\n\nMore information about this result is given in the [linked paper](https://www.geosci-model-dev-discuss.net/gmd-2019-128/).\n"
 },
 {
  "repo": "OceanMixingCommunity/Standard-Mixing-Routines",
  "language": "Matlab",
  "readme_contents": "# Standard-Mixing-Routines\nRepository for standard software used by oceanographic mixing community.\n\nThis repository is inteded to be a repo for \"best-practices\" software used to perform standard calculations in the Oceanographic mixing community (such as Thorpe scales, finescale parameterizations etc). Benefits include:\n* Enable reproducibility of analyses.\n* Enable comparison of different datasets using same code.\n* Allow easy re-calculations if bug is found later in code, or \u201cbest-practices\u201d change.\n* Allow testing of one\u2019s own code against other versions.\n* Easy, well-documented citations of techniques for publication\n\nWe are just starting this repo and getting input from the community about how it should be set up. If you have any suggestions, we would love to hear them. Email andypicke (at) gmail (dot) com . Thanks!\n\n![Vintage photo of Amy Waterhouse](/ifildpkheoobnopb.jpg?raw=true)\n\n\n## How to Contribute\n* You can upload code to <https://www.dropbox.com/request/s0pWpFVFkdijwEQQMl0K>\n* Eventually we envision that everyone in the community will contribute and help maintain the repository. But at first we think it will be easier for a small group to upload and organize things.\n* Find a bug or problem with codes? Open an 'issue' to notify team members and create an official record.\n* Want to make changes? First, 'fork' the repository (make your own separate copy), then make changes, then open a 'pull request'. Once approved, it can be merged into the master branch.\n* Don't use git often and don't want to remember all the terminal commands? Download GitHub for desktop <https://desktop.github.com/>, a nice and easy to use front-end.\n\n## Possible topics\n* Thorpe Scales / overturns\n* Finescale Parameterizations (shear/strain)\n* Calculation of stratification\n* Theoretical spectra (Batchelor/Kraichnan)\n* Small sample datasets that can be used to test code and compare results vs other versions.\n* Summaries of sensitivity studies, method comparisons etc.\n\n## Links\n* Climate Process Team: <http://www-pord.ucsd.edu/~jen/cpt/>\n* OSU Ocean Mixing Group: <http://mixing.coas.oregonstate.edu/>\n* Guide to GitHub workflow: <https://guides.github.com/introduction/flow/>\n* CVmix: <http://cvmix.github.io/>\n* Amy's Microstructure Database : <http://microstructure.ucsd.edu> ( see <https://github.com/OceanMixingCommunity/Standard-Mixing-Routines/blob/master/Examine_mixing_data.ipynb> for example of working w/ data in python)\n* pycurrents (python utils for oceanographic data analysis) <http://currents.soest.hawaii.edu/hgstage/pycurrents/file/9f335da750d2>\n"
 },
 {
  "repo": "gher-ulg/PhysOcean.jl",
  "language": "Julia",
  "readme_contents": "# PhysOcean\n\n[![Build Status](https://github.com/gher-ulg/PhysOcean.jl/workflows/CI/badge.svg)](https://github.com/gher-ulg/PhysOcean.jl/actions)\n[![Build Status Windows](https://ci.appveyor.com/api/projects/status/github/gher-ulg/PhysOcean.jl?branch=master&svg=true)](https://ci.appveyor.com/project/Alexander-Barth/physocean-jl)\n[![codecov.io](http://codecov.io/github/gher-ulg/PhysOcean.jl/coverage.svg?branch=master)](http://codecov.io/github/gher-ulg/PhysOcean.jl?branch=master)\n\n[![documentation stable](https://img.shields.io/badge/docs-stable-blue.svg)](https://gher-ulg.github.io/PhysOcean.jl/stable/)\n[![documentation latest](https://img.shields.io/badge/docs-latest-blue.svg)](https://gher-ulg.github.io/PhysOcean.jl/latest/)\n\n# Content\n\nYou will find here some general tools for Physical Oceanography (state equations, bulk formulas, geostrophic velocity calculations ...). \n\n# Installing\n\nYour need [Julia](http://julialang.org) to use `PhysOcean`. The command line version is sufficient for `PhysOcean`.\nInside Julia, you can download and install the package by issuing:\n\n```julia\nusing Pkg\nPkg.add(\"PhysOcean\")\n```\n\nOr if you want to use the latest version, you can use the following command:\n\n```julia\nusing Pkg\nPkg.add(PackageSpec(name=\"PhysOcean\", rev=\"master\"))\n```\n\n# Testing\n\nA test script is included to verify the correct functioning of the toolbox.\nThe script should be run in a Julia session.\n\n```julia\nusing Pkg\nPkg.test(\"PhysOcean\")\n```\n\n"
 },
 {
  "repo": "oscarbranson/cbsyst",
  "language": "Python",
  "readme_contents": "<div align=\"right\">\n  <a href=\"https://github.com/oscarbranson/cbsyst/actions\"><img src=\"https://github.com/oscarbranson/cbsyst/actions/workflows/tests.yml/badge.svg\" alt=\"GHA\" height=\"18\"></a>\n  <a href=\"https://badge.fury.io/py/cbsyst\"><img src=\"https://badge.fury.io/py/cbsyst.svg\" alt=\"PyPI version\" height=\"18\"></a>\n  <a href=\"https://anaconda.org/conda-forge/cbsyst\"> <img src=\"https://anaconda.org/conda-forge/cbsyst/badges/version.svg\" alt=\"conda-forge version\" height=\"18\"/></a>\n  <a href=\"https://doi.org/10.5281/zenodo.1402261\"> <img src=\"https://zenodo.org/badge/DOI/10.5281/zenodo.1402261.svg\" alt=\"DOI\" height=\"18\"></a>\n</div>\n\n<div align=\"left\">\n  <img src=\"https://raw.githubusercontent.com/oscarbranson/cbsyst/master/logo/cbsyst.png\" alt=\"CBsyst\" height=\"96\">\n</div>\n\n**A Python module for calculating seawater carbon and boron chemistry.** \n\nThis will be particularly useful for anyone thinking about oceans in the distant past, when Mg and Ca concentrations were different. I use [Mathis Hain's MyAMI model](http://www.mathis-hain.net/resources/Hain_et_al_2015_GBC.pdf) to adjust speciation constants for Mg and Ca concentration.\n\n***Tested** in the modern ocean against GLODAPv2 data (see below). Performs as well as Matlab CO2SYS.*\n\n## Work in Progress:\n- [ ] [Compare to CO2SYS](https://github.com/oscarbranson/cbsyst/issues/6), a la [Orr et al (2015)](http://www.biogeosciences.net/12/1483/2015/bg-12-1483-2015.pdf)?\n\nIf anyone wants to help with any of this, please do contribute!\nA full list of bite-sized tasks that need doing is available on the [Issues](https://github.com/oscarbranson/cbsyst/issues) page.\n\n## Acknowledgement\nThe development of `cbsyst` has been greatly aided by [CO2SYS](http://cdiac.ornl.gov/oceans/co2rprt.html), and the [Matlab conversion of CO2SYS](http://cdiac.ornl.gov/ftp/oceans/co2sys/).\nIn particular, these programs represent a gargantuan effort to find the most appropriate coefficient formulations and parameterisations from typo-prone literature.\nCO2SYS has also provided an invaluable benchmark throughout development.\n\n## Data Comparison\nI have used the [GLODAPv2 data set](cbsyst/test_data/GLODAP_data/Olsen_et_al-2016_GLODAPv2.pdf) to test how well `cbsyst` works with modern seawater.\n\n### Method:\nImport the entire GLODAPv2 data set, remove all data where `flag != 2` (2 = good data), and exclude all rows that don't have all of (salinity, temperature, pressure, tco2, talk, phosphate, silicate and phtsinsitutp) - i.e. salinity, temperature, pressure, nutrients and all three measured carbonate parameters.\nThe resulting dataset contains 79,896 bottle samples. \nThe code used to process the raw GLODAPv2 data is available [here](cbsyst/test_data/GLODAP_data/get_GLODAP_data.py).\n\nNext, calculate the carbonate system from sets of two of the measured carbonate parameters, and compare the calculated third parameter to the measured third parameter (i.e. calculate Alkalinity from pH and DIC, then compared calculated vs. measured Alkalinities). The code for making these comparison plots is [here](cbsyst/test_data/GLODAP_data/plot_GLODAPv2_comparison.py).\n\n### Results:\n**Calculated pH** (from DIC and Alkalinity) is offset from measured values by -0.0011 (-0.030/+0.027).\n![Calculated vs Measured pH](cbsyst/test_data/GLODAP_data/Figures/pH_comparison.png)\n\n**Calculated Alkalinity** (from pH and DIC) is offset from measured values by 0.39 (-11/+12) umol/kg.\n![Calculated vs Measured TA](cbsyst/test_data/GLODAP_data/Figures/TA_comparison.png)\n\n**Calculated DIC** (from pH and Alkalinity) is offset from measured values by -0.38 (-11/+10) umol/kg.\n![Calculated vs Measured DIC](cbsyst/test_data/GLODAP_data/Figures/DIC_comparison.png)\n\nReported statistics are median \u00b195% confidence intervals extracted from the residuals (n = 79,896).\n\nData are idential to within rouding errors as values calculated by Matlab CO2SYS (v1.1).\n\n### Conclusions:\n`cbsyst` does a good job of fitting the GLODAPv2 dataset!\n\n## Technical Details\n### Constants\nConstants calculated by an adaptation of [Mathis Hain's MyAMI model](http://www.mathis-hain.net/resources/Hain_et_al_2015_GBC.pdf). \nThe [original MyAMI code](https://github.com/MathisHain/MyAMI) is available on GitHub.\nA stripped-down version of MyAMI is [packaged with cbsyst](cbsyst/MyAMI_V2.py).\nIt has been modified to make it faster (by vectorising) and more 'Pythonic'.\nAll the Matlab interface code has been removed.\n\nConstants not provided by MyAMI (KP1, KP2, KP3, KSi, KF) are formulated following [Dickson, Sabine & Christian's (2007) 'Guide to best practices for ocean CO<sub>2</sub> measurements.'](http://cdiac.ornl.gov/oceans/Handbook_2007.html).\n\nPressure corrections are applied to the calculated constants following Eqns. 38-40 of [Millero et al (2007)](cbsyst/docs/Millero_2007_ChemicalReview.pdf), using (typo-corrected) constants in their Table 5.\nAll constants are on the pH Total scale.\n\n### Calculations\nSpeciation calculations follow [Zeebe and Wolf-Gladrow (2001)](https://www.elsevier.com/books/co2-in-seawater-equilibrium-kinetics-isotopes/zeebe/978-0-444-50946-8).\nCarbon speciation calculations are described in Appendix B, except where Alkalinity is involved, in which cases the formulations of [Ernie Lewis' CO2SYS](http://cdiac.ornl.gov/oceans/co2rprt.html) are used.\nBoron speciation calculations in Eqns. 3.4.43 - 3.4.46.\n\nBoron isotopes are calculated in terms of fractional abundances instead of delta values, as outlines [here](cbsyst/docs/B_systematics.pdf).\nDelta values can be provided as an input, and are given as an output.\n\n\n# Installation\n\n**Requires Python 3.5+**. \nDoes *not* work in 2.7. Sorry.\n\n### PyPi\n```bash\npip install cbsyst\n```\n\n### Conda-Forge\n```bash\nconda install cbsyst -c conda-forge\n```\n\n## Example Usage\n\n```python\nimport cbsyst as cb\nimport numpy as np\n\n# Create pH master variable for demo\npH = np.linspace(7,11,100)  # pH on Total scale\n\n# Example Usage\n# -------------\n# The following functions can be used to calculate the\n# speciation of C and B in seawater, and the isotope\n# fractionation of B, given minimal input parameters.\n#\n# See the docstring for each function for info on\n# required minimal parameters.\n\n# Carbon system only\nCsw = cb.Csys(pHtot=pH, DIC=2000.)\n\n# Boron system only\nBsw = cb.Bsys(pHtot=pH, BT=433., dBT=39.5)\n\n# Carbon and Boron systems\nCBsw = cb.CBsys(pHtot=pH, DIC=2000., BT=433., dBT=39.5)\n\n# NOTE:\n# At present, each function call can only be used to\n# calculate a single minimal-parameter combination -\n# i.e. you can't pass it multiple arrays of parameters\n# with different combinations of parameters, as in\n# the Matlab CO2SYS code.\n\n# Example Output\n# --------------\n# The functions return a Bunch (modified dict with '.' \n# attribute access) containing all system parameters\n# and constants.\n#\n# Output for a single input condition shown for clarity:\n\nout = cb.CBsys(pHtot=8.1, DIC=2000., BT=433., dBT=39.5)\nout\n\n>>> {'ABO3': array([ 0.80882931]),\n     'ABO4': array([ 0.80463763]),\n     'ABT': array([ 0.80781778]),\n     'BO3': array([ 328.50895695]),\n     'BO4': array([ 104.49104305]),\n     'BT': array([ 433.]),\n     'CO2': array([ 9.7861814]),\n     'CO3': array([ 238.511253]),\n     'Ca': array([ 0.0102821]),\n     'DIC': array([ 2000.]),\n     'H': array([  7.94328235e-09]),\n     'HCO3': array([ 1751.7025656]),\n     'Ks': {'K0': array([ 0.02839188]),\n      'K1': array([  1.42182814e-06]),\n      'K2': array([  1.08155475e-09]),\n      'KB': array([  2.52657299e-09]),\n      'KSO4': array([ 0.10030207]),\n      'KW': array([  6.06386369e-14]),\n      'KspA': array([  6.48175907e-07]),\n      'KspC': array([  4.27235093e-07])},\n     'Mg': array([ 0.0528171]),\n     'S': array([ 35.]),\n     'T': array([ 25.]),\n     'TA': array([ 2333.21612227]),\n     'alphaB': array([ 1.02725]),\n     'dBO3': array([ 46.30877684]),\n     'dBO4': array([ 18.55320208]),\n     'dBT': array([ 39.5]),\n     'deltas': True,\n     'fCO2': array([ 344.68238018]),\n     'pCO2': array([ 345.78871573]),\n     'pHtot': array([ 8.1]),\n     'pdict': None}\n\n# All of the calculated output arrays will be the same length as the longest\n# input array.\n\n# Access individual parameters by:\nout.CO3\n\n>>> array([ 238.511253])\n\n# Output data for external use:\ndf = cb.data_out(out, 'example_export.csv')\n\n# This returns a pandas.DataFrame object with all C and B parameters.\n# It also saves the data to the specified file. The extension of the\n# file determined the format it is saved in (see data_out docstring).\n\n```\n\n## Technical Note: Whats is a `Bunch`?\n\nFor code readability and convenience, I've used Bunch objects instead of traditional dicts.\nA [Bunch](cbsyst/helpers.py#L6) is a modification of a dict, which allows attribute access via the dot (.) operator.\nApart from that, it works *exactly* like a normal dict (all the usual methods are available transparrently).\n\n**Example:**\n```python\nfrom cbsyst.helpers import Bunch\n\n# Make a bunch\nbun = Bunch({'a': 1,\n             'b': 2})\n\n# Access items of bunch...\n# as a dict:\nbun['a']\n\n>>> 1\n\n# as a Bunch:\nbun.a\n\n>>> 1\n```"
 },
 {
  "repo": "gher-ulg/DivaPythonTools",
  "language": "Jupyter Notebook",
  "readme_contents": "[![DOI](https://zenodo.org/badge/44103456.svg)](https://zenodo.org/badge/latestdoi/44103456)\n\n# Diva Python Tools\n\nA set of python modules to help users with\n1. the preparation of the Diva input files: data, contours, parameters;\n2. the execution of the Diva interpolation tool,\n3. the reading of output files (analysis, finite-element mesh),\n4. the input and output plotting.\n\n## Getting started \n\n### Prerequisites\n\nThe [Diva](https://github.com/gher-ulg/diva) interpolation tool has to be installed and compiled on your machine. See the related [documentation](https://github.com/gher-ulg/DIVA/blob/master/README.md#installing) for the installation.\n\n### Installing\n\nClone the package:\n```bash\ngit clone git@github.com:gher-ulg/DivaPythonTools.git\n```\nor download the latest stable [release](https://github.com/gher-ulg/DivaPythonTools/releases).\n\nInside DivaPythonTools directory execute:\n```python\npip install -r requirements.txt\npython setup.py install\n```\n\nAfter this you should use it as:\n```python\nfrom pydiva import pydiva2d, pydiva4d\n```\n\n## Module description\n\nThe main modules are [`pydiva2d`](./pydiva2d.py) and [`pydiva4d`](./pydiva4D.py), which define the classes for the 2D and 4D version of Diva, respectively.\n\n### pydiva2d\n\nThe module defines classes corresponding to the main Diva input (data, parameters, contours) and output files (analysed and error fields, finite-element mesh).\n\n### pydiva4d\n\nThe module defines classes to run the 4D version of Diva.\n\n## Plots\n\nThe figures can be generated with and without the [Basemap](https://github.com/matplotlib/basemap) module (Plot on map projections). \n\nSome examples obtained with mixed-layer depth (MLD) data are shown below. The complete example to generate these plots is inside the Notebooks directory [(run_diva2D_MLD)](./Notebooks/run_diva2D_MLD.ipynb).\n\nThe [Notebooks](./Notebooks) directory contains additional examples showing how to run 2D and 4D cases.\n\n### Data values\nScatter plot showing the data positions and values.    \n![Data](./figures/datapoints.png)\n\n### Contours\nBy default, each sub-contour is displayed in a different color.     \n![Contour](./figures/contours.png)\n\n### Finite-element mesh\nTriangular mesh covering the region of interest.     \n![Mesh](./figures/mesh.png)\n\n### Analysed fields\nPseudo-color plot of the gridded field obtained by the interpolation.     \n![Analysis](./figures/analysis.png)\n\n### Combined information\nData, contours, mesh and analysis on the same figure.     \n![Combined](./figures/AnalysisMeshData.png)\n\n## Acknowledgments\n\n[Diva](https://github.com/gher-ulg/DIVA) developments have benefited from the users' feedback and numerous comments, especially during the editions of the Diva workshops.\n\nThe present module was initiated in the frame of [SeaDataCloud](SeaDataCloud) project.  \n\n\n\n"
 },
 {
  "repo": "IHCantabria/THREDDSExplorer",
  "language": "Python",
  "readme_contents": "## Synopsis\n\nTHREDDS Explorer is a QGIS-based plug-in designed to make it easy for users to access georeferenced data accessible through any THREDDS based server.  \n\nData catalogues and maps are exposed to the user through a simple user interface, which allows him to choose any map or explore all contents of the server without resorting to exploring the default web based THREDDS interface.\n\nThis plug-in should work with most THREDDS servers, and will be able to retrieve any layer provided through WMS, WMS-T and/or WCS services published by the server.\n\n[![THREDDS Explorer](https://raw.githubusercontent.com/IHCantabria/THREDDSExplorer/master/doc/video.jpg)](https://vimeo.com/167414368)\n\n## Installation\n\n**Dependencies:** THREDDSExplorer requires the *processing* plug-in by V. Olaya, available within the standard QGIS Installation (also at https://plugins.qgis.org/plugins/processing/).\n\nInstalling the plug-in basically involves copying the code in a QGIS \"plugins\" directory, as detailed below.\n\nPlease note that it is mandatory that the folder where the code resides is called \"THREDDSExplorer\". If you download the code in ZIP format the default name (\"THREDDSExplorer-$VERSION\") must be changed to \"THREDDSExplorer\".\n\n### For Windows\n\nThe folder where the code should be copied is the following, substituting \"`%USERNAME%`\" with your user name:\n\n\tC:\\Users\\%USERNAME%\\AppData\\Roaming\\QGIS\\QGIS3\\profiles\\default\\python\\plugins\n\n\n### For Linux\n\nNot tested yet. \n\n### For Mac OS X\n\nNot tested yet.\n\n## User Manual\n\nFor more information you can find a PDF manual under the \"doc\" folder.\n"
 },
 {
  "repo": "iuryt/Panthalassan",
  "language": "Jupyter Notebook",
  "readme_contents": "# Panthalassan\n\n|[Portugu\u00eas](https://github.com/iuryt/Panthalassan/blob/main/README_pt.md)|\n\n**Disclosure: This is a work in progress!!**\n\nThe idea is to develop a material for a course in Oceanographic Data Analysis and Modelling.\n\n## Justification\n\nProgramming is becoming an essential tool for most of the research in oceanography. With global numerical simulations down to submesoscale and higher-resolution observations, processing and analyzing big data in oceanography is only possible by coding the analyses by yourself and making use of top-level tools and parallel processing in collaborative platforms. Many under/graduate courses have tried to overcome this gap in their programs by integrating computational science into their main curriculum. The lack of resources in developing country institutions unbalance the opportunities in this area, but there are many open-source and cloud tools that could be used to introduce students to this new world. The language barrier is another problem. Most of the programming tutorials are in English, with pratically no online course for oceanographic data analysis in Portuguese and other languages. How can we overcome the language and technological barriers and give developing-country students the opportunity to learn programming data analysis? There are multiple answers to this question, but collaborative learning is usually the way to develop the independence of the students in their own learning process.\n\n## The project\n\nThe subject covers acquiring, presenting, and analyzing the most common types ofobservational data and model output, such as Argo floats, satellite-derived sea-surface temperature, altimetry, moored data, CTD casts, and general circulation models. This course is designed to provide enough experience for the students to learn how to read, understand and plot time series, profiles, cross-sections, maps and do analyses using the top-level module formulti-dimensional data, the xarray project. The main objective is to give enough experience with the main tools and develop the self-awareness and independence of the students in their own learning process.\n\n## The currently available tutorials\n\n| Tutorial    | Badge       |\n| ----------- | ----------- |\n| PIRATA      | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/iuryt/Panthalassan/blob/main/notebooks/01-PIRATA.ipynb)      |\n| BESM      | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/iuryt/Panthalassan/blob/main/notebooks/02-BESM.ipynb)      |\n| CTD (csv)      | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/iuryt/Panthalassan/blob/main/notebooks/03-CTD.ipynb)      |\n| NEMO      | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/iuryt/Panthalassan/blob/main/notebooks/04-NEMO.ipynb)      |\n| MEOP      | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/iuryt/Panthalassan/blob/main/notebooks/05-MEOP.ipynb)      |\n"
 },
 {
  "repo": "SciTools/python-stratify",
  "language": "Jupyter Notebook",
  "readme_contents": "# Stratify\n\nInterpolation for restratification, particularly useful for Nd vertical interpolation of atmospheric and oceanographic datasets\n\n[![cirrus-ci](https://api.cirrus-ci.com/github/SciTools-incubator/python-stratify.svg)](https://cirrus-ci.com/github/SciTools-incubator/python-stratify)\n[![conda-forge](https://img.shields.io/conda/vn/conda-forge/python-stratify?color=orange&label=conda-forge&logo=conda-forge&logoColor=white)](https://anaconda.org/conda-forge/python-stratify)\n[![PyPI](https://img.shields.io/pypi/v/stratify?color=orange&label=pypi&logo=python&logoColor=white)](https://pypi.org/project/stratify/)\n[![codecov](https://codecov.io/gh/SciTools-incubator/python-stratify/branch/master/graph/badge.svg?token=v1R1bJ4kYr)](https://codecov.io/gh/SciTools-incubator/python-stratify)\n[![pre-commit.ci status](https://results.pre-commit.ci/badge/github/SciTools-incubator/python-stratify/master.svg)](https://results.pre-commit.ci/latest/github/SciTools-incubator/python-stratify/master)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n[![License](https://img.shields.io/github/license/SciTools-incubator/python-stratify?style=plastic)](https://github.com/SciTools-incubator/python-stratify/blob/master/LICENSE)\n[![Contributors](https://img.shields.io/github/contributors/SciTools-incubator/python-stratify?style=plastic)](https://github.com/SciTools-incubator/python-stratify/graphs/contributors)\n[![Binder](http://mybinder.org/badge.svg)](http://mybinder.org:/repo/scitools-incubator/python-stratify)\n\n## Introduction\n\nDiscover the capabilities of `stratify` with this introductory [Jupyter Notebook](https://github.com/SciTools-incubator/python-stratify/blob/master/index.ipynb).\n\n![](https://SciTools-incubator.github.io/python-stratify/summary.png)\n\n## Installation\n\n```shell\nconda install -c conda-forge python-stratify\n```\n```shell\npip install python-stratify\n```\n\n## License\nStratify is licenced under a [BSD 3-Clause License](LICENSE).\n\n"
 },
 {
  "repo": "ocean-data-challenges/2020a_SSH_mapping_NATL60",
  "language": "Jupyter Notebook",
  "readme_contents": "[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.4045400.svg)](https://doi.org/10.5281/zenodo.4045400)\n\n# SSH Mapping Data Challenge 2020a\n\nThis repository contains codes and sample notebooks for downloading and processing the SSH mapping data challenge.\n\nThe quickstart can be run online by clicking here:\n[![Binder](https://binder.pangeo.io/badge_logo.svg)](https://binder.pangeo.io/v2/gh/ocean-data-challenges/2020a_SSH_mapping_NATL60/master?filepath=quickstart.ipynb)\n\n## Motivation\n\nThe goal is to investigate how to best reconstruct sequences of Sea Surface Height (SSH) maps from partial satellite altimetry observations. This data challenge follows an _Observation System Simulation Experiment_ framework: \"Real\" full SSH are from a numerical simulation with a realistic, high-resolution ocean circulation model: the reference simulation. Satellite observations are simulated by sampling the reference simulation based on realistic orbits of past, existing or future altimetry satellites. A baseline reconstruction method is provided (see below) and the practical goal of the challenge is to beat this baseline according to scores also described below and in Jupyter notebooks.\n\n### Reference simulation\nThe reference simulation is the NATL60 simulation based on the NEMO model (Ajayi et al. 2020 doi:[10.1029/2019JC015827](https://doi.org/10.1029/2019JC015827)). The simulation is run without tidal forcing. \n\n### Observations\nThe SSH observations include simulations of Topex-Poseidon, Jason 1, Geosat Follow-On, Envisat, and SWOT altimeter data. This nadir altimeters constellation was operating during the 2003-2005 period and is still considered as a historical optimal constellation in terms of spatio-temporal coverage. The data challenge simulates the addition of SWOT to this reference constellation. No observation error is considered in this challenge.\n\n### Data sequence and use\n \nThe SSH reconstructions are assessed over the period from 2012-10-22 to 2012-12-02: 42 days, which is equivalent to two SWOT cycles in the SWOT science phase orbit.\nFor reconstruction methods that need a spin-up, the **observations** can be used from 2012-10-01 until the beginning of the evaluation period (21 days). This spin-up period is not included in the evaluation. For reconstruction methods that need learning from full fields, the **reference data** can be used from 2013-01-02 to 2013-09-30. The reference data between 2012-12-02 and 2013-01-02 should never be used so that any learning period or other method-related-training period can be considered uncorrelated to the evaluation period.\n\n![Data Sequence](figures/DC-data_availability.png)\n\n## Leaderboard\n\n| Method     |   \u00b5(RMSE) |   \u03c3(RMSE) |   \u03bbx (degree) |   \u03bbt (days) | Notes                     | Reference        |\n|:-----------|------------------------:|---------------------:|-------------------------:|-----------------------:|:--------------------------|:-----------------|\n| baseline OI 1 nadir |                    0.69 |                 0.03 |                     3.31 |                  33.32 | Covariances not optimized | quickstart.ipynb |\n| baseline OI 4 nadirs |                    0.83 |                 0.04 |                     2.25 |                  15.67 | Covariances not optimized | quickstart.ipynb |\n| baseline OI 1 swot |                    0.85 |                 0.05 |                     1.22 |                  12.38 | Covariances not optimized | quickstart.ipynb |\n| | | | | | | |\n| duacs 4 nadirs |       0.92 |      0.01 |          1.42 |       12.0 | Covariances DUACS | eval_duacs.ipynb |\n| bfn 4 nadirs  |       0.92 |      0.02 |          1.23 |       10.6 | QG Nudging | eval_bfn.ipynb |\n| dymost 4 nadirs |       0.91 |      0.01 |          1.36 |       11.79 | Dynamic mapping | eval_dymost.ipynb |\n| miost 4 nadirs |       0.93 |      0.01 |          1.35 |       10.19 | Multiscale mapping | eval_miost.ipynb |\n| 4DVarNet 4 nadirs :trophy: |       **0.94** |      **0.01** |          **0.83** |       **8.01** | 4DVarNet mapping | eval_4dvarnet.ipynb |\n| | | | | | | |\n| duacs 1 swot + 4 nadirs |       0.92 |      0.02 |          1.22 |       11.15 | Covariances DUACS | eval_duacs.ipynb |\n| bfn 1 swot + 4 nadirs  |       0.93 |      0.02 |           0.8 |        10.09 | QG Nudging | eval_bfn.ipynb |\n| dymost 1 swot + 4 nadirs |       0.93 |      0.02 |           1.2 |        10.07 | Dynamic mapping | eval_dymost.ipynb |\n| miost 1 swot + 4 nadirs |       0.94 |      **0.01** |          1.18 |       10.14 | Multiscale mapping | eval_miost.ipynb |\n| 4DVarNet 1 swot + 4 nadirs :trophy: |       **0.95** |      **0.01** |          **0.62** |        **5.29** | 4DVarNet mapping | eval_4dvarnet.ipynb |\n\n**\u00b5(RMSE)**: average RMSE score.  \n**\u03c3(RMSE)**: standard deviation of the RMSE score.  \n**\u03bbx**: minimum spatial scale resolved.  \n**\u03bbt**: minimum time scale resolved. \n \n## Quick start\nYou can follow the quickstart guide in [this notebook](https://github.com/ocean-data-challenges/2020a_SSH_mapping_NATL60/blob/master/quickstart.ipynb) or launch it directly from <a href=\"https://binder.pangeo.io/v2/gh/ocean-data-challenges/2020a_SSH_mapping_NATL60/master?filepath=quickstart.ipynb\" target=\"_blank\">binder</a>.\n\n## Download the data\nThe data are hosted on the [AVISO+ website](https://www.aviso.altimetry.fr/en/data/products/ocean-data-challenges/2020a-ssh-mapping-natl60.html) and tagged with DOI: 10.24400/527896/a01-2020.002. The website also provides a data handbook. This is the recommended access. This [wiki](https://github.com/ocean-data-challenges/2020a_SSH_mapping_NATL60/wiki/AVISO---account-creation) can help you create an AVISO account to access the data. The data are also temporarily available [here](https://ige-meom-opendap.univ-grenoble-alpes.fr/thredds/catalog/meomopendap/extract/ocean-data-challenges/dc_data1/catalog.html). They are presented with the following directory structure:\n\n```\n. \n|-- dc_obs\n|   |-- 2020a_SSH_mapping_NATL60_topex-poseidon_interleaved.nc\n|   |-- 2020a_SSH_mapping_NATL60_nadir_swot.nc \n|   |-- 2020a_SSH_mapping_NATL60_karin_swot.nc\n|   |-- 2020a_SSH_mapping_NATL60_jason1.nc\n|   |-- 2020a_SSH_mapping_NATL60_geosat2.nc\n|   |-- 2020a_SSH_mapping_NATL60_envisat.nc\n\n|-- dc_ref\n|   |-- NATL60-CJM165_GULFSTREAM_y****m**d**.1h_SSH.nc\n\n```\n\nTo start out download the *observation* dataset (dc_obs, 285M) from the temporary data server, use: \n```shell\nwget https://ige-meom-opendap.univ-grenoble-alpes.fr/thredds/fileServer/meomopendap/extract/ocean-data-challenges/dc_data1/dc_obs.tar.gz\n```\n\nand the *reference* dataset (dc_ref, 11G) using (*this step may take several minutes*): \n\n```shell\nwget https://ige-meom-opendap.univ-grenoble-alpes.fr/thredds/fileServer/meomopendap/extract/ocean-data-challenges/dc_data1/dc_ref.tar.gz\n```\nand then uncompress the files using `tar -xvf <file>.tar.gz`. You may also use `ftp`, `rsync` or `curl`to donwload the data.  \n\n\n## Baseline and evaluation\n\n### Baseline\nThe baseline mapping method is optimal interpolation (OI), in the spirit of the present-day standard for DUACS products provided by AVISO. OI is implemented in the [`baseline_oi`](https://github.com/ocean-data-challenges/2020a_SSH_mapping_NATL60/blob/master/notebooks/baseline_oi.ipynb) Jupyter notebook. The SSH reconstructions are saved as a NetCDF file in the `results` directory. The content of this directory is git-ignored.\n   \n### Evaluation\n\nThe evaluation of the mapping methods is based on the comparison of the SSH reconstructions with the *reference* dataset. It includes two scores, one based on the Root-Mean-Square Error (RMSE), the other based on Fourier wavenumber spectra. The evaluation notebook [`example_data_eval`](https://github.com/ocean-data-challenges/2020a_SSH_mapping_NATL60/blob/master/notebooks/example_data_eval.ipynb) implements the computation of these two scores as they could appear in the leaderboard. The notebook also provides additional, graphical diagnostics based on RMSE and spectra.\n\n## Data processing\n\nCross-functional modules are gathered in the `src` directory. They include tools for regridding, plots, evaluation, writing and reading NetCDF files. The directory also contains a module that implements the baseline method.  \n\n## Acknowledgement\n\nThe structure of this data challenge was to a large extent inspired by [WeatherBench](https://github.com/pangeo-data/WeatherBench).\n"
 },
 {
  "repo": "clstoulouse/motu",
  "language": "Java",
  "readme_contents": "# Motu Project\n@author <tjolibois@cls.fr>: Project manager & Product owner  \n@author <smarty@cls.fr>: Scrum master, Software architect, Quality assurance, Continuous Integration manager   \n\n>How to read this file? \nUse a markdown reader: \nplugins [chrome](https://chrome.google.com/webstore/detail/markdown-preview/jmchmkecamhbiokiopfpnfgbidieafmd?utm_source=chrome-app-launcher-info-dialog) exists (Once installed in Chrome, open URL chrome://extensions/, and check \"Markdown Preview\"/Authorise access to file URL.), \nor for [firefox](https://addons.mozilla.org/fr/firefox/addon/markdown-viewer/)  (anchor tags do not work)\nand also plugin for [notepadd++](https://github.com/Edditoria/markdown_npp_zenburn).\n\n>Be careful: Markdown format has issue while rendering underscore \"\\_\" character which can lead to bad variable name or path.\n\n\n# Summary\n* [Overview](#Overview)\n* [Architecture](#Architecture)\n  * [Overall](#ArchitectureOverall)\n       * [One instance](#ArchitectureOneInstance)  \n\t   * [Scalability](#ArchitectureScalability)  \n  * [Interfaces](#ArchitectureInterfaces)\n     * [Server interfaces](#ArchitectureInterfacesServer)  \n     * [External interfaces with other systems or tools](#ArchitectureInterfacesExternal)  \n  * [Design](#ArchitectureDesign)\n  * [Design details](#ArchitectureDesignD)  \n     * [Motu-web project](#ArchitectureDesignDMW)\n     * [Other projects](#ArchitectureDesignDOthers)\t \n  * [Algorithm details](#ArchiAlgo)  \n     * [Downloading 1 point](#ArchiAlgoDownloading1Point)\n* [Development](#Development)\n  * [Source code](#DEVSRC)\n  * [Development environment](#DEV)\n  * [Compilation](#COMPILATION)\n  * [Packaging](#Packaging)\n* [Installation](#Installation)\n  * [Prerequisites](#InstallPrerequisites)\n     * [Hardware settings](#InstallPrerequisitesHard)\n     * [Software settings](#InstallPrerequisitesSoft)\n\t * [External interfaces](#InstallPrerequisitesExternalInterfaces)\n     * [Several Motu instances on a same host](#InstallPrerequisitesSeveralsInstances)\n  * [Upgrade from Motu v2.x](#UpgradeFromMotu2x)\n  * [Install Motu from scratch](#InstallFromScratch)\n  * [Check installation](#InstallCheck)\n  * [CDO manual installation](#InstallCDO)\n  * [Installation folder structure](#InstallFolders)\n  * [Setup a frontal Apache HTTPd server](#InstallFrontal)\n  * [Security](#InstallSecurity)\n     * [Run Motu as an HTTPS Web server](#InstallSecurityRunHTTPs)\n\t * [Motu and Single Sign On](#InstallSecuritySSO)\n  * [Install a scalable Motu over several instances](#InstallationScalability)\n* [Configuration](#Configuration)\n  * [Configuration directory structure](#ConfigurationFolderStructure)\n  * [Business settings](#ConfigurationBusiness)\n  * [System settings](#ConfigurationSystem)\n  * [Log settings](#LogSettings)\n  * [Theme and Style](#ThemeStyle)\n* [Operation](#Operation)\n  * [Start, Stop and other Motu commands](#SS)\n  * [Monitor performance](#ExpMonitorPerf)\n  * [Logbooks](#Logbooks)\n  * [Add a dataset](#AdminDataSetAdd)\n  * [Tune the dataset metadata cache](#AdminMetadataCache)\n  * [Debug view](#ExploitDebug)\n  * [Clean files](#ExploitCleanDisk)\n  * [Log Errors](#LogCodeErrors)\n     * [Action codes](#LogCodeErrorsActionCode)  \n     * [Error types](#LogCodeErrorsErrorType)  \n* [Motu clients & REST API](#ClientsAPI)\n  * [Python client](#ClientPython)\n  * [OGC WCS API](#OGC_WCS_API)\n  * [REST API](#ClientRESTAPI)\n* [Docker image](#Docker)\n  * [Docker image content](#DockerContent)\n  * [Docker mounted directories](#DockerDirectories)\n  * [Run Motu with Docker](#DockerRun)  \n  \n  \n# <a name=\"Overview\">Overview</a>  \nMotu is a robust web server allowing the distribution of met/ocean gridded data files through the web. \nSubsetter allows user to extract the data of a dataset, with geospatial, temporal and variable criterias. \nThus, user download only the data of interest.  \nA graphic web interface and machine to machine interfaces allow to access data and information on data (metadata).\nThe machine-to-machine interface can be used through a client written in python, freely available here https://github.com/clstoulouse/motu-client-python.\nOutput data files format can be netCDF3 or netCDF4.  \nAn important characteristic of Motu is its robustness: in order to be able to answer many users without crashing, Motu manages its incoming requests in a queue server.  \nThe aim is to obtain complete control over the requests processing by balancing the processing load according to criteria (volume of data to extract, number of requests to fulfill \nfor a user at a given time, number of requests to process simultaneously).  \nMoreover, Motu implements a request size threshold. Motu masters the amount of data to extract per request by computing, without any data processing, the result data size of the request.  \nBeyond the allowed threshold, every request is rejected. The threshold is set in the configuration file.\nMotu can be secured behind an authentication server and thus implements authorization. A CAS server can implement the authentication. \nMotu receives with authentication process user information, including a user profile associated with the account. \nMotu is configured to authorize or not the user to access the dataset or group of datasets which user is trying to access.  \nFor administrators, Motu allows to monitor the usage of the server: the logs produced by Motu allow to know who (login) requests what (dataset) and when, with extraction criterias.\n\n# <a name=\"Architecture\">Architecture</a>  \nMotu is a Java Web Application running inside the Apache Tomcat application server. Motu can be run as a [single application](#ArchitectureSingleInstance) or can be scaled over [several instances](#ArchitectureScalability).\n\n\n## <a name=\"ArchitectureOverall\">Architecture overall</a>  \n\n### <a name=\"ArchitectureOneInstance\">Architecture single instance</a>  \nThe clients [\"motu-client-python\"](#ClientPython) or an HTTP client like a [web browser](#ClientRESTAPI) are used to connect to Motu services.  \nA frontal web, [Apache HTTPd](#InstallFrontal) for example, is used as a reverse proxy to redirect request to Motu server and also to serve the [downloaded](#motuConfig-downloadHttpUrl) data from Motu [download folder](#motuConfig-extractionPath).  \nMotu server, runs on a Apache Tomcat server and can serve files either directly [\"DGF\"](#BSconfigServiceDatasetType) or by delegating extraction to Thredds server with NCSS or OpenDap [protocols](#BSconfigServiceDatasetType).  \nA NFS server is used to share the netcdf files between Thredds and Motu DGF when they are not deployed on the same host.  \nAn (SSO CAS server)[#ConfigurationSystemCASSSO] is used for the authentication of users but Motu can also be deployed without any authentication system.  \nThe Apache HTTPd, on the top right corner is used to [serve the graphic chart](#InstallPublicFilesOnCentralServer) when several Motu Web server are deployed.\n\nThe schema below shows an example of Motu scalability architecture. The \"i1, i2\" are the Motu server deployed. They have to share the same [business configuration file](#ConfigurationBusiness) and the [download folder](#motuConfig-extractionPath).      \n\n![Software architecture](./motu-parent/src/doc/softwareArchitecture.png \"Motu software architecture, one instance\")\n\n\n### <a name=\"ArchitectureScalability\">Architecture scalability</a>  \nTo run Motu over several instances, a [Redis server](#RedisServerConfig) has to be deployed in order to share to request id and status. The download folder of Motu has also to be shared between the different Motu instances.  \nIt can be on a NFS server or a GLusterFS server.  \nThe frontal web server \"Apache HTTPd\" must serve the downloaded files and implement the load balencer between all Motu instances.   \nAll other servers, CAS, NFS remains as on the single instance architecture.   \nThe same source code is used to run Motu with a single architecture or with several instances. It is just done by [configuration](#InstallationScalability).  \nWhen Motu is scalable, one Motu server instance can run a download request, another distinct Motu server instance can respond to a \"get status\" request and a last one can respond the URL of the result file. \n\n![Software architecture](./motu-parent/src/doc/softwareArchitectureScalability.png \"Motu software architecture, scalability\")\n\n\n\n\t   \n# <a name=\"ArchitectureInterfaces\">Interfaces</a>  \n## <a name=\"ArchitectureInterfacesServer\">Server interfaces</a>  \nAll ports are defined in [motu.properties](#ConfigurationSystem) configuration file.\n\n* __HTTP__: Apache tomcat manages incoming requests with HTTP protocol.\n* __HTTPs__: Used to manage HTTPs incoming requests. This is delegated to Apache HTTPd frontal web server. Apache Tomcat is not intended to be used with HTTPs.\n* __AJP__: Used to communicate with an Apache HTTPd frontal server\n* __Socket for Shutdown__: Port opened by Tomcat to shutdown the server\n* __JMX__: Used to monitor the application\n* __Debug__: In development mode, used to remotely debug the application\n  \n## <a name=\"ArchitectureInterfacesExternal\">External interfaces with other systems or tools</a>  \nMotu has interfaces with other systems:  \n\n* __DGF__: Direct Get File: Read dataset from the file system. (See how to [configure it](#AdminDataSetAdd).)\n* __Unidata Thredds Data Server__: It connects with the NCSS or OpenDap HTTP REST API to run download request for example. (See how to [configure it](#AdminDataSetAdd).)\n* __HTTP CAS Server__: Use for Single Sign On (SSO) in order to manager user authentication. (See how to [configure it](#ConfigurationSystem) \"CAS SSO server\" and check [profiles](#BSconfigService) attribute set on the dataset.)\n* __CDO command line tool__: [CDO](#InstallCDO) is used to deal with 2 types of download requests, which are not covered by NCSS service of Thredds Data Server:  \n  * a download request on a __range of depths__,  \n  * a download request that come __across the boundary__ of the datasets (for global datasets)  \n* __Redis__: Stores the request id and status into the Redis server when Motu is scaled over several instances.  \n  \n# <a name=\"ArchitectureDesign\">Design</a>  \nThe Motu application has been designed by implementing the Three-Layered Services Application design. It takes many advantages in maintenance cost efficiency and in the ease of its future evolutivity.  \nThree layers are set in the core \"motu-web\" project:  \n\n* __USL__: User Service Layer: This layer manages all incoming actions through HTTP request\n* __BLL__: Business Logic Layer: This layer manages all Motu business\n* __DAL__: Data Access Layer: This layer manages all access to Motu external interfaces: DGF, Unidata server, CDO, ...\n\nEach layer is an entry point of the application designed with a singleton. These three singletons gives access to high level managers which provides services by implementing a Java interface.\nHigh level managers handle for example the configuration, the request, the catalog, the users.\n\nA common package is also defined to provide utilities: log4j custom format, XML, String ...\n\n# <a name=\"ArchitectureDesignD\">Design details</a>  \nThe main project is \"motu-web\". This project is divided in three main layers detailed below:  \n\n## <a name=\"ArchitectureDesignDMW\">Motu-web project</a> \n### <a name=\"ArchitectureDesignDLayers\">Layers</a> \n#### <a name=\"ArchitectureDesignDUSL\">USL</a>  \n* __usl/servlet/context/MotuWebEngineContextListener.java__: Apache tomcat ServletContextListener used to init and stop the application.  \n* __usl/request__: All requests are managed with the \"motu/web/servlet/MotuServlet.java\" by following a command pattern. \"action\" HTTP parameter matches one of the \"usl/request/actions\" classes.  \n* __usl/response__: XML and [Apache velocity](https://velocity.apache.org/) data model.  \n* __usl/wcs__: All WCS requests and responses are managed in this package. The servlet entry point is defined with the class \"motu/web/servlet/MotuWCSServlet.java\".\n\n#### <a name=\"ArchitectureDesignDBLL\">BLL</a>  \n* __bll/catalog__: Catalog and Product managers. Package \"bll/catalog/product/cache\" contains the product cache.\n* __bll/config__: Configuration and version manager.  \n* __bll/exception__: Business exceptions. MotuException is a generic one. MotuExceptionBase defines all other business exceptions.\n* __bll/messageserror__: Message error manager\n* __bll/request__: The queue server used to download requests\n* __bll/users__: User manager\n\n#### <a name=\"ArchitectureDesignDDAL\">DAL</a>    \n* __dal/catalog__: OpenDap, TDS or FILE catalog access.  \n* __dal/config__: Configuration (/motu-web/src/main/resources/schema/MotuConfig.xsd) and version manager (configuration, distribution, products).  \n* __dal/messageserror__: Manage messages for a specific error (/motu-web/src/main/resources/MessagesError.properties).\n* __dal/request__: NCSS, OpenDAP and CDO\n* __dal/tds__: NCSS and OpenDap datamodel\n* __dal/users__: User manager\n\n### <a name=\"ArchitectureDesignDDynamics\">Daemon threads</a> \n\"motu-web\" project starts daemon threads for:  \n\n* __bll/catalog/product/cache__: Keep a product cache and refresh it asynchronously to improve response time \n* __bll/request/cleaner__: Clean files (extracted files, java temp files) and request status (stored into java map or list objects)\n* __bll/request/queueserver__: Contains a thread pool executor to treat download requests\n* __dal/request/cdo__: A queue server used to manage requests using CDO tool to avoid using too much RAM. As CDO uses a lot of RAM memory, \nrequests that require CDO to be processed are in sequence and only one is processed at a time\n\n\n\n\n## <a name=\"ArchitectureDesignDOthers\">Other projects</a>\n* __motu-api-message__: JAXB API: All errors types, status mode, ... /motu-api-message/src/main/schema/XmlMessageModel.xsd  \n* __motu-api-rest__: @Deprecated. Not used since Motu v3.\n* __motu-distribution__: Deployment tools (ant script, install folder structure, ...)\n* __motu-library-cas__: Used to manage JAVA HTTP client with CAS server.\n* __motu-library-converter__: JodaTime, ...\n* __motu-library-inventory__: Used for DGF access. JAXBmodels are /motu-library-inventory/src/main/resources/fr/cls/atoll/motu/library/inventory/Inventory.xsd and CatalogOLA.xsd\n* __motu-parent__: Maven parent, eclipse launchers, documentation\n* __motu-poducts__: Source code and scripts used to build archive motu-products.tar.gz (JDK, Apache tomcat, CDO tools)\n* __motu-scripts__: ./motu bash script\n* __motu-web__: Main Motu project. See [Motu-web project](#ArchitectureDesignDMW) for details.\n  \n  \n## <a name=\"ArchiAlgo\">Algorithm details</a>  \n\n### <a name=\"ArchiAlgoDownloading1Point\">Downloading 1 point</a>  \nSchema below displays a subset of a dataset variable as an array of 2 longitudes and 2 latitudes. At each intersection, we have got 1 real value (10, 11, 12, 13) as defined in the gridded data.  \nBut which result value is returned by Motu when the request target a location between those longitudes and latitudes ?   \nAs we can see below, 4 areas are displayed and the nearest value from the requested location is returned.\n\n![Downloading 1 point](./motu-parent/src/doc/downwloading1point.png \"Motu algorithm: Downloading 1 point\")\n\t \n# <a name=\"Development\">Development</a>  \n\n## <a name=\"DEVSRC\">Source code</a>\nSource code can be downloaded directly from Github.  \n\n```  \nmkdir motugithub  \ncd motugithub  \ngit clone https://github.com/clstoulouse/motu.git  \n    #In order to work on a specific version  \ngit tag -l  \ngit checkout motu-X.Y.Z  \ngit status  \ncd motu  \ncd motu-parent  \n```\n\n## <a name=\"DEV\">Development environment</a>  \n\n### Configure Eclipse development environment\n* Add variable in order to run/debug Motu on your localhost:  \nFrom Eclipse menu bar: Run/Debug > String substitution  \nMOTU_HOME=J:\\dev\\cmems-cis-motu\\motu-install-dir  \nThis variable represent the folder where Motu is installed.  \n\n* From a file explorer, create folders:  \n$MOTU_HOME/log  \n$MOTU_HOME/config  \n$MOTU_HOME/data/public/download  \n\n* Copy configuration files from Eclipse to configuration folder:  \nNote: If you do not have any motu-config folder available, default configuration files are folders are available in the \"/motu-web/src/main/resources\" folder  \nIf \"motu-config\" exists, copy:  \ncp $eclipse/motu-config/src/config/common/config $MOTU_HOME/config  \ncp $eclipse/motu-config/src/config/cls/dev-win7 $MOTU_HOME/config  \n\n \n* Add an application server in Eclipse: Window>Preferences>Server>Runtime environment  \nName=Apache Tomcat 7.0  \nTomcat installation directory=C:\\dvlt\\java\\servers\\tomcat\\apache-tomcat-7.0.65  \n\n* J2EE perspective > Under the Servers view > Right click > New > Server  \nServer Name: Tomcat v7.0 Server at localhost  \n\n* Edit /Servers/Tomcat v7.0 Server at localhost/server.xml and add  \n```\n<Context docBase=\"J:/dev/cmems-cis-motu/motu-install-dir/data-deliveries\" path=\"/mis-gateway/deliveries\" />\n```  \njust under the line:  \n```\n<Host appBase=\"webapps\" autoDeploy=\"true\" name=\"localhost\" unpackWARs=\"true\">\n```  \nNow Tomcat can serve downloaded files directly   \n \n### Run/Debug Motu\n\nClick Debug configurations...> Under Apache Tomcat, debug \"Motu Tomcat v7.0 Server at localhost\"\n\nOpen a web browser and test:  \nhttp://localhost:8080/motu-web/Motu?action=ping  \n\nit displays \"OK - response action=ping\"  \n\n\nFor more details about Eclipse launchers, refers to /motu-parent/README-eclipseLaunchers.md.\n\n\n## <a name=\"COMPILATION\">Compilation</a>  \n\nMaven is used in order to compile Motu.  \nYou have to set maven settings in order to compile.  \nCopy/paste content below in a new file settings.xml and adapt it to your information system by reading comments inside.\n\n```xml  \n    <settings>  \n    <!-- localRepository: Path to the maven local repository used to store artifacts. (Default: ~/.m2/repository) -->  \n    <localRepository>J:/dev/cmems-cis-motu/testGitHub/m2/repository&lt;/localRepository>  \n    <!-- proxies: Optional. Set it if you need to connect to a proxy to access to Internet -->  \n    <!--   \n    <proxies>  \n       <proxy>  \n          <id>cls-proxy</id>  \n          <active>true</active>  \n          <protocol>http</protocol>  \n          <host></host>  \n          <port></port>  \n          <username></username>  \n          <password></password>  \n          <nonProxyHosts></nonProxyHosts>  \n        </proxy>  \n      </proxies>  \n    -->   \n    <!-- Repositories used to download Maven artifacts in addition to https://repo.maven.apache.org   \n         cls-to-ext-thirdparty : contains patched libraries and non public maven packaged libraries  \n         geotoolkit: contains geographical tools libraries  \n    -->  \n    <profiles>    \n       <profile>  \n         <id>profile-cls-cmems-motu</id>  \n         <repositories>  \n            <repository>  \n              <id>cls-to-ext-thirdparty</id>  \n              <name>CLS maven central repository, used for CMEMS Motu project</name>  \n              <url>http://mvnrepo.cls.fr:8081/nexus/content/repositories/cls-to-ext-thirdparty</url>  \n            </repository>  \n            <repository>  \n              <id>geotoolkit</id>  \n              <name>geotoolkit</name>  \n              <url>http://maven.geotoolkit.org/</url>  \n            </repository>  \n        </repositories>  \n       </profile>  \n     </profiles>  \n     </settings>\n```\n\nThis step is used to generate JAR (Java ARchives) and WAR (Web application ARchive).   \n\nIn the GitLab Continuous Integration context, the access to the Maven repository are configured with maven password encryption using the ci/settings-security.xml master key, according to https://maven.apache.org/guides/mini/guide-encryption.html.\nIt is recommended to use maven encryption, and for changing a password, ensure that the encryption is done on a maven repository using the same master key than the one used on the continuous integration server, or update with your local master key of your own settings-security.xml. \n\n```  \nmkdir motu  \ncd motu   \n  #Copy paste the content above inside settings.xml  \nvi settings.xml  \n  #Get source code of the last Motu version  \ngit clone https://github.com/clstoulouse/motu.git  \ncd motu/motu-parent  \n  #  This remove the maven parent artifact from pom.xml, or remove lines below manually:  \n  #  <parent>  \n  #        <artifactId>cls-project-config&lt;/artifactId>  \n  #        <groupId>cls.commons&lt;/groupId>  \n  #        <version>1.2.00&lt;/version>  \n  # </parent>  \nsed -i '6,10d' pom.xml  \n  #Compile the source code  \nmvn -s ../../settings.xml -gs ../../settings.xml -Pprofile-cls-cmems-motu -Dmaven.test.skip=true clean install  \n...  \n[INFO] BUILD SUCCESS  \n...    \n``` \n\nAll projects are built under target folder.  \nThe Motu war is built under \"/motu-web/target/motu-web-X.Y.Z-classifier.war\".  \nIt embeds all necessary jar libraries.  \n\n## <a name=\"Packaging\">Packaging</a>  \nThis packaging process can be run only on CLS development environment.\nThis is an helpful script used to packaged as tar.gz the different projects (products, distribution (server and client), configuration, themes).   \nSo if you try to run it outside of CLS development environment, you will have to tune and remove many things to run it successfully (in particular all which is related to motu-config and motu-products).\nThis step includes the compilation step. Once all projects are compiled, it groups all archives in a same folder in order to easy the final delivery.  \nYou have to set ANT script inputs parameter before running it.  \nSee /motu-distribution/build.xml header to get more details about inputs.  \n```  \ncd /motu-distribution  \nant  \ncd target-ant/delivery/YYYYMMDDhhmmssSSS  \n```  \n\n4 folders are built containing archives:  \n\n* src: contains sources of motu application and the configuration files\n   * motu-$version-src.tar.gz  \n   * motu-client-python-$version-src.tar.gz  \n   * motu-config-$version-src.tar.gz  \n   * motu-web-static-files-$version-src.tar.gz  \n* motu: contains the built application archive and the products (java, tomcat, cdo) archive  \n   * motu-distribution-$version.tar.gz\n   * motu-products-$version.tar.gz\n* config: contains two kind of archives:\n  * motu-config-X.Y.Z-classifier-$timestamp-$target.tar.gz:  the built configurations for each target platform  \n  * motu-web-static-files-X.Y.Z-classifier-$timestamp-$target.tar.gz: The public static files (css, js) for each target platform \n* motu-client  \n  * motu-client-python-$version-bin.tar.gz\n\n# <a name=\"Installation\">Installation</a>  \n\n\n\n## <a name=\"InstallPrerequisites\">Prerequisites</a>  \n\nIn this chapter some paths are set. For example \"/opt/cmems-cis\" is often written to talk about the installation path.\nYou are free to install Motu in any other folder, so in the case, replace \"/opt/cmems-cis\" by your installation folder.  \nThis installation is used to install Motu on a [single instance](#ArchitectureSingleInstance). To scale Motu on [several instances](#ArchitectureScalability), refers to [Install a scalable Motu over several instances](#InstallationScalability).\n\n### <a name=\"InstallPrerequisitesHard\">Motu host, hardware settings</a>\nOS target: Linux 64bits (Tested on centos-7.2.1511)\n\nMinimal configuration for an __operational usage__:  \n\n* __CPU__: 4 CPU, 2,4GHz\n* __RAM__: 32 Gb RAM\n* __Storage__: \n  * Motu installation folder 15Gb: can be install on the OS partition (default folder /opt/cmems-cis)\n  * Motu download folder 200Gb: by default [/opt/cmems-cis/motu/data/public/download](#InstallFolders)  \n    Has to be installed in a dedicated partition to avoid freezing Motu if disk is full.\nNote that the available space of the download folder has to be tuned, depending on:  \n     * The number of users which run requests at the same time on the server\n     * The size of the data distributed\n   \nOnce started, you can [check performance](#ExpMonitorPerf).\n   \nFor __test usage__ we recommend:  \n\n* __CPU__: 2 CPU, 2,4GHz\n* __RAM__: 10 Gb RAM\n* __Storage__: \n  * Motu installation folder 15Gb\n  * Motu download folder 50Gb: by default [motu/data/public/download](#InstallFolders)  \n\n\n### <a name=\"InstallPrerequisitesSoft\">Motu host, software settings</a>\nMotu embeds all its dependencies (Java , Tomcat, CDO). All versions of these dependencies will be visible in the folder name once the Motu product archive is extracted.  \nFor example:  \n```\nls -1 /opt/cmems-cis/motu/products  \napache-tomcat-7.0.69  \ncdo-group  \njdk1.7.0_79  \nREADME  \nversion-products.txt  \n```  \n\nSo bash shell is only required on the Linux host machine.  \n\n### <a name=\"InstallPrerequisitesExternalInterfaces\">External interfaces</a>\nMotu is able to communicate with different external servers:  \n\n* __Unidata | THREDDS Data Server (TDS)__: Motu has been only tested with TDS v4.6.14 2019-07-29. The links to this server are set in the [Business settings](#ConfigurationBusiness) and are used to run OpenDap or subsetter interfaces. If Motu runs only with DGF, this server is not required.  \nNote that some specific characters have to be relaxed, e.g. when TDS is installed on Apache Tomcat, add attribute relaxedQueryChars=\"&lt;&gt;[\\]{|}\" in the connector node by editing conf/server.xml from your TDS tomcat installation folder:  \n```  \n<Connector relaxedQueryChars=\"&lt;&gt;[\\]{|}\" port=\"8080\" ...  \n```\nas reported in this [forum topic](https://groups.google.com/a/opendap.org/d/msg/support/ixTqhDXoLZQ/IT0lvZQ7CAAJ).  \nWithout this configuration Motu server can raised exeptions visible in the Motu \"errors.log\", e.g.:  \n```\nERROR fr.cls.atoll.motu.web.bll.catalog.product.cache.CacheUpdateService.updateConfigService Error during refresh of the describe product cache, config service=..., productId=...  \nfr.cls.atoll.motu.web.bll.exception.MotuException: Error in NetCdfReader open - Unable to aquire dataset - location data:  \nCaused by: java.io.IOException: http://.../thredds/dodsC/$dataset is not a valid URL, return status=400  \n```  \n  \n  \n \n* __Single Sign-On - CAS__: The link to this server is set in the [System settings](#ConfigurationSystem). If Motu does not use SSO, this server is not required.\n\nThe installation of these two servers is not detailed in this document. Refer to their official web site to know how to install them.\n\n  \n### <a name=\"InstallPrerequisitesSeveralsInstances\">Several Motu instances on a same host</a>\nIf you need to instance several instances of Motu server on a same host, you have to:  \n\n* __RAM__: set 32Go of RAM for each instance. For example, two Motu instances on a same host requires 64Go  \n* __Storage__: allocate disk space, in relationship with the Motu usage. Download dedicated partition can be shared or dedicated.  \n* __Folders__: Install each Motu instance in a dedicated folder:  \n  * /opt/motu1/motu, \n  * /opt/motu2/motu, \n  * ..., \n  * /opt/motuN/motu  \n\n  \n## <a name=\"UpgradeFromMotu2x\">Upgrade from Motu v2.x</a>    \n\nCheck this section only if you have installed Motu v2.x and you want to install Motu 3.x.\nIn this section we consider that your Motu installation folder of version 2.x is \"/opt/atoll/misgw/\".  \n\n### Upgrade the software\nFirst stop Motu v2.x: /opt/atoll/misgw/stop-motu.  \nThen install the version 3.x of [Motu from scratch](#InstallFromScratch). Before starting the new Motu version, upgrade its configuration by ready section below.  \nOnce the version 3.x of Motu runs well, you can fully remove the folder of version 2.x is \"/opt/atoll/misgw/\".  \nTo avoid any issue, perhaps backup the folder of Motu v2.x before removing it definitively.  \n``` \nmotu2xInstallFolder=/opt/atoll/misgw  \nrm -rf $motu2xInstallFolder/deliveries  \nrm -rf $motu2xInstallFolder/motu-configuration-common-2.1.16  \nrm -rf $motu2xInstallFolder/motu-configuration-sample-misgw-1.0.5  \nrm -rf $motu2xInstallFolder/motu-web  \nrm -rf $motu2xInstallFolder/start-motu  \nrm -rf $motu2xInstallFolder/stop-motu  \nrm -rf $motu2xInstallFolder/tomcat7-motu  \nrm -rf $motu2xInstallFolder/tomcat-motu-cas  \n```  \n\n\n\n### Upgrade the configuration\n\n#### Business configuration, Product & dataset: motuConfiguration.xml  \nThe new version of Motu is compatible with the motuConfiguration.xml file configured in Motu v3.x.  \nSo you can use exactly the same file, but it is important to update some fields to improve performance and future compatibility. \nCopy your old motuConfiguration.xml file to the folder /opt/cmems-cis/motu/config, for example:  \n```\ncp  /opt/atoll/misgw/motu-configuration-sample-misgw/resources/motuConfiguration.xml /opt/cmems-cis/motu/config\n```  \n\nThen update attributes below:  \n\n* use the TDS subsetter protocol to improve performance of product download  \n   * __motuConfig/configService/catalog__\n      * __ncss__: See [ncss protocol](#BSmotuConfigNCSS)\n      * __urlSite__: Update URL to use the TDS subsetter URL. See [Add a dataset, TDS NCSS protocol](#AdminDataSetAdd)\n* check the attributes used to serve downloaded datasets  \n   * __motuConfig__\n      * __extractionPath__ See [extractionPath](#motuConfig-extractionPath)\n      * __downloadHttpUrl__ Set the URL used to serve files from extractionPath\n* remove all @deprecated attributes listed below to ease future migrations. You can read the attribute description in [Business configuration](#ConfigurationBusiness).  \n   * __XML header__ Remove header below  \n```\n< !DOCTYPE rdf:RDF [  \n<!ENTITY myoceanUrn \"http://purl.org/myocean/ontology/service/database#\">  \n]>  \n```\n   * __motuConfig__\n      * __maxSizePerFile__ This attribute definition has been updated. See [maxSizePerFile parameter configuration](#motuConfig-defaultService)  \n      * __maxSizePerFileTDS__ Rename this attribute to maxSizePerFileSub. See [maxSizePerFileSub parameter configuration](#motuConfig-defaultService)  \n      * __runGCInterval__ Remove the attribute\n      * __httpDocumentRoot__ Remove the attribute\n      * __useAuthentication__ Remove the attribute\n      * __defaultActionIsListServices__ Remove the attribute\n      * __useProxy__, __proxyHost__, __proxyPort__, __proxyLogin__, __proxyPwd__ Remove the attributes\n      * __defaultService__  Remove the attribute, See [defaultService](#motuConfig-defaultService) . It was previously used to declare a default config Service. Now this attribute sets a default action. If the action set is unknown, an error log will be written and user is redirected to the listServices action which is the default one.\n   * __motuConfig/configService__\n      * __defaultLanguage__ Remove the attribute\n      * __defaultPriority__ Remove the attribute\n      * __httpBaseRef__ Remove the attribute\n      * __veloTemplatePrefix__ Remove the attribute\n   * __motuConfig/queueServerConfig__\n      * __defaultPriority__ Remove the attribute\n   * __motuConfig/queues__\n      * __batch__ Remove the attribute\n      * __lowPriorityWaiting__ Remove the attribute\n   * __motuConfig/configFileSystem__ Remove the node\n* after starting Motu, if there is issues with the graphic chart, check the attributes below.\n   * __motuConfig__\n      * __httpBaseRef__ See [httpBaseRef](#motuConfig-httpBaseRef) attribute\n   * __motuConfig/configService__\n      * __httpBaseRef__ Remove the attribute\n\n### Upgrade DGF\nResource URN attributes are not handled in the same way in Motu v3.  \nIn Motu __v2.x__, URN attributes had values set with an ontology URL:  \n``` \n<resource urn=\"http://purl.org/myocean/ontology/product/database#dataset-bal-analysis-forecast-phys-V2-dailymeans\">\n```   \n\nIn Motu __v3.x__, you have to upgrade URN attributes with only the value found after the # character:  \n``` \n<resource urn=\"dataset-bal-analysis-forecast-phys-V2-dailymeans\">\n```   \n  \n\n\n\n#### Log files\nIn CMEMS-CIS context the log file motuQSlog.xml is stored in a specific folder in order to be shared.  \nYou have so to check that with the new version this log file is well written in the shared folder.\nHere is where this log files were written in Motu v2.x:  \n``` \ngrep -i \"motuQSlog.xml\" /opt/atoll/misgw/motu-configuration-sample-misgw-1.0.5/resources/log4j.xml\n<param name=\"file\" value=\"/opt/atoll/misgw/tomcat-motu-cas/logs/motuQSlog.xml\" />\n```  \nThe folder set in the value attribute shall be the same as the one defined in new the Motu configuration file. Replace $path below by the folder path:  \n``` \ngrep -i \"motuQSlog.xml\" /opt/cmems-cis/motu/config/log4j.xml\nfileName=\"$path/motuQSlog.xml\"\nfilePattern=\"$path/motuQSlog.xml.%d{MM-yyyy}\"\n``` \n\n\n\n\n\n\n  \n## <a name=\"InstallFromScratch\">Install Motu from scratch</a>  \n\nMotu installation needs two main step: the software installation and optionally the theme installation.  \nThe software installation brings by default the CLS theme. The theme installation is there to customize or change this default theme.  \n\n### Install Motu software, for example on a <a name=\"IntallDU\">Dissemination Unit</a>    \n\nCopy the installation archives and extract them.  \n```\ncd /opt/cmems-cis  \ncp motu-products-A.B.tar.gz .  \ncp motu-distribution-X.Y.Z.tar.gz .  \ncp motu-config-X.Y.Z-$BUILDID-$TARGET-$PLATFORM.tar.gz .  \ntar xzf motu-products-A.B.tar.gz  \ntar xzf motu-distribution-X.Y.Z.tar.gz  \ntar xzf motu-config-X.Y.Z-$BUILDID-$TARGET-$PLATFORM.tar.gz  \ncd motu\n```\n\nAt this step, Motu is able to start. But static files used for customizing the web theme can be installed.  \nIn the CMEMS context, the installation on a dissemination unit is ended, static files are installed on a [central server](#InstallPublicFilesOnCentralServer).  \n\nNow you can configure the server:  \n* Set the [system properties](#ConfigurationSystem): http port, ...\n* Configure [dataset](#ConfigurationBusiness)\n* Configure the [logs](#LogSettings)\n  \nRefer to [configuration](#Configuration) in order to check your configuration settings.  \n\nMotu is installed and configured. You can [start Motu server](#SS).  \nThen you can [check installation](#InstallCheck).\n\n\n### Install Motu theme (public static files)\n\nAs a dissemination unit administrator, in CMEMS context, this section is not applicable.  \n\nPublic static files are used to customized Motu theme. When several Motu are installed, a central server eases the installation and the update by \nreferencing static files only once on a unique machine. This is the case in the CMEMS context, where each dissemination unit host a Motu server, and \na central server hosts static files.  \nIf you runs only one install of Motu, you can install static files directly on Motu Apache tomcat server.\n\n#### <a name=\"InstallPublicFilesOnCentralServer\">On a central server</a>    \nExtract this archive on a server.\n```\ntar xvzf motu-web-static-files-X.Y.Z-classifier-$timestamp-$target.tar.gz  \n```\nThen use a server to make these extracted folders and files accessible from an HTTP address.\n \nExample: The archive contains a motu folder at its root. Then a particular file is \"motu/css/motu/motu.css\" and this file is served by the URL   http://resources.myocean.eu/motu/css/motu/motu.css in the CMEMS CIS context.   \n\n\n\n#### <a name=\"IntallPublicFilesOnMotuTomcat\">Directly on Motu Apache tomcat server</a>    \n \nIf you do not use a central entity to serve public static files, you can optionally extract the archive \nand serve files directly by configuring Motu.  \nFirst extract the archive:   \n```\ntar xzf motu-web-static-files-X.Y.Z-classifier-$timestamp-$target.tar.gz -C /opt/cmems-cis/motu/data/public/static-files   \n```\n\nThen edit \"motu/tomcat-motu/conf/server.xml\" in order to serve files from Motu.  \nAdd then \"Context\" node as shown below. Note that severals \"Context\" nodes can be declared under the Host node.  \n```\n[...]  \n<Host appBase=\"webapps\" [...]  \n        <!-- Used to serve public static files -->  \n        <Context docBase=\"/opt/cmems-cis/motu/data/public/static-files/motu\" path=\"/motu\"/>    \n [...]  \n```  \n  \nFinally in motuConfiguration.xml, remove all occurrences of the attribute named: [httpBaseRef](#motuConfig-httpBaseRef) in motuConfig and configService nodes. (Do not set it empty, remove it).\n\n\nIf you want to set another path instead of \"/motu\", you have to set also the business configuration parameter named [httpBaseRef](#motuConfig-httpBaseRef).  \n\n \n## <a name=\"InstallCheck\">Check installation</a>  \n\n### Start motu\n```\n./motu start \n```\n\n### Check messages on the server console\n\nWhen you start Motu, the only message shall be:  \n```\ntomcat-motu - start\n```\n\nOptionaly, when this is your first installation or when a software update is done, an INFO message is displayed:  \n```\nINFO: War updated: tomcat-motu/webapps/motu-web.war [$version]  \n```  \n  \n  \nIf any other messages appear, you have to treat them.\n\nAs Motu relies on binary software like CDO, error could be raised meaning that CDO does not runs well.  \n```\nERROR: cdo tool does not run well: $cdo --version  \n[...]\n```  \n\nIn this case, you have to install CDO manually.  \n\n### Check Motu web site available\n\nOpen a Web browser, and enter:\nhttp://$motuUrl/motu-web/Motu?action=ping  \nWhere $motuUrl is: ip adress of the server:tomcat port\nRefer to [configuration](#Configuration) regarding the tomcat port\n\nResponse has to be:   \n```  \nOK - response action=ping\n```  \n\nOpen a Web browser, and enter:\nhttp://$motuUrl/motu-web/Motu  \nIf nothing appears, it is because you have to [add dataset](#AdminDataSetAdd).  \n\n\n### Check Motu logs\nCheck that no error appears in Motu [errors](#LogbooksErrors) log files.\n\n## <a name=\"InstallCDO\">CDO manual installation</a>  \nThis section has to be read only if Motu does not start successfully.  \nSelect one option below to install \"cdo\". If you have no idea about cdo installation, choose the Default option.\n\n* [Default option: Install cdo](#InstallCDOHelp)\n* [cdo is already installed on this machine](#InstallCDOAlreadyInstalled)\n* [Try MOTU without cdo installation](#InstallCDONoInstall)\n* [How cdo is built?](#InstallCDOUnderstand)  \n  \n### <a name=\"InstallCDOHelp\">Install cdo</a>  \n\"cdo\" (Climate Data operators) are commands which has to be available in the PATH when Motu starts.   \nBy default, Motu provides a built of CDO and add the \"cdo\" command to the PATH, but with some Linux distribution it is necessary to install it.  \nMotu provides some help in order to install CDO.  \n\nFirst check your GLibC version:  \n```\nldd --version  \nldd (GNU libc) 2.12  \n[...]  \n```  \n\nIf your GlibC lower than 2.14, you have to install GLIBC 2.14, but to highly recommend to upgrade your Linux operating system to get an up to date GLIBC version:  \n__INSTALL GLIBC 2.14__  \n```\nexport MotuInstallDir=/opt/cmems-cis  \ncd $MotuInstallDir/motu/products/cdo-group  \nwget http://ftp.gnu.org/gnu/glibc/glibc-2.14.tar.gz  \ntar zxvf glibc-2.14.tar.gz  \ncd glibc-2.14  \nmkdir build  \ncd build  \nmkdir $MotuInstallDir/motu/products/cdo-group/glibc-2.14-home  \n../configure --prefix=$MotuInstallDir/motu/products/cdo-group/glibc-2.14-home  \nmake -j4  \nmake install  \ncd $MotuInstallDir/motu  \n```  \n\n\n__Now check if \"cdo\" runs well__:  \n```\nexport MotuInstallDir=/opt/cmems-cis  \n$MotuInstallDir/motu/products/cdo-group/cdo.sh --version  \nClimate Data Operators version x.y.z (http://mpimet.mpg.de/cdo)  \n[...]  \n```  \n\nIf error appear like ones below, it certainly means that GLIC is not in the LD_LIBRARY_PATH.\n```\n$MotuInstallDir/motu/products/cdo-group/cdo-x.z.z-home/bin/cdo: error while loading shared libraries: libhdf5.so.10: cannot open shared object file: No such file or directory\n```  \nor  \n```\ncdo: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by cdo)\ncdo: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /opt/cmems-cis-validation/motu/products/cdo-group/hdf5-1.8.17-home/lib/libhdf5.so.10)\n```\n\nIn this case, edit $MotuInstallDir/products/cdo-group/cdo.sh and add \"$GLIBC-home/lib\" to LD\\_LIBRARY\\_PATH.   \n\nNow check again if \"cdo\" runs well.\n\nIf it runs well, you can now start Motu.  \n\n\n### <a name=\"InstallCDOAlreadyInstalled\">cdo is already installed on this machine</a>  \nIf \"cdo\" is installed in another folder on the machine, you can add its path in \"$MotuInstallDir/motu/motu\" script:  \n\n```  \n__setPathWithCdoTools() {  \n  PATH=$MOTU_PRODUCTS_CDO_HOME_DIR/bin:$PATH  \n}  \n```  \n\nOptionnaly set LD_LIBRAY_PATH in $MotuInstallDir/products/cdo-group/setLDLIBRARYPATH.sh  \n\n\n\n### <a name=\"InstallCDONoInstall\">Try MOTU without cdo installation</a>  \nNote that without CDO, some functionalities on depth requests or on download product won't work successfully.\nIf any case, you can disable the CDO check by commented the check call:  \n\n* Disable check:  \n```\ncd /opt/cmems-cis/motu/  \nsed -i 's/  __checkCDOToolAvailable/#  __checkCDOToolAvailable/g' motu\n```  \n* Enable check:  \n```\ncd /opt/cmems-cis/motu/  \nsed -i 's/#  __checkCDOToolAvailable/  __checkCDOToolAvailable/g' motu\n```  \n  \n  \n  \n  \n### <a name=\"InstallCDOUnderstand\">How cdo is built?</a>  \nCDO is automaticcly build from the script $MotuInstallDir/motu/products/cdo-group/install-cdo.sh\nAlso in order to get full details about CDO installation, you can get details in /opt/cmems-cis/motu/products/README and\nsearch for 'Download CDO tools'.  \n\n\n\n## <a name=\"InstallFolders\">Installation folder structure</a>  \n  \nOnce archives have been extracted, a \"motu\" folder is created and contains several sub-folders and files:  \n__motu/__  \n\n* __config:__ Folder which contains the motu configuration files. Refers to [Configuration](#Configuration) for more details.\n* __data:__ Folder used to managed Motu data.\n  * __public__: Folders which contain files exposed to public. It can be published through a frontal Apache HTTPd Web server, through Motu Apache Tomcat or any other access.\n     * __download__: Folder used to save the products downloaded. This folder is sometimes elsewhere, for example in Motu v2: /datalocal/atoll/mis-gateway/deliveries/. A best practice is to create a symbolic link to a dedicated partition to avoid to freeze Motu when there is no space left.   \n     * __inventories__: This folder can be used to store the DGF files.  \n     * __transaction__: This folder is used to serve the [transaction accounting logs](#LogbooksTransactions)\n     * __static-files__: Used to store public static files. This folder can be served by a frontal Apache HTTPd Web server or Motu Apache Tomcat. In the CMEMS-CIS context, it is not used as static files are deployed on a central web server.      \n* __log:__ Folder which contains all log files. Daily logging are suffixed by yyyy-MM-dd.\n  * __logbook.log:__ Motu application logs (errors and warning are included)\n  * __warnings.log:__ Motu application warnings\n  * __errors.log:__ Motu application errors\n  * __motuQSlog.xml,motuQSlog.csv:__ Motu queue server logs messages (transaction accounting logs), format is either xml or csv\n* __motu file:__ Script used to start, stop Motu application.  Refers to [Start & Stop Motu](#SS) for more details.\n* __pid:__ Folder which contains pid files of the running Motu application.\n  * __tomcat-motu.pid:__ Contains the UNIX PID of the Motu process.\n* __products:__ Folder which contains Java, Tomcat ($CATALINE_BASE folder) and CDO products.\n  * __apache-tomcat-X.Y.Z:__ Apache tomcat default installation folder from http://tomcat.apache.org/\n  * __cdo-group:__ CDO tools from https://code.zmaw.de/projects/cdo\n  * __jdkX.Y.Z:__ Oracle JDK from http://www.oracle.com/technetwork/java\n  * __version-products.txt:__ Contains the version of the current Motu products.\n* __tomcat-motu:__ Tomcat is deployed inside this folder. This folder is built by setting the $CATALINE_HOME environment variable.\n* __version-distribution.txt file:__ Contains the version of the current Motu distribution.\n\n\n\n\n\n## <a name=\"InstallFrontal\">Setup a frontal Apache HTTPd server</a>  \nApache HTTPd is used as a frontal HTTP server in front of Motu Http server.\nIt has several aims:  \n\n* Delegate HTTP requests to Motu HTTP server    \n* Serve directly extracted dataset files written after a download action. The folder in which requests are written is [configurable](#motuConfig-extractionPath). URL used to download those files is \"http://$ipMotuServer/mis-gateway/deliveries\". This URL is [configurable](#motuConfig-downloadHttpUrl).  \n* Serve the download transaction logbook files. The folder in which log files are written is [configurable](#LogSettings).  \n* Manage errors like 403, 404. Motu server manages errors web page by displaying a custom message. Redirect to the URL \"http://$ipMotuServer/motu-web/Motu?action=httperror&code=$errorCode\" by replacing $errorCode with the HTTP error code.  \n* Optionally acts as a load balancer when several instances of Motu.\n\n\nSee sample of the Apache HTTPd configuration in the Motu installation folder: __config/apache-httpd-conf-sample__  \nThe configuration is described for Apache2 contains files:\n\n* __001_httpd-vhosts-motu.conf:__ Main apache configuration file used for Motu, replace __$serverHostName__ by the server host, and $webmasterEmail by the webmaster or administrator email.\n* __apache2.conf:__ Use to show how timeout parameter shall be set\n* __enableApacheModules.sh__ Describe the Apache modules to enable\n\nWhen an SSO cas server is used, you have to st the property [cas-auth-serverName](#ConfigurationSystemCASSSO) to http://$serverHostName\n\nApache HTTPd can be used at different levels. The Apache HTTPd above is the one installed on the same machine as Motu.\nAn Apache HTTPd can be used as a frontal to manage Apache HTTPd load balancing. In the case, you can set up with the following example:  \n\n```  \n # Use to authenticate users which want to download transaction files  \n< Location /datastore-gateway/transactions/* >  \n|--AuthType Basic  \n|--AuthName \"XXX\"  \n|--AuthUserFile /XXX/password.conf  \n|--Require valid-user  \n< / Location>   \n  \n # Used to serve URL requested after a CAS authentication  \n # Because Motu SSO client set a redirection URL directly to its webapp name  \n # so we have to take into account the webapp name in Apache HTTPd  \nProxyPass /motu-web http://$motuTomcatIp:$motuTomcatPort/motu-web  \nProxyPassReverse /motu-web http://$motuTomcatIp:$motuTomcatPort/motu-web  \n        \n # Used to serve Motu requests   \n    # /mis-gateway-servlet These rules are used for retro compatibility between Motu v2.x and Motu v3.x  \nProxyPass /mis-gateway-servlet http://$motuTomcatIp:$motuTomcatPort/motu-web  \nProxyPassReverse /mis-gateway-servlet http://$motuTomcatIp:$motuTomcatPort/motu-web  \nProxyPreserveHost On  \n    # /motu-web-servlet This URL is sometimes used.  \n    # It can be customized depending of your current installation. If you have any doubt, keep this rule.  \nProxyPass /motu-web-servlet http://$motuTomcatIp:$motuTomcatPort/motu-web  \nProxyPassReverse /motu-web-servlet http://$motuTomcatIp:$motuTomcatPort/motu-web  \n                \nProxyPass /datastore-gateway/transactions http://$apacheHTTPdOnMotuHost/datastore-gateway/transactions  \nProxyPassReverse /datastore-gateway/transactions http://$apacheHTTPdOnMotuHost/datastore-gateway/transactions  \n\nProxyPass /datastore-gateway/deliveries http://$apacheHTTPdOnMotuHost/datastore-gateway/deliveries  \nProxyPassReverse /datastore-gateway/deliveries http://$apacheHTTPdOnMotuHost/datastore-gateway/deliveries  \n\n< Location /motu-web-servlet/supervision>  \n|--Order allow,deny  \n|--Allow from All  \n< /Location>  \n```  \n\n\n\n## <a name=\"InstallSecurity\">Security</a>  \n\n### <a name=\"InstallSecurityRunHTTPs\">Run Motu as an HTTPS Web server</a>  \nMotu is a web server based on Apache Tomcat.  \nIn order to secure HTTP connections with a client, HTTPs protocol can be used.  \nYou have two choices:  \n\n* __Motu as a standalone web server__  \n  In this case only Motu is installed.  \n  Refers to the Apache Tomcat official documentation to know how to set SSL certificates: [SSL/TLS Configuration HOW-TO](#https://tomcat.apache.org/tomcat-9.0-doc/ssl-howto.html)\n* __Motu with an Apache HTTPd frontal web server__  \n  In this case Motu is installed and also a frontal Apache HTTPd server.  \n  Refers to the Apache HTTPd official documentation: [SSL/TLS Strong Encryption: How-To](#https://httpd.apache.org/docs/2.4/ssl/ssl_howto.html)  \n  [Apache HTTPd](#InstallFrontal) communicates with Apache Tomcat with the [AJP protocol](#ConfigurationSystem).\n\n### <a name=\"InstallSecuritySSO\">Motu and Single Sign On</a>  \nIn order to manage SSO (Single Sign On) connections to Motu web server, Motu uses an HTTPs client.  \nAll documentation about how to setup is written in chapter [CAS SSO server](#ConfigurationSystemCASSSO).\n\n\n## <a name=\"InstallationScalability\">Install a scalable Motu over several instances</a>  \nScalability is provided with two main components, the Redis database, for sharing the status of the requests between Motu instances, and Tomcat with the session replication mechanism to share login and authentication data.\n### Scalability prerequisites:\n* install a [Redis server](https://redis.io/). (Motu has been tested with Redis version 4.0.8, 64 bit). Redis shares the request ids and status between all Motu instances\n* share the [download folder](#motuConfig-extractionPath) between all instances with a NFS mount, GlusterFS or any other file sharing system.\n* set a frontal web server to serve the [downloaded](#motuConfig-downloadHttpUrl) files from the Motu server and to load balance the requests between all Motu servers.  \n  \n### Scalability configuration:\n\n#### Motu configuration:\n* $installDir/motu/config/__motuConfiguration.xml__: set the Redis settings in the [business configuration file](#RedisServerConfig). For example:\n```xml\n<motuConfig dataBlockSize=\"1024000\" ...>\n   ....\n   <redisConfig host=\"localhost\" port=\"16379\"/>\n   ....\n</motuConfig>\n```  \n  \n#### Motu apache Tomcat configuration:\n* $installDir/motu/tomcat-motu/conf/__web.xml__ file: add the _distributable_ element inside of the _web-app_ element\n```xml\n<web-app xmlns=\"http://xmlns.jcp.org/xml/ns/javaee\"\n  xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n  xsi:schemaLocation=\"http://xmlns.jcp.org/xml/ns/javaee\n                      http://xmlns.jcp.org/xml/ns/javaee/web-app_4_0.xsd\"\n  version=\"4.0\">\n    ....\n    <distributable />\n    ....\n</web-app>\n```  \n  \n* $installDir/motu/tomcat-motu/conf/__server.xml__ file\n  * add a _Cluster_ element in the _Engine_ element\n```xml  \n<Server port=\"16005\" shutdown=\"SHUTDOWN\">\n  <Service name=\"Catalina\">\n    <Engine name=\"Catalina\" defaultHost=\"localhost\">\n      ....\n      <Cluster className=\"org.apache.catalina.ha.tcp.SimpleTcpCluster\"\n                       channelSendOptions=\"6\">\n        <Channel className=\"org.apache.catalina.tribes.group.GroupChannel\">\n          <Membership className=\"org.apache.catalina.tribes.membership.McastService\"\n                      address=\"228.0.0.4\"\n                      port=\"45564\"\n                      frequency=\"500\"\n                      dropTime=\"3000\"/>\n          <Receiver className=\"org.apache.catalina.tribes.transport.nio.NioReceiver\"\n                    address=\"auto\"\n                    port=\"4000\"\n                    autoBind=\"100\"\n                    selectorTimeout=\"5000\"\n                    maxThreads=\"6\"/>\n      \n          <Sender className=\"org.apache.catalina.tribes.transport.ReplicationTransmitter\">\n            <Transport className=\"org.apache.catalina.tribes.transport.nio.PooledParallelSender\"/>\n          </Sender>\n          <Interceptor className=\"org.apache.catalina.tribes.group.interceptors.TcpFailureDetector\"/>\n          <Interceptor className=\"org.apache.catalina.tribes.group.interceptors.MessageDispatchInterceptor\"/>\n        </Channel>\n        <Valve className=\"org.apache.catalina.ha.tcp.ReplicationValve\" filter=\"\"/>\n        <Valve className=\"org.apache.catalina.ha.session.JvmRouteBinderValve\"/>\n        <Deployer className=\"org.apache.catalina.ha.deploy.FarmWarDeployer\"\n                  tempDir=\"/tmp/war-temp/\"\n                  deployDir=\"/tmp/war-deploy/\"\n                  watchDir=\"/tmp/war-listen/\"\n                  watchEnabled=\"false\"/>\n        <ClusterListener className=\"org.apache.catalina.ha.session.ClusterSessionListener\"/>\n      </Cluster>\n      ....\n    </Engine>\n  </Service>\n</Server>\n```  \n  Ensure that the configured multicast ip is authorized for multicast (here `228.0.0.4`). The configured broadcast port (here `45564`) has to be available.  \n  Each Motu instance will get allocated a port in the configurable range starting at the port `4000` as configured in the _Receiver_ element. Ensure those ports are available.  \n  >_channelSendOptions_ at `6` ensures that a request creating a session on a server of the cluster is shared among all other servers and acknowledged, before of being responded. \n  * Ensure no `jvmRoute` field is configured in the _Engine_ element:\n```xml\n<Server port=\"16005\" shutdown=\"SHUTDOWN\">\n  <Service name=\"Catalina\">\n     ....\n     <Engine name=\"Catalina\" defaultHost=\"localhost\">\n     <!-- instead of <Engine name=\"Catalina\" defaultHost=\"localhost\" jvmRoute=\"server1\"> -->\n         ....\n     </Engine>\n     ....\n  </Service>\n</Server>\n```  \n  \n* $installDir/motu/tomcat-motu/conf/__context.xml__: add a _Manager_ in the _Context_ element.\n```xml\n<Context>\n  ....\n  <Manager className=\"org.apache.catalina.ha.session.DeltaManager\"\n           expireSessionsOnShutdown=\"false\"\n           notifyListenersOnReplication=\"true\"\n           persistAuthentication=\"true\"/>\n  ....\n</Context>\n```  \n  \n#### Frontal load balancer configuration (httpd or other)\nDeactivate sticky session mechanisms:\n* for __httpd__ _VirtualHost_ blocks\n  * no _route_ attribute in `BalancerMember` directives:\n```xml\n<Proxy balancer://cluster/>\nBalancerMember http://my-host:6080/motu-web\nBalancerMember http://my-host:7080/motu-web\n</Proxy>\n```  \n  * no _stickysession_ attribute in `ProxyPass` and `ProxyPassReverse` directives:\n```xml\nProxyPass /motu-web/ balancer://cluster/\nProxyPassReverse /motu-web/ balancer://cluster/\n``` \n* for __HAProxy__ configurations, do not use `stick on ` directives on IP or session  \n  \n# <a name=\"Configuration\">Configuration</a>  \n\nThis chapter describes the Motu configuration settings.  \nAll the configuration files are set in the $installDir/motu/config folder.  \n\n* [Configuration directory structure](#ConfigurationFolderStructure)\n* [Business settings](#ConfigurationBusiness)\n* [System settings](#ConfigurationSystem)\n* [Log settings](#LogSettings)\n\n  \n## <a name=\"ConfigurationFolderStructure\">Configuration directory structure</a>  \ncd $installDir/motu/config\n  \n* __config:__ Folder which contains the motu configuration files.\n  * __[motu.properties](#ConfigurationSystem):__ JVM memory, network ports of JVM (JMX, Debug) and Tomcat (HTTP, HTTPS, AJP, SHUTDOWN). CAS SSO server settings.\n  * __[motuConfiguration.xml](#ConfigurationBusiness):__ Motu settings (Service, Catalog via Thredds, Proxy, Queues, ....)\n  * __[log4j.xml](#LogSettings):__ Log4j v2 configuration file\n  * __[standardNames.xml](#ConfigStandardNames):__ Contains the standard names\n  * __version-configuration.txt:__ Contains the version of the current Motu configuration.\n  \n## <a name=\"ConfigurationBusiness\">Business settings</a>  \n### motuConfiguration.xml: Motu business settings  \nThis file is watched and updated automatically. This means that when Motu is running, this file has to be written in a atomic way.  \n\nYou can configure 3 main categories:  \n\n* [MotuConfig node : general settings](#BSmotuConfig)\n* [ConfigService node : catalog settings](#BSconfigService)\n* [QueueServerConfig node : request queue settings](#BSqueueServerConfig)\n* [RedisConfig node : Redis server config](#RedisServerConfig)\n  \n  \nIf you have not this file, you can extract it (set the good version motu-web-X.Y.Z.jar):  \n```\n/opt/cmems-cis/motu/products/jdk1.7.0_79/bin/jar xf /opt/cmems-cis/motu/tomcat-motu/webapps/motu-web/WEB-INF/lib/motu-web-X.Y.Z.jar motuConfiguration.xml\n```   \n    \n  \nIf you have this file from a version anterior to Motu v3.x, you can reuse it. In order to improve global performance, you have to upgrade some fields:  \n* [ncss](#BSmotuConfigNCSS) Set it to \"enabled\" to use a faster protocol named subsetter rather than OpenDap to communicate with TDS server. ncss must be enabled only with regular grid. The datasets using curvilinear coordinates (like ORCA grid) can not be published with ncss. Thus, ncss option must be set to disable or empty.  \n* [httpBaseRef](#motuConfig-httpBaseRef) shall be set to the ULR of the central repository to display the new theme  \n* [ExtractionFilePatterns](#BSmotuConfigExtractionFilePatterns) to give a custom name to the downloaded dataset file  \n  \n  \n\n#### <a name=\"BSmotuConfig\">Attributes defined in motuConfig node</a>  \n\n##### <a name=\"motuConfig-defaultService\">defaultService</a>  \nA string representing the default action in the URL /Motu?action=$defaultService  \nThe default one is \"listservices\".  \nAll values can be found in the method USLRequestManager#onNewRequest with the different ACTION_NAME.  \n\n##### dataBlockSize\nAmount of data in Ko that can be requested in a single query from Motu to TDS. Default is 2048Kb.  \nIf this amount is lower than the [maxSizePerFile](#maxSizePerFile) (in MegaBytes), Motu will launch several sub-requests to TDS to gather all the data.  \nHigher value (up to the [maxSizePerFile](#maxSizePerFile)) leads to less requests, but with higher data volume to transfer by request from TDS to Motu. And the [tds.http.sotimeout](#TdsHttpSoTimeout) has to be long enough for letting TDS the time to read and transfer the whole data to Motu.  \nLower values will imply more requests, but shorter. It consumes more CPU on both Motu and TDS with the advantages to allow TDS to answer to other parallel request it would receive, and to reduce communication times for each request.  \nConcerning performance, we tried 200Mb and 1024Mb for a 1024Mb request, and durations were similar, but note that the shapes of the sub-requests may not match the shapes of the netcdf files, and that could imply a supplementary delay for hard drive data storage.  \n>If the [tds.http.sotimeout](#TdsHttpSoTimeout) can be set to a high value (such as 900s for a 1Gb max size request), the safest is to use the [maxSizePerFile](#maxSizePerFile) value for the *dataBlockSize* parameter (mind the units Kb/Mb).  \n>Else from the max timeout the environment can support (external constraints or \"004-27\" error, see [tds.http.sotimeout](#TdsHttpSoTimeout)), extrapolate a volume of data that can be transfered during this delay, lower it to keep a margin, and use it as the *dataBlockSize*.\n\n##### maxSizePerFile\nThis parameter is only used with a catalog type set to \"FILE\" meaning a DGF access.  \nIt allows download requests to be executed only if data extraction is lower than this parameter value.  \nUnit of this value is MegaBytes.  \nDefault is 1024 MegaBytes.  \nExample: maxSizePerFile=\"2048\" to limit a request result file size to 2GB.  \n\n##### maxSizePerFileSub\nThis parameter is only used with a catalog type used with Opendap or Ncss.  \nIt allows download requests to be executed only if data extraction is lower that this parameter value.  \nUnit of this value is MegaBytes.  \nDefault is 1024 MegaBytes.  \nExample: maxSizePerFileSub=\"2048\" to limit request result file size to 2GB.\n\n##### maxSizePerFileTDS\n@Deprecated from v3 This parameter is not used and has been replaced by maxSizePerFile and maxSizePerFileSub.   \nNumber of data in Megabytes that can be written and download for a Netcdf file. Default is 1024Mb.\n\n##### <a name=\"motuConfig-extractionPath\">extractionPath</a>  \nThe absolute path where files downloaded from TDS are stored.  \nFor example: /opt/cmems-cis/motu/data/public/download\nIt is recommended to set this folder on an hard drive with very good performances in write mode.\nIt is recommended to have a dedicated partition disk to avoid freezing Motu if the hard drive is full.\nBy default value is $MOTU_HOME/data/public/download, this folder can be a symbolic link to another folder.  \nString with format ${var} will be substituted with Java property variables. @See System.getProperty(var)  \n\n##### <a name=\"motuConfig-downloadHttpUrl\">downloadHttpUrl</a>\nHttp URL used to download files stored in the \"extractionPath\" described above. It is used to allow users to download the result data files.  \nThis URL is concatenated to the result data file name found in the folder \"extractionPath\".  \nWhen a frontal HTTPd server is used, it is this URL that shall be configured to access to the folder \"extractionPath\".  \nString with format ${var} will be substituted with Java property variables. @See System.getProperty(var)  \n\n##### <a name=\"motuConfig-httpBaseRef\">httpBaseRef</a>  \nHttp URL used to serve files from to the path where archive __motu-web-static-files-X.Y.Z-classifier-buildId.tar.gz__ has been extracted.  \nFor example: \n\n* When __httpBaseRef__ is set to an __URL__, for example __\"http://resources.myocean.eu/motu\"__, this URL serves a folder which contains ./css/motu/motu.css.  \nFor example, it enables to serve the file http://resources.myocean.eu/motu/css/motu/motu.css  \n* When __httpBaseRef__ is set to __\".\"__, it serves static files which are included by default in Motu application\n* When __httpBaseRef__ is __removed__ (not just empty but attribute is removed), it serves a path accessible from URL $motuIP/${motuContext}/motu\n\n__IMPORTANT__: When Motu URL starts with \"HTTPS\", if you set an URL in __httpBaseRef__, this URL has also to start with \"HTTPS\". On the contrary, \nwhen Motu URL starts with \"HTTP\", if you set an URL in __httpBaseRef__, this URL can start with \"HTTP\" or \"HTTPS\".\n\n        \n##### <a name=\"#BScleanExtractionFileInterval\">cleanExtractionFileInterval</a>\nIn minutes, oldest result files from extraction request which are stored in the folder set by [extractionpath](#extractionpath) are deleted. This check is done each \"runCleanInterval\" minutes.    \nDefault = 60min\n\n##### <a name=\"BScleanRequestInterval\">cleanRequestInterval</a>  \nIn minutes, oldest status (visible in [debug](#ExploitDebug) view) than this time are removed from Motu. This check is done each \"runCleanInterval\" minutes.  \nDefault = 60min\n\n##### <a name=\"#BSrunCleanInterval\">runCleanInterval</a>\nIn minutes, the waiting time between each clean process. The first clean work is triggered when Motu starts.  \nA clean process does:  \n\n* delete files inside java.io.tmpdir\n* delete all files found in extractionFolder bigger than extractionFileCacheSize is Mb\n* delete all files found in extractionFolder oldest than cleanExtractionFileInterval minutes\n* remove all status oldest than [cleanRequestInterval](#BScleanRequestInterval) minutes\n\nDefault = 1min\n\n##### <a name=\"BSmotuConfigExtractionFilePatterns\">extractionFilePatterns</a>  \nPatterns (as regular expression) that match extraction file name to delete in folders:\n\n* java.io.tmpdir\n* extractionPath\n\nDefault is \".*\\.nc$|.*\\.zip$|.*\\.tar$|.*\\.gz$|.*\\.extract$\"  \n\n\n##### extractionFileCacheSize\nSize in Mbytes.  \nA clean job runs each \"runCleanInterval\". All files with a size higher than this value are deleted by this job.\nIf value is zero, files are not deleted.  \nDefault value = 0.\n\n##### <a name=\"describeProductCacheRefreshInMilliSec\">describeProductCacheRefreshInMilliSec</a>\nProvide the delay to wait to refresh the meta-data of products cache after the last refresh.  \nMotu has a cache which is refreshed asynchronously. Cache is first refreshed as soon as Motu starts.   \nThen Motu waits for this delay before refreshing again the cache.  \nThis delay is provided in millisecond.  \nThe default value is 60000 meaning 1 minute.  \n  \nLogbook file (motu/log/logbook.log) gives details about time taken to refresh cache, for example:   \n```  \nINFO  CatalogAndProductCacheRefreshThread.runProcess Product and catalog caches refreshed in 2min 19sec 75msec  \n```\nLogbook file gives details per config service ($configServiceId) about dedicated time taken to refresh cache, for example:   \n```  \nINFO  CatalogAndProductCacheRefreshThread.runProcess Refreshed statistics: $configServiceId@Index=0min 34sec 180msec, $configServiceId@Index=0min 31sec 46msec, ...   \n```  \nThey are sorted by config service which has taken the most time first.  \n@Index All config services are refreshed sequentially. This index is the sequence number for which this cached has been refreshed.\n  \nExample of archived data with several TB of data. Cache is refreshed daily: describeProductCacheRefreshInMilliSec=86400000   \nExample of real time data with several GB of data. Cache is refreshed each minute: describeProductCacheRefreshInMilliSec=60000    \n\n##### runGCInterval\n@Deprecated from v3 This parameter is not used. \n\n##### httpDocumentRoot\n@Deprecated from v3 This parameter is not used. \nDocument root of the servlet server.   \n\n##### wcsDcpUrl \nOptional attribute. Used to set the tag value \"DCP\" in the response of the [WCS GetCapabilities](#GetCapabilities) request with a full URL.\nThe WCS DCP URL value is define using the following priority order:\n\t- The value of this parameter defines on the motuConfiguration.xml file. The value can be directly the URL to use or the name of a java property define between {} which contains the value of the URL.\n\t- The java property \"wcs-dcp-url\" value\n\t- The URL of the web server on which Motu webapps is deployed \nThis attribute can be set when you use a frontal web server to serve the WCS requests, e.g. http://myFrontalWebServer/motu/wcs and your frontal is an HTTP proxy to http://motuWebServer/motu-web/wcs.  \n        \n##### useAuthentication\n@Deprecated from v3 This parameter is not used. It is redundant with parameter config/motu.properties#cas-activated.\n\n\n##### defaultActionIsListServices\n@Deprecated from v3 This parameter is not used.  \n\n##### Configure the Proxy settings  \n@Deprecated from v3 This parameter is not used.\nTo use a proxy in order to access to a Threads, use the [JVM properties](#ConfigurationSystem), for example:  \n\n```  \ntomcat-motu-jvm-javaOpts=-server -Xmx4096M  ... -Dhttp.proxyHost=monProxy.host.fr -Dhttp.proxyPort=XXXX -Dhttp.nonProxyHosts='localhost|127.0.0.1'\n```  \n\n\n* __useProxy__  \n* __proxyHost__  \n* __proxyPort__  \n* __proxyLogin__  \n* __proxyPwd__ \n\n\n##### <a name=\"refreshCacheToken\">refreshCacheToken</a>   \n\nThis token is a key value which is checked to authorize the execution of the cache refresh when it is request by the administrator .\nIf the token value provided by the administrator doesn't match the configured token value, the refresh is not executed and an error is returned.\nA default value \"a7de6d69afa2111e7fa7038a0e89f7e2\" is configured but it's hardly recommended to change this value. If this token is not changed, it is a security breach and \na log ERROR will be written while the configuration will be loaded.\nThe value can contains the characters [A-Za-z] and specials listed here ( -_@$*!:;.,?()[] )\nIt's recommended to configure a token with a length of 29 characters minimum.\n\n##### downloadFileNameFormat  \nFormat of the file name result of a download request.  \n2 dynamic parameters can be used to configure this attribute:  \n\n* __@@requestId@@__: this pattern will be replaced in the final file name by the id of the request.  \n* __@@productId@@__: this pattern will be replaced in the final file name by the id of the requested product.  \n\nIf this attribute is not present, default value is: \"@@productId@@_@@requestId@@.nc\"\n\n##### motuConfigReload  \nConfigure how motu configuration is reloaded.  \nArguments are only 'inotify' or an 'integer in seconds'. 'inotify' is the default value.  \n* __'inotify'__: reload as soon as the file is updated (works only on local file system, not for NFS file system).  \n* __'integer in seconds'__: reload each X second the configuration in 'polling' mode. If this integer is equals or lower than 0, it disables the refresh of the configuration.  \n\n\n#### <a name=\"BSconfigService\">Attributes defined in configService node</a>  \n\n##### <a name=\"BSconfigServiceName\">name</a>  \nString to set the config service name\nIf the value of this attribute contains some special caracters, those caracters have not to be encoded.\nFor example, if the value is an URL, the caracters \":\" and \"/\" have not to be encoded like \"%2E\" or \"%3A\".\n\n##### group\nString which describes the group\n\n##### description\nString which describes the service\n\n##### profiles\nOptional string containing one value, several values separated by a comma or empty (meaning everybody can access).  \nUsed to manage access right from a SSO cas server.  \nIn the frame of CMEMS, three profiles exist:  \n\n* internal: internal users of the CMEMS project  \n* major: major accounts  \n* external: external users  \n\nOtherwise, it's possible to configure as many profiles as needed.  \nProfiles are configured in LDAP within the attribute \"memberUid\" of each user. This attribute is read by CAS and is sent to Motu \nonce a user is logged in, in order to check if it matches profiles configured in Motu to allow a user accessing the data.  \nIn LDAP, \"memberUid\" attribute can be empty, contains one value or several values separated by a comma.  \n\n##### veloTemplatePrefix\nOptional, string used to target the default velocity template. It is used to set a specific theme.  \nValue is the velocity template file name without the extension.  \nDefault value is \"index\".\n\n##### <a name=\"refreshCacheAutomaticallyEnabled\">refreshCacheAutomaticallyEnabled</a>\nOptional, boolean used to determine if the current config service have its cache updated automatically by Motu or not.\nDefault value is \"true\". \n\"true\" means that the config service cache update is executed automatically by Motu.\n\n##### httpBaseRef\nOptional, used to override [motuConfig httpBaseRef](#motuConfig-httpBaseRef) attribute for this specific service.\n\n##### defaultLanguage\n@Deprecated from v3 This parameter is not used.\n\n\n#### Attributes defined in catalog node\n\n##### <a name=\"BSconfigServiceDatasetName\">name</a>  \nThis catalog name refers a TDS catalog name available from the URL: http://$ip:$port/thredds/m_HR_MOD.xml\nExample: m_HR_OBS.xml \n\n##### <a name=\"BSconfigServiceDatasetType\">type</a>    \n* tds: Dataset is downloaded from TDS server. In this case, you can use [Opendap or NCSS protocol](#BSmotuConfigNCSS).\n* file: Dataset is downloaded from DGF\n\nExample: tds\n\n##### <a name=\"BSmotuConfigNCSS\">ncss</a>  \nOptional parameter used to enable or disable the use of NetCDF Subset Service (NCSS) in order to request the TDS server.\nncss must be enabled only with regular grid. The datasets using curvilinear coordinates (like ORCA grid) can not be published with ncss. Thus, ncss option must be set to disable or empty.\nWithout this attribute or when empty, Motu connects to TDS with Opendap protocol. If this attribute is set to \"enabled\" Motu connects to TDS with NCSS protocol in order to improve performance.   \nWe recommend to use \"enabled\" for regular grid datasets. \nValues are: \"enabled\", \"disabled\" or empty.\n\n##### urlSite\n* TDS URL  \nFor example: http://$ip:$port/thredds/  \n\n* DGF URL  \nFor example: file:///opt/publication/inventories\n\n#### <a name=\"BSqueueServerConfig\">Attributes defined in queueServerConfig node</a>  \n\n##### maxPoolAnonymous\nMaximum number of request that an anonymous user can send to Motu before throwing an error message.  \nValue of -1 means no check is done so an unlimited number of user can request the server.  \nDefault value is 10  \nIn case where an SSO server is used for authentication, this parameter is not used. In this you you will be able to fix a limit by setting \"maxPoolAuth\" parameter value.  \n\n##### maxPoolAuth\nMaximum number of request that an authenticated user can send to Motu before throwing an error message.  \nValue of -1 means no check is done so an unlimited number of user can request the server.  \nDefault value is 1\nIn case where no SSO server is used for authentication, this parameter is not used. In this you you will be able to fix a limit by setting \"maxPoolAnonymous\" parameter value.  \n\n##### defaultPriority\n@Deprecated from v3 This parameter is not used.\n\n\n#### Attributes defined in queues\n##### id\nAn id to identify the queue.\n\n##### description\nDescription of the queue.\n\n##### batch\n@Deprecated from v3 This parameter is not used.\n\n##### Child node: maxThreads\nUse to build a java.util.concurrent.ThreadPoolExecutor an to set \"corePoolSize\" and \"maximumPoolSize\" values.  \nDefault value is 1  \nThe total number of threads should not be up to the total number of core of the processor on which Motu is running.  \n\n##### Child node: maxPoolSize\nRequest are put in a queue before being executed by the ThreadPoolExecutor. Before being put in the queue, the queue size\nis checked. If it is upper than this value maxPoolSize, an error message is returned.\nValue of -1 means no check is done.  \nDefault value is -1\n\n\n##### Child node: dataThreshold\nSize in Megabyte. A request has a size. The queue in which this request will be processed is defined by the request size.\nAll queues are sorted by size ascending. A request is put in the last queue which has a size lower than the request size.\nIf the request size if higher than the bigger queue dataThreshold, request is not treated and an error message is returned.  \nThis parameter is really useful when a Motu is used to server several kind of file size and you want to be sure that file with a specific size does no slow down request of small data size.  \nIn this case you can configure two queues and set a number of threads for each in order to match the number of processors. The JVM, even if requests for high volume are running, will be able to\nprocess smallest requests by running the thread on the other processor core. Sp processing high volume requests will not block the smallest requests.  \n\n\n##### Child node: lowPriorityWaiting\n@Deprecated from v3 This parameter is not used.\n\n#### <a name=\"RedisServerConfig\">Attributes defined in redisConfig node</a>  \nThis optional node is used to run Motu in a [scalable architecture](#ArchitectureScalability). Do not add this node when you just run one single Motu instance.  \nOnce this node is added, Motu stores all its request ids and status in Redis.  \n\n##### host\nDefine the host (ip or server name) where is deployed the Redis server od Redis cluster used by Motu to share the RequestId and RequestStatus data.\nDefault value is localhost\n\n##### port\nDefine the port used by the Redis server or Redis cluster used by Motu to share the requestId and RequestStatus data.\nDefault value is 6379  \n\n##### prefix\nDefine the prefix used to build the RequestId value of the shared RequestStatus data.\nDefault value is requestStatus\n\n##### isRedisCluster \nDefine if the redis server in in cluster mode.\nThis is a boolean value.\nBy default is set to false and the cluster mode is not activate.\nTo activate the cluster, the value have to be set on true.\n\n## <a name=\"ConfigurationSystem\">System settings</a>  \n\n### motu.properties: Motu system settings  \n\nSystem settings are configured in file config/motu.properties  \nAll parameters can be updated in the file.  \n\n* [Java options](#ConfigurationSystemJavaOptions)\n* [Tomcat network ports](#ConfigurationSystemTomcatNetworkPorts)\n* [CAS SSO server](#ConfigurationSystemCASSSO)\n\n#### <a name=\"ConfigurationSystemJavaOptions\">Java options</a>\nThe three parameters below are used to tune the Java Virtual Machine, and the __tomcat-motu-jvm-javaOpts__ parameter can include any Java property in the form \"-D\\<java property name\\>=\\<value\\>\":  \n   &#35; -server: tells the Hostspot compiler to run the JVM in \"server\" mode (for performance)  \n__tomcat-motu-jvm-javaOpts__=-server -Xmx4096M -Xms512M -XX:MetaspaceSize=128M -XX:MaxMetaspaceSize=512M  \n__tomcat-motu-jvm-port-jmx__=9010  \n__tomcat-motu-jvm-address-debug__=9090  \n__tomcat-motu-jvm-umask__=tomcat|umask|0000 [(More details...)](#ConfigurationSystemTomcatUmask)\n\n##### <a name=\"ConfigurationSystemTomcatUmask\">Tomcat umask</a>\nBy default, if tomcat-motu-jvm-umask is not set, motu sets the umask with result of the command `umask`  \n__tomcat-motu-jvm-umask__=umask|tomcat|0000  \n* __umask__:  By default, if tomcat-motu-jvm-umask is not set, motu sets the umask with result of the command `umask`  \n* __tomcat__: Apache Tomcat process forces umask to 0027 (https://tomcat.apache.org/tomcat-8.5-doc/security-howto.html)  \n* __0000__:   Custom umask value  \nValues 0002 or umask are recommended if Motu download results are served by a frontal web server\n\n##### <a name=\"TdsHttpSoTimeout\">Java property tds.http.sotimeout</a>\nBy default this parameter is at \"300\". It represents the maximum delay in seconds for TDS to answer a MOTU request (reading timeout of the socket).  \nFor queries involving lots of files, TDS might need more than the default 5 minutes to answer, and to avoid the error \"004-27 : Error in NetcdfWriter finish\", this parameter can be set to a higher value in:  \n__tomcat-motu-jvm-javaOpts__=-server [...] -XX:MaxPermSize=512M -Dtds.http.sotimeout=4000\n\n##### <a name=\"TdsHttpConnTimeout\">Java property tds.http.conntimeout</a>\nBy default this parameter is at \"60\". It represents the maximum delay in seconds for TDS to accept a MOTU request (connection timeout on the socket).  \nThe paramater can be customized and added in:  \n__tomcat-motu-jvm-javaOpts__=-server [...] -XX:MaxPermSize=512M -Dtds.http.conntimeout=100\n\n#### <a name=\"ConfigurationSystemTomcatNetworkPorts\">Tomcat network ports</a>\nThe parameters below are used to set the different network ports used by Apache Tomcat.  \nAt startup, these ports are set in the file \"$installdir/motu/tomcat-motu/conf/server.xml\".    \nBut if this file already exist, it won't be replaced. So in order to apply these parameters, remove the file \"$installdir/motu/tomcat-motu/conf/server.xml\".  \n  \n__tomcat-motu-port-http__=9080  \n  &#35; HTTPs is in a common way managed from a frontal Apache HTTPd server. If you really need to use it from Tomcat, you have to tune the SSL certificates and the protocols directly in the file \"$installdir/motu/tomcat-motu/conf/server.xml\".  \n__tomcat-motu-port-https__=9443  \n__tomcat-motu-port-ajp__=9009  \n__tomcat-motu-port-shutdown__=9005  \n\n#### <a name=\"ConfigurationSystemCASSSO\">CAS SSO server</a>\n\n   &#35;  true or false to enable the SSO connection to a CAS server  \n__cas-activated__=false  \n  \n   &#35;  Cas server configuration to allow Motu to access it  \n   &#35;  @see https://wiki.jasig.org/display/casc/configuring+the+jasig+cas+client+for+java+in+the+web.xml  \n     \n   &#35;  The  start of the CAS server URL, i.e. https://cas-cis.cls.fr/cas  \n__cas-server-url__=https://cas-cis.cls.fr/cas   \n\n   &#35;  The Motu HTTP server URL, for example: http://misgw-ddo-qt.cls.fr:9080 or http://motu.cls.fr   \n   &#35;  If you use a frontal HTTPd server, you have to known if its URL will be called once the user will be login on CAS server.  \n   &#35;  In this case, set the Apache HTTPd server. The value will be http://$apacheHTTPdServer/motu-web/Motu So, in Apache HTTPd, you have to redirect this URL to the Motu Web server  \n__cas-auth-serverName__=http://$motuServerIp:$motuServerPort   \n\n   &#35;  The proxy callback HTTPs URL of the Motu server ($motuServerIp is either the Motu host or the frontal Apache HTTPs host ip or name. $motuServerHttpsPort is optional if default HTTPs port 443 is used, otherwise it is the same value as defined above with the key \"tomcat-motu-port-https\", or it is the port defined for the HTTPs server on the frontal Apache HTTPd)  \n__cas-validationFilter-proxyCallbackUrl__=https://$motuServerIp:$motuServerHttpsPort/motu-web/proxyCallback  \n  \n  \n__IMPORTANT__: Motu uses a Java HTTPs client to communicate with the CAS server. When the CAS server has an untrusted SSL certificate, you have to add it to Java default certificates or to add the Java property named \"javax.net.ssl.trustStore\" to target a CA keystore which contains the CAS Server SSL CA public key.\nFor example, add this property by setting Java option [tomcat-motu-jvm-javaOpts](#ConfigurationSystem):  \n```\ntomcat-motu-jvm-javaOpts=-server -Xmx4096M -Xms512M -XX:MetaspaceSize=128M -XX:MaxMetaspaceSize=512M -Djavax.net.ssl.trustStore=/opt/cmems-cis/motu/config/security/cacerts-with-cas-qt-ca.jks\n```\n\nThe following part is not relevant in the CMEMS context as the SSO CAS server has been signed by a known certification authority.  \nIf you need to run tests with your own SSO CAS server without any certificate signed by a known certification authority, you have to follow the following steps.  \n\nHow to build the file cacerts-with-cas-qt-ca.jks on Motu server?  \n\n* Download the certificate file (for example \"ca.crt\") of the authority which has signed the CAS SSO certificate on the CAS server machine (/opt/atoll/ssl/ca.crt) and copy it to \"${MOTU_HOME}/config/security/\", then rename it \"cas-qt-ca.crt\"\n* Copy the default Java cacerts \"/opt/cmems-cis-validation/motu/products/jdk1.7.0_79/jre/lib/security/cacerts\" file into ${MOTU_HOME}/config/security/\n  and rename this file to \"cacerts-with-cas-qt-ca.jks\"  \n  ```\n  cp /opt/cmems-cis-validation/motu/products/jdk1.7.0_79/jre/lib/security/cacerts /opt/cmems-cis/motu/config/security/  \n  mv /opt/cmems-cis/motu/config/security/cacerts /opt/cmems-cis/motu/config/security/cacerts-with-cas-qt-ca.jks  \n  ```\n* Then import \"cas-qt-ca.crt\" inside \"cacerts-with-cas-qt-ca.jks\", Trust the certificate=yes  \n  ```\n  /opt/cmems-cis-validation/motu/products/jdk1.7.0_79/bin/keytool -import -v -trustcacerts -alias $CAS_HOST_NAME -file cas-qt-ca.crt -keystore cacerts-with-cas-qt-ca.jks -keypass XXX  \n  ```  \n\n#### <a name=\"ConfigStandardNames\">NetCdf standard names</a>  \nWhen NetCdf variables are read in data files, either by Threads or directly by Motu, Motu wait for a standard name metadata sttribute to be found for each variable as requiered by the [CF convention](#http://cfconventions.org/Data/cf-standard-names/docs/guidelines.html).\nDue to any production constraints, some netcdf files does not have any standard_name attribute.  \nIn the case, you can add directly in the configuration folder, a file named standardNames.xml in order to map a standard_name to a netcdf variable name.  \nYou can find an example in Motu source: /motu-web/src/main/resources/standardNames.xml  \n\n#### Supervision\nTo enable the status supervision, set the parameter below:  \n__tomcat-motu-urlrewrite-statusEnabledOnHosts__=localhost,*.cls.fr\n\nThis parameter is used to set the property below in the WEB.XML file:  \n```\n        <!-- Documentation from http://tuckey.org/urlrewrite/manual/3.0/\n        you may want to allow more hosts to look at the status page\n        statusEnabledOnHosts is a comma delimited list of hosts, * can\n        be used as a wildcard (defaults to \"localhost, local, 127.0.0.1\") -->\n        <init-param>  \n            <param-name>statusEnabledOnHosts</param-name>  \n            <param-value>${tomcat-motu-urlrewrite-statusEnabledOnHosts}</param-value>  \n        </init-param>  \n```  \n\nFor more detail read:  \norg.tuckey UrlRewriteFilter FILTERS : see http://tuckey.org/urlrewrite/manual/3.0/  \n\n  \n## <a name=\"LogSettings\">Log settings</a>  \n\nLog are configured by using log4j2 in file config/log4j.xml  \n\n### Motu queue server logs: motuQSlog.xml, motuQSlog.csv\n\nThis log files are used to compute statistics about Motu server usage.  \nTwo format are managed by this log, either XML or CSV.  \nTo configure it, edit config/log4j.xml  \n\n##### Log format: XML or CSV  \nUpdate the fileFormat attribute of the node \"MotuCustomLayout\": <MotuCustomLayout fileFormat=\"xml\">\nA string either \"xml\" or \"csv\" to select the format in which log message are written.  \nAlso update the log file name extension of the attributes \"fileName\" and \"filePattern\" in order to get a coherent content in relationship with value set for MotuCustomLayout file format.  \nIf this attribute is not set, the default format is \"xml\".  \n``` \n        <RollingFile name=\"log-file-infos.queue\"   \n            fileName=\"${sys:motu-log-dir}/motuQSlog.xml\"   \n            filePattern=\"${sys:motu-log-dir}/motuQSlog.xml.%d{MM-yyyy}\"    \n            append=\"true\">   \n            <!-- fileFormat=xml or csv -->  \n            <MotuCustomLayout fileFormat=\"xml\" />  \n            <Policies>  \n                <TimeBasedTriggeringPolicy interval=\"1\" modulate=\"true\"/>  \n            </Policies>  \n        </RollingFile>  \n``` \n\n##### Log path\nIn the dissemination unit, Motu shares its log files with a central server.  \nLog files have to be save on a public access folder.  \nSet absolute path in \"fileName\" and \"filePattern\" attributes. This path shall be serve by the frontal Apache HTTPd or Apache Tomcat.\n  \nFor example, if you want to share account transaction log files, you edit config/log4j.xml. \nUpdate content below:  \n``` \n<RollingFile name=\"log-file-infos.queue\" fileName=\"${sys:motu-log-dir}/motuQSlog.xml\"\n            filePattern=\"${sys:motu-log-dir}/motuQSlog.xml.%d{MM-yyyy}\"\n```   \n with:  \n``` \n<RollingFile name=\"log-file-infos.queue\" fileName=\"/opt/cmems-cis/motu/data/public/transaction/motuQSlog.xml\"\n            filePattern=\"/opt/cmems-cis/motu/data/public/transaction/motuQSlog.xml.%d{MM-yyyy}\"\n```   \nNote that both attributes __fileName__ and __filePattern__ have been updated.  \nThen the frontal [Apache HTTPd server](#InstallFrontal) has to serve this folder.\n\n\n\n## <a name=\"ThemeStyle\">Theme and Style</a>  \nIn Motu you can update the theme of the website. There is 2 mains things in order to understand how it works?  \n\n* [Template] velocity: The velocity templates are used to generated HTML pages from Java objects.  \n* [Style] CSS, Images and JS: These files are used to control style and behaviour of the web UI.\n\nBy default, the template and style are integrated in the \"war\". But the Motu design enable to customize it easily.\n\n* [Template] velocity: You can change all templates defined in:\nmotu/tomcat-motu/webapps/motu-web/WEB-INF/lib/motu-web-2.6.00-SNAPSHOT.jar/velocityTemplates/*.vm\nby defining them in motu/config/velocityTemplates.\n\nThe main HTML web page structure is defined by the index.vm velocity template. For example, in you create a file motu/config/velocityTemplates/index.vm containing an empty html page, website will render empty web pages.  \n\"index.vm\" is the default theme. The name can be updated for each motuConfig#configService by setting veloTemplatePrefix=\"\".\nBy default veloTemplatePrefix=\"index\".\n\n\n* [Style] CSS, Images and JS: Those files are integrated with the default theme motu-web-2.6.00-SNAPSHOT.war/css/*, motu-web-2.6.00-SNAPSHOT.war/js/*. These files can be downloaded from an external server which enable to benefit to several mMotu server at he same time. The external server name can be updated for each motuConfig#configService by setting httpBaseRef=\"\".  \nBy default httpBaseRef search static files from the Motu web server, for example:  \n``` \nservice.getHttpBaseRef()/css/motu/screen/images/favicon.ico\"\n``` \n\n\n\n\n\n# <a name=\"Operation\">Operation</a>    \n\n## <a name=\"SS\">Start, Stop and other Motu commands</a>    \nAll operations are done from the Motu installation folder.  \nFor example:  \n``` \ncd /opt/cmems-cis/motu \n```\n\n### Start Motu\nStart the Motu process.  \n``` \n./motu start  \n```\n\n### Stop Motu  \nAt the shutdown of Motu, the server waits for none of the pending or in progress request to be in execution.  \nIf it's the case, the server waits the end of the request before shutdown.  \nNote that after waiting 10 minutes, server will automatically shutdown without waiting any running requests.  \nSo command below can respond quickly if no requests are in the queue server or takes time to process them.  \n``` \n./motu stop\n``` \n\nIf you needs to understand what Motu is waiting for, you can check the logbook:  \n``` \ntail -f log/logbook.log  \nStop in progress...  \nStop: Pending=0; InProgress=2  \nStop: Pending=0; InProgress=2  \n...  \nStop: Pending=0; InProgress=1  \nStop: Pending=0; InProgress=0  \n...  \nStop done  \n``` \n\nDuring the stop step, from a web browser, the user will be able to ends its download request if a front web server (Apache HTTPd) serves the statics files and the downloaded product.\nIn case where Motu is installed as a standalone web server, user will get a 500 HTTP error. For example in development or qualification environment, \nthis could lead to block the download of the files if Motu is used to serve both static and requested product files.\n\n\n### Advanced commands\n#### Restart Motu\n``` \n./motu restart\n``` \n\n#### Status of the Motu process\n``` \n./motu status\n``` \n\nStatus are the following: \n \n* __tomcat-motu started__ A pid file exists\n* __tomcat-motu stopped__ No pid file exists\n\n#### Help about Motu parameters\n``` \n./motu ?\n``` \n\n## <a name=\"ExpMonitorPerf\">Monitor performance</a> \n\nOnce started, you can use the Linux command \"top\" to check performance:  \n\n* __load average__ the three numbers shall be low and under the number of CPU (lscpu | grep Proc). For example if you have 4 processors this indicator can rise up to 4 but not above. If it is above, you have to add more CPU power.\n* __%CpuX, parameter wa__ This indicator shall be near 0 to indicate that processes does not wait to access to the disks. When this number is above 0.5 you have to improve access disk performance.\n* __KiB Mem__ Be sure that free memory is available. If it is less than 5000000, meaning less than 5GB, you have to add RAM memory in order to manage pic load. \n\nExample of top command:  \n\n``` \ntop - 11:07:01 up 19:46,  3 users,  ***load average: 0,05, 0,09, 0,25***  \nTasks: 395 total,   2 running, 393 sleeping,   0 stopped,   0 zombie  \n%Cpu0  :  1,0 us,  1,0 sy,  0,0 ni, 98,1 id,   ***0,0 wa***,  0,0 hi,  0,0 si,  0,0 st  \n%Cpu1  :  1,0 us,  0,0 sy,  0,0 ni, 99,0 id,  ***0,0 wa***,  0,0 hi,  0,0 si,  0,0 st  \nKiB Mem : 10224968 total,  ***4034876 free***,  3334576 used,  2855516 buff/cache    \n...\n``` \n\n  \n## <a name=\"Logbooks\">Logbooks</a>    \n\nLog messages are generated by Apache Log4j 2. The configuration file is \"config/log4j.xml\".  \nBy default, log files are created in the folder $MOTU_HOME/log. This folder contains Motu log messages.  \nTomcat log messages are generated in the tomcat-motu/logs folder.  \n\n* __Motu log messages__\n  * __logbook.log__: All Motu log messages including WARN and ERROR(without stacktrace) messages.\n  * __warnings.log__: Only Motu log messages with a WARN level\n  * <a name=\"LogbooksErrors\">__errors.log__</a>: Only Motu log messages with an ERROR level. When this file is not empty, it means that at least an error has been generated by the Motu application.\n  * <a name=\"LogbooksTransactions\">__motuQSlog.xml__, __motuQSlog.csv__</a>: Either a \"CSV\" or \"XML\" format which logs all queue events.\n     * CSV: On one unique line, writes:  \n    [OK | ERR;ErrCode;ErrMsg;ErrDate];  \n    queueId;queueDesc;\n    requestId;  \n    elapsedWaitQueueTime;elapsedRunTime;elapsedTotalTime;totalIOTime;preparingTime;readingTime;  \n    inQueueTime;startTime;endTime;  \n    amountDataSize;  \n    downloadUrlPath;extractLocationData;  \n    serviceName;TemporalCoverageInDays;ProductId;UserId;UserHost;isAnonymousUser;  \n    variable1;variable2;...;variableN;  \n    temporalMin,temporalMax;  \n    LatitudeMin;LongitudeMin;LatitudeMax;LongitudeMax:\n    DepthMin;DepthMax;\n     * XML: XStream is used to serialized a Java Object to XML from fr.cls.atoll.motu.web.bll.request.queueserver.queue.log.QueueLogInfo  \n     Same data are represented.\n     * Field details\n         * queueId, queueDesc: Queue used to process the request. Id and description found in config/motuConfiguration.xml\n         * requestId: A timestamp representing the request id.\n         * inQueueTime: Timestamp with format \"yyyy-MM-dd' 'HH:mm:ss.SSS\" when the request has been put in the queue\n         * startTime: Timestamp with format \"yyyy-MM-dd' 'HH:mm:ss.SSS\" when the request has been started to be processed\n         * endTime: Timestamp with format \"yyyy-MM-dd' 'HH:mm:ss.SSS\" when the request has been ended to be processed\n         * elapsedWaitQueueTime: Duration in milliseconds, [startTime - inQueueTime]\n         * elapsedRunTime: Duration in milliseconds, [endTime - startTime]\n         * elapsedTotalTime: Duration in milliseconds, [endTime - inQueueTime]\n         * totalIOTime: Duration in nanoseconds: reading + writing + copying + compressing times.\n         * readingTime: Duration in nanoseconds.\n         * writingTime: Duration in nanoseconds.\n         * preparingTime: Duration in nanoseconds, same value as reading time.\n         * copyingTime: Duration in nanoseconds, only set in DGF mode.\n         * compressingTime: Duration in nanoseconds, only set in DGF mode.\n         * amountDataSize: Size in MegaBytes\n         * downloadUrlPath: URL to download the product\n         * extractLocationData: Absolute path on the server\n         * serviceName: The service name found in the configuration file motuConfiguration.xml\n         * TemporalCoverageInDays: duration in days\n         * ProductId: Product id\n         * UserId: User login if user is not anonymous, otherwise its host or IP address from which he is connected\n         * UserHost: Host or ip address from which user is connected\n         * isAnonymousUser: true or false                \n         * variable1;variable2;...;variableN; Extracted variable names\n         * temporalMin,temporalMax: Temporal coverage\n         * LatitudeMin;LongitudeMin;LatitudeMax;LongitudeMax: Geographical coverage (latitude:-90;+90; longitude:180;+180)\n         * DepthMin;DepthMax;: Depth coverage   \n  * __velocity.log__: Logs generated by the http://velocity.apache.org/ technology to render HTML web pages.\n\n* __Tomcat log messages__: This folder contains all Apache Tomcat log files. The file below is important to check startup logs:  \n  * __catalina.out__: Catalina output matching the environment variable CATALINA_OUT.\n    \n\n## <a name=\"AdminDataSetAdd\">Add a dataset</a>    \nIn order to add a new Dataset, you have to add a new configService node in the [Motu business configuration](#ConfigurationBusiness).  \nWhen Motu read data through TDS (Opendap or NCSS service) url, the data shall be configured in TDS before this configuration is saved in Motu. The [TDS configuration](https://www.unidata.ucar.edu/software/thredds/v4.6/tds/catalog/index.html) is not explained here.  \n\nWithin CMEMS, the datasets are organized in a tree structure, where the product granularity appears above the dataset granularity.  \nTo be noticed:  \n\n* All gridded dataset shall be configured in TDS, to be served through the subsetter of Motu  \n* A product is a coherent group of datasets. The product is the granularity used in the catalogue of CMEMS  \n* In the XML tree structure of the TDS configuration, each product shall be configured through a unique node  \n* This node shall correspond to one XML file in the TDS configuration (for example GLOBAL_ANALYSIS_PHYS_001_016.xml) and shall be further referenced in the motuConfiguration.xml file as one <catalog name> (for example <catalog name=\" GLOBAL_ANALYSIS_PHYS_001_016.xml>)  \n* The value of the \"name\" attribute of the element <dataset> shall be identical to the Product Name (from CMEMS Product Information Table). \nIn the example below named \u201cCMEMS DU xxx Thredds Catalog\u201d there are three datasets. The following catalog tree presents a hierarchical organization for this catalog.\n      \n```       \n<  CMEMS DU xxx Thredds Catalog >  \n| ------ < GLOBAL_ANALYSIS_PHYS_001_016  >   \n|------- < dataset-armor-3d-v5-myocean >  \n|----------------- < GLOBAL_REP_PHYS_001_013  >   \n|------- < dataset-armor-3d-rep-monthly-v3-1-myocean >                                                                     \n|------- < dataset-armor-3d-rep-weekly-v3-1-myocean>   \n``` \n\nThe Motu configuration (motuConfiguration.xml) should reference the node corresponding to one XML file in the TDS configuration.\n\n  \nExamples:  \n\n* __TDS NCSS protocol__:  \nThis is the fastest protocol implemented by Motu. Motu select this protocol because type is set to \"tds\" and ncss is set to \"enabled\".  \n\n``` \n<configService description=\"Free text to describe your dataSet\" group=\"HR-Sample\" httpBaseRef=\"\" name=\"HR_MOD-TDS\" veloTemplatePrefix=\"\" profiles=\"external\">  \n        <catalog name=\"m_HR_MOD.xml\" type=\"tds\" ncss=\"enabled\" urlSite=\"http://$tdsUrl/thredds/\"/>  \n</configService>  \n```  \n  \n* __TDS Opendap protocol__:  \nHere OpenDap is used because it is the default protocol when tds type is set and ncss is not set or is disable.  \n\n``` \n<configService description=\"Free text to describe your dataSet\" group=\"HR-Sample\" httpBaseRef=\"\" name=\"HR_MOD-TDS\" veloTemplatePrefix=\"\" profiles=\"external\">  \n        <catalog name=\"m_HR_MOD.xml\" type=\"tds\" ncss=\"\" urlSite=\"http://$tdsUrl/thredds/\"/>  \n</configService>  \n```  \n\n* __DGF protocol__:   \nThis protocol is used to access to local files. With this protocol user download the full data source file and can run only temporal extractions on the dataset (As a reminder, a dataset is temporal aggregation of several datasource files.    \n\n```\n<configService description=\"Free text to describe your dataSet\" group=\"HR-Sample\" profiles=\"internal, external, major\" httpBaseRef=\"\" name=\"HR_MOD-TDS\" veloTemplatePrefix=\"\">  \n           <catalog name=\"catalogFILE_GLOBAL_ANALYSIS_PHYS_001_016.xml\" type=\"file\" urlSite=\"file:///opt/cmems-cis-data/data/public/inventories\"/>  \n</configService>  \n```\n\nAn an example, the file __catalogFILE_GLOBAL_ANALYSIS_PHYS_001_016.xml__ contains:  \n\n```\n< ?xml version=\"1.0\" encoding=\"UTF-8\"?>  \n<!DOCTYPE rdf:RDF [  \n<!ENTITY atoll \"http://purl.org/cls/atoll/ontology/individual/atoll#\">  \n]>  \n<catalogOLA xmlns=\"http://purl.org/cls/atoll\" name=\"catalog GLOBAL-ANALYSIS-PHYS-001-016\">  \n        <resourcesOLA>  \n                <resourceOLA urn=\"dataset-armor-3d-v5-myocean\" inventoryUrl=\"file:///opt/cmems-cis-data/data/public/inventories/dataset-armor-3d-v5-myocean-cls-toulouse-fr-armor-motu-rest-file.xml\"/>  \n        </resourcesOLA>  \n</catalogOLA>    \n```\n\nFile __dataset-armor-3d-v5-myocean-cls-toulouse-fr-armor-motu-rest-file.xml__:  \n\n```\n< ?xml version=\"1.0\" encoding=\"UTF-8\"?>  \n<!DOCTYPE rdf:RDF [  \n<!ENTITY atoll \"http://purl.org/cls/atoll/ontology/individual/atoll#\">  \n<!ENTITY cf \"http://purl.org/myocean/ontology/vocabulary/cf-standard-name#\">  \n<!ENTITY cu \"http://purl.org/myocean/ontology/vocabulary/cf-unofficial-standard-name#\">  \n<!ENTITY ct \"http://purl.org/myocean/ontology/vocabulary/forecasting#\">  \n<!ENTITY cp \"http://purl.org/myocean/ontology/vocabulary/grid-projection#\">  \n]>  \n<inventory lastModificationDate=\"2016-01-27T00:10:10+00:00\" xmlns=\"http://purl.org/cls/atoll\" updateFrequency=\"P1D\">  \n  <service urn=\"cls-toulouse-fr-armor-motu-rest-file\"/>  \n  <resource urn=\"dataset-armor-3d-v5-myocean\">  \n    <access urlPath=\"file:///data/atoll/armor/armor-3d-v3/\"/>  \n    <geospatialCoverage south=\"-82\" north=\"90\" west=\"0\" east=\"359.75\"/>  \n    <depthCoverage min=\"0\" max=\"5500\" units=\"m\"/>  \n    <timePeriod start=\"2014-10-01T00:00:00+00:00\" end=\"2016-01-26T23:59:59+00:00\"/>  \n    <theoricalTimePeriod start=\"2014-10-01T00:00:00+00:00\" end=\"2016-01-26T23:59:59+00:00\"/>  \n    <variables>  \n      <variable name=\"zvelocity\" vocabularyName=\"http://mmisw.org/ont/cf/parameter/eastward_sea_water_velocity\" units=\"m/s\"/>  \n      <variable name=\"height\" vocabularyName=\"http://purl.org/myocean/ontology/vocabulary/cf-standard-name#height_above_geoid\" units=\"m\"/>  \n      <variable name=\"mvelocity\" vocabularyName=\"http://mmisw.org/ont/cf/parameter/northward_sea_water_velocity\" units=\"m/s\"/>  \n      <variable name=\"salinity\" vocabularyName=\"http://mmisw.org/ont/cf/parameter/sea_water_salinity\" units=\"1e-3\"/>  \n      <variable name=\"temperature\" vocabularyName=\"http://mmisw.org/ont/cf/parameter/sea_water_temperature\" units=\"degC\"/>  \n    </variables>  \n  </resource>  \n  <files>  \n    <file name=\"ARMOR3D_TSHUV_20141001.nc\" weight=\"327424008\" modelPrediction=\"http://www.myocean.eu.org/2009/resource/vocabulary/forecasting#\" startCoverageDate=\"2014-10-01T00:00:00+00:00\" endCoverageDate=\"2014-10-07T23:59:59+00:00\" creationDate=\"2015-03-17T00:00:00+00:00\" availabilitySIDate=\"2016-01-27T00:10:10+00:00\" availabilityServiceDate=\"2016-01-27T00:10:10+00:00\" theoreticalAvailabilityDate=\"2015-03-17T00:00:00+00:00\"/>  \n    <file name=\"ARMOR3D_TSHUV_20141008.nc\" weight=\"327424008\" modelPrediction=\"http://www.myocean.eu.org/2009/resource/vocabulary/forecasting#\" startCoverageDate=\"2014-10-08T00:00:00+00:00\" endCoverageDate=\"2014-10-14T23:59:59+00:00\" creationDate=\"2015-03-17T00:00:00+00:00\" availabilitySIDate=\"2016-01-27T00:10:10+00:00\" availabilityServiceDate=\"2016-01-27T00:10:10+00:00\" theoreticalAvailabilityDate=\"2015-03-17T00:00:00+00:00\"/>  \n    ...  \n    <file name=\"ARMOR3D_TSHUV_20160120.nc\" weight=\"327424008\" modelPrediction=\"http://www.myocean.eu.org/2009/resource/vocabulary/forecasting#\" startCoverageDate=\"2016-01-20T00:00:00+00:00\" endCoverageDate=\"2016-01-26T23:59:59+00:00\" creationDate=\"2016-01-26T11:11:00+00:00\" availabilitySIDate=\"2016-01-27T00:10:10+00:00\" availabilityServiceDate=\"2016-01-27T00:10:10+00:00\" theoreticalAvailabilityDate=\"2016-01-26T11:11:00+00:00\"/>  \n  </files>  \n</inventory>  \n```  \n\n\n## <a name=\"AdminMetadataCache\">Tune the dataset metadata cache</a>  \nIn order to improve response time, Motu uses an in-memory cache which stores datasets metadata. This cache is indexed by config service.    \nYou can tune the cache behaviour in order to manage both real time and archived datasets effectively.      \nAt startup, Motu loads datasets metadata of each configService by turn. Once done, cache is refreshed either periodically in an automatic manner or either when asked by triggering a [specific action](#ClientAPI_RefreshCache).  \nThe cache is kept in memory and all Motu requests are based on it. When a cache refresh is asked, a second cache loads new metadata and when fully loaded, Motu main cache is replaced. So until the full loading, old cache is used in Motu responses.   \n\nFor config services which manages real time datasets, meaning datasets which are daily updated, you can set the following configuration in motuConfiguration.xml:\n```  \n<?xml version=\"1.0\"?>\n<motuConfig  ...\n<configService ... refreshCacheAutomaticallyEnabled=\"true\"\n...\n```  \n\nFor config services which manages archived datasets, meaning dataset which not updated frequently for example only once a week, you can set the following configuration in motuConfiguration.xml:\n```  \n<?xml version=\"1.0\"?>\n<motuConfig  ...\n<configService ... refreshCacheAutomaticallyEnabled=\"false\"\n...\n```  \nIn this case, when you want to refresh metadata cache of these datasets, you can use this dedicated [action](#ClientAPI_RefreshCache).\n\n\n## <a name=\"ExploitDebug\">Debug view</a>  \nFrom a web browser access to the Motu web site with the URL:  \n``` \n/Motu?action=debug  \n``` \n\nYou can see the different requests and their [status](#ClientAPI_Debug).  \nYou change the status order by entering 4 parameters in the URL:  \n``` \n/Motu?action=debug&order=DONE,ERROR,PENDING,INPROGRESS\n``` \n\n\n \n \n## <a name=\"ExploitCleanDisk\">Clean files</a>  \n\n## <a name=\"ExploitCleanDiskLogbook\">Logbook files</a>   \nLogbook files are written by Apache Tomcat server and Motu application.  \n \n### <a name=\"ExploitCleanDiskLogbookTomcat\">Apache Tomcat Logbook files</a>  \nTomcat writes log files in folder tomcat-motu/logs.  \nYou can customize this default configuration by editing tomcat-motu/conf/logging.properties  \nThis file is the default file provided by Apache Tomcat.\nThere is a daily rotation so you can clean those files to fullfill the harddrive.   \ncrontab -e   \n0 * * * * find /opt/cmems-cis/motu/tomcat-motu/logs/*.log* -type f -mmin +14400 -delete >/dev/null 2>&1   \n0 * * * * find /opt/cmems-cis/motu/tomcat-motu/logs/*.txt* -type f -mmin +14400 -delete >/dev/null 2>&1   \n\n### <a name=\"ExploitCleanDiskLogbookMotu\">Motu Logbook files</a>  \nLogbook files are written in the folder(s) configured in the log4j.xml configuration file.  \nAll logs are generated daily except for motuQSLog (xml or csv) which are generated monthly.  \nYou can clean those files to avoid to fullfill the harddrive.   \ncrontab -e   \n0 * * * * find /opt/cmems-cis/motu/log/*.log* -type f -mmin +14400 -delete >/dev/null 2>&1  \n0 * * * * find /opt/cmems-cis/motu/log/*.out* -type f -mmin +14400 -delete >/dev/null 2>&1  \n0 * * * * find /opt/cmems-cis/motu/log/*.xml* -type f -mmin +144000 -delete >/dev/null 2>&1  \n0 * * * * find /opt/cmems-cis/motu/log/*.csv* -type f -mmin +144000 -delete >/dev/null 2>&1  \n  \nNote that Motu is often tuned to write the motuQSLog in a dedicated folder. So you have to clean log files in this folder too. For example:  \n0 * * * * find /opt/cmems-cis/motu/data/public/transaction/*.xml* -type f -mmin +144000 -delete >/dev/null 2>&1  \n0 * * * * find /opt/cmems-cis/motu/data/public/transaction/*.csv* -type f -mmin +144000 -delete >/dev/null 2>&1 \n\n## <a name=\"LogCodeErrors\">Log Errors</a>   \n\n### The code pattern\nThe error codes of Motu as the following format \"XXXX-Y\":\n  \n* [XXXX](#LogCodeErrorsActionCode) code matching the action which is executed when the error is raised. This part is the \"ActionCode\".  The action is in general a HTTP request and matches the following HTTP parameter http://$server/motu-web/Motu?action=.\n* [Y](#LogCodeErrorsErrorType) code which identifies the part of the program from which the error was raised. This part is the \"ErrorType\".\n  \n  \nFor example, the web browser can display:  \n011-1 : A system error happened. Please contact the administrator of the site. \n\nHere, we have the error code in order to understand better what happens. But the end user has a generic message and no detail is given to him. These end user messages are described in the file \"/motu-web/src/main/resources/MessagesError.properties\". The file provided with the project is a default one and can be customized for specific purposes. Just put this file in the \"config\" folder, edit it and restart Motu to take it into account. So when a user has an error, it just have to tell you the error code and you can check the two numbers with the descriptions below.  \n\n\n### <a name=\"LogCodeErrorsActionCode\">Action codes</a>  \n\nThe Action Code        =>    A number matching the HTTP request with the action parameter.\n\n001        =>    UNDETERMINED\\_ACTION           \n002        =>    PING\\_ACTION                   \n003        =>    DEBUG\\_ACTION                  \n004        =>    GET\\_REQUEST\\_STATUS\\_ACTION     \n005        =>    GET\\_SIZE\\_ACTION               \n006        =>    DESCRIBE\\_PRODUCT\\_ACTION       \n007        =>    TIME\\_COVERAGE\\_ACTION          \n008        =>    LOGOUT\\_ACTION                 \n010        =>    DOWNLOAD\\_PRODUCT\\_ACTION       \n011        =>    LIST\\_CATALOG\\_ACTION           \n012        =>    PRODUCT\\_METADATA\\_ACTION       \n013        =>    PRODUCT\\_DOWNLOAD\\_HOME\\_ACTION  \n014        =>    LIST\\_SERVICES\\_ACTION              \n015        =>    DESCRIBE\\_COVERAGE\\_ACTION         \n016        =>    ABOUT\\_ACTION  \n018        =>    WELCOME\\_ACTION  \n019        =>    REFRESH\\_CACHE\\_ACTION  \n020        =>    HEALTHZ\\_ACTION  \n021        =>    CACHE\\_STATUS\\_ACTION  \n\n### <a name=\"LogCodeErrorsErrorType\">Error types</a>  \n\nThe Error Type Code    =>    A number defining a specific error on the server.\n\n0         =>    No error.  \n1         =>    There is a system error. Please contact the Administrator.    \n2         =>    There is an error with the parameters. There are inconsistent.         \n3         =>    The date provided into the parameters is invalid.         \n4         =>    The latitude provided into the parameters is invalid.  \n5         =>    The longitude provided into the parameters is invalid.         \n6         =>    The range defined by the provided dates is invalid.         \n7         =>    The memory capacity of the motu server is exceeded.         \n8         =>    The range defined by the provided latitude/longitude parameters is invalid.         \n9         =>    The range defined by the provided depth parameters is invalid.         \n10        =>    The functionality is not yet implemented.         \n11        =>    There is an error with the provided NetCDF variables.         \n12        =>    There is not variables into the variable parameter.         \n13        =>    NetCDF parameter error. Example: Invalid date range, invalid depth range, ...         \n14        =>    There is an error with the provided NetCDF variable. Have a look at the log file to have more information.         \n15        =>    The number of maximum request in the queue server pool is reached. it's necessary to wait that some requests are finished.         \n16        =>    The number of maximum request for the user is reached. It's necessary to wait that some requests are finished for the user.         \n18        =>    The priority of the request is invalid in the queue server manager. Have a look at the log file to have more information.         \n19        =>    The id of the request is not know by the server. Have a look at the log file to have more information.         \n20        =>    The size of the request is greater than the maximum data managed by the available queue. It's impossible to select a queue for this request. It's necessary to narrow the request.         \n21        =>    The application is shutting down. it's necessary to wait a while before the application is again available.         \n22        =>    There is a problem with the loading of the motu configuration file. Have a look at the log file to have more information.         \n23        =>    There is a problem with the loading of the catalog configuration file. Have a look at the log file to have more information.         \n24        =>    There is a problem with the loading of the error message configuration file. Have a look at the log file to have more information.         \n25        =>    There is a problem with the loading of the netcdf file. Have a look at the log file to have more information.         \n26        =>    There is a problem with the provided parameters. Have a look at the log file to have more information.         \n27        =>    There is a problem with the NetCDF generation engine. Have a look at the log file to have more information.         \n28        =>    The required action is unknown. Have a look at the log file to have more information.  \n29        =>    The product is unknown.  \n30        =>    The service is unknown.  \n31        =>    The request cut the ante meridian. In this case, it's not possible to request more than one depth. It's necessary to change the depth selection and to select in the \"from\" and the \"to\" the values that have the same index into the depth list.  \n32        =>  \tDue to a known bug in Thredds Data Server, a request cannot be satisfied wit netCDF4. User has to request a netCDF3 output file.  \n101\t\t  =>\tWCS specific error code : A WCS mandatory parameter is missing  \n102\t\t  =>\tWCS specific error code : A WCS parameter doesn't match the mandatory format  \n103\t\t  =>\tWCS specific error code : The WCS version parameter is not compatible with the Motu WCS server  \n104\t\t  =>\tWCS specific error code : A system error append.  \n105\t\t  =>\tWCS specific error code : The coverage ident doesn't exist  \n106\t\t  =>\tWCS specific error code : The list of coverage id is empty  \n107\t\t  =>\tWCS specific error code : The provided parameter used to define a subset is invalid  \n108\t\t  =>\tWCS specific error code : The provided axis label doesn't match any available label  \n\n  \n# <a name=\"ClientsAPI\">Motu clients & REST API</a>  \n\nYou can connect to Motu by using a web browser or a client.\n\n## <a name=\"ClientPython\">Python client</a>   \nMotu offers an easy to use Python client. Very useful in machine to machine context, it enables to download data by running a python script.   \nProject and all its documentation is available at [https://github.com/clstoulouse/motu-client-python](https://github.com/clstoulouse/motu-client-python).\n  \n## <a name=\"OGC_WCS_API\">OGC WCS API</a>  \nMotu offers a Web Service interface which implements the OGC WCS standard described, in particular, by the two following documents on the OGC web site:  \n\n* [09-110r4_WCS_Core_2.0.1.pdf](https://portal.opengeospatial.org/files/09-110r4)\n* [09-147_WCS_2.0_Extension_--_KVP_Protocol.pdf](http://portal.opengeospatial.org/files/?artifact_id=36263&format=pdf)\n\nAvailable Web Services are:  \n\n* [Get capabilities](#GetCapabilities)\n* [Describe coverage](#DescribeCoverage)\n* [Get coverage](#GetCoverage)\n\nParameters can be added to each request and they are described with their cardinality [x,y].  \n\n* [0,1] is an optional parameter.   \n* [1] is a mandatory parameter.  \n* [0,n] is an optional parameter which can be set several times.  \n* [1,n] is a mandatory parameter which can be set several times. \n\n### <a name=\"GetCapabilities\">WCS: Get Capabilities</a>\n\nThe GetCapabilities request retrieves all available products defined on Motu server.\n\n\n__URL__: http://localhost:8080/motu-web/wcs?service=WCS&version=2.0.1&request=GetCapabilities\n\n__Parameters__:  \n\n* __service [1]:__ Value is fixed to \"WCS\"\n* __version [1]:__ Value is fixed to \"2.0.1\"\n* __request [1]:__ Value is fixed to \"GetCapabilties\"\n\n__Return__: \nA XML document as shown below:\n\n<pre>\n  <code>\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?&gt;\n&lt;ns3:Capabilities version=\"2.0.1\" xmlns:ns6=\"http://www.opengis.net/swe/2.0\" xmlns:ns5=\"http://www.opengis.net/gmlcov/1.0\" xmlns:ns2=\"http://www.w3.org/1999/xlink\" xmlns:ns1=\"http://www.opengis.net/ows/2.0\" xmlns:ns4=\"http://www.opengis.net/gml/3.2\" xmlns:ns3=\"http://www.opengis.net/wcs/2.0\"&gt;\n    &lt;ns1:ServiceIdentification&gt;\n        &lt;ns1:Title&gt;Motu&lt;/ns1:Title&gt;\n        &lt;ns1:Abstract&gt;Motu WCS service&lt;/ns1:Abstract&gt;\n        &lt;ns1:ServiceType&gt;OGC WCS&lt;/ns1:ServiceType&gt;\n        &lt;ns1:ServiceTypeVersion&gt;2.0.1&lt;/ns1:ServiceTypeVersion&gt;\n        &lt;ns1:Profile&gt;http://www.opengis.net/spec/WCS/2.0/conf/core&lt;/ns1:Profile&gt;\n        &lt;ns1:Profile&gt;http://www.opengis.net/spec/WCS_protocol-binding_get-kvp/1.0/conf/get-kvp&lt;/ns1:Profile&gt;\n    &lt;/ns1:ServiceIdentification&gt;\n    &lt;ns1:OperationsMetadata&gt;\n        &lt;ns1:Operation name=\"GetCapabilities\"&gt;\n            &lt;ns1:DCP&gt;\n                &lt;ns1:HTTP&gt;\n                    &lt;ns1:Get ns2:href=\"http://localhost:8080/motu-web/wcs\"/&gt;\n                &lt;/ns1:HTTP&gt;\n            &lt;/ns1:DCP&gt;\n        &lt;/ns1:Operation&gt;\n        &lt;ns1:Operation name=\"DescribeCoverage\"&gt;\n            &lt;ns1:DCP&gt;\n                &lt;ns1:HTTP&gt;\n                    &lt;ns1:Get ns2:href=\"http://localhost:8080/motu-web/wcs\"/&gt;\n                &lt;/ns1:HTTP&gt;\n            &lt;/ns1:DCP&gt;\n        &lt;/ns1:Operation&gt;\n        &lt;ns1:Operation name=\"GetCoverage\"&gt;\n            &lt;ns1:DCP&gt;\n                &lt;ns1:HTTP&gt;\n                    &lt;ns1:Get ns2:href=\"http://localhost:8080/motu-web/wcs\"/&gt;\n                &lt;/ns1:HTTP&gt;\n            &lt;/ns1:DCP&gt;\n        &lt;/ns1:Operation&gt;\n    &lt;/ns1:OperationsMetadata&gt;\n    &lt;ns3:ServiceMetadata&gt;\n        &lt;ns3:formatSupported&gt;application/netcdf&lt;/ns3:formatSupported&gt;\n    &lt;/ns3:ServiceMetadata&gt;\n    &lt;ns3:Contents&gt;\n        &lt;ns3:CoverageSummary&gt;\n            &lt;ns3:CoverageId&gt;HR_MOD_NCSS-TDS@HR_MOD&lt;/ns3:CoverageId&gt;\n            &lt;ns3:CoverageSubtype&gt;ns3:GridCoverage&lt;/ns3:CoverageSubtype&gt;\n        &lt;/ns3:CoverageSummary&gt;\n\n            ...\n\n\t\t&lt;/ns3:Contents&gt;\n&lt;/ns3:Capabilities&gt;\n  </code>\n</pre>\n\n\n\n### <a name=\"DescribeCoverage\">WCS: Describe Coverage</a>\n\nThe DescribeCoverage request retrieves the parameters description and the list of available  variables.\nFor the parameters description, low and high values are provided.\n\n__URL__: http://localhost:8080/motu-web/wcs?service=WCS&version=2.0.1&request=DescribeCoverage&coverageId=$coverageId\n\n__Parameters__:  \n\n* __service [1]:__ Value is fixed to \"WCS\"\n* __version [1]:__ Value is fixed to \"2.0.1\"\n* __request [1]:__ Value is fixed to \"DescribeCoverage\"\n* __coverageId [1]:__ list of identifiers of the required coverages. Each coverage identifiers are separated by a comma (,). CoverageId are returned by the [GetCapabilities](#GetCapabilities) service.\n\n__Return__: \nA XML document as shown below:\n\n<pre>\n  <code>\n  &lt;?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?&gt;\n&lt;ns4:CoverageDescriptions xmlns:ns6=\"http://www.opengis.net/ows/2.0\"\n\txmlns:ns5=\"http://www.opengis.net/swe/2.0\" xmlns:ns2=\"http://www.w3.org/1999/xlink\"\n\txmlns:ns1=\"http://www.opengis.net/gml/3.2\" xmlns:ns4=\"http://www.opengis.net/wcs/2.0\"\n\txmlns:ns3=\"http://www.opengis.net/gmlcov/1.0\"&gt;\n\t&lt;ns4:CoverageDescription ns1:id=\"$covergaeId\"&gt;\n\t\t&lt;ns1:boundedBy&gt;\n\t\t\t&lt;ns1:Envelope\n\t\t\t\tuomLabels=\"latitude longitude depth date in seconds since 1970, 1 jan\"\n\t\t\t\taxisLabels=\"Lat Lon Height Time\"&gt;\n\t\t\t\t&lt;ns1:lowerCorner&gt;-80.0 -180.0 0.0 1.3565232E9&lt;/ns1:lowerCorner&gt;\n\t\t\t\t&lt;ns1:upperCorner&gt;90.0 180.0 5728.0 1.4657328E9&lt;/ns1:upperCorner&gt;\n\t\t\t&lt;/ns1:Envelope&gt;\n\t\t&lt;/ns1:boundedBy&gt;\n\t\t&lt;ns4:CoverageId&gt;$covergaeId&lt;/ns4:CoverageId&gt;\n\t\t&lt;ns1:domainSet&gt;\n\t\t\t&lt;ns1:Grid dimension=\"4\"\n\t\t\t\tuomLabels=\"latitude longitude depth date in seconds since 1970, 1 jan\"\n\t\t\t\taxisLabels=\"Lat Lon Height Time\" ns1:id=\"Grid000\"&gt;\n\t\t\t\t&lt;ns1:limits&gt;\n\t\t\t\t\t&lt;ns1:GridEnvelope&gt;\n\t\t\t\t\t\t&lt;ns1:low&gt;-80 -180 0 1356523200&lt;/ns1:low&gt;\n\t\t\t\t\t\t&lt;ns1:high&gt;90 180 5728 1465732800&lt;/ns1:high&gt;\n\t\t\t\t\t&lt;/ns1:GridEnvelope&gt;\n\t\t\t\t&lt;/ns1:limits&gt;\n\t\t\t&lt;/ns1:Grid&gt;\n\t\t&lt;/ns1:domainSet&gt;\n\t\t&lt;ns3:rangeType&gt;\n\t\t\t&lt;ns5:DataRecord&gt;\n\t\t\t\t&lt;ns5:field name=\"uice\"&gt;\n\t\t\t\t\t&lt;ns5:Quantity&gt;\n\t\t\t\t\t\t&lt;ns5:uom code=\"m s-1\" /&gt;\n\t\t\t\t\t&lt;/ns5:Quantity&gt;\n\t\t\t\t&lt;/ns5:field&gt;\n\t\t\t\t&lt;ns5:field name=\"salinity\"&gt;\n\t\t\t\t\t&lt;ns5:Quantity&gt;\n\t\t\t\t\t\t&lt;ns5:uom code=\"1e-3\" /&gt;\n\t\t\t\t\t&lt;/ns5:Quantity&gt;\n\t\t\t\t&lt;/ns5:field&gt;\n\t\t\t\t&lt;ns5:field name=\"vice\"&gt;\n\t\t\t\t\t&lt;ns5:Quantity&gt;\n\t\t\t\t\t\t&lt;ns5:uom code=\"m s-1\" /&gt;\n\t\t\t\t\t&lt;/ns5:Quantity&gt;\n\t\t\t\t&lt;/ns5:field&gt;\n\t\t\t\t&lt;ns5:field name=\"hice\"&gt;\n\t\t\t\t\t&lt;ns5:Quantity&gt;\n\t\t\t\t\t\t&lt;ns5:uom code=\"m\" /&gt;\n\t\t\t\t\t&lt;/ns5:Quantity&gt;\n\t\t\t\t&lt;/ns5:field&gt;\n\t\t\t\t&lt;ns5:field name=\"u\"&gt;\n\t\t\t\t\t&lt;ns5:Quantity&gt;\n\t\t\t\t\t\t&lt;ns5:uom code=\"m s-1\" /&gt;\n\t\t\t\t\t&lt;/ns5:Quantity&gt;\n\t\t\t\t&lt;/ns5:field&gt;\n\t\t\t\t&lt;ns5:field name=\"v\"&gt;\n\t\t\t\t\t&lt;ns5:Quantity&gt;\n\t\t\t\t\t\t&lt;ns5:uom code=\"m s-1\" /&gt;\n\t\t\t\t\t&lt;/ns5:Quantity&gt;\n\t\t\t\t&lt;/ns5:field&gt;\n\t\t\t\t&lt;ns5:field name=\"temperature\"&gt;\n\t\t\t\t\t&lt;ns5:Quantity&gt;\n\t\t\t\t\t\t&lt;ns5:uom code=\"K\" /&gt;\n\t\t\t\t\t&lt;/ns5:Quantity&gt;\n\t\t\t\t&lt;/ns5:field&gt;\n\t\t\t\t&lt;ns5:field name=\"ssh\"&gt;\n\t\t\t\t\t&lt;ns5:Quantity&gt;\n\t\t\t\t\t\t&lt;ns5:uom code=\"m\" /&gt;\n\t\t\t\t\t&lt;/ns5:Quantity&gt;\n\t\t\t\t&lt;/ns5:field&gt;\n\t\t\t\t&lt;ns5:field name=\"fice\"&gt;\n\t\t\t\t\t&lt;ns5:Quantity&gt;\n\t\t\t\t\t\t&lt;ns5:uom code=\"1\" /&gt;\n\t\t\t\t\t&lt;/ns5:Quantity&gt;\n\t\t\t\t&lt;/ns5:field&gt;\n\t\t\t&lt;/ns5:DataRecord&gt;\n\t\t&lt;/ns3:rangeType&gt;\n\t&lt;/ns4:CoverageDescription&gt;\n&lt;/ns4:CoverageDescriptions&gt;\n  </code>\n</pre>\n\n  \n### <a name=\"GetCoverage\">WCS: Get Coverage</a>\n\nThe GetCoverage request is used to run an extraction on a dataset using some filtering parameters and a list of required variables.\n\n__URL__:  \n\n* __DGF__: http://localhost:8080/motu-web/wcs?service=WCS&version=2.0.1&request=GetCoverage&coverageId=$coverageId&subset=Time(1412157600,1412244000)\n* __Subetter__: http://localhost:8080/motu-web/wcs?service=WCS&version=2.0.1&request=GetCoverage&coverageId=$coverageId&subset=Time(1412157600,1412244000)&subset=Lat(50,70)&subset=Lon(0,10)&subset=Height(0,5728)&rangeSubset=temperature,salinity\n\n\n__Parameters__:   \n\n* __service [1]:__ Value is fixed to \"WCS\"\n* __version [1]:__ Value is fixed to \"2.0.1\"\n* __request [1]:__ Value is fixed to \"GetCoverage\"\n* __coverageId [1]:__ the identifier of the required coverage. CoverageId are returned by the [GetCapabilities](#GetCapabilities) service.\n* __subset [1,n]:__ the list of filtering parameters.  \n\t* To define one filtering parameter, the following format have to be respected:<br/>\n\tFor the Time parameter:\n\t```\n\tSUBSET=Time(lowTimeValue,highTimeValue)\n\t```\n\tUnit is epoch since 1st January 1970, in UTC. E.g. Thu Dec 01 2016 00:00:00 is set to 1480550400000.  \n\t* To define multiple filtering parameters, the following format have to be respected:<br/>\n\tFor the Latitude and the Longitue:\n\t```\n\tSUBSET=Lat(lowLatValue,highLatValue)&SUBSET=Lon(lowLonValue,highLonValue)\n\t```  \n\tIn order to know which subset filters can be applied, you have to run a [DescribeCoverage](#DescribeCoverage) request.\n* __rangesubset:__ the list of required variables for the coverage. Each variable have to be separated by a comma (,)\n\n__Return__: \nA Netcdf file. When you request for one point, a specific algorithm is used, see [Downloading 1 point](#ArchiAlgoDownloading1Point).\n\n\n## <a name=\"ClientRESTAPI\">MOTU REST API</a>   \n__MOTU REST API__ defines a set of services accessible from an HTTP URLs.  \nAll URLs have always the same pattern: http://motuServer/${context}/Motu?action=$actionName  \nOther parameters can be added and they are described with their cardinality [x,y].  \n\n* [0,1] is an optional parameter.   \n* [1] is a mandatory parameter.  \n* [0,n] is an optional parameter which can be set several times.  \n* [1,n] is a mandatory parameter which can be set several times.  \n\n__$actionName is an action, they are all listed below:__  \n  \n* XML API\n   * [Describe coverage](#ClientAPI_DescribeCoverage)  \n   * [Describe product](#ClientAPI_DescribeProduct)  \n   * [Request status](#ClientAPI_RequestStatus)  \n   * [Get size](#ClientAPI_GetSize)  \n   * [Time coverage](#ClientAPI_GetTimeCov)  \n   * [Download product](#ClientAPI_DownloadProduct)  \n* HTML Web pages\n   * [About](#ClientAPI_About)  \n   * [Debug](#ClientAPI_Debug)  \n   * [Download product](#ClientAPI_DownloadProduct)  \n   * [List catalog](#ClientAPI_ListCatalog)  \n   * [List services](#ClientAPI_ListServices)  \n   * [Product download home](#ClientAPI_ProductDownloadHome)  \n   * [Product medatata](#ClientAPI_ProductMetadata)    \n   * [Welcome](#ClientAPI_welcome)  \n* Plain Text \n   * [Ping](#ClientAPI_Ping)  \n   * [Refresh config services metadata cache](#ClientAPI_RefreshCache) \n   * [Healthz](#ClientAPI_healthz)  \n* JSON\n   * [Supervision](#ClientAPI_supervision)  \n   * [CacheStatus](#ClientAPI_CacheStatus)  \n\n\n \n### <a name=\"ClientAPI_About\">About</a>    \nDisplay version of the archives installed on Motu server  \n__URL__: http://localhost:8080/motu-web/Motu?action=about  \n\n__Parameters__: No parameter.  \n\n__Return__: An HTML page. Motu-static-files (Graphic chart) is refreshed thanks to Ajax because its version file can be installed on a distinct server.   \nExample:  \n```\nMotu-products: 3.0  \nMotu-distribution: 2.6.00-SNAPSHOT  \nMotu-configuration: 2.6.00-SNAPSHOT-20160623173246403  \nMotu-static-files (Graphic chart): 3.0.00-RC1-20160914162955422  \n```\n\n\n### <a name=\"ClientAPI_Debug\">Debug</a>    \nDisplay all requests status managed by Motu server in the last [cleanRequestInterval](#BScleanRequestInterval) minutes.\nTables are sorted by time ascending.  \n4 status are defined:\n\n* __DONE__: Request has been processed successfully. Result file can be downloaded.  \n* __ERROR__: Request has not been processed successfully. No result file is available.  \n* __PENDING__: Request has been received by the server. Server computes its size and runs some checks, it can take a while.  \n* __INPROGRESS__: Request has been delegated to the queue server which is currently processing it.  \n\n\n__URL__: http://localhost:8080/motu-web/Motu?action=debug  \n\n__Parameters__:  \n* __order__ [0,1]: Change the order of items INPROGRESS,PENDING,ERROR,DONE. All items shall be set.  \nexample: http://localhost:8080/motu-web/Motu?action=Debug&order=DONE,ERROR,PENDING,INPROGRESS  \nWithout this parameter, default order is: INPROGRESS,PENDING,ERROR,DONE  \n\n__Return__: An HTML page  \n  \n  \n### <a name=\"ClientAPI_DescribeCoverage\">Describe coverage</a>    \nGet coverage data in relationship with a dataset.  \n__URL__: http://localhost:8080/motu-web/Motu?action=describecoverage&service=HR_MOD-TDS&datasetID=HR_MOD  \n\n__Parameters__:  \n\n* __service__ [1]: The [service name](#BSconfigServiceName)  \n* __datesetID__ [1]: The [dataset ID](#BSconfigServiceDatasetName)  \n\n__Return__: A XML document  \n\n```\n<dataset xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"  xsi:noNamespaceSchemaLocation=\"describeDataset.xsd\" name=\"HR_MOD\" id=\"HR_MOD\">  \n<boundingBox>  \n  <lon min=\"-180.0\" max=\"179.91668701171875\" units=\"degrees_east\"/>\n  <lat min=\"-80.0\" max=\"-80.0\" units=\"degrees_north\"/>\n</boundingBox>\n<dimension name=\"time\" start=\"2012-12-26T12:00:00.000+00:00\" end=\"2016-06-12T12:00:00.000+00:00\" units=\"ISO8601\"/>  \n<dimension name=\"z\" start=\"\" end=\"\" units=\"m\"/>  \n<variables>  \n<variable id=\"northward_sea_water_velocity\" name=\"v\" description=\"Northward velocity\" standardName=\"northward_sea_water_velocity\" units=\"m s-1\">  \n<dimensions></dimensions>  \n</variable>  \n...  \n</variables>  \n</dataset>  \n```\n  \n  \n### <a name=\"ClientAPI_DescribeProduct\">Describe product</a>    \nDisplay the product meaning dataset description. Result contains notably: the datasetid, the time coverage, the geospatial coverage, the variable(s) (with the standard_name and unit), eventually the vertical coverage.  \nThere is 2 ways to call describe product, both returning a same response.  \n\n\n#### Way 1   \n  \n__URL__: http://localhost:8080/motu-web/Motu?action=describeproduct&service=HR_MOD-TDS&product=HR_MOD  \n\n__Parameters__:  \n\n* __service__ [1]: The [service name](#BSconfigServiceName)  \n* __product__ [1]: The product id  \n  \n  \n#### Way 2  (Deprecated) \n\n__URL__: http://localhost:8080/motu-web/Motu?action=describeproduct&data=http://$tdsServer/thredds/dodsC/path_HR_MOD&xmlfile=http://$tdsServer/thredds/m_HR_MOD.xml  \n\n__Parameters__:  \n\n* __xmlfile__ [1]: The Thredds dataset, example: http://$tdsServer/thredds/m_HR_MOD.xml  \n* __data__ [1]: The Thredds data, example http://$tdsServer/thredds/dodsC/path_HR_MOD  \n    \n__Return__: An XML document  \n\n```\n<productMetadataInfo code=\"OK\" msg=\"OK\" lastUpdate=\"Not Available\" title=\"HR_MOD\" id=\"HR_MOD\">  \n<timeCoverage code=\"OK\" msg=\"OK\"/>  \n<availableTimes code=\"OK\" msg=\"OK\">  \n1993-01-15T12:00:00Z/2001-01-01T00:00:00Z/P2D,2001-01-01T00:00:00Z/2012-03-01T00:00:00Z/PT6H\n</availableTimes>  \n<availableDepths code=\"OK\" msg=\"OK\">  \n0.49402;1.54138;2.64567;...  \n</availableDepths>  \n<geospatialCoverage code=\"OK\" msg=\"OK\"/>  \n<variablesVocabulary code=\"OK\" msg=\"OK\"/>  \n<variables code=\"OK\" msg=\"OK\">  \n<variable description=\"Northward velocity\" units=\"m s-1\" longName=\"Northward velocity\" standardName=\"northward_sea_water_velocity\" name=\"v\" code=\"OK\" msg=\"OK\"/>  \n<variable description=\"Eastward velocity\" units=\"m s-1\" longName=\"Eastward velocity\" standardName=\"eastward_sea_water_velocity\" name=\"u\" code=\"OK\" msg=\"OK\"/>  \n...  \n</variables>  \n<dataGeospatialCoverage code=\"OK\" msg=\"OK\">  \n<axis code=\"OK\" msg=\"OK\" description=\"Time (hours since 1950-01-01)\" units=\"hours since 1950-01-01 00:00:00\" name=\"time_counter\" upper=\"582468\" lower=\"552132\" axisType=\"Time\"/>  \n<axis code=\"OK\" msg=\"OK\" description=\"Longitude\" units=\"degrees_east\" name=\"longitude\" upper=\"179.91668701171875\" lower=\"-180\" axisType=\"Lon\"/>  \n<axis code=\"OK\" msg=\"OK\" description=\"Latitude\" units=\"degrees_north\" name=\"latitude\" upper=\"90\" lower=\"-80\" axisType=\"Lat\"/>  \n<axis code=\"OK\" msg=\"OK\" description=\"Depth\" units=\"m\" name=\"depth\" upper=\"5727.9169921875\" lower=\"0.4940249919891357421875\" axisType=\"Height\"/>  \n</dataGeospatialCoverage>  \n</productMetadataInfo>  \n```\n\n\n#### availableTimes XML tag \nIn the XML result file the tag \"availableTimes\" provides the list of date where data are available for the requested product.\nThe format of the date follows the convention ISO_8601 used to represent the dates and times. (https://en.wikipedia.org/wiki/ISO_8601)\nForeach available time period, the period definition format is \"StartDatePeriod/EndDatePeriod/DurationBetweenEachAvailableData\".  \nThe \"availableTimes\" contains a list of time period separated by a \",\".\n* __StartDate__ : this the first date of the period where data are available.\n* __EndDate__ : this the last date of the period where data are available.\n* __DurationBetweenEachAvailableData__ : This the period duration between each available data in the interval defined by the the \"StartDate\" and \"EndDate\" date.  \n\n>For DGF datasets, the list of available times is built from the start and end date of each file of the dataset, ignoring the other time values if any. As a consequence, **there might be more available times than those listed in this attribute for DGF datasets** with there are more than 2 time values per file.\n\n##### StartDate and EndDate format\nThe format of the StartDate and EndDate is YYYY-MM-DDThh:mm:ssZ where:\n* __YYYY__ : is the year defined on 4 digits\n* __MM__ : is the number of the month defined on 2 digits\n* __DD__: is the number of the day in the month on 2 digits\n* __hh__: is the hour of the day on 2 digits\n* __mm__: is the minutes of the hour on 2 digits\n* __ss__: is the seconds of the minutes on 2 digits\n\nExamples:\n* 1993-01-15T12:00:00Z\n* 2016-07-25T06:35:45Z\n* 2017-08-31T15:05:08Z\n\n##### DurationBetweenEachAvailableData\nThe formation of the duration is P*nbyers*Y*nbmonths*M*nbdays*DT*nbhours*H*nbminutes*M*nbseconds*S.*nbmillisec* where:\n* __nbyears__ : is the number of years. The ISO_8601 is ambiguous on the number of days in the year. For the Motu project, the number of days is fixed to 365 as in the most of projects.\n* __nbmonths__ : is the number of month. The ISO_8601 is ambiguous on the number of days in the month. For the Motu project, the number of days is fixed to 30 as in the most of projects.\n* __nbdays__ : is the number of day. One day is 24 hours.\n* __nbhours__ : is the number of hours. One hour is 60 minutes.\n* __nbseconds__: is the number of seconds. One seconds is 1000 milliseconds.\n* __nbmillisec__ : is the number of milliseconds.\n\nBy convention, P1M defines a duration of 1 month and PT1M defines a duration of 1 minutes.\n\nExamples:\n* each minute => PT1M\n* each hour => PT1H\n* each 12 hours => PT12H\n* each day => P1D\n* each 15 days => P15D\n* each month => P1M\n \n#### \"lastUpdate\" XML attribute\nNote about \"lastUpdate\" attribute of the \"productMetadataInfo\" field: it has the same value than \"[Last update](#ClientAPI_ListCatalog)\" field of the list catalog page.  \n \n### <a name=\"ClientAPI_DownloadProduct\">Download product</a>    \nRequest used to download a product  \n\n__URL__: http://localhost:8080/motu-web/Motu?action=productdownload  \nexample:  \nhttp://localhost:8080/motu-web/Motu?action=productdownload&service=HR_MOD-TDS&product=HR_MOD&x_lo=-2&x_hi=2&y_lo=-2&y_hi=2&output=netcdf&t_lo=2016-06-12+12%3A00%3A00&t_hi=2016-06-12+12%3A00%3A00&z_lo=0.49&z_hi=5727.92  \n\n__Parameters__:  \n\n* __service__ [1]: The [service name](#BSconfigServiceName)  \n* __product__ [1]: The product id  \n* __variable__ [0,n]: physical variables to be extracted from the product. When no variable is set, all the variables of the dataset are extracted.  \n* __y_lo__ [0,1]: low latitude of a geographic extraction. Default value is -90.  \n* __y_hi__ [0,1]: high latitude of a geographic extraction. Default value is 90.  \n* __x_lo__ [0,1]: low longitude of a geographic extraction. Default value is -180.  \n* __x_hi__ [0,1]: high longitude of a geographic extraction. Default value is 180.  \n* __z_lo__ [0,1]: low vertical depth . Default value is the min available depth. If the lo value is greater than the hi value, the 2 values are switched. If the depth range is out of the available range, Motu computes the best range into the available range. Value of this parameter is a double or \"Surface\" string which has a value of 0.0.   \n* __z_hi__ [0,1]: high vertical depth. Default value is the max available depth. If the hi value is lower than the lo value, the 2 values are switched. If the depth range is out of the available range, Motu computes the best range into the available range. Value of this parameter is a double or \"Surface\" string which has a value of 0.0.   \n* __t_lo__ [0,1]: Start date of a temporal extraction. If not set, the default value is the first date/time available for the dataset. Format is  \"yyy-MM-dd\" or \"yyyy-MM-dd HH:mm:ss\" or \"yyyy-MM-ddTHH:mm:ss\" and depends on the requested dataset.  \n* __t_hi__ [0,1]: End date of a temporal extraction. If not set, the default value is the last date/time available for the dataset. Format is \"yyy-MM-dd\" or \"yyyy-MM-dd HH:mm:ss\" or \"yyyy-MM-ddTHH:mm:ss\" and depends on the requested dataset.    \n* __output__ [0,1]: netcdf. Due to a TDS issue, only netcdf is available. netcdf4 will be available as soon as TDS will have resolved its issue.\n* __mode__ [0,1]: Specify the desired result mode. Enumeration value from [url, console, status] represented as a string. If no mode, \"url\" value is the default mode.  \n\n   * mode=__url__: URL of the delivery file is directly returned in the HTTP response as an HTML web page. Then Javascript read this URL to download file. The request is processed in a synchronous mode.  \n   * mode=__console__: the response is a 302 HTTP redirection to the delivery file to be returned as a binary stream. The request is processed in a synchronous mode.  \n   * mode=__status__: request is submitted and [the status](#ClientAPI_RequestStatus) of the request processing is immediately returned as an XML. The request is processed in an asynchronous mode.  \n   Web Portal submits the request to the Dissemination Unit Subsetter and gets an immediate response of the Subsetter. \n   This response contains the identifier and the status of the order (pending, in progress, done, error).\n   So long as the order is not completed (done or error), Web Portal requests the status of the order at regular and fair intervals (> 5 seconds) \n   and gets an immediate response. When the status is \u201cdone\u201d, Web Portal retrieves the url of the file to download, from the status response. \n   Then Web Portal redirects response to this url. \n   The Web Browser opens a binary stream of the file to download and shows a dialog box to allow the user saving it as a local file.  \n\n__Return__: Several ways depending of the selected http parameter mode. When you request for one point, a specific algorithm is used, see [Downloading 1 point](#ArchiAlgoDownloading1Point).  \n\n\n\n \n### <a name=\"ClientAPI_RequestStatus\">Request status</a>    \nGet a request status to get more details about a download state.  \n\n__URL__: http://localhost:8080/motu-web/Motu?action=getreqstatus&requestid=123456789  \n\n__Parameters__:  \n\n* __requestid__ [1]: A request id.  \n\n__Return__: An XML document or an HTML page if requestId does not exists.    \nValidated by the schema /motu-api-message/src/main/schema/XmlMessageModel.xsd#StatusModeResponse  \nExample:  \n\n```\n<statusModeResponse code=\"004-0\" msg=\"\" scriptVersion=\"\" userHost=\"\" userId=\"\" dateSubmit=\"2016-09-19T16:56:22.184Z\" localUri=\"/$pathTo/HR_MOD_1474304182183.nc\" remoteUri=\"http://localhost:8080/motu/deliveries/HR_MOD_1474304182183.nc\" size=\"1152.0\"dateProc=\"2016-09-19T16:56:22.566Z\" requestId=\"1474304182183\" status=\"1\"/>\n```\n  \nSize is in MegaBytes or at NaN while still not estimated.\n\n\n\n \n### <a name=\"ClientAPI_GetSize\">Get size</a>    \nGet the size of a download request. Result contains the size of the potential result file, with a unit, and the maximum allowed size for this service.  \n\n__URL__: http://localhost:8080/motu-web/Motu?action=getsize  \n\n__Parameters__:  \n\nParameters below are exactly the same as for [Download product](#ClientAPI_DownloadProduct)   \n\n* __service__ [1]: The [service name](#BSconfigServiceName)  \n* __product__ [1]: The product id  \n* __variable__ [0,n]: physical variables to be extracted from the product. When no variable is set, all the variables of the dataset are extracted.  \n* __y_lo__ [0,1]: low latitude of a geographic extraction. Default value is -90.  \n* __y_hi__ [0,1]: high latitude of a geographic extraction. Default value is 90.  \n* __x_lo__ [0,1]: low longitude of a geographic extraction. Default value is -180.  \n* __x_hi__ [0,1]: high longitude of a geographic extraction. Default value is 180.  \n* __z_lo__ [0,1]: low vertical depth . Default value is the min available depth. If the lo value is greater than the hi value, the 2 values are switch. If the depth range is out of the available range, Motu compute the best range into the available range.  \n* __z_hi__ [0,1]: high vertical depth. Default value is the max available depth. If the hi value is less than the lo value, the 2 values are switch. If the depth range is out of the available range, Motu compute the best range into the available range.\n* __t_lo__ [0,1]: Start date of a temporal extraction. If not set, the default value is the first date/time available for the dataset. Format is yyy-mm-dd or yyyy-dd h:m:s or yyyy-ddTh:m:s.  \n* __t_hi__ [0,1]: End date of a temporal extraction. If not set, the default value is the last date/time available for the dataset. Format is yyy-mm-dd or yyyy-dd h:m:s or yyyy-ddTh:m:s.  \n  \n__Return__: An XML document.    \nThe unit is \"KB\" means Kilobyte.\nValidated by the schema /motu-api-message/src/main/schema/XmlMessageModel.xsd#RequestSize  \nExample:  \n\n```\n<requestSize code=\"005-0\" msg=\"OK\" unit=\"kb\" size=\"1.5104933E8\" maxAllowedSize=\"9.961472E8\"/>  \n```  \n\n### <a name=\"ClientAPI_ListCatalog\">List catalog</a>    \nDisplay information about a catalog (last update timestamp) and display link to access to download page and dataset metadata.\n\nNote about field \"__Last update__\" : On MOTU start-up, this date is set to the most recent date of the dataset if it is before of the current day, or to the \"last update date\" returned by the TDS server also if it is before current day, or else to \"Not available\".  \nOn MOTU cache update, if the most recent available date of the dataset changes, the \"last update date\" is set with the current date.  \n\n__URL__: http://localhost:8080/motu-web/Motu?action=listcatalog&service=HR_MOD-TDS  \n\n__Parameters__:   \n\n* __service__ [1]: The [service name](#BSconfigServiceName)  \n\n__Return__: An HTML page   \n\n\n### <a name=\"ClientAPI_ListServices\">List services</a>    \nDisplay the service web page \n__URL__: http://localhost:8080/motu-web/Motu?action=listcatalog&service=HR_MOD-TDS  \n\n__Parameters__:  \n\n* __catalogtype__ [0,1]: The [catalog type](#BSconfigServiceDatasetType) used to filter by type.  \n\n__Return__: An HTML page   \n\n\n### <a name=\"ClientAPI_Ping\">Ping</a>    \nUsed to be sure that server is up. You can also use the [supervision](#ClientAPI_supervision) URL.  \n\n__URL__: http://localhost:8080/motu-web/Motu?action=ping  \n\n__Parameters__: No parameter.  \n\n__Return__: An plain text  \n\n```  \nOK - response action=ping    \n```     \n\n### <a name=\"ClientAPI_RefreshCache\">Refresh config services metadata cache</a>    \nForce the refresh of the cache of [config service](#BSconfigService) metadata instead of waiting the [automatic refresh](#describeproductcacherefreshinmillisec).   \nThis action is secured and is only triggered when a valid [token](#refreshCacheToken) is given.    \nMoreover a list of config services needed to be refreshed is shared with the automatic update process.     \nThis add robustness because a job refreshes only cache of the config services which are is the list. So when this action is called several times, if a config service in already in this waiting list, it is not added a second time.\nA soon as a cache for a config service is refreshed, config service is removed from this waiting list.   \n\n__URL__: http://localhost:8080/motu-web/Motu?action=refreshcache&token=tokenValid&configServiceNames=all  \n\n__Parameters__:\n\n* __token__ [1] : Used to secure this action. The [token](#refreshCacheToken) configured in the motuConfiguration.xml file which allowed the execution of the refresh. See this section for [the token configured](#refreshCacheToken)\n* __configServiceNames__ [1] : [all,onlyauto,$configServiceNames] 3 options to tune how the cache will be resfreshed.  \n   * __all__ : Refresh immediately all the config service. \n   * __onlyauto__ : Refresh immediately only the config services which enable the [automatic refresh](#refreshCacheAutomaticallyEnabled).\n   * __$configServiceNames__ : Refresh immediately all the config services listed. Value of this parameter is a list of all [config service](#BSconfigServiceName) name is separated by a comma character, e.g. configServiceNames=AAA,BBB,CCC\n\n__Return__: A plain text which specify if the refresh is launched or if an error occurred, e.g. \"OK: config service AAA cache refresh in progress\" or \"ERROR: Unknwon config service UnknownConfigService\"\n\n```  \nOK cache refresh in progress   \n```  \n\n\n### <a name=\"ClientAPI_healthz\">Healthz</a>  \nGives healthz information about Motu server health. \n\n__URL__: http://localhost:8080/motu-web/Motu?action=healthz\n\n__Parameters__: No parameter\n  \n__Return__: An http status and a short message:\n - http status **202** (*accepted*) when started and cache still not refreshed, with the message \"*Server started and refresh in progress (remaining  X / Y).*\"  Where X is the number of Catalog to put in the cache, over the total number Y.  \n   This message is also displayed when the Web context gets destroyed and cache gets build again, or also when the configuration file is modified and reloaded.\n - http status **200** when running and ready, with the message \"*Server is ready.*\"\n\n\n### <a name=\"ClientAPI_welcome\">Welcome</a>  \nHTML page which gives access to several web pages, in particular the Motu listservices web page.\n\n__URL__:  \n* http://localhost:8080/motu-web/  \n* http://localhost:8080/motu-web/Motu?action=welcome\n\n__Parameters__: No parameter\n  \n__Return__: An HTML web page  \n\n### <a name=\"ClientAPI_ProductDownloadHome\">Product download home</a>    \nDisplay an HTML page in order to set the download parameters.  \n\n__URL__: http://localhost:8080/motu-web/Motu?action=productdownloadhome&service=HR_OBS-TDS&product=HR_OBS  \n\n__Parameters__:  \n\n* __service__ [1]: The [service name](#BSconfigServiceName)  \n* __product__ [1]: The product id  \n  \n__Return__: An HTML page  \n\n\n\n### <a name=\"ClientAPI_ProductMetadata\">Product metadata Home</a>    \nDisplay an HTML page with the geographical and temporal coverage, the last dataset update and the variables metadata.  \n\nNote about field \"Date\" of \"__Last dataset update__\" section: this date has the same value than the \"[Last update](#ClientAPI_ListCatalog)\" field.  \n\n__URL__: http://localhost:8080/motu-web/Motu?action=listproductmetadata&service=HR_OBS-TDS&product=HR_OBS  \n\n__Parameters__:  \n\n* __service__ [1]: The [service name](#BSconfigServiceName)  \n* __product__ [1]: The product id  \n  \n__Return__: An HTML page  \n\n\n\n### <a name=\"ClientAPI_GetTimeCov\">Time coverage</a>    \nDisplay an HTML page with the geographical and temporal coverage, the last dataset update and the variables metadata.   \n\n__URL__: http://localhost:8080/motu-web/Motu?action=gettimecov&service=HR_MOD-TDS&product=HR_MOD  \n\n__Parameters__:  \n\n* __service__ [1]: The [service name](#BSconfigServiceName)  \n* __product__ [1]: The product id  \n  \n__Return__: A XML document  \n\n```xml  \n<timeCoverage code=\"007-0\" msg=\"OK\" end=\"2016-09-17T00:00:00.000Z\" start=\"2007-05-13T00:00:00.000Z\"/>\n```  \n\n### <a name=\"ClientAPI_supervision\">Supervision</a>  \nGives information about Motu server.  \nFor more details, see [https://jolokia.org/reference/html/agents.html].\n\n__URL__: http://localhost:8080/motu-web/supervision\n\n__Parameters__: No parameter\n  \n__Return__: A JSON document  \n\n```json  \n{\"timestamp\":1474638852,\"status\":200,\"request\":{\"type\":\"version\"},\"value\":{\"protocol\":\"7.2\",\"config\":{\"agentId\":\"10.1.20.198-18043-2df3a4-servlet\",\"agentType\":\"servlet\"},\"agent\":\"1.3.3\",\"info\":{\"product\":\"tomcat\",\"vendor\":\"Apache\",\"version\":\"7.0.69\"}}}\n```  \n\n### <a name=\"ClientAPI_CacheStatus\">CacheStatus</a>  \nGives the status of the dataset cache of Motu.  \nOn start-up, Motu reads its configuration file, and gets a list of \"configService\" nodes referencing a dataset catalog with an URL, and starts caching them. \n\n__URL__: http://localhost:8080/motu-web/Motu?action=cachestatus\n\n__Parameters__: No parameter\n  \n__Return__: A JSON document  \n\n```json  \n{\"cachestatus\":\n\t{\"state\":\n\t\t{\"nbTotal\":8,\"nbSuccess\":6,\"nbFailure\":2,\"lastUpdate\":\"2019-11-19T10:56:37.357Z\",\"lastUpdateDuration\":\"PT1M17.561S\"},\n \t \"configServices\": [\n \t \t{\"Sea_Surface_Temperature_Global-TDS\":\n \t \t\t{\"state\":\n \t \t\t\t{\"status\":\"FAILURE\",\"lastUpdate\":\"2019-11-19T10:57:56.398Z\",\"lastUpdateDuration\":\"\"},\n \t \t\t \"conf\":\n \t \t\t \t{\"refreshCacheAutomaticallyEnabled\":true,\"type\":\"tds\",\"ncss\":\"enabled\"}\n \t \t\t}\n \t \t},\n \t \t{\"HR_MOD_NCSS-TDS\":\n \t \t\t{\"state\":\n \t \t\t\t{\"status\":\"SUCCESS\",\"lastUpdate\":\"2019-11-19T10:57:01.436Z\",\"lastUpdateDuration\":\"PT1.688S\"},\n \t \t\t \"conf\":\n \t \t\t \t{\"refreshCacheAutomaticallyEnabled\":true,\"type\":\"tds\",\"ncss\":\"enabled\"}\n \t \t\t}\n \t \t},\n \t \t.....]\n \t },\n \"version\":\n \t{\"motu-products\":\"Unknow version\",\"motu-distribution\":\"Unknow version\",\"motu-configuration\":\"3.11.04-20190716151835979\"}\n }\n```  \nThe ***nbTotal*** is the total number of ConfigServices.  \nThe ***nbSuccess*** and the ***nbFailure*** are the number of currently loaded ConfigServices in success/failure.  \nNote that ***lastUpdate*** and ***lastUpdateDuration*** fields can be empty if the system hasn't still refreshed the cache or if the access failed.\n\n### <a name=\"ClientAPI_welcome\">Welcome</a>  \nHTML page which gives access to several web pages, in particular the Motu listservices web page.\n\n__URL__:  \n* http://localhost:8080/motu-web/  \n* http://localhost:8080/motu-web/Motu?action=welcome\n\n__Parameters__: No parameter\n  \n__Return__: An HTML web page  \n\n  \n# <a name=\"Docker\">Motu Docker distribution</a>  \n\nMotu comes in a Docker release based on CentOS.\n\n## <a name=\"DockerContent\">Docker image content</a>   \nThe Motu specific elements are located under /opt/motu.  \nThe required libraries are installed on the Docker image.  \nThe Tomcat is located under /opt/motu/tomcat-motu.  \nThe CDO scripts (merge.sh and cdo.sh) can be found in the folder /opt/motu/products/cdo-group.\n  \n## <a name=\"DockerDirectories\">Docker mounted directories</a>  \nTo configure the Motu Docker image, some of the folders have to be mounted from the execution environment:\n* the **motu configuration** folder to mount to /opt/motu/config\n  * log4j.xml\n  * motuConfiguration.xml\n  * motu.properties\n  * standarNames.xml\n  * version-configuration.txt\n  * velocityTemplates/motu.vm\n  * security (folder with the configuration elements for cas authentification)\n* the produced **data files** folder outputed on /opt/motu/data/download/public\n\nOther folders for logs and Apache configuration are optionnals:\n* the **log** folder to mount to /opt/motu/log  \n  The produced logs (errors.log, logbook.log, wrnings.log and motuQSlog.xml) will be available in this folder.\n* the **apache tomcat log** folder to mount to /opt/motu/tomcat-motu/logs  \n  In this folder the host-manager, catalina, and access logs will be created.\n* the **tomcat configuration** folder to mount to /opt/motu/tomcat-product/conf  \n  If this volume is not mounted, the default Apache configuration is used. In that case, the downloads have to be handled by another element of the installation, to which the ddownloadHttpUrl property of the motuConfiguration file will refer.  \n  * logging.properties\n  * server.xml\n    For example to set the Apache HTTPd \"Context\" directive to handle the result files downloading with the MOTU server.\n  * web.xml\n  * other files that could be used for specific needs (configuring catalina, jaspic or the tomcat users)  \n  \nConfiguration files are similar to standard Motu configuration files with some exceptions to take into account:\n* Motu is installed under /opt/motu (and not /opt/cmems-cis/motu)\n* The download directory is under /opt/motu/data/download/public \n\n## <a name=\"DockerRun\">Run Motu with Docker</a>  \n\nThe following command:\n\n```  \ndocker run -d -v /data/shared/motu/result:/opt/motu/data/download/public -v /home/motu/motu/config_qt/motu_config:/opt/motu/config -v /home/motu/motu/config_qt/apache_config:/opt/motu/tomcat-motu/conf -v /home/motu/motu/config_qt/log_motu:/opt/motu/log -v /home/motu/motu/config_qt/log_apache:/opt/motu/tomcat-motu/logs -p 8080:8080 -p 8443:8443 -p 8009:8009 -p 8005:8005 registry-ext.cls.fr:443/motu/motu/motu-distribution\n\n```  \n  \nWill run the Motu server. Logs (from logbook, warning and error MOTU files) can be accessed using:\n```\ntail /var/log/syslog\n```\n\nIf a reverse proxy for downloading the results is to be added in the platform where MOTU server docker image is deployed, use:\n```  \ndocker run -d -v /data/shared/motu/result:/var/www -v /home/motu/motu/config_qt/motu_config:/opt/motu/config -p $NGINX_PORT:8070 -e NGINX_PORT=$NGINX_PORT -e MOTU_DOWNLOAD_PATH=/motu-web -e MOTU_URL=http://$(hostname):18080 -d registry-ext.cls.fr:443/motu/motu/motu-nginx\n\n```  \n\n"
 },
 {
  "repo": "VACUMM/vacumm",
  "language": "Python",
  "readme_contents": "VACUMM\n======\n\n.. image:: https://zenodo.org/badge/22859/VACUMM/vacumm.svg\n   :target: https://zenodo.org/badge/latestdoi/22859/VACUMM/vacumm\n.. image:: https://travis-ci.org/VACUMM/vacumm.svg?branch=master\n    :target: https://travis-ci.org/VACUMM/vacumm\n\nVACUMM provides generic and specialized tools for the validation of ocean models,\nand more especially the MARS model from `IFREMER <http://www.ifremer.fr>`_.\nThe heart of VACUMM is a\n`library <http://www.ifremer.fr/vacumm/library/index.html>`_  written mainly\nin the `Python <http://www.python.org>`_ language,\nwhose `core <http://www.ifremer.fr/vacumm/library/misc.html>`_\ncan be used for the **preprocessing** and the\n**postprocessing** of oceanic and atmospheric data coming from models or observations.\nThe library for instance also has specialized modules for managing outputs from\n`models <http://www.ifremer.fr/vacumm/library/data/model.html>`_ and making advanced\n`diagnostics <http://www.ifremer.fr/vacumm/library/diag.html>`_.\n\n.. code-block:: python\n\n    >>> from vcmq import *\n    >>> sst = DS(data_sample('mars3d.xy.nc'), 'mars').get_sst()\n    >>> map2(sst)\n\n\nFeatures\n--------\n\n- A huge documentation with a gallery, a lot of examples and the complete API:\n  http://www.ifremer.fr/vacumm\n- Full UV-CDAT support and extensions.\n- Matplotlib/basemap graphics with advanced plotting objects like geographical mapping tools.\n- Numerous utilities for manipulating and converting time data.\n- Regridding and interpolation of random or gridded data, in 1D or 2D, with curvilinear grid support.\n- Helper routines for inspecting and reading NetCDF objects in single or multiple file datasets.\n- Generic and specialized 1D and 2D filters working on masked variables.\n- Specialized physical and numerical diagnostics, like dynamics, thermodynamics, spectral analyses, tides, etc.\n- Support and extension of CF conventions for searching or formatting variables.\n- Miscellaneous location utilities such as readers of sigma coordinates for ocean models, or Arakawa grid converters.\n- High level generic interface for reading and post-processing NetCDF data from standard or known dataset, such as model outputs or satellite products.\n- Statistical accumulator for large datasets.\n- Interfaces for working with random and gridded bathymetries, and with shorelines.\n- Utilities for working with masks and selection of spatial data.\n- Utilities for working with input and output remote files.\n- Advanced logging classes.\n- Extensions to sphinx for Fortran and Python.\n- A collection of scripts for some diagnostics.\n\n\nDependencies\n------------\n\nMandatory:\n`CDAT <http://uvcdat.llnl.gov>`_ (or more specifically\n`cdms2 <http://uvcdat.llnl.gov>`_,\n`cdutil <http://uvcdat.llnl.gov>`_,\n`genutil <http://uvcdat.llnl.gov>`_ from CDAT, and\n`matplotlib <https://matplotlib.org>`_,\n`basemap <https://matplotlib.org/basemap>`_),\n`configobj <http://www.voidspace.org.uk/python/configobj.html>`_.\n\nOptional:\n`seawater <https://pypi.python.org/pypi/seawater>`_,\n`PIL <https://pypi.python.org/pypi/PIL>`_,\n`pytz <http://pytz.sourceforge.net>`_,\n`paramiko <http://www.paramiko.org>`_,\n`xlwt <https://pypi.python.org/pypi/xlwt>`_,\n`sphinx-fortran <https://pypi.python.org/pypi/sphinx-fortran>`_,\n`cmocean <https://pypi.python.org/pypi/cmocean>`_.\n\n\nDownload\n--------\n\nTo download VACUMM sources, please go to this page:\nhttp://www.ifremer.fr/vacumm/user.install.download.html\n\n\nInstallation\n------------\n\nFrom sources::\n\n    $ python setup.py install\n\nUsing `conda <http://conda.pydata.org/docs/index.html>`_::\n\n    $ conda install -c vacumm -c conda-forge -c cdat  vacumm\n\nFor more information, please go to this:\nhttp://www.ifremer.fr/vacumm/user.install.installations.html\n\nRelease notes\n-------------\n\nRelease notes for each version are available here:\nhttp://www.ifremer.fr/vacumm/appendix.release.html\n\n\nDocumentation\n-------------\n\nThe documentation is available here:\nhttp://www.ifremer.fr/vacumm\n\n\nLicense\n-------\n\nVACUMM is under the :ref:`CeCiLL <appendix.license>` license,\nwhich is compatible with well knwon GPL license.\n\n\nSupport\n-------\n\nYou can submit `issues <https://github.com/VACUMM/vacumm/issues>`_\nand `pull requests <https://github.com/VACUMM/vacumm/issues>`_\nfrom the GitHub site.\n\nStephane Raynaud (raynaud (at) gmail.com),\nGuillaume Charria (Guillaume.Charria (at) ifremer.fr).\n\nSee the contact page:\nhttp://www.ifremer.fr/vacumm/contact.html\n\n\n"
 },
 {
  "repo": "jerabaul29/OpenMetBuoy-v2021a",
  "language": "C++",
  "readme_contents": "**A question? => open an issue; An idea of improvement? => open an issue and / or submit a pull request.**\n\n# OpenMetBuoy-v2021a (OMB / OMB-v21a): an easy to build, affordable, customizable, open source instrument for oceanographic measurements.\n\n**Our pledge**: many fields within Geosciences are relying on in-situ data collection. Traditionally, collecting in-situ data is expensive, with commercial instruments easily reaching several thousands of USDs even if only quite simple functionalities are provided. However, the recent emergence of open source communities within micro controllers and low level firmware for these opens new possibilities for developing low-cost, high performance instruments based on widely available and affordable components. This repository is focusing on developing an oceanic buoy based on such open source code and low cost components. Our solution is around 10 times cheaper than the least expensive similar commercial instrument we know of, and is actually superior to the commercial instrument in a number of regards, for example, for measurement in the polar regions. We hope that the present open source release, beyond sharing the design of our specific instrument, will encourage many more actors to share their developments within in-situ instrumentation and participate in building an open source community around geophysical instrumentation.\n\nLink to our paper, if you find our work useful and build up on it, please consider citing it:\n\n```\nRabault, Jean, et al.\n\"OpenMetBuoy-V2021: an easy-to-build, affordable, customizable, open source instrument for\n  oceanographic measurements of drift and waves in sea ice and the open ocean.\"\nGeosciences (2022).\n```\n\n- As a preprint: https://www.researchgate.net/publication/357712696_OpenMetBuoy-v2021_an_easy-to-build_affordable_customizable_open_source_instrument_for_oceanographic_measurements_of_drift_and_waves_in_sea_ice_and_the_open_ocean , and as pdf on this repository (see **latest_preprint_MDPI_2022.pdf**),\n- As a published paper (open access, CC-BY license): https://www.mdpi.com/2076-3263/12/3/110 .\n\nThe fully assembled instrument gets space into a box 12x12x9 cm, including 3 D-size battery holders (at the bottom, empty on the picture, should use typically 3 SAFT LSH20 D-size batteries), all the components, and the 9dof sensor:\n\n<img src=\"https://github.com/jerabaul29/OpenMetBuoy-v2021a/blob/main/instrument_hardware/jpg/from_side.jpg\" width=\"600\" />\n\nThis is a typical example of dataset collected by an instrument that drifted in the Caribbeans: the track of the instrument and the wave measurements are indicated by the black curve, while satellite measurements of significant wave height (from the Sentinel satellites in particular) are shown on intersecting transsects with colored lines:\n\n![caribbeans_deployment](https://user-images.githubusercontent.com/8382834/155947239-19cf49fe-5649-4f91-b513-fb7ae9e2e0ad.jpg)\n\nUsing the standard box and components, and 3 Li LSH20 batteries, the box floats quite well and works just fine, and will behave quite similar to an iSphere surface drifter for example (note that you will have to add some floatability if you use alkaline batteries, that are much heavier):\n\n![box_no_extra_buoy](https://github.com/jerabaul29/OpenMetBuoy-v2021a/blob/main/instrument_hardware/jpg/floating_3LSH20_box.jpg)\n\nWe also describe how to build a standard, upper ocean drifter buoy, which allows to assemble a complete instrument+buoy setup looking like the following picture (to give a sense of scale, the PVC tube is about 1m long); this will drift more like the 1m depth current:\n\n<img src=\"https://github.com/jerabaul29/OpenMetBuoy-v2021a/blob/main/instrument_hardware/jpg/buoy_with_instrument.jpg\" width=\"600\" />\n\n## Key data\n\nThese are explained in more details in the paper, see in particular Tables 1, 2, 3, and the associated text, for more details.\n\n- total cost of the parts: 562USD (Nov. 2021 price).\n- typical assembly time: around 1-1.5hr per buoy when producing a small series efficiently.\n- cost of the iridium communications: 42USD / month if using only GPS, up to 108USD / month when using GPS and high frequency wave measurements (Nov. 2021 price).\n- battery autonomy: i) 4.6 months using 2 Li D-cells, with GPS and wave measurements activated. ii) Over 1 year using 2 Li D-cells and GPS tracking only. iii) Battery life scales linearly with the number of D-cells included in the instrument.\n- typical detection threshold for waves in ice: 0.5cm at 16s period, even better detection threshold is obtained at higher frequency due to using an IMU to perform the measurements.\n\n## Overview of the content and organisation of the repository\n\n- https://github.com/jerabaul29/OpenMetBuoy-v2021a/tree/main/development_environment : this contains the instructions on how to set up your development environment to write firmwares, either for the \"legacy\" (Arduino 1.8-based), or the \"rewrite\" (PlatformIO-based) firmware.\n- https://github.com/jerabaul29/OpenMetBuoy-v2021a/tree/main/instrument_hardware : this describes the hardware needed, and the parts to source, for building different versions of the instrument, and includes the assembly instructions.\n- https://github.com/jerabaul29/OpenMetBuoy-v2021a/tree/main/legacy_firmware : this contains the legacy firmware (version IDE 1.8), which at the moment is the only one ready to use. It provides both the source code (in different versions), and some pre-compiled binaries with default parameters.\n\n## Coding environment\n\nI am using only Linux in the form of Ubuntu LTS distributions. Though the instructions here can likely be adapted to other distributions / OSes (possibly with the Windows Linux Subsystem or similar, see for example https://github.com/jerabaul29/OpenMetBuoy-v2021a/issues/53 ), if you want to use something else than Ubuntu 20.04 LTS or another modern Ubuntu LTS, you are on your own! Also, some of the PlatformIO building flags rely on using a Linux system. These could be adapted to also work on Windows, but I am not interesed in spending the time to get it to work at the moment on neither Windows not MacOS (but contributions can be welcome).\n\n## About the \"legacy\" vs \"rewrite\" firmwares\n\n### \"Legacy\" instrument firmware\n\nThe coding environment for the \"legacy\" instrument was based on Arduino IDE-1.8 and the SparkFun Ambiq Apollo3 Arduino core, in a slightly older version of the \"bare metal\" core. It is well working, but a bit messy as it was developed \"as things went\". For now, this is the only one that is ready for use (see \"Instrument firmware under rework\" section).\n\n### Instrument firmware under rework\n\nThe \"legacy\" instrument firmware has been developed \"as things were going\". It works well and is robust, but the codebase needs a good rework to clean it. The \"reworked firmware\" is a work in progress, without any guarantees on when it will be done, and is available at: https://github.com/jerabaul29/OpenMetBuoy-v2021a/tree/feat/platformio-tracker and \"related\" branches. It is based on:\n\n- C++ with an Arduino flavor,\n- Visual Studio Code and PlatformIO,\n- the SparkFun Ambiq Apollo3 Arduino core, in \"bare metal\" (i.e. no RTOS), flavor.\n\nSee the corresponding branch for more instructions.\n\nIf you want to develop a \"cousin\" instrument of our own, I would recommend that you base yourself on this development environment.\n\n## A note about other programming languages\n\nC++ has some pros and cons, and it is possible to use other languages for low level programming. You can find a project that i) targets similar MCUs and development boards, ii) uses Rust-Lang as an alternative to C++, at: https://github.com/gauteh/sfy . If you want to develop a new project from scratch, it may be worth considering if you want to use our C++ or the Rust firmware as a starting point :) (NOTE: I see the pont of Rust and I want to transition to it in the - hopefully - not so far future, but for now I am still working in C++, the code there is from a colleague :) ).\n\n## Contribution policy\n\nIn case of any question / bug report, please use the Github issues tracker system. I will very likely **not** answer to private emails, as I want all discussions to be open, become part of the repo and, therefore, the \"meta documentation\" of the project, and serve the community.\n\nAll materials on this repo are released under the MIT license, unless explicitly stated otherwise. Nothing on this repository comes with any warranty. I do not guarantee any form of support.\n\n## Quirks / errors\n\nThis is a large code base, that I largely developed on my own with a very limited time budget, so there are some quirks / errors. I will fix the ones that lead to wrong results, but minor quirks may not be fixed fully. This is to list such quirks.\n\n- **dyslexia frequency vs. period**: the instrument, internally, computes the zero crossing and the mean *frequency*, not *period* as the code says (i.e. there is a relation by ```f(x)=1/x``` between what is named in the code and what is actually computed). This would be error prone to fix in all firmwares, so I choose to i) keep it \"as it is\" in the firmware, and ii) fix this a posteriori in the decoder by taking the inverse there. For more discussions, see: https://github.com/jerabaul29/OpenMetBuoy-v2021a/issues/36\n\n## A few deployments I know about\n\nIf you deploy the OMB, feel free to let me know in an issue! These are so far the deployments I know of (no guarantee this is up to date). This is just to show that we do have solid experience with the OMB, and that we have had many successful depoyments:\n\n- 2 test buoys: NABOS expedition, Arctic, September 2021: 1 in the open water in the MIZ, 1 on ice\n- 1 test drifter: OneOcean cruise, Caribbeans, November 2021: open water, freely drifting\n- 3 ice drifters: Japanese Antarctic program, Antarctica, February 2022: 3 instruments on the sea ice\n- 2 ice drifters: Seal pups monitoring cruise 2022, East Greenland sea, March 2022: on the ice in the MIZ, 2 drifters\n- 20 ice drifters: CIRFA 2022 cruise, East Greenland sea, April 2022: 20 instruments on the sea ice\n- 15 drifting buoys: CIRFA 2022 cruise, Barents sea, April 2022: 15 instruments, drifting in open water\n- 20 ice drifters: AWI summer cruise 2022, Barents sea: 20 instruments, on the sea ice\n- 15 ice trackers: Hovercraft 2022 cruise, Yamal plateau / East Greenland sea, August 2022: 15 instruments on the sea ice\n\n## Just for fun - a not so serious corner of the Readme :)\n\nIf you have more fun / goofy / strange pictures or stories that can fit in here, just share them as an issue or open a pull request!\n\n### Floatenstein, the ugly drifter\n\nA buoy that needed to fly by plane (so Li batteries not allowed) had to be equipped with 3 Alkaline D cells in series instead. Alkaline batteries are heavier than Li batteries, so it was not floating. The solution was to add two chunks of styrofoam, wrapped in duct tape, fixed with cable nilon strips, all of it tightened with bathroom silicon. That made for an ugly, sticky, stinky instrument, but meh, it crossed the Caribbeans without any issue, so don't worry about how your instruments look like - as long as they work! :) .\n\n<img src=\"https://github.com/jerabaul29/OpenMetBuoy-v2021a/blob/main/instrument_hardware/jpg/floatenstein_light.jpg\" width=\"600\" />\n\nPicture credits: me :) .\n\n### The Arctic is tough, even if you're expensive\n\nInstrumentation has a tendency to fail early and \"randomly\" in the Arctic - and harsh field conditions do not care about how much money you cost! This is an example where the telemetry from a Datawell Mk II deployed outside of Longyearbyen, Svalbard, was lost after just 2 hours. We were worried that the mooring had failed, or the buoy had sunk. In reality, due to the conditions (1 deg C water, -15 deg C air and some wind and waves), the Mk II had got iced and tilted. So next time your ocean buoy fails in the Arctic, don't take it too hard on yourself - it is likely not your fault, and it does not matter how much your instrument cost - stick to the OpenMetBuoy.\n\n<img src=\"https://github.com/jerabaul29/OpenMetBuoy-v2021a/blob/main/instrument_hardware/jpg/IMG_1099(1).JPG\" width=\"400\" />\n\nPicture credits Atle Jensen.\n\n### There is nothing as tasty as a free lunch\n\nIf you work in the Arctic, don't use bright colors, blinking LEDs, and chunky flags that make your instruments visible from far away - for a polar bear, there is nothing as tasty as a free lunch, even if it tastes plastic. According with some friends who are experts on Arctic expeditions, polar bears have a particular affinity for round instruments that they can play football with, large instruments they can use as a trampoline, and soft plastic / rubber that they can use to sharpen their teeths. Oh, and cables too! Record to break according to my friend: 250kUSD destruction of scientific instrumentation within 12 minutes...\n\n<img src=\"https://github.com/jerabaul29/OpenMetBuoy-v2021a/blob/main/instrument_hardware/jpg/687px-Polar_bears_chewing_cables.jpg\" width=\"600\" />\n\nPicture credits: this is actually from wikimedia commons, I did not have a good picture lying around: https://commons.wikimedia.org/wiki/File:Polar_bears_chewing_cables.jpg .\n\n### Trashing planet Earth, one step at a time - I feel really bad about this one\n\nMost (all?) of the floating trash we throw in the oceans, ultimately comes back on the shores - or get caught in the Vortex. Also floating scientific instruments. I feel really bad about this, but at least our instruments are much smaller than typical commercial instruments (so, for a given number of measurements, we release less volume and mass of trash, so less trash footprint), and, at least, this is for a good cause (advancing human knowledge and protecting the world oceans), and not a random sacrifice to the pagan god of over-consumption and fast fashion... Still, if you have ideas on how to reduce the environmental footprint of the OMB, open an issue and help us get better :) .\n\nThis picture is actually from one of my \"old, v2018 instruments\", that drifted all the way from the Arctic MIZ North of Svalbard, until it got stranded on the Northen coast of Iceland. Someone taking a walk on the beach was kind enough to open it, read the waterproof notice with our contact information, and let us know it was found. Pretty rusted!\n\n<img src=\"https://github.com/jerabaul29/OpenMetBuoy-v2021a/blob/main/instrument_hardware/jpg/electronics_trash.jpg\" width=\"600\" />\n\nCredits: Picture Finn Plny, sharing Atle Jensen; we are going to pick up the box, and try to get the raw data from the SD card.\n\n### Go big or go home\n\nThe aim of the OpenMetBuoy (OMB) is to increase the number of measurement points you can get at constant budget, NOT to reduce your overall field instrumentation budget! So, go big or go home! This is what you get for the cost of 4 or 5 classical commercial instruments: enough components to build 40 OMBs!\n\n<img src=\"https://github.com/jerabaul29/OpenMetBuoy-v2021a/blob/main/instrument_hardware/jpg/go_big_go_home.jpg\" width=\"600\" />\n\nCredits: Takehiko Nose, preparing to assemble 50 OMBs.\n\n### \"Dolly\" the search-and-rescue doll\n\nThe OMB has also been used to test search and rescue systems - hard time floating alone in the middle of the Norwegian sea! Credits: Forsvarets 330 Skv. Avd Sola.\n\n<img src=\"https://github.com/jerabaul29/OpenMetBuoy-v2021a/blob/main/instrument_hardware/jpg/search_and_rescue_doll\" width=\"600\" />\n"
 },
 {
  "repo": "gauteh/sfy",
  "language": "Rust",
  "readme_contents": "[![sfy-data](https://github.com/gauteh/sfy/actions/workflows/sfy-data.yml/badge.svg)](https://github.com/gauteh/sfy/actions/workflows/sfy-data.yml)\n[![sfy-buoy](https://github.com/gauteh/sfy/actions/workflows/sfy-buoy.yml/badge.svg)](https://github.com/gauteh/sfy/actions/workflows/sfy-buoy.yml)\n[![sfy-processing](https://github.com/gauteh/sfy/actions/workflows/sfy-processing.yml/badge.svg)](https://github.com/gauteh/sfy/actions/workflows/sfy-processing.yml)\n\n# The small friendly buoy\n\n* [sfy-buoy](sfy-buoy/) - the firmware for the buoy.\n* [hardware](hardware/Hardware.md) | [build-tutorial](https://www.hackster.io/gaute-hope/ocean-buoy-to-measure-waves-drift-using-low-power-cellular-16ad09) | [bill-of-materials](https://docs.google.com/spreadsheets/d/e/2PACX-1vRE62P6-pCVzig-hSsqVcr2DABZ5LlB4lt1ZFfrct_tdcxoljO3zjmq7vGT1-jjqNiVCXLdns6XSkHF/pubhtml?gid=0&single=true) - hardware and assembly instructions.\n* [sfy-data](sfy-data/) - the server scraping or receiving data from deployed\n    buoys.\n* [sfy-processing](sfy-processing/) - python libraries and tools for reading and post-processing received data.\n* [sfy-dashboard](sfy-dashboard/) - web interface for displaying latest position\n    and overview of buoys.\n\nThe buoys deployed in the surf on the coast of Norway:\n\n[![Wave buoys in the surf zone at J\u00e6ren](http://img.youtube.com/vi/qK1Di7pjYFI/0.jpg)](http://www.youtube.com/watch?v=qK1Di7pjYFI \"Wave buoys in the surf zone at J\u00e6ren\")\n\n# Acknowledgements\n\nThis work is based on the [OpenMetBuoy-v2021a](https://github.com/jerabaul29/OpenMetBuoy-v2021a), see [Rabault et. al. (2022)](https://www.mdpi.com/2076-3263/12/3/110).\n"
 },
 {
  "repo": "JuliaOcean/JuliaOceanSciencesMeeting2020",
  "language": null,
  "readme_contents": "# Julia Ocean Sciences Meeting 2020 workshop\n\n**Post OSM20 update :**\n\n- **video recording of the workshop session** is [available here](https://youtu.be/xOeB-uwoMI0)\n- [workshop](https://agu.confex.com/agu/osm20/meetingapp.cgi/Session/92105) title : Julia (language) users and tools for oceanography\n- time : **took place on** Tuesday, 18 February 2020 @ 12:45 - 13:45\n- abstract:\n\nThere has been a visible uptick in oceanography and climate applications of the Julia language since it reached the v1.0 milestone last year. The growth and appeal for this language were recently highlighted by Nature magazine. It seems very timely to offer this rapidly growing community of open source developers and users an opportunity to meet in person, advertise their recent efforts, and engage with the oceanographic community at large. A tentative agenda for this workshop would include: a brief general presentation of Julia, a survey of existing efforts, a hackathon-type session with both experienced and new users interacting, and open-ended discussion time.\n\n## Workshop outline\n\n_below is the current, still evolving, workshop outline_\n\n### intro (G.F. 15')\n\n- workshop outline and motivation\n- why should you want to use Julia?\n- how can you get started with Julia?\n\t- [julialan.org](https://julialang.org)\n- what are Julia's salient features?\n- state of the ocean & climate stack?\n\n### state of the Julia ocean stack (10 x 2' contributions)\n\n\nAs for almost all open source efforts, listed packages should be regarded as collaborative & ongoing development projects. They are under version control, documented as much as possible, and can generally be installed via `Julia`'s awesome [package manager](https://julialang.github.io/Pkg.jl/v1/).\n\n_The list is a work in progress. Also categories 1. 2. and 3. below are approximate -- some packages arguably belong in 1. 2. and 3._\n\n1. plotting, I/O, and data analysis packages\n\t- [Plots.jl](http://docs.juliaplots.org/latest/), [PyPlot.jl](https://github.com/JuliaPy/PyPlot.jl), [Makie.jl](http://makie.juliaplots.org/stable), ... (volunteer ?)\n\t- [NetCDF.jl](https://juliageo.org/NetCDF.jl/dev/), [NCTiles.jl](https://github.com/gaelforget/NCTiles.jl), [Zarr.jl](https://meggart.github.io/Zarr.jl/latest/), [CSV.jl](https://juliadata.github.io/CSV.jl/stable/), ... (volunteer ?)\n\t- [DataFrames.jl](http://juliadata.github.io/DataFrames.jl/stable/), [ClimateTools.jl](https://juliaclimate.github.io/ClimateTools.jl/stable/), [ESDL.jl](https://github.com/esa-esdl/ESDL.jl), ... (volunteer ?)\n\n2. common ocean models, analysis tools, and data sets\n\t- [GlobalOceanNotebooks](https://github.com/JuliaClimate/GlobalOceanNotebooks), [MeshArrays.jl](https://juliaclimate.github.io/MeshArrays.jl/dev/), [IndividualDisplacements.jl](https://juliaclimate.github.io/IndividualDisplacements.jl/dev/), [ArgoData.jl](https://gaelforget.github.io/ArgoData.jl/dev/), [ClimateTasks.jl](https://gaelforget.github.io/ClimateTasks.jl/dev/), ... (@gaelforget)\n\t- [AIBECS.jl](https://briochemc.github.io/AIBECS.jl/stable/), [WorldOceanAtlasTools.jl](https://github.com/briochemc/WorldOceanAtlasTools.jl), [OceanographyCruises.jl](https://github.com/briochemc/OceanographyCruises.jl), ... (@briochemc)\n\t- [NCDatasets.jl](https://alexander-barth.github.io/NCDatasets.jl/dev/), [DIVAnd.jl](https://gher-ulg.github.io/DIVAnd.jl/latest/), [PhysOcean.jl](https://github.com/gher-ulg/PhysOcean.jl) (@Alexander-Barth, to be confirmed)\n\n3. models written in Julia\n\n\t- [ShallowWaters.jl](https://github.com/milankl/ShallowWaters.jl) (@milankl)\n\t- [Oceananigans.jl](https://github.com/climate-machine/Oceananigans.jl) (@ali-ramadan or @glwagner)\n\t- [geophysicalflows.jl](https://github.com/FourierFlows/GeophysicalFlows.jl) (@navidcy or @glwagner)\n\n_New to github etc? See e.g. [these guides](https://guides.github.com) and `Lecture03` in [these tutorials](https://github.com/PraCTES/MIT-PraCTES); it's all user-friendly._\n\n### Q & A (20')\n\nPlease feel free to add questions ahead of, during, and after the workshop in [this thread](https://github.com/gaelforget/JuliaOceanSciencesMeeting2020/issues/4) for example.\n\n### looking forward (G.F. 5')\n\n- this repo; during & after OSM020\n- github organizations & contributors\n- JuliaClimate and JuliaOcean organizations\n"
 },
 {
  "repo": "TEOS-10/GibbsSeaWater.jl",
  "language": "Julia",
  "readme_contents": "[![Build Status](https://github.com/TEOS-10/GibbsSeaWater.jl/workflows/CI/badge.svg)](https://github.com/TEOS-10/GibbsSeaWater.jl/actions)\n[![Build status Windows](https://ci.appveyor.com/api/projects/status/77kj4lug424x20y9/branch/master?svg=true)](https://ci.appveyor.com/project/Alexander-Barth/gibbsseawater-jl-ojx2d/branch/master)\n[![codecov.io](http://codecov.io/github/TEOS-10/GibbsSeaWater.jl/coverage.svg?branch=master)](http://codecov.io/github/TEOS-10/GibbsSeaWater.jl?branch=master)\n\n\n# GibbsSeaWater.jl\n\nGibbsSeaWater.jl is a Julia wrapper for [GSW-C#master](https://github.com/TEOS-10/GSW-C/), which is the C implementation of the Thermodynamic Equation of Seawater 2010 (TEOS-10).\n\n## Installation\n\nStart Julia and issue the following commands:\n\n```julia\nusing Pkg\nPkg.add(\"GibbsSeaWater\")\n```\n\n## Example\n\nFor arrays, one should use the [vectorized \"dot\" operator](https://docs.julialang.org/en/v1/manual/mathematical-operations/#man-dot-operators-1):\n\n```julia\nC = [45.8;34.7]\nT = [28.9;22.8]\nP = [10.0;50.0]\nSP = gsw_sp_from_c.(C,T,P)\n```\n\n## About TEOS-10\n\nPlease check the [official site](http://www.teos-10.org) and [official repository](https://github.com/TEOS-10), which provide the official implementations (C/Fortran/Matlab/PHP) and the wrappers.\n"
 },
 {
  "repo": "aist-oceanworks/mudrod",
  "language": "Java",
  "readme_contents": "# MUDROD\n## Mining and Utilizing Dataset\u00a0Relevancy from Oceanographic Datasets to Improve Data Discovery and Access\n\nMUDROD now lives at the Apache Software Foundation under the Apache Science Daat Analytics Platform (SDAP) Incubating project.\n\nYou can access the SDAP Website at https://sdap.apache.org\n\nThe SDAP Source Code lives at https://github.com/apache/incubator-sdap-mudrod\n \n# License\nThis source code is licensed under the [Apache License v2.0](http://www.apache.org/licenses/LICENSE-2.0), a\ncopy of which is shipped with this project. \n"
 },
 {
  "repo": "obidam/pcm",
  "language": "Jupyter Notebook",
  "readme_contents": "Profile Classification Modelling (PCM)\n======================================\n[![DOI](https://img.shields.io/badge/DOI--Article-10.1016%2Fj.pocean.2016.12.008-orange.svg)](http://dx.doi.org/10.1016/j.pocean.2016.12.008)  \n\n[![license](https://img.shields.io/github/license/mashape/apistatus.svg)]()\n\n**Profile Classification Modelling** is a scientific analysis approach based on vertical profiles classification that can be used in a variety of oceanographic problems (front detection, water mass identification, natural region contouring, reference profile selection for validation, etc ...).  \nIt is being developed at Ifremer/LOPS in collaboration with IMT Atlantique since 2015, and has become mature enough (with publication and communications) to be distributed and made publicly available for continuous improvements with a community development.\n\n**Ocean dynamics** and its 3-dimensional structure and variability is so complex that it is very difficult to develop objective and efficient diagnostics of horizontally and vertically coherent oceanic patterns. However, identifying such **patterns** is crucial to the understanding of interior mechanisms as, for instance, the integrand giving rise to Global Ocean Indicators (e.g. heat content and sea level rise). We believe that, by using state of the art **machine learning** algorithms and by building on the increasing availability of ever-larger **in situ and numerical model datasets**, we can address this challenge in a way that was simply not possible a few years ago. Following this approach, **Profile Classification Modelling** focuses on the smart identification of vertically coherent patterns and their spatial distribution.\n\n*References*: \n\n- Maze, G., et al. Coherent heat patterns revealed by unsupervised classification of Argo temperature profiles in the North Atlantic Ocean. *Progress in Oceanography*, 151, 275-292 (2017)  \n    [http://dx.doi.org/10.1016/j.pocean.2016.12.008](http://dx.doi.org/10.1016/j.pocean.2016.12.008)\n- Maze, G., et al. Profile Classification Models. *Mercator Ocean Journal*, 55, 48-56 (2017).   \n    [http://archimer.ifremer.fr/doc/00387/49816](http://archimer.ifremer.fr/doc/00387/49816)\n- Maze, G. A Profile Classification Model from North-Atlantic Argo temperature data. *SEANOE Sea scientific open data edition*.  \n    [http://doi.org/10.17882/47106](http://doi.org/10.17882/47106)\n\n## Python package\n[![Python 2.7](https://img.shields.io/badge/python-2.7-blue.svg)](https://www.python.org/downloads/release/python-270/) [![](https://img.shields.io/badge/xarray-0.10.0-blue.svg)](http://xarray.pydata.org/en/stable/) \n\nWe are currently developing a Python package to work with PCM easily.  \n**You can check out the first release on the [pyXpcm](https://github.com/obidam/pyxpcm) homepage.**  \nOtherwise you can still [look at a\nclassic PCM workflow on this notebook](https://github.com/obidam/pcm/blob/master/python/PCM-workflow-classic-demo.ipynb).\n\n## Matlab toolbox\n[![DOI](https://img.shields.io/badge/DOI--Matlab-10.5281%2Fzenodo.400018-orange.svg)](http://dx.doi.org/10.5281/zenodo.400018) [![](https://img.shields.io/badge/matlab->R2016b-blue.svg)]()  \n\nThis is the original code for PCM used in [Maze et al (2017)](http://dx.doi.org/10.1016/j.pocean.2016.12.008).  \n\n[You can get started with the Matlab toolobx following this wiki page](https://github.com/obidam/pcm/blob/master/matlab/README.md)\n\nNote that @gmaze will provide help to use it, but won't maintain the code. Contact us if you want to contribute !\n\n"
 },
 {
  "repo": "FinalTheory/oceanography-numerical-calculations",
  "language": "HTML",
  "readme_contents": "Calculation of Marine and Hydrologic Factors\n============================================\n\n\n\n## \u4e00\u3001[\u6f6e\u6c50\u7684\u6700\u5c0f\u4e8c\u4e58\u8c03\u548c\u5206\u6790](http://nbviewer.jupyter.org/github/FinalTheory/oceanography-numerical-calculations/blob/master/Harmonic%20Analysis/HomeWork1.ipynb)\n\n- \u57fa\u4e8eFortran\u4e0ePython\u7684\u6df7\u5408\u7f16\u7a0b\n- \u4f7f\u7528Numpy\u7684Array\u5bf9\u8c61\u4f5c\u4e3a\u901a\u7528\u7684\u6570\u636e\u5b58\u50a8\u5bb9\u5668\n- \u5728IPython Notebook\u4e0a\u5b8c\u6210\u57fa\u672c\u7a0b\u5e8f\u7684\u7f16\u5199\u548c\u7f16\u8bd1\n- \u6700\u540e\u8f93\u51fa\u4e3aipynb\u6587\u4ef6\uff0c\u65b9\u4fbf\u5404\u79cd\u5171\u4eab\u4ee5\u53ca\u91cd\u73b0\u7ed3\u679c\n- \u6240\u6709\u6d89\u53ca\u7684\u6570\u636e\u6587\u4ef6\u548c\u4ee3\u7801\u5747\u53ef\u4ee5\u4ece\u8be5Repo\u4e2d\u83b7\u53d6\uff0c\u4e5f\u53ef\u4ee5\u76f4\u63a5\u6253\u5305\u4e0b\u8f7d\n\n\n## \u4e8c\u3001[\u5317\u592a\u5e73\u6d0b\u6d77\u533aSST\u6570\u636e\u7684EOF\u5206\u89e3\u4ee5\u53ca\u5206\u6790](http://nbviewer.jupyter.org/github/FinalTheory/oceanography-numerical-calculations/blob/master/EOF%20Analysis/HomeWork2.ipynb)\n\n- \u4f7f\u7528Basemap\u7ed8\u56fe\u5e93\u5b8c\u6210\u4f5c\u56fe\n- \u5206\u6790\u5199\u5f97\u76f8\u5f53\u6c34\uff0c\u8fd8\u8bf7\u89c1\u8c05\n- \u4f7f\u7528eofs\u5de5\u5177\u8fdb\u884cEOF\u5206\u89e3\uff0c\u6548\u7387\u66f4\u9ad8\uff1b\u5728\u505a\u5927\u89c4\u6a21\u8ba1\u7b97\u65f6\uff0c\u76f8\u5bf9\u4e0ematlab\u7a0b\u5e8f\u53ef\u4ee5\u51cf\u5c11\u4e00\u534a\u7684\u5185\u5b58\u5360\u7528\uff0c\u8282\u7701\u5341\u500d\u7684\u65f6\u95f4\n\n## \u4e09\u3001\u4e8c\u7ef4\u6f6e\u6ce2\u6570\u503c\u6a21\u62df\u7a0b\u5e8f\n\n- \u4f7f\u7528Fortran\u8bed\u8a00\u5b8c\u6210\u547d\u4ee4\u884c\u8fd0\u7b97\u7a0b\u5e8f\u7684\u7f16\u5199\n- \u8be5\u8fd0\u7b97\u6838\u5fc3\u63a5\u53d717\u4e2a\u8f93\u5165\u53c2\u6570\uff0c\u7528\u4ee5\u51b3\u5b9a\u662f\u5426\u5ffd\u7565\u8fd0\u52a8\u65b9\u7a0b\u4e2d\u7684\u67d0\u4e9b\u9879\uff0c\u4ee5\u53ca\u5404\u4e2a\u7cfb\u6570\u7684\u53d6\u503c\u7b49\n- \u4f7f\u7528Python\u7684Tkinter\u6a21\u5757\u7f16\u5199\u56fe\u5f62\u754c\u9762\uff0c\u7528numpy\u8bfb\u53d6\u5e76\u5904\u7406\u6570\u636e\uff0c\u5e76\u4e0eMatPlotLib\u8fdb\u884c\u6574\u5408\u4ee5\u5b9e\u73b0\u7ed8\u56fe\u3001\u52a8\u753b\u7b49\u529f\u80fd\n- \u8fd0\u884c\u6548\u679c\u5982\u4e0b\uff1a\n<img src=\"https://github.com/FinalTheory/oceanography-numerical-calculations/raw/master/Numerical%20Simulation/demonstration.gif\" width=1130>\n\n## \u56db\u3001\u4e8c\u7ef4\u6f6e\u6ce2\u4f34\u968f\u540c\u5316\u6a21\u578b\n"
 },
 {
  "repo": "TEOS-10/GSW-Matlab",
  "language": "MATLAB",
  "readme_contents": "# GSW-Matlab (Code only)\nGibbs-SeaWater (GSW) Oceanographic Toolbox in Matlab - Code only !!!\n\nThis is a work in progress repository, thie files contained in this repository are only the code file (.m files)  contained in GSW(Matlab). It is intended to be a method of assisting those who are translating the code into their prefered language.\n\nDo not download this code and treat it as an distributed release of the GSW code - it is not under any circumstance.\n\nThe ONLY location to download software is from the TEOS-10 website http://www.TEOS-10.org/\n\nPaul and Trevor.\n"
 },
 {
  "repo": "pyoceans/pocean-core",
  "language": "Python",
  "readme_contents": "# \ud83c\udf10 pocean-core\n\n[![Build Status](https://travis-ci.org/pyoceans/pocean-core.svg?branch=master)](https://travis-ci.org/pyoceans/pocean-core)\n[![Build status](https://ci.appveyor.com/api/projects/status/gds2iavceg5unj0a?svg=true)](https://ci.appveyor.com/project/ocefpaf/pocean-core)\n[![license](https://img.shields.io/github/license/pyoceans/pocean-core.svg)](https://github.com/pyoceans/pocean-core/blob/master/LICENSE.txt)\n[![GitHub release](https://img.shields.io/github/release/pyoceans/pocean-core/all.svg)\n\n\n\ud83d\udc0d + \ud83c\udf0a\n\nA python framework for working with met-ocean data\n\n## Resources\n+ **Documentation:** <https://pyoceans.github.io/pocean-core/docs/>\n+ **API:** <https://pyoceans.github.io/pocean-core/docs/api/pocean.html>\n+ **Source Code:** <https://github.com/pyoceans/pocean-core/>\n+ **Git clone URL:** <https://github.com/pyoceans/pocean-core.git>\n"
 },
 {
  "repo": "boshek/rsoi",
  "language": "R",
  "readme_contents": "---\noutput: github_document\n---\n\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n```{r, echo = FALSE}\nknitr::opts_chunk$set(\n  collapse = TRUE,\n  comment = \"#>\",\n  fig.path = \"man/figures/\", \n  warning = FALSE\n)\n```\n\n\n\n[![License: GPL v3](https://img.shields.io/badge/License-GPL%20v3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0) \n[![R-CMD-check](https://github.com/boshek/rsoi/workflows/R-CMD-check/badge.svg)](https://github.com/boshek/rsoi/actions)\n\n[![CRAN\\_Status\\_Badge](https://www.r-pkg.org/badges/version/rsoi)](https://cran.r-project.org/package=rsoi) [![CRAN Downloads](https://cranlogs.r-pkg.org/badges/rsoi?color=brightgreen)](https://CRAN.R-project.org/package=rsoi) [![cran checks](https://cranchecks.info/badges/worst/rsoi)](https://cran.r-project.org/web/checks/check_results_rsoi.html)\n\n\n# rsoi\nAn R package to download the most up to date of these climate indices:\n\n- Southern Oscillation Index\n- Oceanic Nino Index \n- North Pacific Gyre Oscillation\n- North Atlantic Oscillation\n- Arctic Oscillation\n- Antarctic Oscillation\n- Multivariate ENSO Index Version 2\n- Pacific Decadal Oscillation\n- Dipole Mode Index\n\n## Installation\nFor the development version \n\n```{r, eval = FALSE, echo = TRUE}\ninstall.packages(\"rsoi\")\n\nlibrary(rsoi)\nlibrary(tibble)\n```\n\n```{r, eval = TRUE, echo = FALSE}\nlibrary(rsoi)\nlibrary(tibble)\n```\n\n## Usage\nDownload Oceanic Nino Index data\n```{r, eval = TRUE, echo = TRUE, warning=FALSE, message=FALSE}\noni <- download_oni()\nhead(oni)\n```\n\nAnd a quick plot to illustrate the data:\n```{r plot, eval = TRUE, echo = TRUE, warning=FALSE, message=FALSE}\nbarcols <- c('#edf8b1','#7fcdbb','#2c7fb8')\n\nbarplot(oni$ONI, names.arg = oni$Date, ylab = \"Oceanic Nino Index\" , \n    col = barcols[oni$phase], border = NA, space = 0,\n    xaxt = \"n\")\n```\n\n## Inspired by\nThe idea for this package borrows heavily from the `rpdo` package. `rsoi` now supercedes `rpdo` as a source of data in R for Pacific Decadal Oscillation. \n\n\n"
 },
 {
  "repo": "mjharriso/MIDAS",
  "language": "Python",
  "readme_contents": "[![Build Status](https://travis-ci.org/mjharriso/MIDAS.svg?branch=master)](https://travis-ci.org/mjharriso/MIDAS)\n\n# DESCRIPTION\n\n MIDAS (MIDAS Midas is Data Analysis Software)\n is a Python package primarily for processing\n gridded data stored in CF-compliant NetCDF/HDF5 format\n (http://cfconventions.org).\n\n * spatial interpolation between quadrilateral meshes\n * temporal interpolation between calendar dates (datetime)\n * conservative re-mapping in the vertical dimension (MOM6/ALE)\n * spatial integration/averaging with generalized horizontal/vertical coordinates\n * temporal averaging (Datetime).\n\n MIDAS was first developed by Matt Harrison as an employee of NOAA/GFDL\n and has been used for the generation of realistic global model configurations\n and post-run analysis scripts (https://github.com/NOAA-GFDL/MOM6-examples.git)\n\n\n This work is licensed under the Creative Commons\n Attribution-NonCommercial-ShareAlike 3.0 Unported License.\n To view a copy of this license, visit\n http://creativecommons.org/licenses/by-nc-sa/3.0/\n or send a letter to Creative Commons, 444 Castro Street,\n Suite 900, Mountain View, California, 94041, USA.\n\n# SIMPLE INSTALL (reduced functionality)\n\nIf you just want the basics, and are not planning to use the remapping features, then simply type (bash)\n\n```\nINSTALL_PATH='/your/python/module/path'\npython setup_nofort.py install --prefix=$INSTALL_PATH\n\n```\n\nIf you have root access and want to use default installation paths, then omit the prefix option.  If you are using conda, then the default location is specific to your current environment.\n\n\n# CONDA INSTALLATION\n\n1. Download and install Anaconda\n\n```\n(wget https://repo.anaconda.com/archive/Anaconda3-5.1.0-Linux-x86_64.sh;./Anaconda3-5.1.0-Linux-x86_64.sh)\n```\n\n2. Update your existing shell to add conda to your path\n\n```\nsource ~/.bashrc\n```\n\n3. Now update conda\n\n```\nconda update conda\nconda install conda-build\n```\n\n4. Install gfortran development libraries (if needed)\n\n```\nsudo apt-get install libgfortran-6-dev\n```\n\noptionally install mpich2 and associated libraries\n\n```\n(sudo apt-get install libmpich2-dev)\n```\n\nOr else if you do not have root privileges\n\n```\n(. activate;wget http://www.mpich.org/static/downloads/3.2.1/mpich-3.2.1.tar.gz;cd Downloads;tar xvf mpich-3.2.1.tar.gz;cd mpich-?3.2.1;./configure --enable-sha\\\nred --prefix=$CONDA_PREFIX;make; make install)\n```\n\n\n# MIDAS INSTALLATION\n\nOn linux platforms, simply type\n\n```\nmake\n```\n\nThis will take some time, since the Makefile will be downloading and compiling several large packages, like hdf5 and netcdf. Why are we compiling everything when there are pre-compiled binaries avaialble from the Anaconda cloud? Problems arise when linking c and c++ libraries to fortran APIs. There are often strict requirements for matching versions and glibc compatability which make using pre-compiled code unfeasible.\n\nFor platforms with multiple users, it is recommended that the compiled libraries and Python packages be made available through a local channel.\n\n# MIDAS STEP-BY-STEP INSTALLATION\n\nFor best results, build the following libraries yourself - conda does not handle dependencies for linking c and c++ libraries to fortran APIs - consider yourself lucky if you can work with pre-compiled packages and associated libraries\n\n```\n(conda create --name MIDAS)\ngit clone git@github.com:MJHarrison-GFDL/conda-recipes.git\n(. activate MIDAS; cd conda-recipes/zlib;conda build .;conda install --use-local zlib)\n(. activate MIDAS; cd conda-recipes/hdf5;conda build .;conda install --use-local hdf5)\n(. activate MIDAS; cd conda-recipes/libnetcdf;conda build .;conda install --use-local libnetcdf)\n(. activate MIDAS; cd conda-recipes/libnetcdff;conda build .;conda install --use-local libnetcdff)\n```\n\nInstall the netCDF4 python API\n\n```\n(. activate MIDAS;pip install netCDF4)\n```\n\nInstall MIDAS\n\n```\ngit clone git@github.com:mjharriso/MIDAS.git\n(. activate MIDAS; cd MIDAS;git checkout master;. build.sh)\n```\n\n**TROUBLESHOOTING**\n\nif you have a problem with libmkl missing:\n\n```\n(. activate MIDAS; conda install nomkl numpy scipy scikit-learn numexpr)\n(. activate MIDAS; conda remove mkl mkl-service)\n```\n\nmissing libcomm_err.so.3 at runtime?\n\n```\n(ln -s $CONDA_PREFIX/pkgs/krb5-1.14.6-0/lib/libcom_err.so.3 $CONDA_PREFIX/lib/.)\n```\n\n\n**EXAMPLES**\n\n```\ncd examples\nsource activate MIDAS\npython contour_example.py # Fetches OpenDAP URL from NODC and plots with Matplotlib\npython hinterp_example.py # fms/horiz_interp does bi-linear interpolation from the original\n                 \t  # 1-deg to a 5-deg grid with masking\npython hist.py            # volume-weighted histogram of salinity in the Indian Ocean\npython subtile.py         # use subtiling algorithm to calculate un-weighted\n \t       \t\t  # cell average bathymetry and roughness. Example along the Eastern US\nsource deactivate\n```\n\n**HOW TO OBTAIN DOCUMENTATION**\n\n```\nipython\nimport midas.rectgrid as rectgrid\nrectgrid.[Tab]   # complete listing of methods\nrectgrid.quadmesh       # a generic rectangular grid description\n\t\t\t\t    # Can be read from a file or provided as\n\t\t\t\t    # 2-d lat/lon position arrays.\nrectgrid.state?  # Description of state instance\nrectgrid.state.state.[Tab] # available methods for state objects\nrectgrid.state.volume_integral?  # integrate scalars over the domain\n\t\t\t\t     # in 'X','Y','Z','XY' or 'XYZ'\nrectgrid.state?? # View the code\n```\n\n**MORE INFORMATION**\n\ntype\n\n```\n(. activate MIDAS;conda list)\n```\n\nthis returns your current environment which should look something like this:\n\n```\n# Name                    Version                   Build  Channel\nca-certificates           2018.4.16                     0    conda-forge\ncurl                      7.60.0                        0    conda-forge\nhdf5                      1.8.20                        0    local\nkrb5                      1.14.6                        0    conda-forge\nlibnetcdf                 4.4.1                         0    local\nlibnetcdff                4.4.4                         0    local\nlibssh2                   1.8.0                         2    conda-forge\nopenssl                   1.0.2o                        0    conda-forge\nzlib                      1.2.11                        1    local\n\n```"
 },
 {
  "repo": "AtlanticR/bioRworkshops",
  "language": "R",
  "readme_contents": "# Bedford Institute of Oceanography R workshops\n\nThis repository collects materials related to a series of R workshops to be run regularly at the Bedford Institute of Oceanography. While meant to be introductory, the workshops will in general be tailored toward the use of R (and other tools) in the ocean sciences. \n\n## 01_git\n\nAn overview of the [Git](www.git-scm.com) source control system, and why you should use it (to write R code).\n\n## 02_Rbasics\n\nSome basics of the R language, including general syntax, data types, objects and classes, package system overview, and how to get help. \"Super Noob\" workshop given on 2017-10-20 and (mostly) repeated on 2017-11-03.\n\n## 03_Datawrangling\n\nA \"Noob\" workshop, giving many of the details of doing complex data wrangling, given on 2017-10-27.\n\n## 04_readingPlottingAnalysis\n\nThe basis for the \"super noob\" workshop, on 2017-11-17 -- reading data (e.g. csv/spreadsheet), working with the data/columns, making plots, and doing some analysis (fitting models)\n\n## 05_plottingBasics\n\nHow to make plots using *base* graphics in R (i.e. not `ggplot2`, `lattice`, etc). \n\n## 06_datesAndTimes\n\nDealing with dates and times (yay, POSIX!) in R\n\n## 07_groupReinvigoration\n\nThe R noob group has been re-kicked off, starting in 2019! Look at all the fun topics we can cover!\n\n## 08_workflowOrganization\n\nHow to organize your work, use RStudio projects, etc\n\n## 09_intro_ocean_data\n\nSome basics of the Rstats `oce` package for analyzing *real* ocean data\n\n## 10_using_git\n\nA re-visit of the crowd-pleaser, Git! Essential for anyone who bashes on keyboards.\n\n## 11_DataManipulation_dplyr\n\nIntroduction to the grammar of data manipulation using `dplyr`. Test application with the rvdata. \n\n## 12_R_Mapping\n\nIntroduction to mapping in R. \n\n## 16_R_dashboards \n\nOutcomes of the Interactive Tools Workshop \n"
 },
 {
  "repo": "PlanktoScope/PlanktoScope",
  "language": "HTML",
  "readme_contents": "# An open and affordable imaging platform for citizen oceanography\n\n![PlanktoScope Render](docs/readme/planktoscope_cad.webp)\n\n![Plankton collage](docs/readme/plankton_collage.webp)\n\n\n# What is this?\nThe PlanktoScope is an open-source, affordable imaging platform for citizen oceanography. It's built around a Raspberry Pi, a couple of HATs, some stepper motors and a few centimeters of silicon tubes. Its cost is at about $500 in parts.\n\nThe goal of the PlanktoScope is to allow citizen to engage in scientific programs, either at sea or onshore. You can use the PlanktoScope to image the different species of Plankton living in a body of water.\n\n\n## Get the papers!\nThe PlanktoScope has been described in a paper available on the [bioRxiv preprint server](https://www.biorxiv.org/content/10.1101/2020.04.23.056978v1). The first results of this program and its outline are also available as a [preprint](https://www.biorxiv.org/content/10.1101/2020.08.31.263442v1).\n\n|[![PlanktoScope Preprint](docs/readme/planktoscope_pub.webp)](https://www.biorxiv.org/content/10.1101/2020.04.23.056978v1)|[![PlanktonPlanet Preprint](docs/readme/planktonplanet_pub.webp)](https://www.biorxiv.org/content/10.1101/2020.08.31.263442v1)|\n|--------|--------|\n\n\n# Key Features\n- Image small animals and algae living in water\n- Focus stage control\n- Pump control\n- Automatic image capture\n- Automatic segmentation\n\n# How do I build one?\nYou can access the complete documentation on [Read The Docs](https://planktonscope.readthedocs.io/).\n\n# How do I get involved?\nThere are several ways to join the development effort, share your progress with your build or just ask for help.\n\nWe are using slack as a communication platform between interested parties. You can [request to join by filling this form](https://docs.google.com/forms/d/e/1FAIpQLSfcod-avpzWVmWj42_hW1v2mMSHm0DAGXHxVECFig2dnKHxGQ/viewform).\n\nThis repository is also a good way to get involved. Please fill in an issue if you witnessed a bug in the software or hardware. If you are able, you can also join the development effort. Look through the [issues opened](https://github.com/PlanktonPlanet/PlanktoScope/labels/good%20first%20issue) and choose one that piques your interest. Let us know you want to work on it in the comments, we may even be able to guide your beginnings around the code.\n\n# License: Our work is fully open source\n\nDepending on the material they cover, different licenses apply to the files of this repository.\n\nIf you make any change for your use, please fork this repository and publish your improvements. It will help others, and we can integrate those changes in this repository if needed.\n\nAlso, all licenses are contaminating, if you want to use any of this material for a project that you cannot open-source, please contact us using Slack so we can see what can be done to help you and your project.\n\n## Hardware files\nAll hardware files and documentation (everything in the `hardware` directory) is released under a [CERN-OHL-S-2.0 license](https://ohwr.org/cern_ohl_s_v2.txt).\n\n## Software source\nThe source code (everything in the directories `flows` and `scripts`) is released under a [GPL-3.0 license](https://www.gnu.org/licenses/gpl-3.0.en.html).\n\n## Everything else (documentation, pictures, etc...)\nEverything else is released under a [Creative Commons CC-BY-SA license](https://creativecommons.org/licenses/by-sa/4.0/)."
 },
 {
  "repo": "Hunter-Github/GitScience",
  "language": null,
  "readme_contents": "# GitScience\nA list of science- and engineering related repositories on GitHub and in neighboring counties\n\n## Contents\n\n#### Ocean\n\n * [Acoustics (underwater)](https://github.com/Hunter-Github/GitScience#acoustics-underwater)\n * [Oceanography in general](https://github.com/Hunter-Github/GitScience#oceanography-general)\n\n#### Earth\n\n* [Seismology](https://github.com/Hunter-Github/GitScience#seismology)\n\n#### Air\n\n * [Meteorology](https://github.com/Hunter-Github/GitScience#meteorology)\n\n#### Space\n\n * [Orbital mechanics](https://github.com/Hunter-Github/GitScience#orbital-mechanics)\n * [Astronomy](https://github.com/Hunter-Github/GitScience#astronomy)\n\n#### Technology\n\n * [Radio](https://github.com/Hunter-Github/GitScience#radio)\n * [Optics](https://github.com/Hunter-Github/GitScience#optics)\n * [Nuclear technology](https://github.com/Hunter-Github/GitScience#nuclear-technology)\n * [Chemistry](https://github.com/Hunter-Github/GitScience#chemistry)\n\n### Acoustics (underwater)\n\n* [Acoustic Toolbox](http://oalib.hlsresearch.com/Modes/AcousticsToolbox/) MatLab, Fortran\n    * ... [ACT](http://cmst.curtin.edu.au/products/actoolbox.cfm) - GUI for the Acoustic Toolbox\n* [cTraceo](https://github.com/EyNuel/cTraceo) Does not require MatLab\n\n### Oceanography (general)\n\n * [OCE for R](https://github.com/dankelley/oce) - a smorgasbord of useful commands to process oceanographic datasets\n\n### Radio\n\n* SPLAT!\n    * ... [Signal-Server](https://github.com/Cloud-RF/Signal-Server)\n    * ... [splat](https://github.com/jmcmellen/splat)\n* VOACAP\n    * ... [voacapl](https://github.com/jawatson/voacapl) - a port to Linux\n* ITU-R models\n    * ... [propagation](https://github.com/deepaknadig/propagation)\n* [QRadioPredict](http://qradiopredict.sourceforge.net/)\n\n### Orbital mechanics\n\n* [NASA's GMAT](http://gmat.sourceforge.net/)\n* [SGP4 and SDP4 ported to Javascript](https://github.com/shashwatak/satellite-js) - runs nicely on any system with a browser or with a Node.js engine.\n* [SPICE](http://naif.jpl.nasa.gov/naif/toolkit.html) - many routines to use in space operations and astronomy.\n    * ... [in C](http://naif.jpl.nasa.gov/naif/toolkit_C.html)\n    * ... [in Fortran](http://naif.jpl.nasa.gov/naif/toolkit_FORTRAN.html)\n    * ... [bindings for Python (PySPICE)](https://github.com/rca/PySPICE)\n* [Orekit](http://orekit.org/)\n* [benelsen/spacetrack SpaceTrack API](https://github.com/benelsen/spacetrack) - a Node.js wrapper for [SpaceTrack](https://www.space-track.org) (USSTRATCOM) [TLE](https://en.wikipedia.org/wiki/Two-line_element_set) API.\n* [benelsen/orb](https://github.com/benelsen/orb) - helper JavaScript routines for orbital mechanics problems.\n* [The Primary Repository for code relating to Icarus Interstellar's Project Voyager](https://github.com/zachfejes/ProjectVoyager)\n* [Trajectory optimization tool](https://github.com/Arrowstar/ksptot) - multiple gravity assist planning code.\n* [virtual Apollo Guidance Computer](https://github.com/rburkey2005/virtualagc) - awesome simulation of real-life Apollo Guidance Computer by Ron Burkey.\n* [Basilisk](http://hanspeterschaub.info/bskMain.html) (hat tip to [ChrisR](https://space.stackexchange.com/users/1391/chrisr)) a modular C/C++ astrodynamics simulation framework with Python scripting. Likely the highest fidelity attitude control simulation, algorithms used in ADCS of the upcoming EMM mission.\n* [smd](https://github.com/ChristopherRabotin/smd) - (hat tip to [ChrisR](https://space.stackexchange.com/users/1391/chrisr)) a mission propagator for continuous thrusting via way-point targeting. Can also be used for statistical orbital determination given range and range-rate information. Written in Go.\n* [poliastro](https://github.com/poliastro/poliastro/) (hat tip to [ChrisR](https://space.stackexchange.com/users/1391/chrisr)) a set of Python routines for astrodynamics with an emphasis on interplanetary mission design.\n\n### Seismology\n\n* [SPECFEM-3D](https://github.com/geodynamics/specfem3d)\n* [SPECFEM-3D-Globe](https://github.com/geodynamics/specfem3d_globe)\n* [SPECFEM-2D](https://github.com/geodynamics/specfem2d)\n* [SEM2DPACK](http://sem2d.sourceforge.net)\n\n### Meteorology\n\n* [PyAOSlib](https://github.com/PyAOS/aoslib) - auxiliary routines in Fortran and Python\n\n### Optics\n\n* [poppy](https://github.com/mperrin/poppy)\n* [Py6S](https://github.com/robintw/Py6S) - interface to 6S radiative transfer codes\n\n### Nuclear technology\n\n* [Method of characteristics](https://github.com/mit-crpg/OpenMOC) - nuclear reactor physics calculations from MIT.\n* [Models for nuclear reactor benchmarks](https://github.com/mit-crpg/benchmarks) - another repo from the same MIT group.\n\n### Astronomy\n\n* [NTL Asteroid Data Hunter](https://github.com/nasa/NTL-Asteroid-Data-Hunter) - an asteroid search program with a neat GUI \n* [Binary classification of RR (AB) stars](https://github.com/johnh2o2/rrlyrclassification)\n* [Tkinter program to visualize lightcurves](https://github.com/johnh2o2/pyvislc)\n* [A code to compute the abundances of chemical species in the interstellar medium](https://github.com/smaret/astrochem) - software written by S\u00e9bastien Maret.\n* [SunPy](https://github.com/sunpy/sunpy) - Python for solar physics.\n\n### Chemistry\n\n* [GROMACS](https://github.com/gromacs/gromacs) - a molecular simulation toolkit, main site [here](http://www.gromacs.org/).\n\n"
 },
 {
  "repo": "ArgoCanada/argoFloats",
  "language": "R",
  "readme_contents": "if (!interactive()) png(\"exampleTS.png\", width=7, height=3.5, unit=\"in\", res=120, pointsize=8)\nlibrary(argoFloats)\nlibrary(oce)\n## 1. Get worldwide float-profile index, saving to ~/data/argo by default.\nindexAll <- getIndex()\n## 2. Narrow to a 30km-radius circle centred on Abaco Island, The Bahamas.\nindex <- subset(indexAll,\n                circle=list(longitude=-77.06,latitude=26.54,radius=30))\n## 3. Get NetCDF files for these profiles, saving to ~/data/argo by default.\nprofiles  <- getProfiles(index)\n## 4. Read the NetCDF files.\nargos <- readProfiles(profiles)\n## 5. Examine QC flags, and set questionable data to NA.\nargosClean <- applyQC(argos)\npar(mfrow=c(1, 2))                     # want two-panel plot\npar(mar=c(3.5, 3.5, 2.0, 2.0))         # tighten margins\n## 6. Plot a map with bathymetry, indicating number of profiles.\nplot(index, which=\"map\")\npoints(-77.06, 26.54, pch=\"*\", cex=3)  # show centre of focus\nmtext(paste(argosClean[[\"length\"]], \"profiles\"))\n## 7. Plot a TS diagram\nplot(argosClean, which=\"TS\")\nif (!interactive()) dev.off()\n\n"
 },
 {
  "repo": "omuse-geoscience/omuse",
  "language": "Fortran",
  "readme_contents": "# OMUSE #\n\nOMUSE stands for Oceanographic MUltipurpose Software Environment. It is a \npackage to conduct numerical experiments in oceanography and other Earth \nsciences. Example OMUSE applications can be found in the examples \n[repository](https://github.com/omuse-geoscience/omuse-examples).\n\n### For whom is OMUSE? ###\n\nOMUSE aims to be useable by any researcher or student with a basic knowledge of \nthe Python programming language.\n\n### What is this repository for? ###\n\nThis repository contains the source tree for OMUSE, including OMUSE specific framework\ncomponents and community codes.\n\n### How do I get set up? ###\n\nWhile there are some packages available on [pipy](www.pypi.org), we recommend at the moment \nto do a pip developer install:\n\n- setup a python environment, e.g. using virtualenv, and activate it.\n- in a suitable working directory clone the [AMUSE](https://github.com/amusecode/amuse) repository: `git clone https://github.com/amusecode/amuse`\n- go into the created directory: `cd amuse`\n- do the developer install from here: `pip install -e . [MPI]` The MPI is optional. \n- Going back to the working directory (`cd ..`) also clone the OMUSE repository: `git clone https://github.com/omuse-geoscience/omuse`,\n- go into the source directory `cd omuse` and set the environment variable `DOWNLOAD_CODES`, e.g. `export DOWNLOAD_CODES=latest`.\n- now, do `pip install -e .` from the root of the package\n- type `python setup.py build_codes --inplace` to build the codes. \n- the file `build.log` will report any errors in the build process.\n\nThis installs amuse-devel and omuse-devel. The community codes of OMUSE can \nbe build manually by going into each directory:\n\n + src/omuse/community/adcirc\n + src/omuse/community/swan\n + etc\n\nand typing: first `make download` (for some) and then `make`\n\nOMUSE has been tested on OSX and linux machines, with ifort and gfortran \ncompilers, on desktop machines and on the Carthesius supercomputer.\n\nIn addition to the AMUSE dependencies, OMUSE needs/ can use the following \npackages:\n\n + matplotlib basemap\n + netCDF and netCDF for fortran and the python bindings\n + GRIB_API\n\n### Documentation ###\n\nDocumentation can be found [here](https://omuse.readthedocs.io). In addition the base  [AMUSE documentation](https://amuse.readthedocs.io) can be consulted.\n\n### Reporting issues ###\n\nIssues can be reported at the OMUSE issue tracker; for framework issues, \nreport them at the AMUSE [repository](https://github.com/amusecode/amuse).\n\n### Contribution guidelines ###\n\nContributions are welcome. Note that most framework development happens at \nthe AMUSE [repository](https://github.com/amusecode/amuse) A primer for \nwriting code interfaces and other documentation can be found on the amuse \n[website](www.amusecode.org).\n"
 },
 {
  "repo": "DocOtak/gsw-xarray",
  "language": "Python",
  "readme_contents": ".. |CI Status| image:: https://github.com/docotak/gsw-xarray/actions/workflows/ci.yml/badge.svg\n  :target: https://github.com/DocOtak/gsw-xarray/actions/workflows/ci.yml\n  :alt: CI Status\n.. |Documentation Status| image:: https://readthedocs.org/projects/gsw-xarray/badge/?version=latest\n  :target: https://gsw-xarray.readthedocs.io/en/latest/?badge=latest\n  :alt: Documentation Status\n.. |pypi| image:: https://badge.fury.io/py/gsw-xarray.svg\n   :target: https://badge.fury.io/py/gsw-xarray\n   :alt: pypi package\n.. |conda forge| image:: https://img.shields.io/conda/vn/conda-forge/gsw-xarray\n   :target: https://anaconda.org/conda-forge/gsw-xarray\n\ngsw-xarray: Wrapper for gsw that adds CF attributes\n===================================================\n|CI Status| |Documentation Status| |pypi| |conda forge|\n\ngsw-xarray is a wrapper for `gsw python <https://github.com/TEOS-10/GSW-python>`_\nthat will add CF attributes to xarray.DataArray outputs.\nIt is meant to be a drop in wrapper for the upstream GSW-Python library and will only add these attributes if one argument to a function is an xarray.DataArray.\n\nYou can find the documentation on `gsw-xarray.readthedocs.io <https://gsw-xarray.readthedocs.io/>`_.\n\nUsage\n-----\n\n.. code:: python\n\n   import gsw_xarray as gsw\n\n   # Create a xarray.Dataset\n   import numpy as np\n   import xarray as xr\n   ds = xr.Dataset()\n   id = np.arange(3)\n   ds['id'] = xr.DataArray(id, coords={'id':id})\n   ds['CT'] = ds['id'] * 10\n   ds['CT'].attrs = {'standard_name':'sea_water_conservative_temperature'}\n   ds['SA'] = ds['id'] * 0.1 + 34\n   ds['SA'].attrs = {'standard_name':'sea_water_absolute_salinity'}\n\n   # Apply gsw functions\n   sigma0 = gsw.sigma0(SA=ds['SA'], CT=ds['CT'])\n   print(sigma0.attrs)\n\nOutputs\n\n::\n\n   {'standard_name': 'sea_water_sigma_t', 'units': 'kg/m^3'}\n\nDon't worry about usage with non xarray array objects, just use in all places you would the upstream library:\n\n.. code:: python\n\n   sigma0 = gsw.sigma0(id * 10, id * 0.1 + 34)\n   print(type(sigma0), sigma0)\n\nOutputs\n\n::\n\n   <class 'numpy.ndarray'> [-5.08964499  2.1101098   9.28348219]\n\n\nWe support (and convert the unit if necessary) the usage of pint.Quantities and the usage of xarray wrapped Quantities.\nSupport for pint requires the installation of two optional dependencies: ``pint`` and ``pint-xarray``.\nIf all the inputs to a gsw function are Quantities, the returned object will also be a Quantity belonging to the same UnitRegistry.\n\n.. warning::\n\n   Quantities must all belong to the same pint.UnitRegistry, a ValueError will be thrown if there are mixed registries.\n\n.. warning::\n\n   If one input is a Quantity, all inputs must be Quantities (and/or xarray wrapped Quantities), except for the `axis` and `interp_method` arguments.\n   For mixed usage of Quantities and non Quantities, a ValueError will be thrown.\n\n.. code:: python\n\n   import pint_xarray\n   import gsw_xarray as gsw\n\n   # Create a xarray.Dataset\n   import numpy as np\n   import xarray as xr\n   ds = xr.Dataset()\n   id = np.arange(3)\n   ds['id'] = xr.DataArray(id, coords={'id':id})\n   ds['CT'] = ds['id'] * 10\n   # make sure there are unit attrs this time\n   ds['CT'].attrs = {'standard_name':'sea_water_conservative_temperature', 'units': 'degC'}\n   ds['SA'] = ds['id'] * 0.1 + 34\n   ds['SA'].attrs = {'standard_name':'sea_water_absolute_salinity', 'units': 'g/kg'}\n\n   # use the pint accessor to quantify things\n   ds = ds.pint.quantify()\n\n   # Apply gsw functions\n   sigma0 = gsw.sigma0(SA=ds['SA'], CT=ds['CT'])\n   # outputs are now quantities!\n   print(sigma0)\n\nOutputs\n\n::\n\n   <xarray.DataArray 'sigma0' (id: 3)>\n   <Quantity([27.17191038 26.12820162 24.03930887], 'kilogram / meter ** 3')>\n   Coordinates:\n     * id       (id) int64 0 1 2\n   Attributes:\n       standard_name:  sea_water_sigma_t\n\nThe usage of xarray wrapped Quantities is not required, you can use pint directly (though the ``pint-xarray`` dep still needs to be installed).\n\n.. code:: python\n\n   import gsw_xarray as gsw\n   import pint\n   ureg = pint.UnitRegistry()\n   SA = ureg.Quantity(35, ureg(\"g/kg\"))\n   CT = ureg.Quantity(10, ureg.degC)\n   sigma0 = gsw.sigma0(SA=SA, CT=CT)\n   print(sigma0)\n\nOutputs\n\n::\n\n   26.824644457868317 kilogram / meter ** 3\n\nAs gsw-xarray converts arguments to the proper unit when Quantities are used, we can e.g. use the temperature in Kelvin:\n\n.. code:: python\n\n   CT = ureg.Quantity(10, ureg.degC).to('kelvin')\n   sigma0 = gsw.sigma0(SA=SA, CT=CT)\n   print(sigma0)\n\nOutputs\n\n::\n\n   26.824644457868317 kilogram / meter ** 3\n\n.. note::\n   If you do not wish to use the unit conversion ability, you need to pass dequantified Quantities\n   (e.g. `da.pint.dequantify()` for pint-xarray or `arg.magnitude` for pint.Quantity).\n\n.. warning::\n   On the opposite, gsw-xarray will not check the units if non Quantity arguments are used.\n   If you wish to use unit conversion, please pass quantified arguments (if your xarray.Dataset /\n   xarray.DataArray has the 'units' attribute, you can use `da.pint.quantify()`)\n\n.. note::\n   We recommend that you use the `cf-xarray <https://cf-xarray.readthedocs.io/en/latest/units.html>`_ registry for units,\n   as it implements geophysical units as `degree_north`, `degrees_north`, etc.\n   gsw-xarray internally uses `degree_north` and `degree_east` for latitude and longitude unit names.\n   If they are not found in the unit registry, they will be replaced by `degree`.\n\n   The function `gsw.SP_from_SK` uses part per thousand for SK. 'ppt' is already used for picopint,\n   so the expected unit is replaced by '1'.\n\n\nInstallation\n------------\nPip\n...\n\n.. code:: bash\n\n    pip install gsw-xarray\n\n\nConda\n.....\n\nInside a conda environment:  ``conda install -c conda-forge gsw-xarray``.\n\nPipenv\n......\n\nInside a pipenv environment: ``pipenv install gsw-xarray``.\n\n\nContributor guide\n-----------------\n\nAll contributions, bug reports, bug fixes, documentation improvements,\nenhancements, and ideas are welcome.\nIf you notice a bug or are missing a feature, fell free\nto open an issue in the `GitHub issues page <https://github.com/DocOtak/gsw-xarray/issues>`_.\n\nIn order to contribute to gsw-xarray, please fork the repository and\nsubmit a pull request. A good step by step tutorial for starting with git can be found in the\n`xarray contributor guide <https://xarray.pydata.org/en/stable/contributing.html#working-with-the-code>`_.\nA main difference is that we do not use conda as python environment, but poetry.\n\nSet up the environment\n......................\n\nYou will first need to `install poetry <https://python-poetry.org/docs/#installation>`_.\nThen go to your local clone of gsw-xarray and launch installation:\n\n.. code:: bash\n\n   cd /path/to/your/gsw-xarray\n   poetry install\n\nYou can then activate the environment by launching a shell\nwithin the virtual environment:\n\n.. code:: bash\n\n   poetry shell\n\nYou can check that the tests pass locally:\n\n.. code:: bash\n\n   pytest gsw_xarray/tests\n\nRelease (for maintainers only)\n..............................\n\nTODO...\n"
 },
 {
  "repo": "njwilson23/narwhal",
  "language": "Python",
  "readme_contents": "# Narwhal\n\n[![Build Status](https://travis-ci.org/njwilson23/narwhal.svg?branch=master)](https://travis-ci.org/njwilson23/narwhal)\n\n## Oceanographic data analysis in Python\n\nNarwhal is a Python module built on [pandas](http://pandas.pydata.org/) and\n[matplotlib](http://matplotlib.org/). Narwhal is designed for manipulating and\nvisualizing oceanographic data.\n\nData are organized into self-describing `Cast` and `CastCollection` data\nstructures. Convenience methods and functions are included for:\n\n- interpolation\n- density and depth calculation\n- buoyancy frequency estimation\n- baroclinic mode analysis\n- water type fraction inversion\n\n### Quickly visualize results\n\nThe `narwhal.plotting` submodule contains convenience methods for creating T-S\ndiagrams, cast plots, and section plots. Here's some data from off the coast of\nnortheastern Greenland:\n\n![T-S diagram](https://github.com/njwilson23/narwhal/raw/gh-pages/images/ts-plot2_v7.png)\n\n![Section diagram](https://github.com/njwilson23/narwhal/raw/gh-pages/images/sill_velocity.png)\n\n### Python wrapper for the thermodynamic equation of state\n\nNarwhal provides a *ctypes* wrapper for the\n[Gibbs Seawater Toolbox](http://www.teos-10.org/pubs/gsw/html/gsw_contents.html)\nin the `narwhal.gsw` submodule, making things like the following possible:\n\n    density = narwhal.gsw.rho(cast[\"sa\"], cast[\"ct\"], cast[\"p\"])\n\nCurrently, GSW 3.05 is packaged with Narwhal.\n\n### Data should not be tied to software\n\nFor storage, data is serialized to JSON or HDF files. These common formats are\nopen and easily imported into other analysis packages (such as MATLAB), or\nvisualization libraries (such as D3).\n\n## Installation\n\n    git clone https://github.com/njwilson23/narwhal.git\n    pip install -r narwhal/requirements.txt\n    pip install narwhal\n\n### Dependencies\n\n- Python 2.7+ or Python 3.4+\n- pandas\n- matplotlib\n- scipy\n- requests\n- dateutil\n- six\n- C-compiler (for GSW)\n- h5py (optional, required for HDF read/write)\n\nIf [Karta](https://github.com/fortyninemaps/karta) is installed, it will be used\nfor fast and accurate geographical calculations.\n\nNarwhal is experimental. See also\n[python-oceans](https://github.com/ocefpaf/python-oceans) and\n[oce](https://github.com/dankelley/oce) (R).\n"
 },
 {
  "repo": "sea-mat/sea-mat",
  "language": "HTML",
  "readme_contents": "# sea-mat\n\n[![Build Status](https://travis-ci.org/sea-mat/sea-mat.svg?branch=master)](https://travis-ci.org/sea-mat/sea-mat)\n\nA collaborative effort to organize and distribute Python tools for the Oceanographic Community\n\nSee the rendered page here:\nhttp://sea-mat.github.io/sea-mat/\n"
 },
 {
  "repo": "Energy-Pathways-Group/GLOceanKit",
  "language": "MATLAB",
  "readme_contents": "GLOceanKit\n===========\nGLOceanKit is a collection of models and analysis tools for physical oceanography.\n\nThe code is in written in two different languages: Matlab and Objective-C, but not all models or analysis tools are available in both languages.\n\n[Matlab](Matlab/)\n-------\nThe Matlab directory contains the following subdirectories of models and tools,\n- [Advection-Diffusion Estimation](Matlab/AdvectionDiffusionEstimation) Tools for computing estimating velocity field parameters (strain, vorticity, divergence). From Oscroft, Sykulski & Early (2021).\n- [Advection-Diffusion Models](Matlab/AdvectionDiffusionModels) Code for generating particles in advection diffusion models with boundaries.\n- [Boussinesq2D](Matlab/Boussinesq2D) 2D nonlinear spectral Boussinesq model.\n- [Diffusivity](Matlab/Diffusivity) A collection of analysis tools for computing relative diffusivity from particles.\n- [InternalModes](Matlab/InternalModes) Tools solving the vertical mode eigenvalue problem with very high accuracy. From Early, Lelong & Smith (2020).\n- [InternalWaveModel](Matlab/InternalWaveModel) A linear internal wave model.\n- [InternalWaveSpectrum](Matlab/InternalWaveSpectrum) Tools for computing the Garrett-Munk spectrum and its approximations.\n- [OceanBoundaryLayer](Matlab/OceanBoundaryLayer) A few simple ocean boundary layer models taken from Elipot and Gille (2009).\n- [Quasigeostrophy](Matlab/Quasigeostrophy) Tools for analyzing the output of the Quasigeostrophic model.\n\n[Objective-C](GLOceanKit/)\n-------\nContains internal modes routines, internal wave model, and a QG model.\n\ngit-lfs\n--------\nThis repo links to the lfs for some precomputed internal wave modes, but does not download them by default. To override these settings, see [this comment](https://github.com/git-lfs/git-lfs/issues/2717). I think, that if you just do,\n`git config lfs.fetchexclude \"\"`\nthen it'll remove the exclusion and you can started to download those big files.\n"
 },
 {
  "repo": "c-proof/pyglider",
  "language": "Roff",
  "readme_contents": "![](docs/_static/PyGliderHorizontal.svg)\n\nPython tools for interacting with ocean glider data.   PyGlider takes data from\nTeledyne/Webb Slocum gliders and Alseamar SeaExplorers and creates CF-compliant\nNetCDF files.\n\nFor documentation, please see <http://pyglider.readthedocs.io>\n\n### Contact\n\nGet in touch with us using Discussion above or by opening an issue.\n"
 },
 {
  "repo": "MPOcanes/MPO624-2018",
  "language": "Jupyter Notebook",
  "readme_contents": "# MPO 624 CLASS FOR SPRING 2018\n\n## Welcome!\nThe course is all here. Fork it so you can contribute back! \n\n### Software accounts and installs: \n\n1.  https://github.com/MPOcanes/MPO624-2018/blob/master/INSTALL_JUPYTER_UNIDATA.md\n1. https://github.com/MPOcanes/MPO624-2018/blob/master/GITHUB_QUICKSTART.md\n1. Install the IDV\n   * Install the IDV nightly build from Undiata. https://www.unidata.ucar.edu/downloads/idv/nightly/index.jsp You will need to create a light sign-in, just so they know who\u2019s using it. Great free software, my group (and Xingchen in class) are involved with integrating it into Jupyter notebooks. \n1. Matlab \n   * Matlab is free to you while you are at UM. You will need to create an account with them, verify license tokens, etc.  It is very powerful, and very well documented with lots of hand-holding features, but it is commercial and proprietary. I will not use my course to create a generation of addicts to it, but you might end up using it -- lots of scientists do, and a big body of working, legacy code is nothing to sneer at. You can install it from here: http ://it.miami.edu/a-z-listing/matlab/index.html\n\n\n-------\n#### Random links\n\nBeginner's intro to github: https://github.com/Github-Classroom-Cybros/Learn-Git-Github\n\nHow to use repos in teaching https://classroom.github.com/videos\n\nList of emojis you can use in Markdown: https://gist.github.com/rxaviers/7360908\n\nSpecial characters to cut and paste https://en.wikipedia.org/wiki/List_of_XML_and_HTML_character_entity_references\n\n"
 },
 {
  "repo": "thunderhoser/ai2es_xai_course",
  "language": "Python",
  "readme_contents": "# ai2es_xai_course\nNotebooks for AI2ES (NSF Institute for Research on Trustworthy Artificial Intelligence in Weather, Climate, and Coastal Oceanography) short course on XAI (explainable artificial intelligence).\n"
 },
 {
  "repo": "rowg/hfrprogs",
  "language": "MATLAB",
  "readme_contents": "HFR_Progs: High Frequency Radar Program Suite\n\n$Id: README 351 2007-03-09 01:31:02Z dmk $\n\n*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n\nMore detailed documentation will eventually be available at:\n\nProject webpage: https://github.com/rowg/hfrprogs\n\n*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n\nThis suite covers basic processing and analysis of HF Radar data.  It\nis an updated and expanded version of HFRadarmap (by Mike Cook) or\nHFRC (by David Kaplan).  The principal authors of this suite are David\nKaplan (UCSC), Mike Cook (NPS) and Dan Atwater (UCSC/NPS).\n\nMuch of this toolbox is released under the Gnu General Public License,\nthe text of which is included with the toolbox at\nHFR_Progs/license.gpl.txt.  In a nutshell, this license says that the\ncode may be modified and redistributed freely, provided that all\nmodifications to the source code are also made freely available.\n\n*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n\nInstallation Quickstart:\n------------------------\n\nThese instructions are for suitably configured Unix-like systems\n(e.g., Linux, Mac OSX).\n\n1) Unpack\n\n$ unzip HFR_Progs-VERSION.zip\n\n2) Configure Matlab\n\n$ cd HFR_Progs\n$ make\n$ make all_gshhs\n$ cd matlab\n$ matlab\nMATLAB>> HFRPdemo\n\n3) Configure Perl (if desired)\n\n$ sudo perl -MCPAN -e \"install 'MODULE::NAME'\"\n\nIf using the Perl scripts, at a minimum the following modules are\nnecessary: Date::Calc, List::Compare, Net::SSH, Net::SCP,\nConvert::ASN1\n\n*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n\nInstallation:\n-------------\n\n1) Unpack the suite in a suitable location\n\nThe suite is distributed in a zipped archive.  On unix-like systems,\nthis can be expanded by:\n\n$ unzip HFR_Progs-VERSION.zip\n\nThis will create a directory called HFR_Progs-VERSION with the suite\ninside.\n\n2) Install the external Matlab packages\n\nThe core of this suite is the Matlab functions and scripts.  This code\ndepends on several freely available software packages: m_map, openMA,\nt_tide, arrow, mexnc.  All but m_map are to one degree or another\noptional.  These packages can be downloaded off the web and placed in\nthe HFR_Progs/matlab/external_matlab_packages directory.\n\nTo ease installation of these packages, we have created a Makefile\nthat will automatically download and install the packages on suitably\nconfigured systems.  These systems must have make, wget, tar and unzip\ninstalled.\n\nAssuming that these packages are installed, one can download and\ninstall m_map, openMA, t_tide and arrow with:\n\n$ cd HFR_Progs\n$ make\n\nYou may also want to use the high definition coastlines with m_map:\n\n$ make all_gshhs\n\nIf you want to generate netCDF files for use with Gnome, then you will\nneed to install the netCDF package and the mexnc Matlab interface for\nnetCDF.  Installing netCDF is fairly straightforward and explained at\nhttp://www.unidata.ucar.edu/software/netcdf/.  Once installed,\ninstalling and compiling mexnc may be possible with:\n\n$ make all_mexnc\n$ make compile_mexnc NETCDF=/PATH/TO/NETCDF\n\nThis requires a working mex executable.\n\n3) Install Perl modules\n\nPerl may be useful for moving files around and converting some file\nformats.  It is not required for using the Matlab parts of the\ntoolbox.\n\nIf you wish to use the Perl scripts, then certain Perl modules must be\ninstalled (it is assumed Perl itself is already installed).  The\nappropriate way to install these packages depends on the system, but\nthe following should work on many systems:\n\n$ sudo perl -MCPAN -e \"install 'MODULE::NAME'\"\n\nor perhaps\n\n$ su\n$ perl -MCPAN -e \"install 'MODULE::NAME'\"\n\nAt a minimum, the following modules are necessary: Date::Calc,\nList::Compare, Net::SSH, Net::SCP, Convert::ASN1\n\n4) Running matlab\n\nTo use the Matlab toolbox, the HFR_Progs/matlab directory and most\nsubdirectories must be on the Matlab path.  One way to achieve this is\nby starting Matlab from the HFR_Progs/matlab directory.  If Matlab is\nstarted from another directory, then the following should add the\nappropriate directories (and perform a couple of basic tasks, such as\ninitializing the random number generator):\n\nMATLAB>> cd HFR_Progs/matlab\nMATLAB>> startup\n\nAfter this has been done, you can change to any working directory and\nthe toolbox will be available.  You could also modify\nHFR_Progs/matlab/startup.m to fit your needs.\n\nThere is a demonstration of the toolbox along with some sample data in\nHFR_Progs/matlab/demo:\n\nMATLAB>> HFRPdemo\n\nWe are in the process of writing drivers that automate basic\nprocessing of HF Radar data.  These can be found in\nHFR_Progs/matlab/drivers.\n\nThere are a number of readme files inside the HFR_Progs/matlab\ndirectory and subdirectories that provide additional information about\nspecific aspects of the toolbox.\n\n\n"
 },
 {
  "repo": "jonathansharp/CO2-System-Extd",
  "language": "MATLAB",
  "readme_contents": "# CO2-System-Extd\n\n<a href=\"https://zenodo.org/badge/latestdoi/198885961\"><img src=\"https://zenodo.org/badge/198885961.svg\" alt=\"DOI\"></a> [![View CO2SYSv3 for MATLAB on File Exchange](https://www.mathworks.com/matlabcentral/images/matlab-file-exchange.svg)](https://www.mathworks.com/matlabcentral/fileexchange/78378-co2sysv3-for-matlab)\n\n## ABOUT\n\nThis repository includes software compatible with MATLAB and GNU Octave for calculating marine CO2 system variables (CO2SYS.m), computing partial derivatives of calculated CO2 system variables with respect to inputs (derivnum.m), and propagating uncertainties for CO2 system calculations (errors.m). This software performs similarly to previously released versions of CO2SYS.m (v1: https://cdiac.ess-dive.lbl.gov/ftp/co2sys/CO2SYS_calc_MATLAB_v1.1/; v2: https://github.com/jamesorr/CO2SYS-MATLAB), and includes the following extended capabilities, additions, and bug fixes (among other minor changes):\n \n1) Can accept input parameters of [CO3], [HCO3], and [CO2], and propagate their uncertainties\n2) Includes NH3 and HS as alkalinity contributors, and propagates their uncertainties\n3) Uses separate inputs to specify choices for characterizations of K1K2, KSO4, KF, and TB\n4) Does not evaluate input parameters equal to -999 or NaN\n5) Exits pH iteration loops that do not converge and indicates where a problem occurred\n6) Provides exactly identical pH results for a given input line, no matter the other lines of input parameters (this is not necessarily the case for prior versions of CO2SYS.m)\n7) Uses an updated definition of the ideal gas constant (https://physics.nist.gov/cgi-bin/cuu/Value?r)\n8) Fixes bugs in CO2SYS.m Revelle factor calculation and derivnum.m output conditions\n9) Includes K1 and K2 constants defined by Sulpis et al. (2020), K2 constant defined by Schockman and Byrne (2021), KF constant defined by Perez and Fraga (1987), and KSO4 constant of Waters and Millero (2013)\n10) Determines initial pH in iterative solvers using the approach of Munhoven (2013), detailed further in Humphreys et al. (submitted), rather than simply using an initial estimate of 8.0 each time.\n11) Obtains free scale pH properly within iterative pH solvers no matter the input scale, rather than making the simplification that input pH is always on the total scale.\n12) Includes substrate-inhibitor ratio (Bach, 2015) as an output argument from CO2SYS.\n13) Calculates uncertainties at output conditions that are associated with equilibrium constants with respect to equilibrium constants at output conditions, rather than input conditions as previously. This essentially assume pK uncertainty is constant regardless of temperature and pressure.\n14) Calculates derivatives and errors for the Revelle factor.\n15) errors.m includes optional calcium concentration uncertainty input as discussed in Dillon et al. (2020)\n\nAlso included in this repository is a routine to compare CO2SYSv3 to CO2SYSv2 (compare_versions.m), a routine to calculate total concentrations of conservative elements (Na, Mg, Cl, etc.) from CO2SYS output (TOTALS.m), and an example function to run CO2SYSv3 and plot some of the output (example_CO2SYS.m).\n\n## HISTORY\n\nCO2SYS was initially developed by Lewis and Wallace (1998) for MS DOS, later adapted for MS Excel and MATLAB by Pierrot (2006). The code was vectorized, refined, and optimized for computational speed by van Heuven (2011). Options for error propagation were added by Orr et al. (2018). This software builds upon those previous versions.\n\n## INSTALLATION AND USE\n\nDownload the files in this repository and place them in a directory that is in the MATLAB search path. Or add the directory where they are located to the search path (https://www.mathworks.com/help/matlab/matlab_env/add-remove-or-reorder-folders-on-the-search-path.html).\n\nTo perform CO2 system calculations, use CO2SYS.m as directed in the function's help text. All sub-routines that will be called upon are contained within the CO2SYS.m function.\n\nTo propagate uncertainties in CO2 system calculations, use errors.m as directed in the function's help text. The errors.m routine will call upon CO2SYS.m and derivnum.m. As such, this version of errors.m is compatible only with CO2SYSv3.\n\nTo compute partial derivatives of calculated CO2 system variables with respect to input parameters, use derivnum.m as directed in the function's help text. The derivnum.m routine will call upon CO2SYS.m. As such, this version of derivnum.m is compatible only with CO2SYSv3.\n\n## CITATION\n\nThe full citation for CO2SYSv3 (Sharp et al., 2020) is given below. Cite this version if using CO2SYSv3 for CO2 system calculations or propagating errors in CO2 system calculations using the extended errors.m or derivnum.m routines provided here.\n\nIf using any CO2SYS program for CO2 system calculations, cite also the original CO2SYS DOS work of Lewis and Wallace (1998).\n\nIf using the CO2SYS MATLAB program for CO2 system calculations, cite also the work of van Heuven et al. (2011).\n\nIf using the derivnum.m and/or errors.m programs for CO2 system error propagations, cite also the work of Orr et al. (2018).\n\n## REFERENCES\n\nBach, L. T. (2015). Reconsidering the role of carbonate ion concentration in calcification by marine organisms. Biogeosciences 12(16), 4939\u20134951.\n\nDillon, W. D. N., Dillingham, P. W., Currie, K. I., McGraw, C. M., 2020. Inclusion of uncertainty in the calcium-salinity relationship improves estimates of ocean acidification monitoring data quality. Marine Chemistry 226, 103872.\n\nHumphreys, M.P., Lewis, E.R., Sharp, J.D., Pierrot, D. PyCO2SYS: marine carbonate system calculations in Python. Submitted to Geoscientific Model Development.\n\nLewis, E., Wallace, D. W. R., 1998. Program Developed for CO2 System Calculations. ORNL/CDIAC-105. Carbon Dioxide Information Analysis Center, Oak Ridge National Laboratory, Oak Ridge, TN.\n\nMunhoven, G., Mathematics of the total alkalinity\u2013pH equation \u2013 pathway to robust and universal solution algorithms: the SolveSAPHE package v1.0.1. Geoscientific Model Development 6, 1367\u20131388, 2013\n\nOrr, J.C., Epitalon, J.-M., Dickson, A. G., Gattuso, J.-P., 2018. Routine uncertainty propagation for the marine carbon dioxide system. Marine Chemistry 207, 84-107.\n\nSharp, J.D., Pierrot, D., Humphreys, M.P., Epitalon, J.-M., Orr, J.C., Lewis, E.R., Wallace, D.W.R. (2021, May 20). CO2SYSv3 for MATLAB (Version v3.2.0). Zenodo. http://doi.org/10.5281/zenodo.3950562\n\nSulpis, O., Lauvset, S. K., and Hagens, M., 2020. Current estimates of K1* and K2* appear inconsistent with measured CO2 system parameters in cold oceanic regions. Ocean Science Discussions, 1-27.\n\nvan Heuven, S., Pierrot, D., Rae, J.W.B., Lewis, E., Wallace, D.W.R., 2011. MATLAB Program Developed for CO2 System Calculations. ORNL/CDIAC-105b. Carbon Dioxide Information Analysis Center, Oak Ridge National Laboratory, Oak Ridge, TN.\n"
 },
 {
  "repo": "iuryt/ocean_gyre_tank",
  "language": "Jupyter Notebook",
  "readme_contents": "# Ocean Gyre in a Tank (Numerical Experiment)\n\nThis is the MITgcm simulation for the Ocean General Circulation in a rotating tank.\n\nBased on \"Insights of the non-linear solution of Munk\u2019s ocean circulation theory from a rotating tank experiment\" - *Ocean and Coastal Research*, 2020. ([10.1590/2675-2824069.20-011psp](https://doi.org/10.1590/2675-2824069.20-011psp))\n\n<img src=\"https://github.com/iuryt/ocean_gyre_tank/blob/master/notebooks/psi_model.png\" data-canonical-src=\"https://github.com/iuryt/ocean_gyre_tank/blob/master/notebooks/psi_model.png\" width=\"400\" height=\"auto\" />\nA: Linear solution. B: Nonlinear solution.\n\n## How to set up the experiment\n\n1. Follow the [Getting Started](https://mitgcm.readthedocs.io/en/latest/getting_started/getting_started.html) section on MITgcm documentation to set up the model.\n2. Clone this experiment to the MITgcm folder (you can also download the repository and extract it to MITgcm folder.)\n3. Create the `build` and `run` folders inside `ocean_gyre_tank`. \n4. Go to `build` and compile the model with `mpi` (see the [MITgcm documentation](https://mitgcm.readthedocs.io/en/latest/)).\n5. Copy the executable `mitgcmuv` to the `run` folder.\n6. Create a symbolic link to the files in `input` for `run` folder.\n\n## How to generate the initial conditions and forcing\n\nIn `notebooks` there is a file called `Init.ipynb` that creates the bathymetry and wind forcing.\nThe data will be saved to `input` folder. You may have to change the grid spacing in `input/data` or number of points in `code/SIZE.h` if you change the code on the notebooks.\n\n## How to configure the experiment\n\nThe file `data` in `input` folder has all the parameters needed for the experiment.\nYou can change to the linear case setting `.FALSE.` for `momAdvection`.\n\n## How to run the experiment\n\nThe current configuration on `code/SIZE.h` works in parallel using 4 cores (see [Documentation](https://mitgcm.readthedocs.io/en/latest/) to learn how to set up for a different number of cores).\nIf the experiment is already configured you just have to run `mpirun -np 4 ./mitgcmuv` in `run` folder.\n\n## How to read the data from the output\n\nThe notebook `notebooks/Analysis.ipynb` its a tutorial that explains how to read and plot the output from this experiment.\n\n"
 },
 {
  "repo": "janosh/ocean-artup-gatsby",
  "language": "JavaScript",
  "readme_contents": "> ### \u26a0\ufe0f This repo was superseded by [the new Ocean artUp site](https://github.com/janosh/ocean-artup) written in Svelte.\n\n<p align=\"center\">\n  <a href=\"https://ocean-artup.eu\"><img src=\"src/assets/favicon.png\" alt=\"Favicon\" width=150></a>\n</p>\n\n<h1 align=\"center\">\n  <a href=\"https://ocean-artup.eu\">Ocean artUp</a>\n</h1>\n\n<h3 align=\"center\">\n\n[![License](https://img.shields.io/github/license/janosh/ocean-artup?label=License)](/license)\n![GitHub Repo Size](https://img.shields.io/github/repo-size/janosh/ocean-artup?label=Repo+Size)\n![GitHub last commit](https://img.shields.io/github/last-commit/janosh/ocean-artup?label=Last+Commit)\n\n</h3>\n\nThis repo powers the [Gatsby](https://gatsbyjs.org) site hosted at [ocean-artup.eu](https://ocean-artup.eu). The design and layout make heavy use of [CSS grid](https://css-tricks.com/snippets/css/complete-guide-grid) and [styled-components](https://styled-components.com). It is fully responsive, supports a two-level navbar animated with [`react-spring`](https://react-spring.io) (on mobile), [fluid typography](https://css-tricks.com/snippets/css/fluid-typography), [Algolia search](https://algolia.com) and [Contentful](https://contentful.com). This site is maintained by [Janosh Riebesell](https://janosh.dev).\n\nOcean artUp is a research project funded by an [Advanced Grant](https://cordis.europa.eu/project/rcn/205206_en.html) of the European Research Council. It aims to study the feasibility, effectiveness, associated risks and potential side effects of artificial upwelling in increasing ocean productivity, raising fish production, and enhancing oceanic CO<sub>2</sub> sequestration.\n\n## Installation\n\nTo get this site running locally, you need installed [`git`](https://git-scm.com), [`gatsby-cli`](https://gatsbyjs.org/packages/gatsby-cli) and [`yarn`](https://yarnpkg.com) (or [`npm`](https://npmjs.com)). Then follow these steps:\n\n1. Clone the repo to your machine and change into its directory.\n\n   ```sh\n   git clone https://github.com/janosh/ocean-artup \\\n   && cd ocean-artup\n   ```\n\n2. Optionally setup `git` hooks (recommended if you intend to open a PR).\n\n   ```sh\n   git config core.hooksPath src/utils/gitHooks \\\n   && chmod -R u+x src/utils/gitHooks\n   ```\n\n3. Install dependencies.\n\n   ```sh\n   yarn\n   ```\n\n4. Start the dev server.\n\n   ```sh\n   gatsby develop\n   ```\n\n## Deployment\n\nThe easiest way to get this site published is as follows:\n\n1. Create an account with [netlify](https://netlify.com).\n2. Install the [`netlify-cli`](https://netlify.com/docs/cli).\n3. Login to your account.\n\n   ```sh\n   netlify login\n   ```\n\n4. Connect your GitHub repo with your netlify account for [continuous deployment](https://netlify.com/docs/cli/#continuous-deployment).\n\n   ```sh\n   netlify init\n   ```\n\n5. Finally deploy the site with\n\n   ```sh\n   netlify deploy\n   ```\n"
 },
 {
  "repo": "gher-ulg/Diva-Workshops",
  "language": "Jupyter Notebook",
  "readme_contents": "[![Build Status](https://github.com/gher-ulg/Diva-Workshops/workflows/CI/badge.svg)](https://github.com/gher-ulg/Diva-Workshops/actions)\n[![DOI](https://zenodo.org/badge/108153788.svg)](https://zenodo.org/badge/latestdoi/108153788)\n\n# DIVA Workshops and training\n\nThis page provides the [Jupyter](https://jupyter.org/) notebooks (examples and exercises) for the `DIVAnd` user workshops and training sessions organised in the frame of H2020 [SeaDataCloud](https://www.seadatanet.org/) project.     \n\n[`Diva`](https://github.com/gher-ulg/DIVA) and [`DIVAnd`](https://github.com/gher-ulg/divand.jl) are software tools designed to generate gridded fields from in-situ observations.\n\nNotebooks can be previewed with the service [nbviewer](https://nbviewer.jupyter.org/github/gher-ulg/Diva-Workshops/tree/master/notebooks/).\n\n## Objectives\n\n* The [1st workshop](https://gher-ulg.github.io/Diva-Workshops/Previous/Diva-workshop-2018-Liege.html) (Li\u00e8ge, 3-6 April 2018) was focused on the creation of gridded products and climatologies using `DIVAnd`. The organizational details are available.\n\n* Within the 2nd SeaDataCloud training course (19-26 June 2019), the objective is to introduce participants to the `Julia` language, the Jupyter notebooks and the new Virtual Research Environment.\n\n* The [2nd workshop](https://gher-ulg.github.io/Diva-Workshops/2020/) (Bologna, 27-30 January 2020) was attended by beginners, intermediate and advanced users, and the goal was to help them create new products with `DIVAnd`.\n\n## Recommendation for the products\n\nA list of [recommendations](./tricks/climato_recommendation.md) for the preparation of products.\n\n## Tips and tricks\n\n### Export ODV to netCDF\n\n[Instructions](./tricks/ODV_netCDF_export.md) and screen-shots detailing how to create a netCDF\nfrom an ODV collection.\n\n### Adding EDMO code and CDI to CORA data sets\n\n[Instructions](./tricks/ODV_EDMO_CDI.md) and screen-shots to show how to add metadata to a CORA\ndataset.\n\n### Manipulation of a netCDF with nco\n\n[Examples](./tricks/cutting_netcdf.md)\n\n### Viewing netCDF file\n\n[netCDF](./tricks/netCDV_viewing.md)\n\n### Saving the attributes in a text file\n\nThis is useful when working on a machine with no internet connexion:      \n[save_attributes_file.ipynb](notebooks/postprocessing/save_attributes_file.ipynb)\n\n# Binder\n\nMost notebooks need more resources that what is can currently available on Binder. The introduction notebooks (introduction to OI and variationa analysis) however work\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/gher-ulg/Diva-Workshops/master?filepath=notebooks%2F1-Intro%2F04-OI-variational-analysis-introduction.ipynb).\n"
 },
 {
  "repo": "hackforthesea/awesome-marine-hacking",
  "language": null,
  "readme_contents": "# Resources for Ocean Hacking\n\n[![Donate on Patreon](https://camo.githubusercontent.com/6446a7907a4d4f8de024ec85750feb07d7914658/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f70617472656f6e2d646f6e6174652d79656c6c6f772e737667)](https://www.patreon.com/user?u=4619046)\n\n## Blogs\n- [UNder the C](https://underthecblog.org/) - An Ocean Science Blog by Grad Students in Marine Science at UNC- Chapel Hill\n- [oceanbites](https://oceanbites.org/) - The latest oceanography literature, explained\n- [Saltwater Science](https://www.nature.com/scitable/blog/saltwater-science) - Marine science is fascinating, diverse, and relevant to us all. From tiny plankton to enormous mammals, currents, climate and corals, three scientists break down the science of the sea.\n- [The Drop-In Blog @ Moss Landing Marine Labs](https://mlmlblog.wordpress.com/)\n- [Wood's Hole Oceanographic Institution](http://www.whoi.edu/main/blogs-expeditions) - Blogs and Expeditions\n- [Scripp's Institute of Oceanography](https://scripps.ucsd.edu/news/blogs-and-expeditions) - Blogs and Expeditions\n- [Ocean Alliance](http://whale.org) - Ocean Alliance Blog\n- [Notey Blogs on Oceanography](http://www.notey.com/blogs/oceanography) - Aggregated list of interesting marine blogs and blog posts.\n- [Southern Fried Science](http://www.southernfriedscience.com/)\n- [oceans4ever](http://oceans4ever.com/)\n- [Skidaway Institutes](https://oceanscience.wordpress.com/) - A blog for the Skidaway Institute of Oceanography faculty and staff\n- [Ocean Portal](http://ocean.si.edu/blog) - Smithsonian Museum of Natural History Ocean Blog\n- [V & the Sea](http://vdives.blogspot.com/)\n- [Words in Mocean](https://wordsinmocean.com/)\n- [Ya like Dags?](http://yalikedags.southernfriedscience.com/) - Appreciating the Unappreciated Sharks\n- [Oceans of Opportunity](http://oceanopportunity.com/) - This site is the web portal into the undersea work and world of Michael Lombardi.\n- [Oceana Blog](http://usa.oceana.org/blog)\n- [British Oceanographic Data Center Blog](http://blog.bodc.ac.uk/)\n- [Carl Safina's Blog](http://carlsafina.org/)\n- [Deep Blue Home](http://deepbluehome.blogspot.com/)\n- [Deep Sea News](http://www.deepseanews.com/)\n- [Freaque Waves](http://freaquewaves.blogspot.com/)\n- [Coral Notes from the Field](http://coralnotesfromthefield.blogspot.com/)]\n- [Marine Conservation Institute Blog](https://blog.marine-conservation.org/)\n- [Monkey Face News](http://www.monkeyfacenews.com/my-blog/)\n- [Ocean Doctor](http://oceandoctor.org/)\n- [Oceanographer's Choice](http://www.oceanographerschoice.com/)\n\nHave more to add? [Create an issue!](https://github.com/hackforthesea/resources/issues/new)\n\n## Datasets\n\n### Cornell Lab's Macaulay Library of Natural Sounds\n- https://www.macaulaylibrary.org/\n\n## NOAA Digital Coast\n- https://coast.noaa.gov/digitalcoast/\n\n## Pangaea - Data Publisher for Earth & Environmental Science\n- https://www.pangaea.de/\n\n## Global Biodiversity Information Facility\n- https://www.gbif.org/\n\n#### World Register of Marine Species \n- http://marinespecies.org/aphia.php?p=webservice\n\n#### The International Council for Exploration of the Seas \n- http://www.ices.dk/marine-data/dataset-collections/Pages/default.aspx\n\n#### NOAA.gov Dataset search \n- https://data.noaa.gov/dataset\n\n#### NOAA Sea Turtles Data\n- http://seamap.env.duke.edu/\n- http://www.seaturtlestatus.org/learn/maps/all\n- https://www.sefsc.noaa.gov/species/turtles/strandings.htm\n\n#### Fish Watch - Provides the database on seafood counts \n- http://fishwatch.gov\n\n#### AEA Research\n- https://www.advancingecoag.com/aea-research\n\n#### Economics: National Ocean Watch \n- https://coast.noaa.gov/dataregistry/search/collection/info/enow \n- https://coast.noaa.gov/digitalcoast/training/diving-into-the-ocean-economy-with-economics.html\n\n#### Massachusetts Ocean Resource Information System \n- http://www.mass.gov/eea/agencies/czm/program-areas/mapping-and-data-management/moris/\n\n#### National \n- http://marinecadastre.gov/ \n- https://ioos.noaa.gov/data/\n- https://ioos.us/comt\n\n#### Global: \n\n- http://www.esri.com/industries/oceans \n- http://resources.arcgis.com/en/communities/oceans/ \n- http://shipmaps.exactearth.com/\n\n#### The Ocean Data Portal seems to be sound for quasi-real-time feeds: \n- http://www.oceandataportal.net/portal/\n\n#### Global Temperature and Salinity Profile Programme (GTSPP) dataset: \n- http://coastwatch.pfeg.noaa.gov/erddap/tabledap/erdGtsppBest.html?trajectory,org,type,platform,cruise&distinct()\n\n#### Bottom Sediments of Georges Bank dataset: \n\n- http://woodshole.er.usgs.gov/openfile/of03-001/data/seddata/wigley61/wigley61.zip\n\n#### Portland Fish Exchange's \"landing tool\"\n\n- http://www.pfex.org/info/\n\n#### British Oceanographic Data Center\n\n- https://www.bodc.ac.uk/data/published_data_library/\n\n#### Marine Data Systems Tools\n\n- http://www.searoutes.com/routing?speed=13&panama=true&suez=true&kiel=true\n- http://www.trusteddocks.com/\n- http://www2.searoutes.com/wp-includes/swagger-ui/#/\n\n#### Bureau of Ocean Energy Management \n\n- [Gulf of Mexico Deepwater Bathymetry Map](https://www.boem.gov/Gulf-of-Mexico-Deepwater-Bathymetry/)\n- [Oil and Gas Energy Programs Maps and GIS Data](https://www.boem.gov/Maps-and-GIS-Data/)\n- [Renewable Energy Program Maps and GIS Data](https://www.boem.gov/Renewable-Energy-Program-Mapping-and-Data/)\n- [Statistics and Facts](https://www.boem.gov/Statistics-and-Facts/)\n- [Research and Studies](https://www.boem.gov/Marine-Minerals-Research-and-Studies/)\\\n\n#### Data Dryad Data Sets\n\n- [Starts with Ocean](https://datadryad.org/search-filter)\n\n## Hardware\n- http://ocean-innovations.net/products/\n- http://ocean-innovations.net/companies/global-ocean-design/products/10-polystyrene-spheres/instrumentation-sphere-g-141/\n- http://ocean-innovations.net/companies/linkquest/tracklink-models/tracklink-1500-series/\n- http://www.teledynemarine.com/920-series-atm-926?ProductLineID=8\n- http://www.dupont.com/products-and-services/plastics-polymers-resins/thermoplastics/brands/delrin-acetal-resin.html\n- http://plasticworld.ca/store/index.php?main_page=product_info&products_id=41\n- http://www.novabraid.com/rope/spectec-12/\n\n#### Communications\n- https://www.metocean.com/shop/telematics/satellite-phones/\n- https://www.metocean.com/product/shout-ts/\n- https://www.iridium.com/products/iridium-go/\n- https://www.iridium.com/products/iridium-go/\n\n#### Acoustic Monitoring\n- https://www.metocean.com/shop/defence-security/acoustic-monitoring/ \n\n#### EM/ER\n- http://www.archipelago.ca/fisheries-monitoring/electronic-monitoring/\n- https://www.immervisionenables.com/\n\n## Research\n#### The tides are not what you think - the 29.52 day Spring Neap Cycle\n- http://www.sciencedirect.com/science/article/pii/S0025322706002544\n\n## Previous Hackathon Submissions\n- https://public.tableau.com/profile/cuong.nguyen1823#!/vizhome/Hack_for_the_Sea/\n- https://public.tableau.com/profile/bill.ostaski#!/vizhome/ClimateChangeSourcesandEffects/ClimateChangeSourcesandEffects\n- https://github.com/Feneric/LobstaListen\n- https://github.com/morganrstewart/hackforthesea16\n\n## Other Awesome Repositories\n\n- [FishAnnotator](https://github.com/BGWoodward/FishAnnotator) is an application that facilitates annotation of videos and images of fish. Eventually it will be expanded to automatically create annotations using computer vision algorithms.\n"
 },
 {
  "repo": "TEOS-10/GSW-Fortran",
  "language": "Fortran",
  "readme_contents": "==========================================================================\r\n Gibbs SeaWater (GSW) Oceanographic Toolbox of TEOS-10 (Fortran)\r\n==========================================================================\r\n\r\n This is a subset of functions contained in the Gibbs SeaWater (GSW)\r\n Oceanographic Toolbox of TEOS-10.\r\n\r\n Version 1.0 written by David Jackett\r\n Modified by Paul Barker (version 3.02)\r\n Modified by Glenn Hyland (version 3.04+)\r\n\r\n For help with this Oceanographic Toolbox email: help@teos-10.org\r\n\r\n This software is available from http://www.teos-10.org\r\n\r\n==========================================================================\r\n\r\n gsw_data_v3_0.nc\r\n NetCDF file that contains the global data set of Absolute Salinity Anomaly\r\n Ratio, the global data set of Absolute Salinity Anomaly atlas, and check\r\n values and computation accuracy values for use in gsw_check_function.\r\n The data set gsw_data_v3_0.nc must not be tampered with.\r\n\r\n gsw_check_function.f90\r\n Contains the check functions. We suggest that after downloading, unzipping\r\n and installing the toolbox the user runs this program to ensure that the\r\n toolbox is installed correctly and there are no conflicts. This toolbox has\r\n been tested to compile and run with gfortran.\r\n\r\n==========================================================================\r\n\r\n Build using Make:\r\n cd test\r\n make\r\n ./gsw_check\r\n ./poly_check\r\n\r\n==========================================================================\r\n\r\n Build using CMake:\r\n   static and shared libraries\r\n   build 'gsw_check_functions' and 'gsw_poly_check' on demand\r\n   provides a package configuration\r\n   Git submodules support\r\n   Window (Visual Studio), Mac and Linux support\r\n\r\n Windows (using a terminal):\r\n   mkdir build; cd build\r\n   cmake ..\r\n   Open gsw.sln in Visual Studio\r\n\r\n Mac, Linux:\r\n   mkdir build; cd build\r\n   cmake ..\r\n   make\r\n   ctest\r\n\r\n Use a Git submodule (using extern/gsw as target folder):\r\n   git submodule add https://github.com/TEOS-10/GSW-Fortran extern/gsw\r\n   git submodule init\r\n   git submodule update\r\n    ... add e.g. target_link_libraries(my_target STATIC gsw_static)\r\n    ... in your CMakeLists.txt. It is not necessary to compile and install\r\n    ... the TEOS-10/GSW library - the compilation and linking will be done\r\n    ... due to the target_link_libraries() statement.\r\n\r\n Note that gfortran is the name of the GNU Fortran project, developing a\r\n free Fortran 95/2003/2008 compiler for GCC, the GNU Compiler Collection.\r\n It is available from http://gcc.gnu.org/fortran/\r\n\r\n==========================================================================\r\n"
 },
 {
  "repo": "ngs-docs/2016-metagenomics-sio",
  "language": "HTML",
  "readme_contents": "2016 / October / Metagenomics\n=============================\n\nThis workshop was given on October 12th and 13th, 2016,\nby Harriet Alexander and C. Titus Brown, at the Scripps Institute of\nOceanography.\n\nSee https://2016-metagenomics-sio.readthedocs.io/en/latest/ for the\nrendered version of this site.\n\n----\n\nTo look into:\n\n* https://github.com/chrisquince/DESMAN for binning\n* https://github.com/DRL/blobtools - blobtools viz\n"
 },
 {
  "repo": "gvoulgaris0/WavePart",
  "language": "HTML",
  "readme_contents": "# WavePart ![image](https://user-images.githubusercontent.com/48567321/126871118-68ae6e82-15fe-48e5-ac1e-2d1d0b673a9b.png)\n\n\nSet of Matlab(r) functions for the partition of directional wave spectra to its wind and different swell components. The partitions are initially identified using a watershed defining algorithm  and then are modified following mostly the method described in Hanson and Phillips (2001).\n\nAuthors:  \n  Douglas Cahl and George Voulgaris  \n  School of the Earth, Ocean and Environment  \n  University of South Carolina  \n  Columbia, SC, 29205, USA.  \n  \nCode Updates:\n  -  1/22/2020 - waveparams.m - the mean freq. (fm) estimate was incorrect; it has been corrected.\n  -  1/22/2020 - partition.m was updated to catch cases when a flat spectrum is given as input.\n  -  1/22/2020 - master_partition.m was updated so that it calls waveparams.m with the correct number of outputs.\n  -  1/21/2020 - waveparams.m - the Hrms estimate was incorrect; also the mean direction now is given in -180 to +180 deg range.\n  \nCitation:  \n   -  Douglas, C. and Voulgaris, G., 2019, WavePART: MATLAB(r) software for the partition of directional ocean wave spectra.: Zenodo, doi:10.5281/zenodo.2638500. \n\nRelevant References:  \n   -  J.L. Hanson and O.M. Philips, 2001. Automated Analysis of Ocean Surface Directional  Wave Spectra. Journal of Oceanic and Atmospheric Technology, 18, 278-293.   \n   -  J. Portilla, F.J. Ocampo-Torres, and J. Monbaliu, 2009. Spectral Partitioning and Identification of Wind Sea and Swell.  Journal of Oceanic and Atmospheric Technology, 26, 107-121. DOI: 10.1175/2008JTECHO609.1   \n   -  E. Cheynet, 2019. Pcolor in Polar Coordinates (https://www.mathworks.com/matlabcentral/fileexchange/49040-pcolor-in-polar-coordinates), MATLAB Central File Exchange. Retrieved March 16, 2019.  \n"
 },
 {
  "repo": "gcosne/OceanographyProject",
  "language": "Jupyter Notebook",
  "readme_contents": "# Coupling Oceanic Observation Systems to Study Mesoscale Ocean Dynamics\n\nThe paper related to the code implemented can be found here : [arxiv](https://arxiv.org/abs/1910.08573).\n\n\n**Abstract** : Understanding local currents in the North Atlantic region of the ocean is a key part of modelling heat transfer and global climate patterns. Satellites provide a surface signature of the temperature of the ocean with a high horizontal resolution while in situ autonomous probes supply high vertical resolution, but horizontally sparse, knowledge of the ocean interior thermal structure. The objective of this paper is to develop a methodology to combine these complementary ocean observing systems measurements to obtain a three-dimensional time series of ocean temperatures with high horizontal and vertical resolution. Within an observation-driven framework, we investigate the extent to which mesoscale ocean dynamics in the North Atlantic region may be decomposed into a mixture of dynamical modes, characterized by different local regressions between Sea Surface Temperature (SST), Sea Level Anomalies (SLA) and Vertical Temperature fields. Ultimately we propose a Latent-class regression method to improve prediction of vertical ocean temperature.\n\n**LATEST VERSION OF THE NOTEBOOK can be found on colaboratory here** : [colab](https://colab.research.google.com/drive/1xX_XcPrx6cdHfIDTYd7K7BpJnu5LliDv)\n"
 },
 {
  "repo": "wavebitscientific/ndbc",
  "language": "Python",
  "readme_contents": "# ndbc\n\n[![Build Status](https://travis-ci.org/wavebitscientific/ndbc.svg?branch=master)](https://travis-ci.org/wavebitscientific/ndbc)\n[![GitHub issues](https://img.shields.io/github/issues/wavebitscientific/ndbc.svg)](https://github.com/wavebitscientific/ndbc/issues)\n\nA Python interface to National Data Buoy Center data.\n\n## Getting started\n\n```\npip install git+https://github.com/wavebitscientific/ndbc\n```\n\n## Usage\n\n```python\nfrom ndbc import Station\nfrom datetime import datetime\n\n# initialize without getting the data\nstation = Station(42001)\n\nstation.name\n# 'MID GULF - 180 nm South of Southwest Pass, LA'\n\nstation.lon\n# -89.668\n\nstation.lat\n# 25.897\n\n# initialize and get the data\nstation = Station(42001, datetime(2017,10,1), datetime(2017,11,1))\n\n# get a different time window\nstation.get_stdmet(datetime(2015,1,1), datetime(2017,1,1))\n\n```\n\n## Features\n\n* [x] Standard meteorological data: wind speed and direction, air pressure, air and water temperature, dew-point temperature, wave height, period, and direction.\n* [ ] Omnidirectional (1-d) wave spectrum data\n* [ ] Directional (2-d) wave spectrum data\n* [ ] Derived diagnostics\n* [ ] [What else?](https://github.com/wavebitscientific/ndbc/issues/new)\n"
 },
 {
  "repo": "SHYFEM-model/shyfem",
  "language": "Fortran",
  "readme_contents": "\n#------------------------------------------------------------------------\n#\n#    Copyright (C) 1985-2020  Georg Umgiesser\n#\n#    This file is part of SHYFEM.\n#\n#    SHYFEM is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU General Public License as published by\n#    the Free Software Foundation, either version 3 of the License, or\n#    (at your option) any later version.\n#\n#    SHYFEM is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n#    GNU General Public License for more details.\n#\n#    You should have received a copy of the GNU General Public License\n#    along with SHYFEM. Please see the file COPYING in the main directory.\n#    If not, see <http://www.gnu.org/licenses/>.\n#\n#------------------------------------------------------------------------\n\n    This is the README file for the SHYFEM model\n\n#------------------------------------------------------------------------\n\n    Contact:\n\n    Georg Umgiesser\n    Oceanography, ISMAR-CNR\n    Arsenale Tesa 104, Castello 2737/F\n    30122 Venezia\n    Italy\n\n    Tel.   : +39-041-2407943\n    Fax    : +39-041-2407940\n    E-Mail : georg.umgiesser@ismar.cnr.it\n\n#------------------------------------------------------------------------\n\n    Availability:\n\n    https://github.com/SHYFEM-model/\n\n    Comments and issues should be posted in the Issues section \n    on the abovementioned GitHub repository\n\n#------------------------------------------------------------------------\n\n    Documentation:\n\n    The manual can be found in femdoc/final/shyfem.pdf\n\n    Example applications can be found in examples\n\n#------------------------------------------------------------------------\n\n    The software is also supported by various institutions:\n\n    CNR - ISMAR (Istituto di Scienze Marine)\n    ---------\n\tGeorg Umgiesser, system engineering\n\tChristian Ferrarin, renewal times, sediment transport\n\tMarco Bajo, system solvers, assimilation\n\tDebora Bellafiore, non-hydrostatic modeling, fluid mud\n\tMichol Ghezzo, ecological modeling, lagrangian model\n\tFrancesca De Pascalis, lagrangian model\n\tWilliam McKiver, non-hydrostatic modeling\n\n    CNR - IAS (Istituto di Ambiente Marino Costiero)\n    ---------\n\tAndrea Cucco, lagrangian model, renewal times\n\n    INOGS (Istituto Nazionale di Oceanografia e di Geofisica Sperimentale)\n    ---------\n\tCosimo Solidoro, ecological modeling, BFM\n\tDonata Melaku Canu, ecological modeling, EUTRO\n\tEric Pascolo, OMP parallelization\n \n    MRI (Marine Research Institute, Klaipeda University, Lithuania)\n    ---------\n\tPetras Zemlys, ecological modeling, AquaBC\n\t\n    Istanbul University\n    ---------\n\tAli Erturk, ecological modeling, AquaBC\n \n    Fondazione CMCC (Centro Mediterraneno per i cambiamenti climatici)\n    ---------\n\tSilvia Mocavero, MPI parallization\n\tGiorgio Micaletto, MPI parallization\n\t\n#------------------------------------------------------------------------\n\nDirectory structure\n===================\n\nexamples\texample applications\nfem3d\t\tFEM model and utility routines\nfemadj\t\tRoutines for the adjustment of the grid after the\n\t\tautomatic mesh generation. The program to run is shyadj.\nfemanim\t\tRoutines to generate animations from postscript files\nfembin\t\tBinaries used by the model\nfemcheck\tRoutines to set up and check the installation\nfemdoc\t\tDocumentation, manual, etc.\nfemdummy\tDummy directory\nfemersem\tRoutines for BFM (ERSEM) ecological model\nfemgotm\t\tRoutines for GOTM turbulence clousre model\nfemlib\t\tLibrary routines (populated during compilation)\nfemplot\t\tPlotting routines for post processing. The routine\n\t\tto run is shyplot.\nfemregres\tRoutines to do regression testing. Developers only.\nfemutil\t\tVarious utility programs\ngrid\t\tVisualization routine for GRD files\nhcbs\t\tOutdated files for plotting to monitor\nmesh\t\tAutomatic mesh generator\npost\t\tLibrary for plotting to Postscript files.\ntmp\t\tTemporary directory\n\nCompiling the model\n===================\n\nYou can compile everything from the SHYFEM directory by running the\ncommand \"make fem\". Other commands are:\n\nmake help\tgives help on available make targets\nmake fem\tcompiles everything\nmake doc\tmakes documentation in femdoc (manual)\nmake all\tcompiles (shyfem) and makes documents (doc)\nmake clean\tdeletes objetc and executable files in all subdirectories\nmake cleanall\tas clean, but cleans also libraries\n\n#------------------------------------------------------------------------\n\n"
 },
 {
  "repo": "GenSci/NDBC",
  "language": "Python",
  "readme_contents": "# NDBC\n\n![alt text](http://www.ndbc.noaa.gov/images/nws/noaaleft.jpg \"NOAA\") ![alt text](http://www.ndbc.noaa.gov/images/nws/ndbc_title.jpg \"NDBC\")\n\nThis repository represents my attempts to build out Python class(es)\nto facilitate the acquisition, analysis, and visualization of National\nData Buoy Center (NDBC) data. The goal is to develop a set of APIs to\nfacilitate rapid discovery of data resources, exploratory data analysis,\nand allow integration into automated data workflows.\n\n## NDBC.py\n\nThis file defines the DataBuoy class. The purpose of this class is to\nallow a user to define a specific data buoy they wish to gather data\nfrom and provide the user with methods to collect and analyze this data.\n\nDependencies are listed in `requirements.txt`\n\n## Usage\n\n#### Installation\n\nInstall using pip from PyPI\n\n```\npip install NDBC\n```\n\nThen you are ready to start using this module in exploratory data analyses and scripted workflows.\n\n#### Methods of DataBuoy Class\n\n`.set_station_id`\n\nIf a DataBuoy class has been instantiated without any `station_id` argument, this method allows for setting a station id\n\n```\nfrom NDBC.NDBC import DataBuoy\nDB = DataBuoy()\nDB.set_station_id('46042') # <- Either strings or numbers are acceptable\n```\n\n`.get_station_metadata()`\n\nPerform a scrape of the public webpage for a specified data station and save a dictionary of available metadata to the `.station_info` property. This is only available if a DataBuoy has a valid `station_id` set (either during class instantiation or using\nthe `set_station_id` method).\n\n```\nfrom NDBC.NDBC import DataBuoy\nDB = DataBuoy(46042)\nDB.get_station_metadata()\nDB.station_info\n{   'Air temp height': '4 m above site elevation',\n    'Anemometer height': '5 m above site elevation',\n    'Barometer elevation': 'sea level',\n    'Sea temp depth': '0.6 m below water line',\n    'Site elevation': 'sea level',\n    'Watch circle radius': '1789 yards',\n    'Water depth': '1645.9 m',\n    'lat': '36.785 N',\n    'lon': '122.398 W'}\n```\n\n- `.get_data(datetime_index=False)`\n\nAfter importing, the DataBuoy class is instantiated with the ID of the\nstation from which historical data is sought. Then data may be gathered for\nthe years and months specified. If no time period is specified, the most recent\nfull month available is retrieved.\n\nThe default behavior is to append datetime values built from date part columns (YY, MM, DD, etc.) to a column 'datetime'. If value `True` is passed as the `datetime_index` argument, the datetime values will be used as index values for the returned dataframe. In some cases this is advantageous for time series analyses.\n\n```\nfrom NDBC.NDBC import DataBuoy\n\nn42 = DataBuoy(46042)  # <- String or numeric station ids are valid\n\nn42.get_data(datetime_index=True)  # <- no year, month argumets so latest full month is retrieved. Default data type is 'stdmet'\n\nOct not available.   # <- Where data is missing, messages are returned to the terminal via a logger.warning() call\nSep not available.\n\nn42.data  # <- anticipating additional data collection methods, the .data property returns a dictionary.  Indiviudual\n               data products are returned as pandas DataFrame objects\n\n# Datetime objects are compiled from individual year, month, day, hour, minute columns and used as the index to support\n# slicing data by time frames.\n\n{'stdmet':          WDIR WSPD  GST  WVHT    DPD   APD  MWD    PRES  ATMP  WTMP   DEWP   VIS   TIDE\n2019-07-31 23:50:00  298  3.6  5.2  1.25   7.69  5.37  303  1015.1  13.4  15.2  999.0  99.0  99.00\n2019-08-01 00:50:00  301  5.7  7.2  1.26   7.14  5.42  306  1014.8  13.4  15.3  999.0  99.0  99.00\n2019-08-01 01:50:00  323  6.6  8.3  1.33   7.14  5.47  312  1014.5  13.2  15.1  999.0  99.0  99.00\n2019-08-01 02:50:00  347  5.8  7.7  1.32   7.69  5.15  319  1014.5  12.7  15.1  999.0  99.0  99.00\n2019-08-01 03:50:00  353  5.6  7.2  1.26   7.69  5.31  325  1014.9  12.6  15.0  999.0  99.0  99.00\n...                  ...  ...  ...   ...    ...   ...  ...     ...   ...   ...    ...   ...    ...\n2019-08-31 18:50:00  999  6.2  7.4  0.87  13.79  4.67  186  1014.6  17.0  17.2  999.0  99.0  99.00\n2019-08-31 19:50:00  999  6.8  8.3  0.83  13.79  4.56  178  1014.2  17.2  17.3  999.0  99.0  99.00\n2019-08-31 20:50:00  999  6.5  7.8  0.89  13.79  4.38  195  1013.8  17.5  17.4  999.0  99.0  99.00\n2019-08-31 21:50:00  999  7.5  8.9  0.95  13.79  4.52  190  1013.1  17.5  17.3  999.0  99.0  99.00\n2019-08-31 22:50:00  999  8.0  9.4  0.95  13.79  4.09  171  1012.7  17.7  17.1  999.0  99.0  99.00\n\n[741 rows x 13 columns]}\n```\n\nBy default the `get_data()` function will fetch the most current month's data. However, the function can take lists of years & months ([int]) to specify a timeframe.\n\n```\n>>> n42 = NDBC.DataBuoy('46042')\n>>> n42.get_data(months=[1,2], years=range(2019, 2020), datetime_index=True, data_type='swden)\nYear 2019 not available.\nYear 2020 not available.\n \n>>> n42.data\n{'swden': {'data':                      .0200  .0325  .0375  .0425  .0475  .0525  .0575  .0625  .0675  .0725  .0775  .0825  .0875  ...  .3000  .3100  .3200  .3300  .3400  .3500  .3650  .3850  .4050  .4250  .4450  .4650  .4850\n2021-01-01 00:40:00    0.0    0.0    0.0   0.00   1.17   9.11  24.25  24.95  15.84  20.44  26.48  20.63  12.72  ...   0.28   0.31   0.19   0.20   0.13   0.07   0.06   0.05   0.03   0.01   0.01   0.00    0.0\n2021-01-01 01:40:00    0.0    0.0    0.0   0.00   0.00  13.76  26.55  22.40  24.12  30.09  23.41  15.74  14.95  ...   0.25   0.16   0.12   0.16   0.06   0.16   0.06   0.03   0.05   0.02   0.01   0.00    0.0\n2021-01-01 02:40:00    0.0    0.0    0.0   0.00   0.93   4.40  16.03  33.95  41.48  38.02  31.47  18.88  14.59  ...   0.21   0.15   0.18   0.14   0.14   0.10   0.07   0.05   0.03   0.02   0.01   0.00    0.0\n2021-01-01 03:40:00    0.0    0.0    0.0   0.07   1.14   6.95  27.94  45.68  41.92  30.11  25.03  19.52  10.93  ...   0.22   0.20   0.16   0.09   0.08   0.15   0.09   0.04   0.02   0.01   0.00   0.01    0.0\n2021-01-01 04:40:00    0.0    0.0    0.0   0.00   0.76   3.64  11.23  18.23  29.84  27.19  12.85  11.20   9.77  ...   0.13   0.17   0.14   0.16   0.08   0.08   0.07   0.08   0.05   0.01   0.01   0.00    0.0\n...                    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...  ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...\n2021-02-28 19:40:00    0.0    0.0    0.0   0.00   0.00   0.00   0.06   0.25   1.42   2.50   9.48  11.48   8.46  ...   0.21   0.13   0.11   0.08   0.10   0.04   0.02   0.02   0.03   0.01   0.00   0.00    0.0\n2021-02-28 20:40:00    0.0    0.0    0.0   0.02   0.05   0.08   0.24   1.02   3.97   4.97   4.99   8.31  10.09  ...   0.21   0.07   0.09   0.06   0.05   0.10   0.04   0.03   0.01   0.01   0.00   0.00    0.0\n2021-02-28 21:40:00    0.0    0.0    0.0   0.00   0.00   0.15   0.30   0.36   1.63   4.18   6.85   7.82   7.98  ...   0.12   0.11   0.09   0.08   0.04   0.05   0.06   0.02   0.01   0.01   0.00   0.00    0.0\n2021-02-28 22:40:00    0.0    0.0    0.0   0.00   0.01   0.09   0.10   0.32   2.84   3.82   3.91   4.92   5.17  ...   0.17   0.09   0.13   0.05   0.05   0.08   0.06   0.03   0.01   0.01   0.00   0.00    0.0\n2021-02-28 23:40:00    0.0    0.0    0.0   0.00   0.00   0.00   0.18   0.25   1.78   3.97   5.08   4.98   5.40  ...   0.07   0.10   0.11   0.08   0.08   0.06   0.03   0.02   0.01   0.01   0.00   0.00    0.0\n\n[1413 rows x 47 columns]}}\n\n```\n\nLikely due to my own biases in my research interests, the `get_data()`  function will default to fetching\nstandard meteorological data.  However, users can specify different data packages like so `get_data(data_type='cwind')`.  To view which data packages\nare currently supported examine the `DataBuoy.DATA_PACKAGES` attribute:\n```\n{'cwind': {'name': 'Continous Wind Data', 'url_char': 'c'},\n 'srad': {'name': 'Solar radiation data', 'url_char': 'r'},\n 'stdmet': {'name': 'Standard meteoroligcal data', 'url_char': 'h'},\n 'swden': {'name': 'Spectral Wave Density data', 'url_char': 'w'},\n 'swdir': {'name': 'Spectral wave (alpha1) direction data', 'url_char': 'd'},\n 'swdir2': {'name': 'Spectral wave (alpha2) direction data', 'url_char': 'i'},\n 'swr1': {'name': 'Spectral wave (r1) direction data', 'url_char': 'j'},\n 'swr2': {'name': 'Spectral wave (r2) direction data', 'url_char': 'k'}}\n\n```\n\nUsing the pandas DataFrame to store the returned data provides access to the wide array of methods the pandas package\nprovides.\n\n- `.save(filename(optional))`\n\nSaves an instantiated DataBuoy object as JSON to a file. If `filename` is not specified the file name will follow the\n`databuoy_{station_id}.json` convention.\n\n```\ndb = DataBuoy(46042)\ndb.save('/path/to/file/my_filename.json')\n```\n\n_classmethod_\n\n- `.load(filename)`\n  Instantiate a DataBuoy object from a file, generated by the `.save()` method.\n\n```\ndb = DataBuoy.load('/path/to/file.json')\n```\n"
 },
 {
  "repo": "seaflow-uw/popcycle",
  "language": "R",
  "readme_contents": "Popcycle\n========\n**Popcycle** is an R package that offers a reproducible approach to process, calibrate and curate flow cytometry data collected by SeaFlow.\n\n<img src=\"documentation/images/seaflow-workflow.png?raw=true\" alt=\"Popcycle workflow\"\n\ttitle=\"Popcycle workflow\" align=\"right\" style=\"float\" width=\"500\">\nRaw  data are stored every 3 minutes in a custom binary file format (RAW data) consisting of six to eight 16-bit integer channels, along with [metadata](https://github.com/seaflow-uw/seaflow-sfl) provided by the ship's data system (e.g., time, location, sea surface temperature, salinity, light intensity). The files are stored in day-of-year-labeled directories, each containing raw files with the associated log file.\n\nThe software package performs 4 key analyses:\n1. ```Filtering```: Filter raw particle data down to optimally posistioned particles (OPP).\n2. ```Gating```: Classification of phytoplankton cell populations using a mixture of manual gating and a semi-supervized clusterting algorithm.\n3. ```Light scatter conversion```: Convert light scattering of each particle to cell diameter ([fsc-size-calibration](https://github.com/seaflow-uw/fsc-size-calibration)) and carbon content ([fsc-poc-calibration](https://github.com/seaflow-uw/fsc-poc-calibration)).\n4. ```Population data```: Perform aggregate statistics along with error propagation for the different populations.\n\nFiltered OPP data are stored in hourly [Parquet](https://github.com/apache/parquet-format) files.\nGated data (cell population identification) and calibrated data (diameter and carbon content) for each hourly OPP file are saved in separate hourly VCT Parquet files. The metadata, filtering and gating parameters, and aggregated statistics for each step are saved to a SQLite3 database files.\n\n### Analysis\nTo get started with the analysis, go to the [wiki](https://github.com/seaflow-uw/popcycle/wiki/SeaFlow-data-analysis-tutorial)\n"
 },
 {
  "repo": "FESOM/pyfesom",
  "language": "Jupyter Notebook",
  "readme_contents": "# pyfesom\n\nCollection of tools for basic handling of FESOM ocean model output.\n\n[![MIT licensed](https://img.shields.io/badge/license-MIT-blue.svg)](https://github.com/koldunovn/pyfesom/blob/master/LICENSE) [![Documentation Status](https://readthedocs.org/projects/pyfesom/badge/?version=latest)](http://pyfesom.readthedocs.io/en/latest/?badge=latest)\n\n## Documentation (tools and library)\n\nhttp://pyfesom.readthedocs.io/en/latest/index.html\n\n## Examples of library usage\n\n- [Show fesom mesh](https://github.com/koldunovn/pyfesom/blob/master/notebooks/show_mesh.ipynb)\n- [Plot variable on original grid](https://github.com/koldunovn/pyfesom/blob/master/notebooks/show_variable_on_original_grid.ipynb)\n- [Plot simple diagnostic (mean, std)](https://github.com/koldunovn/pyfesom/blob/master/notebooks/plot_simple_diagnostics.ipynb)\n- [Interpolate to regular grid](https://github.com/koldunovn/pyfesom/blob/master/notebooks/interpolate_to_regular_grid.ipynb)\n- [Compare to climatology](https://github.com/koldunovn/pyfesom/blob/master/notebooks/compare_to_climatology.ipynb)\n\n## Requirements\n\nWe try to support both python 2.7 and possibly 3.4.\n\n- numpy\n- scipy\n- pandas\n- netCDF4\n\n\n\n"
 },
 {
  "repo": "gher-ulg/Documentation",
  "language": null,
  "readme_contents": "# GHER documentation\n\nThis wiki is meant to contain all the information that does not fit specific projects, software tools or personal pages.\n\nTypically the content of the [Software section](http://modb.oce.ulg.ac.be/mediawiki/index.php/Software) of the previous wiki.\nOnce a section has been moved to the current wiki, the corresponding link should be added in the former wiki.\n\n## Conversion from mediawiki\n\nTo convert from `mediawiki` format to `markdown`, use [pandoc](http://pandoc.org/) conversion tool.   \nAn example for the conversion of the Python section:\n```bash\npandoc -f mediawiki -t markdown Python.wiki -o Python.md\n```\nwhere `Python.wiki` is a file whose content is obtained as a copy/paste of the mediawiki source\n\n## Other contents\n\n**Projects:** should be listed at http://labos.ulg.ac.be/gher/projects/ with a link to the project web page.    \n**Personal pages:** links at http://labos.ulg.ac.be/gher/people/.    \n**Software tools:** links to [github](https://github.com/gher-ulg) from http://labos.ulg.ac.be/gher/software-2/.    \n\n## Table of contents\n\nUse [gh-md-toc](https://github.com/ekalinin/github-markdown-toc) on the main markdown file to generate the table of contents (not dynamically). Then copy the result in the sidebar (right-hand side).\n\n\n"
 },
 {
  "repo": "janadr/time_series_analysis",
  "language": "Jupyter Notebook",
  "readme_contents": "# Time series analysis\nPython equivalent code for a course in data analysis in oceanography.\n"
 },
 {
  "repo": "mvdh7/pytzer",
  "language": "Python",
  "readme_contents": "# Pytzer\n\n![Tests](https://github.com/mvdh7/pytzer/workflows/Tests/badge.svg)\n[![Coverage](https://github.com/mvdh7/pytzer/blob/master/.misc/coverage.svg)](https://github.com/mvdh7/pytzer/blob/master/.misc/coverage.txt)\n[![pypi badge](https://img.shields.io/pypi/v/pytzer.svg?style=popout)](https://pypi.org/project/pytzer/)\n[![DOI](https://img.shields.io/badge/DOI-10.5281%2Fzenodo.2637914-informational)](https://doi.org/10.5281/zenodo.2637914)\n[![Docs](https://readthedocs.org/projects/pytzer/badge/?version=latest&style=flat)](https://pytzer.readthedocs.io/en/latest/)\n[![License: GPL v3](https://img.shields.io/badge/License-GPLv3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n\nPytzer is a Python implementation of the Pitzer model for chemical activities in aqueous solutions [[P91](https://pytzer.readthedocs.io/en/jax/refs/#p)] plus solvers to determine the equilibrium state of the system.\n\n**Pytzer is in beta!  Use at your own peril.**\n\n- [Pytzer](#pytzer)\n  - [Installation](#installation)\n    - [For general use](#for-general-use)\n    - [For development](#for-development)\n  - [Documentation](#documentation)\n  - [Citation](#citation)\n\n## Installation\n\nDue to its dependency on [JAX](https://github.com/google/jax), Pytzer can only be installed on Unix systems, although it does work on Windows via [WSL](https://docs.microsoft.com/en-us/windows/wsl/).\n\n### For general use\n\nInstall with pip:\n\n    pip install pytzer\n\nNote that you should also install the dependencies (especially NumPy) using pip, not conda, for best performance.  This happens automatically with the above command if the dependencies are not already installed.\n\nOnce installed, you will need to set the environment variable `JAX_ENABLE_X64=True`.  For example, using conda:\n\n    conda env config vars set JAX_ENABLE_X64=True\n\n### For development\n\nUse the [environment.yml](https://github.com/mvdh7/pytzer/blob/master/environment.yml) file to create a new environment with Conda:\n\n    conda env create -f environment.yml\n\nThen, fork and/or clone this repo to somewhere that your Python can see it.\n\n## Documentation\n\nA work in progress at [pytzer.readthedocs.io](https://pytzer.readthedocs.io/en/latest/).\n\n## Citation\n\nPytzer is maintained by [Dr Matthew P. Humphreys](https://humphreys.science) at the [NIOZ Royal Netherlands Institute for Sea Research](https://www.nioz.nl/en) (Texel, the Netherlands).\n\nFor now, the appropriate citation is:\n\n> Humphreys, Matthew P. and Schiller, Abigail J. (2021). Pytzer: the Pitzer model for chemical activities and equilibria in aqueous solutions in Python (beta).  *Zenodo.*  [doi:10.5281/zenodo.2637914](https://doi.org/10.5281/zenodo.2637914).\n\nPlease report which version of Pytzer you are using.  You can find this in Python with:\n\n```python\nimport pytzer as pz\npz.hello()\n```\n"
 },
 {
  "repo": "mpiannucci/gribberish",
  "language": "Rust",
  "readme_contents": "# gribberish\n\nRead [GRIB 2](https://en.wikipedia.org/wiki/GRIB) files with Rust.\n\nSee [`python`](python/README.md) for usage with `python` and `xarray`\n\nSee [`node`](node/README.md) for usage with `nodejs`\n\n## Getting Started\n\nAdd the package in `Cargo.toml` unser `[dependencies]`:\n\n```toml\ngribberish = { git = \"https://github.com/mpiannucci/gribberish\" }\n```\n\nSee [read.rs](tests/read.rs) for example usage for simple reading, or [message-dump](examples/message-dump/main.rs) for an example of dumping grib metadata to stdout. \n\n## License\n\n[MIT](LICENSE) -  2022 Matthew Iannucci"
 },
 {
  "repo": "miniufo/xinvert",
  "language": "Jupyter Notebook",
  "readme_contents": "# xinvert\n\n![animate plot](https://raw.githubusercontent.com/miniufo/xinvert/master/pics/animateConverge.gif)\n\n\n## 1. Introduction\nResearches on meteorology and oceanography usually encounter [inversion problems](https://doi.org/10.1017/CBO9780511629570) that need to be solved numerically.  One of the classical inversion problem is to solve Poisson equation for a streamfunction $\\psi$ given the vertical component of vorticity $\\zeta$ and proper boundary conditions.\n\n> $$\\nabla^2\\psi=\\zeta$$\n\nNowadays [`xarray`](http://xarray.pydata.org/en/stable/) becomes a popular data structure commonly used in [Big Data Geoscience](https://pangeo.io/).  Since the whole 4D data, as well as the coordinate information, are all combined into [`xarray`](http://xarray.pydata.org/en/stable/), solving the inversion problem become quite straightforward and the only input would be just one [`xarray.DataArray`](http://xarray.pydata.org/en/stable/) of vorticity.  Inversion on the spherical earth, like some meteorological problems, could utilize the spherical harmonics like [windspharm](https://github.com/ajdawson/windspharm), which would be more efficient using FFT than SOR used here.  However, in the case of ocean, SOR method is definitely a better choice in the presence of land/sea mask.\n\nMore importantly, this could be generalized into a numerical solver for elliptical equation using [SOR](https://mathworld.wolfram.com/SuccessiveOverrelaxationMethod.html) method, with spatially-varying coefficients.  Various popular inversion problems in geofluid dynamics will be illustrated as examples.\n\nOne problem with SOR is that the speed of iteration using **explicit loops in Python** will be **e-x-t-r-e-m-e-l-y ... s-l-o-w**!  A very suitable solution here is to use [`numba`](https://numba.pydata.org/).  We may try our best to speed things up using more hardwares (possibly GPU).\n\nClassical problems include:\n- [Poisson equation](https://github.com/miniufo/xinvert/blob/master/notebooks/1.%20Invert%20Poisson%20equation.ipynb);\n- [Gill-Matsuno model](https://github.com/miniufo/xinvert/blob/master/notebooks/2.%20Invert%20Gill-Matsuno%20model.ipynb);\n- [Stommel-Munk model](https://github.com/miniufo/xinvert/blob/master/notebooks/3.%20Wind-driven%20ocean%20circulation.ipynb);\n- [Geopotential equation](https://github.com/miniufo/xinvert/blob/master/notebooks/4.%20Geopotential%20model.ipynb);\n- [Omega equation](https://github.com/miniufo/xinvert/blob/master/notebooks/5.%20Omega%20equation.ipynb);\n- Eliassen balance vortex model;\n- potential vorticity inversion...\n\n---\n## 2. How to install\n**Requirements**\n`xinvert` is developed under the environment with `xarray` (=version 0.15.0), `dask` (=version 2.11.0), `numpy` (=version 1.15.4), and `numba` (=version 0.51.2).  Older versions of these packages are not well tested.\n\n**Install via pip** (not yet)\n```\npip install xinvert\n```\n\n**Install from github**\n```\ngit clone https://github.com/miniufo/xinvert.git\ncd xinvert\npython setup.py install\n```\n\n\n---\n## 3. Example: Helmholtz decomposition\nThis is a classical problem in both meteorology and oceanography that a vector flow field can be deomposed into rotational and divergent parts, where rotational and divergent parts are represented by the streamfunction and velocity potential.  Given vorticity (vor) and divergence (div) as the forcing functions, one can invert the streamfunction and velocity potential as:\n```python\nfrom xinvert import invert_Poisson\n\npsi = invert_Poisson(vor, dims=['lat','lon'], BCs=['extend', 'periodic'])\nchi = invert_Poisson(div, dims=['lat','lon'], BCs=['extend', 'periodic'])\n```\n### 3.1 Atmospheric demonstration\nHere is an atmospheric demonstration with no lateral boundaries:\n```python\nimport xarray as xr\nfrom xinvert import invert_Poisson\n\ndset = xr.open_dataset('data.nc')\n\nvor = dset.vor\n\n# Invert within lat/lon plane, with extend and periodic boundary\n# conditions in lat and lon respectively\npsi = invert_Poisson(vor, dims=['lat','lon'], BCs=['extend', 'periodic'])\n```\n![atmospheric plot](https://raw.githubusercontent.com/miniufo/xinvert/master/pics/atmosExample.png)\n\n\n### 3.2 Oceanic demonstration\nHere is a oceanic demonstration with complex lateral boundaries of land/sea:\n```python\nimport xarray as xr\nfrom xinvert import invert_Poisson\n\ndset = xr.open_dataset('mitgcm.nc')\n\nvor = dset.vor\n\n# Invert within YG/XGplane, with fixed and periodic boundary respectively.\n# Kwarg undef is used as a mask for land value.\npsi = invert_Poisson(vor, dims=['YG','XG'], BCs=['fixed', 'periodic'], undef=0)\n```\n![oceanic plot](https://raw.githubusercontent.com/miniufo/xinvert/master/pics/oceanExample.png)\n\n### 3.3 Animate the convergence of iteration\nOne can see the whole convergence process of SOR iteration as:\n```python\nfrom xinvert import invert_Poisson_animated\n\n# input of vor need to be two dimensional only;\n# psi has one more dimension than vor as iteration, which could be animated over.\n# Here psi has 40 frames and loop 1 per frame (final state is after 40 iterations)\npsi = invert_Poisson_animated(vor[0,0] BCs=['extend', 'periodic'],\n                              loop_per_frame=1, max_loop=40)\n```\n![animate plot](https://raw.githubusercontent.com/miniufo/xinvert/master/pics/animateConverge.gif)\n\nMore examples can be found at these notebooks:\n1. [Poisson equation for streamfunction/velocity potential](https://github.com/miniufo/xinvert/blob/master/notebooks/1.%20Invert%20Poisson%20equation.ipynb)\n2. [Matsuno-Gill model for heat-induced tropical circulation](https://github.com/miniufo/xinvert/blob/master/notebooks/2.%20Invert%20Gill-Matsuno%20model.ipynb)\n3. [Stommel-Munk model for wind-driven ocean circulation](https://github.com/miniufo/xinvert/blob/master/notebooks/3.%20Wind-driven%20ocean%20circulation.ipynb)\n4. [Geopotential equation for balanced mass field](https://github.com/miniufo/xinvert/blob/master/notebooks/4.%20Geopotential%20model.ipynb);\n5. [Omega equation for quasi-geostrophic vertical motion](https://github.com/miniufo/xinvert/blob/master/notebooks/5.%20Omega%20equation.ipynb);\n6. Eliassen balance vortex model for Hadley circulation and cyclones\n7. PV inversion model for the balanced mass and flow \n8. background reference state\n\nmore to be added...\n"
 },
 {
  "repo": "ctroupin/Python-Course-Cadiz",
  "language": "Jupyter Notebook",
  "readme_contents": "# Introduction to Scientific Python\n\nThe main objective of the course is to make users familiar with the Python programming language.     \nThe course is mostly targeted to users without a strong previous experience with Python, but with some notions of programming (MATLAB, Octave, ...). The different applications presented are oriented to Oceanography in general.\n\n## Content\n\nThe content can be divided into 4 parts:\n1. General description of the language.\n2. Reading/writing information from/to files.\n3. Data processing and analysis.\n4. Data visualisation.\n\nThe material is organised into sub-folders:\n- in [notebooks](./notebooks/) you find the `jupyter-notebook` describing the different programs.\n- [exercises](./exercises/) contains some exercises that will be explained and solved during the course.\n\n## How to start?\n\nDuring the course, participants will use virtual machines (VM) on which all the necessary tools are installed, in order to avoid the sometimes tedious installation steps, often depending on the operating system.\n"
 },
 {
  "repo": "jinbow/popy",
  "language": "Python",
  "readme_contents": "popy\n====\n\nCommonly used routines in Physical Oceanography."
 },
 {
  "repo": "TEOS-10/GSW-R",
  "language": "R",
  "readme_contents": "# gsw\n\n[![R-CMD-check](https://github.com/TEOS-10/GSW-R/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/TEOS-10/GSW-R/actions/workflows/R-CMD-check.yaml)\n![RStudio CRAN mirror downloads](http://cranlogs.r-pkg.org/badges/last-month/gsw)\n![RStudio CRAN mirror downloads](http://cranlogs.r-pkg.org/badges/last-week/gsw)\n![RStudio CRAN mirror downloads](http://cranlogs.r-pkg.org/badges/last-day/gsw)\n\ngsw is an R package that provides a connection to the thermodynamic equation of\nseawater as defined at [teos-10.org](http://www.teos-10.org).  An earlier version\nof gsw was referenced to TEOS-10 version 3.03, but the present one is\nreferenced to version 3.05-4, which is current as of Aug 7, 2017. See the\n[CRAN](https://cran.r-project.org/package=gsw) website for check statistics,\netc.\n\nAll the gsw functions reproduce [teos-10.org](http://www.teos-10.org) test\nvalues to a tolerance of 1.5e-8, the default for numerical comparison in R\nworking on a 64-bit machine. (The tests are part of the package-building\nprocess, as is usual in R.)\n\nFunction names, as well as most argument names, match those used in the TEOS-10\n[documentation](http://www.teos-10.org/pubs/gsw/html/gsw_contents.html).\n\n"
 },
 {
  "repo": "castelao/GSW-rs",
  "language": "Rust",
  "readme_contents": "# TEOS-10 GSW Oceanographic Toolbox in Rust\n\nGSW for microcontrollers.\n\n## Talks\n\nWe presented about goals and progress (as of Apr 2022) at the\n[SEA Improving Scientific Software 2022](https://sea.ucar.edu/conference/2022),\n[slides available here](https://github.com/castelao/GSW-rs/tree/talk_SEAS2022/doc/talks).\n\n## Minimum supported Rust version\n\nCurrently the minimum supported Rust version is 1.46.0\n\n## License\n\nLicensed under either of\n\n * Apache License, Version 2.0\n   ([LICENSE-APACHE](LICENSE-APACHE) or http://www.apache.org/licenses/LICENSE-2.0)\n * MIT license\n   ([LICENSE-MIT](LICENSE-MIT) or http://opensource.org/licenses/MIT)\n\nat your option.\n\n## Contribution\n\nUnless you explicitly state otherwise, any contribution intentionally submitted\nfor inclusion in the work by you, as defined in the Apache-2.0 license, shall be\ndual licensed as above, without any additional terms or conditions.\n"
 },
 {
  "repo": "euroargodev/argodmqc_owc",
  "language": "HTML",
  "readme_contents": "# pyowc: OWC salinity calibration in Python\n\n![build](https://github.com/euroargodev/argodmqc_owc/workflows/build/badge.svg)\n[![codecov](https://codecov.io/gh/euroargodev/argodmqc_owc/branch/refactor-configuration/graph/badge.svg)](https://codecov.io/gh/euroargodev/argodmqc_owc)\n[![Requirements Status](https://requires.io/github/euroargodev/argodmqc_owc/requirements.svg?branch=master)](https://requires.io/github/euroargodev/argodmqc_owc/requirements/?branch=refactor-configuration)\n[![Python 3.6](https://img.shields.io/badge/python-3.6-blue.svg)](https://www.python.org/downloads/release/python-360/)\n\n[![Gitter](https://badges.gitter.im/Argo-floats/owc-python.svg)](https://gitter.im/Argo-floats/owc-python?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)\n\nThis software is a python implementation of the \"OWC\" salinity calibration method used in Argo floats Delayed Mode Quality Control.\n\nThis software is in very active development and its usage may change any time. Please [post an issue to get involved if you're interested](https://github.com/euroargodev/argodmqc_owc/issues/new/choose).\n\n# Installation\n\nIf you are using Linux, Windows or macOS, you can simply install this package using `pip` in a virtual environment.\nAssuming your virtual environment is activated:\n\n```bash\npip install pyowc\n```\n\n# Software usage\n\n[![badge](https://img.shields.io/static/v1.svg?logo=Jupyter&label=Pangeo+Binder&message=Click+here+to+try+this+software+online+!&color=blue&style=for-the-badge)](https://binder.pangeo.io/v2/gh/euroargodev/argodmqc_owc/master?filepath=docs%2FTryit.ipynb)\n\nOtherwise, you can run the pyowc code in your local machine in this way:\n\nIn start_with_pycharm.py code, you can specify the WMO float number that you want to do analysis.\nYou can also add more float numbers, then the calculations of all floats will be done at the\nsame time.\n\n```python\nimport pyowc as owc\n\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\nif __name__ == '__main__':\n\n    FLOAT_NAMES = [\"3901960\"]  # add float names here e.g. [\"3901960\",\"3901961\",\"3901962\"]\n    USER_CONFIG = owc.configuration.load()  # fetch the default configuration and parameters\n    print(owc.configuration.print_cfg(USER_CONFIG))\n```\n\n## Parameters for your analysis\n\nParameters for the analysis are set in a configuration.py python code. \nThe configuration has the same parameters as the Matlab software (https://github.com/ArgoDMQC/matlab_owc).\n\n- You can change the default directories to locations of your historical data.\n```python\n        #    Climatology Data Input Paths\n        'HISTORICAL_DIRECTORY': \"data/climatology/\"\n        'HISTORICAL_CTD_PREFIX': \"/historical_ctd/ctd_\"\n        'HISTORICAL_BOTTLE_PREFIX': \"/historical_bot/bot_\"\n        'HISTORICAL_ARGO_PREFIX': \"/historical_argo/argo_\"\n```\n- To run the analysis,you need to have the float source file in .mat format. \n```python\n        #    Float Input Path\n        'FLOAT_SOURCE_DIRECTORY': \"data/float_source/\"\n        'FLOAT_SOURCE_POSTFIX': \".mat\"\n```\n- The output from the analysis will be saved in default directory of the code.You can change \nthe default directories to locations of your constants.\n```python\n        #    Constants File Path\n        'CONFIG_DIRECTORY': \"data/constants/\"\n        'CONFIG_COASTLINES': \"coastdat.mat\"\n        'CONFIG_WMO_BOXES': \"wmo_boxes.mat\"\n        'CONFIG_SAF': \"TypicalProfileAroundSAF.mat\"\n```\n- Final step is to set your objective mapping parameters, e.g.\n```python\n        'MAP_USE_PV': 0\n        'MAP_USE_SAF': 0\n\n        'MAPSCALE_LONGITUDE_LARGE': 8\n        'MAPSCALE_LONGITUDE_SMALL': 4\n        'MAPSCALE_LATITUDE_LARGE': 4\n        'MAPSCALE_LATITUDE_SMALL': 2\n ```\n- Additionally, you can set a specific ranges of theta bounds for salinity anomaly plot.\nThe code will crete two separate plots with set ranges.\n```python \n     #    Plotting Parameters\n        # Theta bounds for salinity anomaly plot\n        'THETA_BOUNDS': [[0, 5], [5, 20]]\n```\n\n## Plots\nThe plots are automatically generated. It is worth to note that only one plot will be \ndisplayed at one time in the PyCharm. The next plot will be displayed after closing\nthe window of the current plot. \n\nThe number of generated plots at specific theta levels (from 1 to 10 theta levels) can be\ncurrently changed in the dashboard.py code. The default is set to 2. The plots will be \ngenerated separately for each theta level.\n\n```python\ndef plot_diagnostics(float_dir, float_name, config, levels=2):\n```\n\n# Building the documentation\n\nIf you wish to build the documentation locally, you will need a virtual environment.\nAssuming your virtual environment is activated, follow these steps:\n\n1. Install the required documentation packages\n    ```bash\n    pip install -r requirements-docs.txt\n    ```\n2. Change directory to the `docs` directory, for example on Linux:\n    ```bash\n    cd docs\n    ```\n3. Run the `sphinx-build` command:\n    ```bash\n    sphinx-build -M html source build -W\n    ```\n\nThis will build the HTML documentation under the `docs/build/html` directory and can be viewed\nusing your normal web browser.\n\n```{admonition} Note\n:class: note\n\nIf you make modifications to the code or documentation configuration, you may need to delete\nthe `docs/source/generated` directory for the documentation to build correctly.\n```\n\n# Software history\n\n- Major refactoring of the software for performance optimisation and to fully embrace the Pythonic way of doing this !\n\n- [UAT: Phase 1, 2, 3](https://github.com/euroargodev/User-Acceptance-Test-Python-version-of-the-OWC-tool) \n\n- Migration of the code from from BODC/NOC git to euroargodev/argodmqc_owc. See https://github.com/euroargodev/User-Acceptance-Test-Python-version-of-the-OWC-tool/issues/10 for more details on the migration. Contribution from [G. Maze](https://github.com/gmaze)\n\n- Alpha experts user testings with [feedbacks available here](https://github.com/euroargodev/User-Acceptance-Test-Python-version-of-the-OWC-tool/issues). Contributions from: [K. Walicka](https://github.com/kamwal), [C. Cabanes](https://github.com/cabanesc), [A. Wong](https://github.com/apswong)\n\n- BODC created [the first version of the code](https://git.noc.ac.uk/bodc/owc-software-python), following the [Matlab implementation](https://github.com/ArgoDMQC/matlab_owc).\n  Contributions from: [M. Donnelly](https://github.com/matdon17), [E. Small](https://github.com/edsmall-bodc),\n   [K. Walicka](https://github.com/kamwal), [A. Hale](https://github.com/halebodc), [T. Gardner](https://github.com/thogar-computer).\n\n\n## New positioning of functions \nNote that functions name are not changed !\n\n- **pyowc/core**\n  - **stats.py**: brk_pt_fit, build_cov, covarxy_pv, covar_xyt_pv, noise_variance, signal_variance, fit_cond, nlbpfun\n  - **finders.py**: find_10thetas, find_25boxes, find_besthit, find_ellipse, nearest_neighbour\n\n- **pyowc/data**\n  - **fetchers.py**: get_region_data, get_region_hist_locations, get_data, get_topo_grid, frontal_constraint_saf\n  - **wrangling.py**: interp_climatology, map_data_grid \n\n- **pyowc/plot**\n  - **dashboard.py**: plot_diagnostics\n  - **plots.py**: cal_sal_curve_plot, sal_var_plot, t_s_profile_plot, theta_sal_plot, trajectory_plot\n  - **utils.py**: create_dataframe\n\n- **pyowc/calibration.py**: update_salinity_mapping, calc_piecewisefit\n\n- **pyowc/configuration.py**: load_configuration, set_calseries, print_cfg\n\n- **pyowc/tests**  # Contain all the unit tests !\n\n- **pyowc/utilities.py**: change_dates, cal2dec, potential_vorticity, wrap_longitudes, sorter, spatial_correlation\n"
 },
 {
  "repo": "OSU-CEOAS-Schmittner/UVic2.9",
  "language": "Fortran",
  "readme_contents": "# UVic2.9 with updated Marine Iron Biogeochemistry\nThis version contains the new marine iron biogeochemistry modifications from Somes et al., (2021; https://doi.org/10.1029/2021GB006948), which has now been implemented into MOBI2.1 updates level 09. Please note some marine biogeochemical parameter changes in run/control.in. \n\n# UVic2.9\nThis is the base code of the University of Victoria (UVic) climate model version 2.9 used at OSU. The source directory should not be changed. It is the original code without updates. \n\n## Further info\n* [OSU-UVic2.9 webpage](https://osu-ceoas-schmittner.github.io/UVic2.9/)\n* Model of Ocean Biogeochemistry and Isotopes [MOBI](https://github.com/andreasschmittner/UVic2.9/wiki/Model-of-Ocean-Biogeochemistry-and-Isotopes-(MOBI))\n* How to use git and github with this code: [OSU-UVic2.9 wiki](https://github.com/OSU-CEOAS-Schmittner/UVic2.9/wiki)\n"
 },
 {
  "repo": "pymoc/pymoc",
  "language": "Python",
  "readme_contents": "[![CircleCI](https://circleci.com/gh/pymoc/pymoc/tree/master.svg?style=shield)](https://circleci.com/gh/pymoc/pymoc/tree/master)\n[![Test Coverage](https://api.codeclimate.com/v1/badges/b03ff00b5c86d7afc364/test_coverage)](https://codeclimate.com/github/pymoc/PyMOC/test_coverage)\n[![Maintainability](https://api.codeclimate.com/v1/badges/b03ff00b5c86d7afc364/maintainability)](https://codeclimate.com/github/pymoc/PyMOC/maintainability)\n[![PyPI version](https://badge.fury.io/py/py-moc.svg)](https://badge.fury.io/py/py-moc)\n[![Documentation](https://img.shields.io/badge/docs-PyMOC-informational)](https://pymoc.github.io)\n[![License](https://img.shields.io/badge/license-MIT-informational)](LICENSE)\n\nPyMOC is a suite of python modules to build simple \"toy\" models for ocean's\nMeridional Overturning Circulation (MOC). \n\nThe model suite consists of several independent modules representing\nvarious ocean regions and dynamics. Specifically, there are modules\nfor calculating the 1-D advective-diffusive buoyancy tendencies averaged \nover ocean basins, given the net isopycnal transports in and out of the column.\nThe isopycnal transports are computed as diagnostic  relationships, with modules\nto calculate the wind- and eddy-driven residual circulation in a southern-ocean-like\nre-entrant channel, as well as the geostrophic exchange between different basins or\nbasin regions. These modules may be coupled to study the circulation in a wide range\nof different configurations.\n\nThe intended audiences for this model are researchers, educators and students\nin the geophysical sciences. The goal is to provide an accessible\nmodel appropriate for newcomers to geophysical modeling, but with physics\nthat reflect the current state of our theoretical understanding of the deep ocean\noverturning circulation.\n\nConfiguration and execution of the PyMOC suite requires relatively little\ntechnical knowledge or computational resources. All modules are written\nin pure Python, and the only core dependencies are the NumPy and SciPy\nlibraries. If configuration of your base system environment is undesirable,\na preconfigured Docker container has been made available with all required\nsoftware libraries pre-installed. \n\nAnybody is more than welcome to contibute to the development of PyMOC,\nbut is asked to adhere to the goal of keeping PyMOC well tested, stable,\nmaintainable, and documented. Further details on installation, configuration,\ncontribution, and issue reporting is available in the [documentation](https://pymoc.github.io).\n"
 },
 {
  "repo": "chouj/POPapers",
  "language": null,
  "readme_contents": "# [POPapers](https://twitter.com/geomatlab)\n\n![](https://img.shields.io/badge/dynamic/json?label=Twitter%20Followers&query=%24.data.totalSubs&url=https%3A%2F%2Fapi.spencerwoo.com%2Fsubstats%2F%3Fsource%3Dtwitter%26queryKey%3Dgeomatlab&color=1da1f2&logo=twitter&longCache=true&style=flat-square)\n\n###### A twitter account to auto-share new papers in the field of (Physical) Oceanography\n\n## Introduction\n\n![https://img.shields.io/badge/Powered%20by-RSS-orange.svg?style=plastic&logo=rss](https://img.shields.io/badge/Powered%20by-RSS-orange.svg?logo=rss)\n\n\"**POPapers**\" means Pysical Oceanography Papers. Actually, it also means \"papers pop out\". ^_^ \n\nThe account was created on 6 Apr 2013, when I wanted to utilize the `RSS` feeds provided by my preferred journals to help myself keep an eye on research frontiers. Soon I found it might be useful to other researchers, so the idea emerged natually when one was aware of the automation / information flow services like [IFTTT](https://ifttt.com) and [Zapier](https://zapier.com). Currently, it has been followed by [600+ worldwide researchers](https://twitter.com/geomatlab/followers) in the fields of oceanography, atmospheric sciences or climate change.\n\nActually, I have been harnessing useful information by using `RSS` technology since 2007. There is a category called \"[About RSS](https://chouj.github.io/categories/rss%E7%9B%B8%E5%85%B3/)\" on my blog. `RSS` is not dead apparently.\n\nIf you like this idea or have been benifited from it, please kindly **star this repo**. Cheers.\n\n## How to setup a similar one in your field\n\n<blockquote class=\"twitter-tweet\" data-partner=\"tweetdeck\"><p lang=\"en\" dir=\"ltr\">It&#39;s pretty easy to create your own as long as your preferred journals provide RSS feeds for recently-published papers. Then follow the instructions in this post [ <a href=\"https://t.co/3epPIhBdN3\">https://t.co/3epPIhBdN3</a>] with a ready twitter account for auto-posting.</p>&mdash; POPapers (@geomatlab) <a href=\"https://twitter.com/geomatlab/status/1125692588302819329?ref_src=twsrc%5Etfw\">May 7, 2019</a></blockquote>\n\n## Scrape RSS feeds of these journals (no particular order)\n\n- AGU journals including _Geophys. Res. Lett._, _J. Geophys. Res. -Ocean_, etc.\n- _J. Phys. Oceanogr._\n- Nature Publish Group journals including _Nature_, _Nat. Geosci._, _Nat. Comm._, _Nat. Clim. Change_, etc.\n- _Acta Oceanol. Sin._\n- _J. Mar. Sci._\n- _Prog. Oceanogr._\n- _Deep Sea Res._\n- _Cont. Shelf Res._\n- _Ocean Dynam._\n- _Dynam. Atmos. Oceans_\n- _Front. Mar. Sci._\n- _J. Fluid Dynam._\n- _Ann. Rev. Mar. Sci._\n- _Ann. Rev. Fluid Dynam._\n- _J. Clim._\n- _J. Oceanogr._\n- _Tellus A_\n- _Ocean Model._\n- _Ocean Sci._\n- _J. Atmos. Ocean. Tech._\n- _PlosOne: Oceanography_\n- _Clim. Dynam._\n- _J. Oper. Oceanogr._\n- _AGU EOS_\n- _ESSOAr_\n- ASLO pubs. eg. _Limnol. Oceanogr._\n- _Oceanogr._\n- _Remote Sen._ Ocean Remote Sensing Section\n- _Geosci. Model Dev._ Subject: Oceanography ![](https://img.shields.io/badge/NEW-APR%2012%202020-green)\n- _Earth Syst. Sci. Data_ Subject: Oceanography ![](https://img.shields.io/badge/NEW-APR%2012%202020-green)\n\n## Acknowledgement\n\n- [Twitter](https://twitter.com) | [IFTTT](https://ifttt.com) | [Zapier](https://zapier.com) | [Integromat](https://www.integromat.com/)\n\nThanks for solidary and reliable services.\n\n- [SUBSTATS](https://substats.spencerwoo.com/)\n\n## Else\n\nI also maintain a [Twitter list](https://twitter.com/chouj/lists/oceanography) full of researchers / organizations / projects in the field of (Physical) Oceanography.\n\n#### Buy me a cup of coffee\n\n[![Donate](https://img.shields.io/badge/Donate-PayPal-green.svg)](https://www.paypal.me/Mesoscale)\n[![Donate](https://img.shields.io/badge/Donate-WeChat-brightgreen.svg)](https://github.com/chouj/donate-page/blob/master/simple/images/WeChatQR.jpg?raw=true)\n[![Donate](https://img.shields.io/badge/Donate-AliPay-blue.svg)](https://github.com/chouj/donate-page/blob/master/simple/images/AlipayQR.jpg?raw=true)\n"
 },
 {
  "repo": "tagbase/tagbase-server",
  "language": "Python",
  "readme_contents": "# Tagbase server\n\n[![Bugs](https://sonarcloud.io/api/project_badges/measure?project=tagbase_tagbase-server&metric=bugs)](https://sonarcloud.io/summary/new_code?id=tagbase_tagbase-server)\n[![Code Smells](https://sonarcloud.io/api/project_badges/measure?project=tagbase_tagbase-server&metric=code_smells)](https://sonarcloud.io/summary/new_code?id=tagbase_tagbase-server)\n[![Coverage](https://sonarcloud.io/api/project_badges/measure?project=tagbase_tagbase-server&metric=coverage)](https://sonarcloud.io/summary/new_code?id=tagbase_tagbase-server)\n[![Duplicated Lines (%)](https://sonarcloud.io/api/project_badges/measure?project=tagbase_tagbase-server&metric=duplicated_lines_density)](https://sonarcloud.io/summary/new_code?id=tagbase_tagbase-server)\n![tagbase-server CI](https://github.com/tagbase/tagbase-server/actions/workflows/build.yml/badge.svg)\n[![Lines of Code](https://sonarcloud.io/api/project_badges/measure?project=tagbase_tagbase-server&metric=ncloc)](https://sonarcloud.io/summary/new_code?id=tagbase_tagbase-server)\n[![Maintainability Rating](https://sonarcloud.io/api/project_badges/measure?project=tagbase_tagbase-server&metric=sqale_rating)](https://sonarcloud.io/summary/new_code?id=tagbase_tagbase-server)\n[![Quality Gate Status](https://sonarcloud.io/api/project_badges/measure?project=tagbase_tagbase-server&metric=alert_status)](https://sonarcloud.io/summary/new_code?id=tagbase_tagbase-server)\n[![Reliability Rating](https://sonarcloud.io/api/project_badges/measure?project=tagbase_tagbase-server&metric=reliability_rating)](https://sonarcloud.io/summary/new_code?id=tagbase_tagbase-server)\n[![Security Rating](https://sonarcloud.io/api/project_badges/measure?project=tagbase_tagbase-server&metric=security_rating)](https://sonarcloud.io/summary/new_code?id=tagbase_tagbase-server)\n[![Technical Debt](https://sonarcloud.io/api/project_badges/measure?project=tagbase_tagbase-server&metric=sqale_index)](https://sonarcloud.io/summary/new_code?id=tagbase_tagbase-server)\n[![Vulnerabilities](https://sonarcloud.io/api/project_badges/measure?project=tagbase_tagbase-server&metric=vulnerabilities)](https://sonarcloud.io/summary/new_code?id=tagbase_tagbase-server)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n\n## Overview\n\n[tagbase-server](https://github.com/tagbase/tagbase-server) is a data management web service for working with eTUFF [1](https://doi.org/10.6084/m9.figshare.10032848.v4) [2](https://doi.org/10.6084/m9.figshare.10159820.v1) and eventually [nc-eTAG](https://github.com/oceandatainterop/nc-eTAG/) files.\n\ntagbase-server facilitates ingestion operations via REST courtesy of the [OpenAPI v3.0.3](https://spec.openapis.org/oas/v3.0.3.html).\n\n## Documentation\n\nSee the [project wiki](https://github.com/tagbase/tagbase-server/wiki) for guidance on \n* [Installation](https://github.com/tagbase/tagbase-server/wiki/Installation)\n* [Operations](https://github.com/tagbase/tagbase-server/wiki/Operations)\n* [Release Management](https://github.com/tagbase/tagbase-server/wiki/Release-Management-Guide)\n* [System Architecture](https://github.com/tagbase/tagbase-server/wiki/Systems-Architecture)\n* [Working with OpenAPI](https://github.com/tagbase/tagbase-server/wiki/Working-with-the-OpenAPI-Specification-a.k.a-openapi.yaml)\n\n## Development, Support and Community\nPlease create a [issue](https://github.com/tagbase/tagbase-server/issues).\n"
 },
 {
  "repo": "jaanga/terrain-srtm30-plus",
  "language": "JavaScript",
  "readme_contents": "[Jaanga](../index.html ) &raquo;<br>[Terrain<br>SRTM30 Plus]( ./index.html )\n===\n\n<div id=rm >\n\t<a href=JavaScript:displayMD(\"#readme.md#rm\"); >Read Me</a>\n</div>\n\n<div id=dn >\n\t<a href=JavaScript:displayMD(\"#dev-notes.md#dn\"); >Dev Notes</a>\n</div>\n\n[SRTM to TMS7+]( ./srtm-to-tms7+/index.html )  \n\n<i class=\"fa fa-external-link\"></i> [Source Code on GitHub ]( https://github.com/jaanga/terrain-srtm30-plus/ )  \n<br>\n\n\n<i class=\"fa fa-external-link\"></i> \n[Copyright and License]( https://github.com/jaanga/jaanga.github.io/blob/master/jaanga-copyright-and-mit-license.md )\n\n<style>div { margin: 15px 0; }</style>"
 },
 {
  "repo": "CNES/aviso-lagrangian",
  "language": "C++",
  "readme_contents": "Lagrangian analysis\r\n===================\r\n\r\nIntroduction\r\n------------\r\n\r\nFinite Size Lyapunov Exponent (FSLE) is a local lagrangian diagnostics that is\r\nwidely used for the study of transport and mixing processes of oceanographic\r\ntracers (Sea surface temperature, Ocean color ...). Its computation is derived\r\nfrom the definition of Finite-Time Lyapunov Exponent that allows the\r\nidentification of  Lagrangian Coherent Structures.\r\n\r\nThe code used to compute LE, was developed in collaboration between LOCEAN (F.\r\nd'Ovidio) and CLS, is available under GNU General Public License\r\n\r\nGiven a time series of meso-scale velocity field, this code computes\r\ncorresponding sub-mesoscale maps of backward or forward FLSE or FTLE. It also\r\ncompute other diagnostics such as orientation of Finite-Time Lyapunov Vectors.\r\n\r\nLicense\r\n-------\r\n\r\nLagrangian is distributed under the GNU Lesser General Public License. See\r\nCOPYING for details.\r\n\r\nDocumentation\r\n-------------\r\n\r\nFor full documentation visit the [Documentation\r\nPage](http://lagrangian.readthedocs.io/en/latest/index.html)."
 },
 {
  "repo": "hackforthesea/2020",
  "language": "HTML",
  "readme_contents": "# Hack for the Sea 2020\n\n<img src=\"https://avatars0.githubusercontent.com/u/21284584?s=200&v=4\" />\n\n## Where + When?\n\nHack for the Sea is loosely scheduled for late October 2020, at [Wheelhouse Cowork](https://wheelhousecowork.com/en) in Gloucester, MA\n\n## Challenges\n\nChallenge submissions are open! Please read and comment on the [Challenge Guide](https://github.com/hackforthesea/2020/issues/1).\n\n## Rules\n\n1. All projects must be released under a [suitable open source license](https://choosealicense.com/) or under the Creative Commons \"Attribution-ShareAlike\" license.\n2. You can start as soon as the challenges are published\n3. This is **not** a recruiting event, nor is it an intellectual property grab\n4. In-person attendence is strongly preferred, but we will make accomodations if necessary\n\n## Beneficiaries\n\n| [Ocean Alliance](https://whale.org/) | [Seaside Sustainability](https://www.seasidesustainability.org/) |\n|:-----:|:-----:|\n| <img src=\"https://whale.org/wp-content/themes/oceanAlliance/images/oceanAllianceLogo@2x.png\" width=250 /> | <img src=\"https://static.wixstatic.com/media/5b886d_68403281bb7a4cb2851a4f3dd2c75340~mv2.png/v1/fill/w_264,h_269,al_c,q_85,usm_0.66_1.00_0.01/LOGO3%20copy.webp\" width=250 /> |\n\n## Sponsors \n\n| [GL Design Co](https://gldesignco.com) |\n|:----:|\n| <img src=\"https://static.wixstatic.com/media/3fd9d1_7cd090011d26453fbbae46c8b1e106e3~mv2.png/v1/fill/w_178,h_84,al_c,q_85,usm_0.66_1.00_0.01/3fd9d1_7cd090011d26453fbbae46c8b1e106e3~mv2.webp\" /> |\n\nStay tuned for more information. For notifications as to when things change, please watch or star this repository, or sign up for our mailing list at https://tinyletter.com/hackforthesea.\n\nPlease send an email to [mark@mrh.io](mailto:mark@mrh.io) to get in touch.\n"
 },
 {
  "repo": "obidam/ds2-2022",
  "language": "Jupyter Notebook",
  "readme_contents": "# DS2 Class 2022, Big Data & Cloud Computing for Oceanography\n\nHome of the 2022 ISblue Big Data & Cloud Computing for Oceanography class (IMT-A, ENSTA, IUEM) given by:\n\n- Pierre Tandeo - pierre.tandeo@imt-atlantique.fr\n- Fr\u00e9d\u00e9ric Paul - frederic.paul@ifremer.fr\n- Sally Close - sally.close@univ-brest.fr\n- Guillaume Maze - Guillaume.Maze@ifremer.fr\n- Carlos\u00a0Granero Belinchon\u00a0- carlos.granero-belinchon@imt-atlantique.fr\n\nThis repo is a place holder for the class practice session and for projects developed by students.\n\n## Practice notebooks\n\nSee https://github.com/obidam/ds2-2022/blob/main/practice/README.md\n\n## Projects\n\nSee https://github.com/obidam/ds2-2022/blob/main/project/README.md\n\n***\n<img src=\"https://github.com/obidam/ds2-2022/raw/main/logo_isblue.jpg\">"
 },
 {
  "repo": "metarelate/metOcean",
  "language": "Python",
  "readme_contents": "metOcean\n=========\n\nmetOcean mapping is a knowledge repository providing information on translating meteorological and oceanographic metadata.\n\nThe knowledge is stored as RDF Turtle datasets in StaticData, modelled using the Metarelate terminology mapping model.\n\nTo contribute to the project, the static data should be used to populate a local triple store which an instance of the metarelate management software may access. \n\nDependencies\n------------\n* metarelate - https://github.com/metarelate/metarelate\n\nTo use this repository with metarelate, set your local environment variables:\n\n* METARELATE_STATIC_DIR='/path/to/your/working/copy/of/metOcean/staticData/'\n* METARELATE_TDB_DIR='/a/path/to/a/writeable/folder/for/a/local/triplestore'\n* METARELATE_DATA_PROJECT='metOcean' \n"
 },
 {
  "repo": "pangeo-gallery/physical-oceanography",
  "language": "Jupyter Notebook",
  "readme_contents": "# Pangeo Gallery Template\n\nThis repository stores an example gallery repo for the Pangeo Gallery.\nIt is configured to automatically build itself using\n[binderbot](https://github.com/pangeo-gallery/binderbot).\nIt is linked, via a git submodule, the the\n[gallery website repo](https://github.com/pangeo-gallery/pangeo-gallery).\nWhenever the notebooks are updated in this, repository\ndispatch is used to trigger a gallery rebuild. This keeps\n[gallery.pangeo.io](http://gallery.pangeo.io) always in sync with this repo.\n\nThe repo contains the following elements:\n\n- A set of jupyter notebooks, numbered in the order that we want them to\n  appear on the gallery website.\n- A configuration file, `binder-gallery.yaml`, which provides important\n  configuration parameters (see [pangeo gallery documentation](http://gallery.pangeo.io)).\n- Github workflows, which make the magic happen! (Don't touch these.)\n"
 },
 {
  "repo": "brianemery/hfr_cs_processing",
  "language": "MATLAB",
  "readme_contents": "## HFR CS PROCESSING TOOLBOX FOR MATLAB ##\n\nv2.0\n\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.5598294.svg)](https://doi.org/10.5281/zenodo.5598294)\n\nTools for processing oceanographic HF radar cross spectra with direction\nfinding methods. \n\n- Provides research quality software for processing HFR data following the\n  methods of Lipa et al. (2006).\n- Employs a home made ship removal algorithm that follows what is known \n  about how it's really done. (Presently disabled) \n- Generalizes the cross spectra data structure for arbitrary arrays.\n- Allows the use of the imageFOL toolbox by Anthony Kirincich \n  (https://github.com/akirincich/imageFOLs).\n- Includes several DF methods used in Emery (2018), and an estimate of \n  MUSIC error from Stoica and Nehorai (1989) used in Emery and Washburn (2018).\n- Uses my version of CODAR's method for single vs dual determination. Can use \n  the Likelihood Ratio (See Emery et. al 2022) for larger arrays. \n- Will work with data from LERA and WERA. It has been used to \n  process LERA data.\n\n\nHOW TO USE IT\n- download it and cd to the unzipped directory\n- run install_hfr_cs_proc.m to modify the MATLAB path\n- run the demo to make sure it works (run_cs_processing_demo.m)\n- edit run_cs_processing.m for more advanced applications \n- edit doa_on_cs.m (~line 80) to include the use of parfor \n\n\nTO DO\n- Could build some edge case tests using simiulations (eg 0-360 \n  transition, etc)\n- This commit includes many more mfiles than are actually needed due to MATLAB's \n  dependency tool - need a better way to isolate tools. \n\n\nACKNOWLEDGMENT\n\nThe release is self contained but includes code from the following people\nand or toolboxes. HFRProgs [1] by David Kaplan set the standard, and data\nstructures here follow the same basic formatting. I've include a few mfiles\nfrom HFRprogs as dependencies. As mentioned above, this toolbox can use \nimageFOL by Anthony Kirincich [2]. The toolbox includes code obtained\nfrom CODAR Ocean Sensors for reading cross spectra files. Version 1.5 \nincludes data from BML1 provided by William Speiser and John Largier. \n\n[1] https://github.com/rowg/hfrprogs  \n[2] https://github.com/akirincich/imageFOLs   \n\n\nREFERENCES\n\nLipa, B., B. Nyden, D. S. Ullman, and E. Terrill, (2006). SeaSonde radial velocities: Derivation and internal consistency. IEEE Journal of Oceanic Engineering, 31 (4) 850?861.  \n\nStoica, P., & Nehorai, A. (1989). MUSIC, maximum likelihood, and the Cramer-Rao bound. IEEE Transactions on Acoustics, Speech, and Signal Processing, 37(5), 720-741.  \n\nEmery, B. and Washburn, L. (2019). Uncertainty Estimates for SeaSonde HF Radar Ocean Current Observations. Journal of Atmospheric and Oceanic Technology 36.2: 231-247.\n\nEmery, B. (2019). Evaluation of Alternative Direction of Arrival Methods for Oceanographic HF Radars. IEEE Journal of Oceanic Engineering, doi: 10.1109/JOE.2019.2914537.  \n\nEmery, B., Kirincich, A., and Washburn, L. (2022). Direction Finding and Likelihood Ratio Detection for \nOceanographic HF Radars. Journal of Atmospheric and Oceanic Technology, 39(2), 223-235. https://journals.ametsoc.org/view/journals/atot/39/2/JTECH-D-21-0110.1.xml  \n\nARCHITECTURE GOALS\n\n- arbitrary array geometry, fft length, etc\n- arbitrary doa method \n\n\nNOTES\n\n- .m files use functions as blocks of code - code folding (cmd =) makes it easy to move among these.\n- Data structures contain variables of similar origin following the HFRProgs\n  convention, rows = locations, cols = time\n- Data structures are initialized with appropriately named function \n  (e.g. doa_struct.m) to enable standardization\n- End product is presently a structure that I'm calling a DOA structure, which is roughly\n  equivalent to a SeaSonde short-time radial (pre-merge), plus a lot more meta data.\n\n\nCODING PRINCIPLES\n\n- Minimize repetition (dont repeat yourself)\n- Make code re-usable and recyclable. Make general functions. \n- Code should be pretty and readable sentences and paragraphs, aid the reader when\n  possible\n- balance between future usages (flexibility) and getting the current job done (purpose built)\n- Good design is simple\n- Write computer programs to make them easy for people to read.\n- Have  a clear division between code that is custom for a particular application, \n  and the general/easily repurposed code\n  \n  \nHOW TO CITE\n\n(See the Zenodo site to cite specific older versions): \n\nEmery, B., 2021: HFR CS Processing Toolbox for MATLAB, software release version 2.0, https://doi.org/10.5281/zenodo.5598294. URL: https://doi.org/10.5281/zenodo.5598294, doi:10.4445281/zenodo.5598294.\n\nUsing Bibtex:\n \n```\n@misc{Emery2021code,\n  author       = {Brian Emery},\n  title        = {{HFR CS Processing Toolbox for MATLAB}, Software Release Version 2.0, https://doi.org/10.5281/zenodo.5598294},\n  version      = {2.0},\n  month        = oct,\n  year         = 2021,\n  doi          = {10.5281/zenodo.5598294},\n  url          = {https://doi.org/10.5281/zenodo.5598294}\n}\n```\n\nVERSION NOTES\n\n1.0 (18 July 2018)\nOriginal version can be found here: [![DOI](https://zenodo.org/badge/84593561.svg)](https://zenodo.org/badge/latestdoi/84593561)\n\n1.5 (16 June 2020)\nUpdates include an install script (install_hfr_cs_proc.m) and demonstration code (run_cs_processing_demo.m) that \nuses test data from BML1. This update also includes the likelihood ratio detection method from an in-prep\nmanuscript (suitable for use with MLE direction finding - more about this at a later time). \n\n2.0 (21 Oct 2021) \nUpdates, improvements and new features related to Emery et al. 2022.\n\n2.1 (1 June 2022)\nUpdates including radial metrics, many code fixes and likely bug introductions. \n"
 },
 {
  "repo": "nikita-0209/downscale-sst",
  "language": "Jupyter Notebook",
  "readme_contents": "# Downscaling Oceanographic Satellite Data with Convolutional Neural Networks\n\nA widely measured variable in the ocean, Sea SurfaceTemperature  (SST),  is  a  strong  indicator  of  pollution, productivity, global climate change and stress to corals and other species.  It is an estimate of the energy in the sea due to the motion of molecules. High resolution satellite sensors are effectively measure Sea Surface Temperature under clear sky conditions.  However under cloudy conditions, high resolution SST Measurements are not available.With  the  help  of  a  deep  learning  architecture,  the available images with low spatial resolution can beenhanced to produce images of high spatial resolution.\n\n## Getting Started\n\nThese instructions will get you a copy of the project up and running on your local machine.\n\n### Prerequisites\n\n* Python3\n* Tensorflow 2.x\n* NetCDF\n* Basemaps Toolkit\n\n### Installing\n\nAfter ensuring you have the above mentioned versions of Tensorflow and Python. The other two prerequisties can be installed in a Colab notebook as:\n\nNetCDF: \n```\n!pip install netCDF4\n```\n\nAnd \n\n```\n!apt-get install libgeos-3.5.0\n!apt-get install libgeos-dev\n!pip install https://github.com/matplotlib/basemap/archive/master.zip\n```\n\n## The Data\n\nGroup of High Resolution Sea Surface Temperature (GHRSST) data engulf SST observations from all kinds of available sources. The  GHRSST was established to foster an international focus and coordination for the development of a new generation of global, multi-sensor, high-resolution near real time SST datasets. Major contribution in this dataset comes from the space-borne satellite radiometers. The Level-4 (L4) product is generated using various objective analysis techniques to produce gap-free SST maps over the global oceans. In this study, we have used this L4 GHRSST products with a regular spatial resolution of ~ 1 km. The data was downloaded from [Physical Oceanography Distributed Active Archive Center,](https://podaac.jpl.nasa.gov/GHRSST)\n\n## Methodology\n\n### Create Data\n\nSea Surface Temperature Data is stored in NetCDF Format. Along with recording the sea surface temperature, these data files denote land values by a particular constant. This constant, known as the fix value differs from one data file to another. To maintain uniformity, all NetCDF files were rewritten to assign a single fix value, (in this case 0) to denote the land values. For purposes of training, each of the given SST fields were divided into overlapping patches. \nThis can be done by [Create Dataset.ipynb](https://github.com/nikita-0209/downsample-sst/blob/master/Create_Dataset.ipynb). Remember to change the paths to where your NetCDF Files are stored. \nAs required by the architecture, the images were normalized to range [0,1] by dividing each pixel with the maximum of pixel values in both the data files.\n\n### Models\n\nSince Super Resolution Convolutional Neural Network had already been tried and tested on bicubic interpolated SST Fields by Aurelien Ducournau and Ronan Fablet in their paper Deep Learning for Ocean Remote Sensing: An Application of Convolutional Neural Networks for Super-Resolution on Satellite-Derived SST Data, initial experiments were carried out on this architecture. Since the results weren't satisfactory, a deeper architecture, namely Very Deep Super Resolution CNN was experimented with. For both the variants, the activation function used is ReLu. Each model is optimised by adaptive moment estimation. A batch size of 64 was chosen. \n\nTo run the SRCNN Architecture:\n```\npython srcnn_server.py --file_name_low <path to hdf5 array of low resolution patches>  --file_name_high  <path to hdf5 array of high resolution patches>\n```\n\nTo run the VDSR Architecture:\n```\npython vdsr_server.py --file_name_low <path to hdf5 array of low resolution patches>  --file_name_high  <path to hdf5 array of high resolution patches>\n```\n\nThe checkpoints of the best models will be saved in ckpts directory. Currently the number of epochs is 100.\n\n## Evaluate\n\nEach model was trained to minimise the mean square error between the predicted and the expected patch. Along with that, a popular metric used for comparing quality of images, Peak Signal Noise Ratio (PSNR) was calculated. The smaller the MSE, the greater is the PSNR and the better is the image quality.\n[Evaluate.ipynb](https://github.com/nikita-0209/downsample-sst/blob/master/Evaluate.ipynb) initialises the model, loads the weights and calcuulates PSNR of the predicted patches.\n\n## Predictions\n\nTo reconstruct the entire SST Field from the predicted patches, run [Prediction.ipynb](https://github.com/nikita-0209/downsample-sst/blob/master/Prediction.ipynb). This model assumes that VDSR was trained. Feel free to replace the architecture if needed.\nThe model is initialised, the weights are loaded, each patch is predicted and appropriately arranged to form the final complete SST Field.\n\nRemember to change paths to the saved hdf5 files of patches and model weights. \nModify the path while writing the created NetCDF File of Predicted SST Fields.\n\n##  Evaluation Results\n\nIn order to gain a better insight into the performance of the model, several fields like Mean of Predicted SST Fields, Domain Averaged Root Mean Square Error (RMSE) and Domain Averaged Bias have been plotted in [Plot Evaluation.ipynb](https://github.com/nikita-0209/downsample-sst/blob/master/Plot_Evaluation.ipynb). The terms Mean SST and Domain Averaged RMSE are self-explanatory with mean SST referring to the mathematical mean of the predicted SST Fields, and the latter referring to the averaged root mean squared error between considered fields. \nThe Domain Averaged Bias is defined as the mean of the differences between the considered SST Fields, say the input and the predicted.\n\n## Detailed Report\n\nA detailed report of this project is available: [here](https://drive.google.com/file/d/1ssvq1EZvxojmPIaApwvClPOaoqPUlmu8/view?usp=sharing).\n\n## Authors\n\n* **Nikita Saxena** \n\n## Acknowledgments\nI express my sincere thanks to Dr. Rashmi Sharma, who provided me with the opportunity to work on this project. I pay my deep sense of gratitude to Dr. Neeraj Agarwal and Dr. Jishad M, without whose valuable guidance and supervision the project couldn't have been completed.\n\n\n"
 },
 {
  "repo": "jklymak/Eos314Text",
  "language": "TeX",
  "readme_contents": "# EOS 314 course notes\n\nSee the pdf for the notes.  Written using https://github.com/Tufte-LaTeX/tufte-latex\n\nCompiled versions can be found at http://ocean-physics.seos.uvic.ca/~jklymak/Eos314Text/\n"
 },
 {
  "repo": "jessecusack/ocean_tools",
  "language": "Python",
  "readme_contents": "Ocean Tools\n=============\nThis repository contains oceanographic data analysis tools.\n\nModules\n-------\n\n* GM: functions for applying the Garrett and Munk internal wave spectra.\n* TKED: functions for estimating turbulent kinetic energy dissipation from finescale observations. Including the finescale parameterisation, Thorpe scale estimates and the large eddy method.\n* gravity_waves: linear inertia-gravity wave dynamics.\n* sandwell: read data from the Smith and Sandwell bathymetric binary file into a numpy array efficiently.\n* utils: miscellaneous functions.\n* window: functions for splitting data into chunks.\n\nAll very much a work in progress.\n\nInstallation\n------------\n\nFirst clone or download the repository. Then install using pip:\n\n``cd ocean_tools``\n\n``pip install .``\n\nOptionally, use the the -e flag, to make it editable.\n\n``pip install -e .``\n"
 },
 {
  "repo": "BCODMO/Ocean-Data-Ontology",
  "language": null,
  "readme_contents": "# Ocean Data Ontology #\n\n![documentation v0.9.0](https://img.shields.io/badge/documentation-v0.9.0-blue.svg)\n\nApplication-level ontology for describing oceanographic datasets\n\n## PREFIX ##\n\n<code>PREFIX odo: <http://ocean-data.org/schema></code>\n\nhttp://prefix.cc/odo\n\n\n## FUNDING ##\n\nThis work is funded by the National Science Foundation under the following awards:\n\nA Data Management System and Portal for Access to Ecological and Biogeochemical Ocean Data - BCO-DMO  \nAward #1031253  \nURL: http://www.nsf.gov/awardsearch/showAward?AWD_ID=1031253\n\nBiological and Chemical Oceanography Data Management Office (BCO-DMO): A System for Access to Ecological and Biogeochemical Ocean Data  \nAward #1435578  \nURL: http://www.nsf.gov/awardsearch/showAward?AWD_ID=1435578\n\n### NOTES ###\n\nRDF/XML: https://ocean-data.org/schema/ontology.xml\nJSON-LD: https://ocean-data.org/schema/ontology.json\nN-Triples: https://ocean-data.org/schema/ontology.nt\nTurtle: https://ocean-data.org/schema/ontology.ttl\n"
 },
 {
  "repo": "OceanOptics/Inlinino",
  "language": "Python",
  "readme_contents": "Inlinino\n========\n[![License: GPL v3](https://img.shields.io/badge/License-GPLv3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0)\n[![Python 3.8](https://img.shields.io/badge/Python-3.8-blue.svg)](https://www.python.org/downloads/)\n[![Documentation Status](https://readthedocs.org/projects/inlinino/badge/?version=latest)](https://inlinino.readthedocs.io/en/latest/?badge=latest)\n\nInlinino is an open-source software data logger for oceanographers. It primarily log measurements from optical instruments deployed on research vessels during month long campaigns. Secondarily, it  provides real-time visualization, which helps users troubleshoot instruments in the field and ensure collection of quality data. Inlinino is designed to interface with either serial (RS-232) or analog instruments. The data received is logged in a timestamped raw format (as communicated by the instrument) or in a comma separated file (csv) for easy importation in data analysis software. Typically, a new log file is created every hour for simplicity of post-processing and easy backups. Instruments supported are: SeaBird TSG, Satlantic PAR, WET Labs ECO sensors (e.g. ECO-BB3, ECO-FLBBCD, ECO-BBFL2, ECO-3X1M, ECO-BB9, ECO-BBRT), WET Labs ACS, Sequoia LISST, and analog sensors through a data acquisition system (DataQ DI-1100 ). Other instruments can be added via the user interface if they output simple ascii data frame, otherwise the code is intended to be modular to support new instruments. \n     \nThe documentation of the project is available at [http://inlinino.readthedocs.io](http://inlinino.readthedocs.io/en/latest/).\n\nAppropriate citation is:\nHaentjens, N. and Boss, E., 2020. Inlinino: A Modular Software Data Logger for Oceanography. DIY Oceanography. DOI: [10.5670/oceanog.2020.112](https://doi.org/10.5670/oceanog.2020.112)\n\n\n### Installation\nInlinino was bundled into a Windows executable and a macOS application. Both are available for download with a quick start guide in the [documentation](https://inlinino.readthedocs.io/en/latest/quick_start.html). Otherwise Inlinino can be installed from source using the setup.py file available on this repository, following the instructions below.\n\nDownload Inlinino code.\n \n    wget https://github.com/OceanOptics/Inlinino/archive/master.zip\n    unzip master.zip\n    cd Inlinino-master\n \nTo install Inlinino (tested with python 3.8 only, should work with newer python versions):\n\n    pip install -r requirements.txt\n\nInlinino can then be started from the folder containing inlinino's source code with.\n\n    python -m inlinino\n\n### Inlinino Software\nThe application is written in Python\u00a03, on top of pySerial, numpy, and PyQt5. The current version works with a \"classic\" Graphical User Interface. A web interface started to be implemented and can be found in the branch `tb-app` of this repository. A command line interface used to be available but was not ported to version >2.0.\n\nThe code is organized in:\n  + `docs`: User Documentation ([ReadTheDocs](https://inlinino.readthedocs.io/))\n  + `inlinino`: Inlinino source code\n    - `instruments/`:  Instrument interfaces, more instrument types can be added there.\n    - `ressources/`: User Interface Layout and Logo.\n    - `*.py`: Core code of Inlinino.\n    - `inlinino_cfg.json`: Applications parameters are saved in this file ([ReadTheDocs](https://inlinino.readthedocs.io/en/latest/cfg.html))\n  + `mcu_firmwares`: Firmwares to upload on a microcontroller for the previous DAQ module (deprecated)\n    - `PASC.cpp`: Precision analog to serial converter (PASC) firmware\n    - `Simulino.cpp `: Instrument simulator to test Inlinino with microcontrollers simulating the behavior of scientific instruments.\n  + `make.py`: Bundles Inlinino application into a .app or .exe depending on platform. pyInstaller must be installed.\n  + `setup.py`: Python environment setup file.\n\nWhen Inlinino is started an engineering log file is created in `logs/inlinino_<YYYYMMDD>_<hhmmss>.log` and keep track of most tasks executed (e.g. user interaction, creation of data log files, warnings, and potential errors).\n\n### Questions and issues\nFor any questions or issues regarding Inlinino please contact [me](mailto:nils.haentjens+inlinino@maine.edu).\n"
 },
 {
  "repo": "SBFRF/pyDIWASP",
  "language": "Python",
  "readme_contents": "# pyDiwasp\nconversion of diwasp package (DIWASP: DIrectional WAve SPectrum analysis Version 1.4) for python\nconverted from https://github.com/metocean/diwasp\n\nI would lOVE help making this into a more pythonic representation of the original diwasp tool.  check issues for needed functionality adds.  \n\n## Toolbox contents:\n### Main functions:\n- dirspec.m           Main function for directional wave analysis\n- readspec.m          Reads in DIWASP format spectrum files\n- writespec.m         Writes DIWASP format spectrum files\n- plotspec.m          Plots DIWASP spectrums\n- testspec.m          Testing function for the estimation methods\n- makespec.m          Makes a fake spectrum and generates fake data for testing dirspec.m\n- infospec.m          Returns information about a directional spectrum\n- data_structures.m   is a help file describing the new Version 1.1 data structures\n\n## Private functions (some can be used as stand alone functions):\n### The transfer functions\n- /private/elev.m\n- /private/pres.m\n- /private/velx.m\n- /private/vely.m\n- /private/velz.m\n- /private/slpx.m\n- /private/slpy.m\n- /private/vels.m\n- /private/accs.m\n\n### The estimation functions\n- /private/DFTM.m\n- /private/EMLM.m\n- /private/IMLM.m\n- /private/EMEP.m\n- /private/BDM.m\n\n### Miscellaneous functions\n- /private/smoothspec.m\n- /private/wavenumber.m\n- /private/makerandomsea.m\n- /private/makewavedata.m\n- /private/Hsig.m\n- /private/gsamp.m\n- /private/check_data.m\n  \n\ncarying original license agreement and copyright\n\n## License agreement\nDIWASP, is free software; you can redistribute it and/or modify it under the terms of the \nGNU General Public License as published by the Free Software Foundation. \nHowever, the DIWASP license includes the following addendum concerning its usage:\nThis software and any derivatives of it shall only be used for educational purposes or \nscientific research without the intention of any financial gain. \nUse of this software or derivatives for any purpose that results in financial gain \nfor a person or organization without written consent from the author is a breach of the license agreement.\nThis software is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; \nwithout even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. \nIn addition the author is not liable in any way for consequences arising from the application of \nsoftware output for any design or decision-making process.\nThe GNU General Public License forms the main part of the license agreement included in the package. \n\nCopyright (C) 2002 David Johnson   Coastal Oceanography Group, CWR, UWA, Perth\n\n"
 },
 {
  "repo": "regeirk/klib",
  "language": "Python",
  "readme_contents": "kLib\n====\n\nA collection of Python functions for applications in oceanography and science \nin general. This module references to the numpy, scipy, pylab and probably \nother Python packages too.\n\nFunctions are grouped in different modules such as statistics, file management, \ngraphics, mapping, interpolation and common functions.\n\nThis library is still in it's infancy and under development. Eventually everything\nwill become depreciated and moved to the `atlantis` module.\n\n\nMODULES\n-------\n\n- common\n- dynamics\n- file (filemngmnt)\n- geostrophy\n- graphics\n- gis\n- interpolate\n- signal\n- stats \n\n\nDISCLAIMER\n----------\n\nThis software may be used, copied, or redistributed as long as it\nis not sold and this copyright notice is reproduced on each copy\nmade. This routine is provided as is without any express or implied\nwarranties whatsoever.\n\n\nINSTALLATION\n------------\n\nCopy all the contents into a location included in the Python search\npath. On Linux distribution one such option is\n\n> `~/.local/lib/python2.x/site-packages/klib`\n\n\nAUTHORS\n-------\n\nSebastian Krieger (sebastian at nublia.com)\n\n\nREVISION\n--------\n\n    8 (2017-02-20 18_05 -0300)\n    7 (2016-06-28 21:59 -0300)\n    6 (2016-02-05 00:00 -0300)\n    5 (2016-05-19 00:00 -0300)\n    4 (2014-02-24 17:26 -0300)\n    3 (2013-03-19 14:52 -0300)\n    2 (2013-01-08 14:09 -0300)\n    1 (2012-02-24 20:37 -0300)\n\n"
 },
 {
  "repo": "biavillasboas/IdealizedWaveCurrent",
  "language": "Python",
  "readme_contents": "\n\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.4045184.svg)](https://doi.org/10.5281/zenodo.4045184)\n\n# Source code for \nVillas B\u00f4as, A. B., B. D. Cornuelle, M. R. Mazloff, S. T. Gille, and F. Ardhuin, Wave-Current Interactions at Meso and Submesoscales: Insights from Idealized Numerical Simulations. J. Phys. Oceanogr., doi: https://doi.org/10.1175/JPO-D-20-0151.1. \n\n# Abstract\nSurface gravity waves play a major role in the exchange of momentum, heat, energy, and gases between the ocean and the atmosphere. The interaction between currents and waves can lead to variations in the wave direction, frequency, and amplitude. In the present work, we use an ensemble of synthetic currents to force the wave model WAVEWATCH III and assess the relative impact of current divergence and vorticity in modifying several properties of the waves, including direction, period, directional spreading, and significant wave height (Hs). We find that the spatial variability of Hs is highly sensitive to the nature of the underlying current and that refraction-caused vorticity in the rotational component of the flow is the main mechanism leading to gradients of Hs. The results obtained using synthetic currents were used to interpret the response of surface waves to realistic currents by running an additional set of simulations using the llc4320 MITgcm output in the California Current region. Our findings suggest that wave parameters could be used to detect and characterize strong gradients in the velocity field, which is particularly relevant for the Surface Water and Ocean Topography (SWOT) satellite as well as several proposed satellite missions.\n# Authors\n* [Bia Villas Boas](https://biavillasboas.github.io/) <<avillasboas@ucsd.edu>>\n* [Sarah T. Gille](http://www-pord.ucsd.edu/~sgille/)\n* [Matthew R. Mazloff](http://scrippsscholars.ucsd.edu/mmazloff)\n* [Bruce D. Cornuelle](http://scrippsscholars.ucsd.edu/bcornuelle)\n* [Fabrice Ardhuin](https://annuaire.ifremer.fr/cv/16811/en/)\n\n# Data\nAll model output analyzed in this paper is availabe for download here https://doi.org/10.6075/J0X928V6\n\n# Funding\nThis project was partlially funded by the [SWOT](https://swot.jpl.nasa.gov/) program with NASA grant NNX16AH67G and 80NSSC20K1136.\nBia Villas B\u00f4as was also partially funded by NASA grant 80NSSC17K0326.\n\n# How to use this repository\n\nAll figures in Villas B\u00f4as et al. (2020) can be reproduced using the Python scripts from this repository and the [model output](https://library.ucsd.edu/dc/collection/bb6217292w). To do so, follow these steps\n\n1. Make a local copy of this repository by either cloning or downloading it.\n\n2. Download the [model output](https://library.ucsd.edu/dc/collection/bb6217292w), untar the files, and move all three directories to `data` in the project root. After doing so, your directory tree should look like this:\n\n```\nIdealizedWaveCurrent/\n\u251c\u2500\u2500 data\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 llc4320\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 model_stats\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 synthetic\n\u251c\u2500\u2500 figs\n\u251c\u2500\u2500 src\n\u2514\u2500\u2500 tools\n```\n3. Make sure that you create an environment with the package versions specified in `environment.yml`. If you are using [Conda](https://docs.conda.io/en/latest/) you can run \n\n`conda env create -f environment.yml`\n\nfrom the project root.\n\n4. If you follow the steps above you should be able to reproduce all figures, by running `python figXX.py` from the `src` directory without having to adjust any paths.\n\n* **Note on rendering matplotlib text with LaTeX:** To ensure that the math fonts in the figures matched the fonts in the paper, the code in this repository requires a [working LaTeX installation](https://matplotlib.org/3.1.0/tutorials/text/usetex.html). If you encounter any problems with this, you may change the line matplotlib.rcParams['text.usetex']=True  to False or just comment it out. \n\n# How to cite this code\n\nIf you wish to use the code from this repository, you may cite it as \n\nVillas B\u00f4as, Ana B. (2020, September 23). Source code for: 'Wave-Current Interactions at Meso and Submesoscales: Insights from Idealized Numerical Simulations'. Zenodo. https://doi.org/10.5281/zenodo.4045183\n"
 },
 {
  "repo": "dankelley/ocedata",
  "language": "R",
  "readme_contents": "# ocedata 0.2.0\n\n<!-- badges: start -->\n\n[![R-CMD-check](https://github.com/dankelley/ocedata/workflows/R-CMD-check/badge.svg)](https://github.com/dankelley/ocedata/actions)\n[![CRAN_Status_Badge](https://www.r-pkg.org/badges/version/ocedata)](https://cran.r-project.org/package=ocedata)\n![RStudio CRAN mirror downloads](https://cranlogs.r-pkg.org/badges/last-month/ocedata)\n![RStudio CRAN mirror downloads](https://cranlogs.r-pkg.org/badges/last-week/ocedata)\n![RStudio CRAN mirror downloads](https://cranlogs.r-pkg.org/badges/last-day/ocedata)\n\n<!-- badges: end -->\n\n\n## About ocedata\n\nThe `ocedata` package supplies some oceanographic datasets, for general use and\nas an adjunct to the `oce` package. Indeed, some of the datasets were once\nsupplied by `oce`, but they were moved to `ocedata` to reduce storage pressure\non the CRAN system, based on the assumption that `oce` will be updated more\nfrequently than `ocedata`.\n\n\n## Installing ocedata\n\nStable versions of `ocedata` may be installed from within R, in the same way as\nother packages, with\n```splus\ninstall.packages(\"ocedata\")\n```\nHowever, this version is only updated a few times a year (pursuant to CRAN\npolicy), so some users install `ocedata` from the github.com website instead,\nto get the latest version. This may be done with\n```splus\nlibrary(devtools)\ninstall_github(\"dankelley/ocedata\", ref=\"main\")\n```\n\n"
 },
 {
  "repo": "gher-ulg/OAK",
  "language": "Fortran",
  "readme_contents": "Installation\n------------\n\nTo install OAK you need:\n\n* A Fortran 90 compiler\n* NetCDF (with Fortran 90 interface)\n* LAPACK library\n* an implementation of the BLAS library (such as ATLAS, OpenBLAS or MKL) \n* cholmod\n\nOptionally, for parallelization, either one of:\n\n* MPI (with Fortran 90 interface)\n* OpenMP (included in Fortran 90 compilers) \n\n\n# Compilation\n\n## gfortran and default BLAS\n\nTo compile with open-source gfortran, parallelized with MPI, at least the following packages are needed:\n\n```\nsudo apt-get install gfortran libatlas-base-dev liblapack-dev openmpi-bin libopenmpi-dev libnetcdf-dev netcdf-bin\ncp config.mk.template config.mk\nmake\n```\n\n# ifort and MKL\n\n## Without CLODMOD:\n\nSet `$MKLROOT`:\n\n```bash\n$ make FORT=ifort NETCDF_CONFIG=/path/to/bin/nf-config CHOLMOD_LIB= BLAS_LIBDIR=$MKLROOT/lib/intel64/ BLAS_LIB=\"-lmkl_intel_lp64 -lmkl_sequential -lmkl_core\" LAPACK_LIB=\n```\n"
 },
 {
  "repo": "eiszapfen2000/tgda",
  "language": "C",
  "readme_contents": "# Ocean Surface Generation and Rendering\nA real-time capable implementation of Tessendorf's choppy wave algorithm [Tessendorf1999a], augmented with wave spectrum models from oceanographic research. Master's thesis and respective poster are to be found [here](https://www.cg.tuwien.ac.at/research/publications/2018/GAMPER-2018-OSG/).\n\n## Screenshots\n\n|   |   |\n|---|---|\n|![alt text](Branches/OpenGL33Core/DATU/figures/21-06-2018_10-44-51_complete.png)|![alt text](Branches/OpenGL33Core/DATU/figures/21-06-2018_12-48-51_complete.png)|\n|![alt text](Branches/OpenGL33Core/DATU/figures/28-05-2018_10-56-10_complete.png)|![alt text](Branches/OpenGL33Core/DATU/figures/21-06-2018_15-47-53_complete.png)|\n\n## Videos\n\n| Overview  | Lods |\n| ------------- | ------------- |\n| [![Overview](https://img.youtube.com/vi/op_NVMRhpL0/0.jpg)](https://www.youtube.com/watch?v=op_NVMRhpL0) | [![Lods](https://img.youtube.com/vi/RiBrIPSPOxo/0.jpg)](https://www.youtube.com/watch?v=RiBrIPSPOxo) |\n\n## Ocean Surface Synthesis\nWe implemented the following oceanographic wave spectra:\n* Pierson-Moskowitz [Pierson1964a]\n* JONSWAP [Hasselmann1973a]\n* Donelan [Donelan1985a]\n* Elfouhaily [Elfouhaily1997a]\n\nFor Pierson-Moskowitz and JONSWAP we employ the directional distribution as introduced by Mitsuyasu et al. [Mitsuyasu1975a], and improved by Hasselmann et al. [Hasselmann1980a]. Donelan and Elfouhaily each incorporate their own directional distribution. \n\n## Fourier Transform\nWe compute the Discrete Fourier Transform with the single precision variant of [FFTW](http://www.fftw.org). The Fast Fourier Transform works fastest on power-of-two resolutions, therefore we restrict the ocean's resolution to such resolutions.\n\n## Ocean Surface Rendering\n* Projected Grid [Johanson2004a]\n* Seamless Ocean Surface Lighting [Bruneton2010a]\n* Ocean Whitecaps [Dupuy2012a]\n\n## Skylight\nPreetham sky light [Preetham1999a, Section 3.1] and sun light [Preetham1999a, Section 3.2].\n\n## Tonemapping\nFor tonemapping purposes we implemented the global tonemapping operator by Reinhard et al. [Reinhard2002a, Equation 4], and the temporal luminance adaptation algorithm by Krawczyk et al. [Krawczyk2005a, Equations 5, 6, 7, 12]. The necessary color space conversions are done according to http://www.brucelindbloom.com/Math.html.\n\n## Literature\n[Bruneton2010a] Eric Bruneton, Fabrice Neyret, and Nicolas Holzschuch. Real-time realistic ocean lighting using seamless transitions from geometry to brdf. Computer Graphics Forum, 29(2):487\u2013496, 2010.\n\n[Donelan1985a] M. A. Donelan, J. Hamilton, andW. H. Hui. Directional spectra of wind-generated waves. Phil. Trans. Roy. Soc. London A, 315:509\u2013562, 1985.\n\n[Dupuy2012a] Jonathan Dupuy and Eric Bruneton. Real-time animation and rendering of ocean whitecaps. In SIGGRAPH Asia 2012 Technical Briefs, SA \u201912, pages 15:1\u201315:3. ACM, 2012.\n\n[Elfouhaily1997a] T. Elfouhaily, B. Chapron, K. Katsaros, and D. Vandemark. A unified directional spectrum for long and short wind-driven waves. J. Geophys. Res., 102(C7):15781\u201315796, 1997.\n\n[Hasselmann1973a] K. Hasselman, T. P. Barnett, E. Bouws, D. E. Carlson, and P. Hasselmann. Measurements of wind-wave growth and swell decay during the joint north sea wave project (jonswap). Deutsche Hydrographische Zeitschrift, 8(12), 1973.\n\n[Hasselmann1980a] D. E. Hasselmann, M. Dunckel, and J. A. Ewing. Directional wave spectra observed during JONSWAP 1973. J. Phys. Oceanogr., 10:1264\u20131280, 1980.\n\n[Johanson2004a] Claes Johanson. Real-time water rendering - introducing the projected grid concept. Master\u2019s thesis, Department of Computer Science, Lund University, 2004.\n\n[Krawczyk2005a] Grzegorz Krawczyk, Karol Myszkowski, and Hans-Peter Seidel. Perceptual effects in real-time tone mapping. In Proceedings of the 21st Spring Conference on Computer Graphics, SCCG \u201905, pages 195\u2013202. ACM, 2005.\n\n[Mitsuyasu1975a] H. Mitsuyasu, F. Tasai, T. Suhara, S. Mizuno, M. Ohkusu, T. Honda, and K. Rikiishi. Observations of the Directional Spectrum of Ocean Waves Using a Cloverleaf Buoy. Journal of Physical Oceanography, 5:750, 1975.\n\n[Pierson1964a] Willard J. Pierson and Lionel Moskowitz. A proposed spectral form for fully developed wind seas based on the similarity theory of S. A. Kitaigorodskii. J. Geophys. Res., 69(24), December 1964.\n\n[Preetham1999a] A. J. Preetham, Peter Shirley, and Brian Smits. A practical analytic model for daylight. In Proceedings of the 26th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH \u201999, pages 91\u2013100. ACM Press/Addison-Wesley Publishing Co., 1999.\n\n[Reinhard2002a] Erik Reinhard, Michael Stark, Peter Shirley, and James Ferwerda. Photographic tone reproduction for digital images. In Proceedings of the 29th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH \u201902, pages 267\u2013276. ACM, 2002.\n\n[Tessendorf1999a] Jerry Tessendorf. Simulating ocean water. In SIGGRAPH course notes. ACM, 1999.\n"
 },
 {
  "repo": "cesar-rocha/niwqg",
  "language": "Python",
  "readme_contents": "Code for a special class of solutions of the Xie & Vanneste (2015) coupled model in a doubly periodic domain.\n\n<img src=\"./docs/figs/organogram.png\" alt=\"NIWQG organogram\"  width=\"500\">\n\n# Installation\nThis software is written in `python3` and depends on `numpy` and `h5py`. I strongly\nrecommend the python3 pre-packaged on [Anaconda](https://www.continuum.io/downloads).\nThis package comes with `numpy`. To install h5py, use Anaconda's package manager:\n```bash\nconda install h5py\n```\n\nFor a more comprehensive installation or update with conda, run\n```\nmake install\n```\n\n### Installing `niwqg`\n\nIf you're a git user, fork and clone your fork of the `niwqg` repository.\nAlternatively, just download the repository by clicking on the link on the\nupper-right corner of this page.\n\nInside the root niwqg directory, install the package:\n\n```bash\npython setup.py install\n```\n\nIf you plan to make changes to the code, then setup the development mode:\n\n```bash\npython setup.py develop\n```\n\n### Testing `niwqg`\nIf you have [`pytest`](https://docs.pytest.org/en/latest/), then run\n```\nmake test\n```\nAlternatively, to in the test directory\n```bash\ncd niwqg/tests\n```\nand run all unit tests using nose:\n```bash\nnosetests\n```\n\nYou can also run this simple [example](./examples/LambDipole_CoupledModel.ipynb)\nand verify the energy budget.\n\n# Documentation\nSome documentation is available [here](docs/Index.ipynb).  Also  check the docstrings of\nclasses and methods.\n\n# Development\nThe code is under rapid development by [@crocha700](https://github.com/crocha700)\nas part of the project \"Stimulated Loss of Balance\" (SLOB) with\n[@glwagner](https://github.com/glwagner) and [@wry55](https://github.com/wry55).\n\nPlease, submit contributions via pull-request of\na cut-off branch (not master).\n\n# Funding\nThis project is funded by the [National Aeronautics and Space Administration](https://www.nasa.gov) under grant NNX16AO5OH.\n"
 },
 {
  "repo": "hydroffice/hyo2_openbst",
  "language": "Jupyter Notebook",
  "readme_contents": "The Open BackScatter Toolchain Project\n======================================\n\n.. image:: https://github.com/hydroffice/hyo2_openbst/raw/master/resources/png/openbst.png\n    :alt: OpenBST logo\n\n|\n\n.. image:: https://img.shields.io/badge/docs-latest-brightgreen.svg\n    :target: https://www.hydroffice.org/manuals/openbst/index.html\n    :alt: Latest Documentation\n\n.. image:: https://ci.appveyor.com/api/projects/status/e73gnrt9n50cu2k5?svg=true\n    :target: https://ci.appveyor.com/project/giumas/hyo2-openbst\n    :alt: AppVeyor Status\n\n.. image:: https://travis-ci.org/hydroffice/hyo2_openbst.svg?branch=master\n     :target: https://travis-ci.org/hydroffice/hyo2_openbst\n     :alt: Travis-CI Status\n\n.. image:: https://api.codacy.com/project/badge/Grade/13c4893c0a7e45ddb40cfdbddd7091a3\n    :target: https://www.codacy.com/app/hydroffice/hyo2_openbst/dashboard\n    :alt: Codacy\n\n.. image:: https://coveralls.io/repos/github/hydroffice/hyo2_openbst/badge.svg?branch=master\n    :target: https://coveralls.io/github/hydroffice/hyo2_openbst?branch=master\n    :alt: Coveralls\n\n|\n\n* Code Repository: `github <https://github.com/hydroffice/hyo2_openbst>`_\n* Project Page: `url <https://www.hydroffice.org/openbst/main>`_\n* License: `LGPLv3 <https://github.com/hydroffice/hyo2_openbst/raw/master/LICENSE>`_\n\n|\n\nGeneral info\n------------\n\nThe Open BackScatter Toolchain (OpenBST) package provides a library and an app for processing acoustic backscatter.\n\n|\n\nCopyright Notice\n----------------\n\nCopyright (c) 2019, University of New Hampshire, Center for Coastal and Ocean Mapping; IFREMER. All rights reserved.\nPortions of this project were developed under a cooperative agreement with NOAA Coast Survey Development\nLaboratory, and contain NOAA-developed code in the public domain.\n"
 },
 {
  "repo": "oiip/oiip-data-viewer",
  "language": "JavaScript",
  "readme_contents": "## Welcome to the OIIP Data Viewer\n\nThis web application was developed as part of the Oceanographic In-Situ data Interoperability Project (OIIP) at NASA/JPL in collaboration with UCAR/Unidata, the Large Pelagics Research Center, and University of Massachusetts, Boston. The primary goal of the application is progress the architecture and design of applications that pair in-situ and remote sensing data products. To that end, it visualizes a select number of oceanographic remote sensing datasets (such as sea surface temperature and ocean current speed) as well as vertically resolved animal tag and ship based sensor tracks.\n\nFor more information on the project, please visit: [https://oiip.jpl.nasa.gov/](https://oiip.jpl.nasa.gov/)\n\nFor a demonstration of the tool, please see: [https://www.youtube.com/watch?v=CgOTwWWMhdc](https://www.youtube.com/watch?v=CgOTwWWMhdc)\n\nThis application is built off of the [Common Mapping Client](https://github.com/nasa/common-mapping-client)\n\n### Development Instructions\n\n**Requirements**\n\nNodeJS\n\n**Install Dependencies**\n\n`npm install`\n\n**Start Dev Server**\n\n`npm start`\n\n**Build for Prod**\n\n`npm run build`\n"
 },
 {
  "repo": "cywhale/ODB",
  "language": "HTML",
  "readme_contents": "# ODB\nMy R works in Ocean Data Bank (ODB), Institute of Oceanography, NTU, Taiwan\n\nBioQuery: an integrated Open API front-end with Shiny App for bio-query (BioQuery) <a href=\"https://bio.odb.ntu.edu.tw/query\" target=\"_blank\">https://bio.odb.ntu.edu.tw/query</a>\n\n    - ?map https://bio.odb.ntu.edu.tw/query?map directly link to map query interface for bio-data, ODB\n    \n    - ?api(=1 - 7) directly open API document \n    \n    - ?help(=1 - 9)&lang=(EN, TW, which can be omitted if EN: English is selected; only TW: traditional Chinese need to be specified) directly open Help and tips manual\n\n#### Links\n1. Shiny R Dashboard for bio-database of ODB, linking PostgreSQL\n\n    - <a href=\"http://www.odb.ntu.edu.tw/biology/?page_id=925\">Current bio-records and status in Bio-database of ODB</a> \n\n2. Shiny R integrated query web-application for bio-database of ODB\n\n    - <a href=\"http://bio.odb.ntu.edu.tw/query/\">Query application for Bio-database of ODB</a>. Update: Jan, 2018\n    \n    - Remarks: <a href=\"http://bio.odb.ntu.edu.tw/index_tech_citations.html\">Technical citations</a> and [notes](Techniques_ref01.md)\n\n3. Manual for usage:\n    - <a href=\"http://bio.odb.ntu.edu.tw/index.html\">Chinese version</a>\n    \n    - <a href=\"http://bio.odb.ntu.edu.tw/index_en.html\">English version</a>\n\n\n_Update: 2018.06.05_\n![twitter notes](https://pbs.twimg.com/media/De6PWyLUYAA5rdQ.jpg)\n\n_Update: 2019.01.15_\n[![Step by step (in Chinese)](https://github.com/cywhale/ODB/blob/master/docs/web_poly_regionalBio02_chinese.png)](https://github.com/cywhale/ODB/blob/master/docs/web_poly_regionalBio02_chinese.png)\n\n"
 },
 {
  "repo": "so-wise/so-fronts",
  "language": "Python",
  "readme_contents": "# Defining Southern Ocean fronts using unsupervised classification\n\n<a href=https://opensource.org/licenses/MIT><img src='https://img.shields.io/badge/License-MIT-blue.svg' alt='License: MIT' /></a>\n<a href=https://www.python.org/downloads/release/python-388/><img src='https://img.shields.io/badge/python-3.8-blue.svg' alt='Python version 3.8.8 used' /></a>\n<a href=\"https://github.com/psf/black\"><img alt=\"Python code style: black\" src=\"https://img.shields.io/badge/code%20style-black-000000.svg\"></a>\n<img src='https://readthedocs.org/projects/so-fronts/badge/?version=latest' alt='Documentation Status / link to documentation' />\n<a href=https://zenodo.org/badge/latestdoi/318541083><img src='https://zenodo.org/badge/318541083.svg' alt='Most recent Zenodo release' /></a>\n\n## Paper: <https://doi.org/10.5194/os-17-1545-2021>\n\n## Preprint: <https://doi.org/10.5194/os-2021-40>\n\n## Presentation at AGU2021: <https://doi.org/10.1002/essoar.10507114.1>\n\n## Short description\n\nIn the Southern Ocean, fronts delineate water masses, which correspond to upwelling\nand downwelling branches of the overturning circulation. Classically, oceanographers\ndefine Southern Ocean fronts as a small number of continuous linear features that\nencircle Antarctica. However, modern observational and theoretical developments are\nchallenging this traditional framework to accommodate more localized views of fronts\n[Chapman et al. 2020].\n\nHere we present code for implementing two related methods for calculating fronts from\noceanographic data. The first method uses unsupervised classification (specifically,\nGaussian Mixture Modeling or GMM) and a novel interclass metric to define fronts.\nThis approach produces a discontinuous, probabilistic view of front location,\nemphasising the fact that the boundaries between water masses are not uniformly sharp\nacross the entire Southern Ocean.\n\nThe second method uses Sobel edge detection to highlight\nrapid changes [Hjelmervik & Hjelmervik, 2019].\nThis approach produces a more local view of fronts,\nwith the advantage that it can highlight the movement\nof individual eddy-like features (such as the Agulhas rings).\n\n1. Chapman, C. C., Lea, M.-A., Meyer, A., Sallee, J.-B. & Hindell, M.\n    Defining Southern Ocean fronts and their influence on biological and\n    physical processes in a changing climate. Nature Climate Change (2020).\n    https://doi.org/10.1038/s41558-020-0705-4\n\n2. Maze, G. et al. Coherent heat patterns revealed by unsupervised\n    classification of Argo temperature profiles in the North Atlantic Ocean.\n    Progress in Oceanography (2017).\n    https://doi.org/10.1016/j.pocean.2016.12.008,\n    https://doi.org/10.5281/zenodo.3906236\n\n3. Hjelmervik, K. B. & Hjelmervik, K. T. Detection of oceanographic fronts\n    on variable water depths using empirical orthogonal functions.\n    IEEE Journal of Oceanic Engineering (2019).\n    https://doi.org/10.1109/JOE.2019.2917456\n\n## I metric for K=5\n\n![I metric for K=5](gifs/boundaries-k5.gif)\n\n## Getting started\n\n- Make the environment:\n\n    ```bash\n    make env\n    ```\n\n- Activate the environment in conda:\n\n     ```bash\n     conda activate ./env\n     ```\n\n- Change the settings in `src.constants` to set download location etc.\n\n- Download data (`get_zip`: 1694.64639 s):\n\n   ```bash\n   python3 src/data_loading/bsose_download.py\n   ```\n\n- Make I-metric:\n\n   ```bash\n   python3 src/models/batch_i_metric.py\n   ```\n\n- Make figures:\n\n   ```bash\n   python3 main.py\n   ```\n\n## Project Organization\n\n```txt\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 Makefile             <- Makefile with commands like `make env` or `make `\n\u251c\u2500\u2500 README.md            <- The top-level README for developers using this project.\n\u251c\u2500\u2500 main.py              <- The main python script to run.\n|\n\u251c\u2500\u2500 figures              <- .png images with non-enumerated names.\n\u2502\n\u251c\u2500\u2500 requirements         <- Directory containing the requirement files.\n\u2502\n\u251c\u2500\u2500 setup.py             <- makes project pip installable (pip install -e .) so src can be imported from jupyter notebooks etc.\n|\n\u251c\u2500\u2500 src                  <- Source code for use in this project.\n|   |\n\u2502   \u251c\u2500\u2500 __init__.py      <- Makes src a Python module\n|   |\n\u2502   \u251c\u2500\u2500 data             <- KO fronts to plot, other data.\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 data_loading     <- Scripts to download and name data.\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 models           <- Make I-metric and Sobel edge detection directory.\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 plot             <- plotting functions directory\n|   |\n\u2502   \u251c\u2500\u2500 plot_utils       <- plotting utilities directory\n|   |\n\u2502   \u251c\u2500\u2500 preprocessing    <- preprocessing scripts (to transform to density etc.).\n|   |\n|   \u251c\u2500\u2500  tests           <- Scripts for unit tests of your functions\n|   | \n|   \u251c\u2500\u2500 animate.py       <- animate i-metric.\n|   \u251c\u2500\u2500 constants.py     <- contains majority of run parameters that can be changed.\n|   \u251c\u2500\u2500 make_figures.py  <- make all figures in one long script.\n|   \u251c\u2500\u2500 move_figures.py  <- Move figures script (now unnecessary). \n|   |                       Changes figure names to Figure-X.png etc.\n|   \u2514\u2500\u2500 time_wrapper.py  <- time wrapper to time parts of the program.\n\u2502\n\u2514\u2500\u2500 setup.cfg            <- setup configuration file for linting rules\n```\n\n## Requirements\n\n- Anaconda, with `conda` working in shell.\n- `make` in shell.\n- Python 3.6+ (final run for paper used `python==3.8.8`)\n\n--------------------------------------------------------\n\nProject template created by the\n[Cambridge AI4ER Cookiecutter](https://github.com/ai4er-cdt/ai4er-cookiecutter).\n"
 },
 {
  "repo": "olmozavala/particleviz",
  "language": "JavaScript",
  "readme_contents": "\n#  <img src=\"docs/logos/logo_sm.png\" width=\"200px\" style=\"border:none\"> Welcome to ParticleViz  \nParticleViz is an Open Source software that is used to animate large number of particles inside dynamic web maps.\nIt is designed mostly for Earth Science scientists that simulate different processes using Lagrangian models.\n\nThe objectives of this software are:\n* Provide efficient visualizations that can help analyze and understand research made through lagrangian modelling in the Earth Sciences, in a fast and easy way. \n* Make it easy to share this research with other colleagues with self-contained websites. \n\n**ParticleViz** currently reads NetCDF outputs from [OceanParcels](https://oceanparcels.org/) and [OpenDrift](https://opendrift.github.io/).\n\n## Status\n![GitHub Repo stars](https://img.shields.io/github/stars/olmozavala/particleviz?style=social)\n![GitHub](https://img.shields.io/github/license/olmozavala/particleviz)\n![GitHub all releases](https://img.shields.io/github/downloads/olmozavala/particleviz/total)\n![GitHub issues](https://img.shields.io/github/issues/olmozavala/particleviz)\n![GitHub commit activity](https://img.shields.io/github/commit-activity/m/olmozavala/particleviz)\n\n## Install\n\n1. Clone the repository.\n\n```shell\ngit clone https://github.com/olmozavala/particleviz.git\ncd particleviz\n```\n\n2. Create a conda environment with the proper dependencies. For this step, you first need to install Anaconda (or Miniconda), more details can be found [here](https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html).\n\n```shell\nconda env create -f particleviz.yml\nconda activate particleviz\n```\n\n3. Enjoy life\n\n## Quick Start\n\nThe simplest way to use **ParticleViz** is to run it specifying the input netcdf from the command line directly (NetCDF should follow [OceanParcels](https://oceanparcels.org/) or [OpenDrift](https://opendrift.github.io/) format).\n\n```shell\npython ParticleViz.py --input_file <path_to_netcdf> \n```\n\nThis will generate the *default* web interface and store the parameters into a configuration file, `Current_Config.json`. It can be edited to customize the interface. You need to be _patient_ the first time you run it because it will install all the Javascript dependencies.\n\nTest it with the *Global_Marine_Debris.nc* example file in the *ExampleData* folder:\n\n```shell\npython ParticleViz.py --input_file ExampleData/Global_Marine_Debris.nc\n```\n<img src=\"docs/media/quickstart.gif\" alt=\"example\" />\n\n## Intro video\nThis is a presentation made at OceanSciences meeting about ParticleViz in March 2022.\n\n[![ParticleViz at OSM](docs/media/video_tm.png)](https://youtu.be/7Xk0DxRMPjQ?t=289)\n\n## Docs\nPlease take a look at the complete docs at [https://olmozavala.github.io/particleviz/](https://olmozavala.github.io/particleviz/)"
 },
 {
  "repo": "guzmanlopez/aws-data-downloader",
  "language": "R",
  "readme_contents": "[![AWS data downloader](https://github.com/guzmanlopez/aws-data-downloader/actions/workflows/main.yaml/badge.svg?branch=main)](https://github.com/guzmanlopez/aws-data-downloader/actions/workflows/main.yaml) [![pages-build-deployment](https://github.com/guzmanlopez/aws-data-downloader/actions/workflows/pages/pages-build-deployment/badge.svg?branch=main)](https://github.com/guzmanlopez/aws-data-downloader/actions/workflows/pages/pages-build-deployment)\n\n# Automatic Weather Station data downloader\n\nEl objetivo de este c\u00f3digo es descargar peri\u00f3dicamente (24 horas) los datos meteorol\u00f3gicos y mareogr\u00e1ficos generados por el Servicio de Oceanograf\u00eda, Hidrograf\u00eda y Meteorolog\u00eda de la Armada ([SOHMA](https://sohma.armada.mil.uy/), Uruguay) para su respaldo y f\u00e1cil acceso por parte de la comunidad.\n\n## Panel interactivo\n\nPara ver todos los datos en figuras interactivas por estaciones meteorol\u00f3gicas y mareogr\u00e1ficas ingresar al [Panel interactivo](https://guzmanlopez.github.io/aws-data-downloader/).\n## Descarga de los datos\n\nA continuaci\u00f3n se detallan tres opciones de c\u00f3mo descargarse todos los datos de este repositorio descargados hasta el momento:\n\n### Opci\u00f3n 1\n\nUtilizando la p\u00e1gina web est\u00e1tica generada por este sitio: \n\n- Ingresar a [Panel interactivo - Descargar datos](https://guzmanlopez.github.io/aws-data-downloader/#descargar-datos).\n- Luego elegir formato CSV (texto plano separado por comas) o Excel haciendo clic en las opciones que se presentan arriba a la izquierda. \n\n### Opci\u00f3n 2\n\nDescargarse el archivo comprimido de todo el repositorio desde aqu\u00ed: [main.zip](https://github.com/guzmanlopez/aws-data-downloader/archive/refs/heads/main.zip)\n\n### Opci\u00f3n 3\n\nUtilizando la terminal, clonar el repositorio:\n\n```{sh}\n# Utilizando HTTPS\ngit clone https://github.com/guzmanlopez/aws-data-downloader.git\n\n# O utilizando SSH\ngit clone git@github.com:guzmanlopez/aws-data-downloader.git\n```\n\nPara actualizar los \u00faltimos datos:\n\n```{sh}\n# En el directorio del repositorio previamente clonado:\ngit pull\n```\n\n"
 },
 {
  "repo": "oceandata/celeste",
  "language": "HTML",
  "readme_contents": "# Project C\u00e9leste \u2013 A developer's guide to Copernicus services driven by examples\n\nGet a quick start with Earth Observation data. Find and copy-paste code samples by use cases, markets, and geo parameters.\nA developer\u2019s guide to Copernicus products and services driven by examples.\n"
 },
 {
  "repo": "ioos/colocate",
  "language": "Jupyter Notebook",
  "readme_contents": "# colocate\nEasy access to oceanographic data co-located in space and time via Python\n\n## Preview/Run:\n\n#### ERDDAP colocate.ipynb:\n[![binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/ioos/colocate/master?filepath=notebooks%2Fcolocate.ipynb)\n[![nbviewer](https://raw.githubusercontent.com/jupyter/design/master/logos/Badges/nbviewer_badge.svg)](https://nbviewer.jupyter.org/github/ioos/colocate/blob/master/notebooks/colocate.ipynb)\n\n#### ERDDAP colocate-dev.ipyb:\n[![binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/ioos/colocate/master?filepath=notebooks%2Fcolocate-dev.ipynb)\n[![nbviewer](https://raw.githubusercontent.com/jupyter/design/master/logos/Badges/nbviewer_badge.svg)](https://nbviewer.jupyter.org/github/ioos/colocate/blob/master/notebooks/colocate-dev.ipynb)\n\n<img src=\"img/colocate_datashader.png\" alt=\"colocate example map plot\" style=\"width: 200px, align: center\" />\n\n## Installation:\n\n### Install with conda:\n```\ngit clone https://github.com/ioos/colocate.git\ncd colocate\nconda env create -f environment.yml\nconda activate colocate\n```\n\n### Run via command line:\n```\nerddap-co-locate\n```\n\n### Run in Jupyter notebook:\n```\njupyter notebook &\n```\n\n### Run in JupyterLab:\n\nThis step may be necessary for ipyleaflet and HoloViz to run correctly in JupyterLab.  Run the following on the command line with the 'colocate' conda environment active:\n```\njupyter labextension install @jupyter-widgets/jupyterlab-manager\njupyter labextension install jupyter-leaflet\njupyter labextension install @pyviz/jupyterlab_pyviz\n```\nThen, start JupyterLab:\n```\njupyter-lab &\n```\n\n### Run with voila:\n```\nvoila colocate.ipynb --enable_nbextensions=True --VoilaConfiguration.file_whitelist=\"['.*']\"\n```\n\n## Local Development:\nIf you want to develop locally, clone from GitHub, `cd` to the cloned repository root directory, and run `pip install` as follows:\n```\ngit clone https://github.com/ioos/colocate.git\ncd colocate\npip install -e . --no-deps --force-reinstall\n```\n\n\n# OceanHackWeek 2019 'co-locators' Project\nDescription of the original OceanHackWeek 2019 project that led to the development of this module.  \n\n## The problem\nCo-locate oceanographic data (served via ERDDAP) by establishing constraints to use as data server query filters.  \n\nSubmit the identical filter criteria to all ERDDAP servers indexed by the [awesome-erddap project](https://github.com/IrishMarineInstitute/awesome-erddap) and visualize the results.\n\n### Application example\nA user is interested in all the available oceanographic data in a region where an eddy just formed. They provide the geospatial bounds of the region and a temporal range and get an aggregated response of all available data.\n\n## Collaborators:\n\n| Name | Year |\n|------------|----|\n|Mathew Biddle|2019|\n|Sophie Chu|2019|\n|Yeray Santana Falcon|2019|\n|Molly James|2019|\n|Pedro Maga\u00f1a|2019|\n|Jazlyn Natalie|2019|\n|Laura Gomez Navarro|2019|\n|Shikhar Rai|2019|\n|Micah Wengren|2019|\n|Jacqueline Tay|2020|\n|Mike Morley|2020|\n|Yuta Norden|2020|\n\n\n### Specific tasks\n- [x] Collect temporal bounds.\n- [x] Collect spatial bounds.\n- [x] _Collect keywords?_\n- [x] Build query url.\n- [x] Do the search.\n- [ ] Evaluate the response.\n- [ ] Manage response.\n- [ ] Geospatial plotting.\n- [ ] Temporal plotting.\n- [ ] Link back to dataset on erddap server.\n- [ ] _Aggregated download?_\n\n\n### Existing methods\n- The Irish Marine Institute has developed a [keyword search across existing ERDDAP\n  servers](https://github.com/IrishMarineInstitute/search-erddaps)\n  - [Here is the list of all ERDDAP's they use.](https://github.com/IrishMarineInstitute/search-erddaps/blob/master/erddaps.json)\n- OHW18 built a [search interface for one ERDDAP server](https://github.com/oceanhackweek/ohw18_erddap-explorer)\n- yodapy: https://github.com/cormorack/yodapy\n\n\n### Proposed methods/tools\n- Jupyter\n- Python\n  - [erddapy](https://github.com/ioos/erddapy)\n  - [Pandas](https://pandas.pydata.org/pandas-docs/stable/getting_started/overview.html)\n\n### Background reading\n\nOptional: links to manuscripts or technical documents for more in-depth analysis.\n"
 },
 {
  "repo": "physoce/physoce-py",
  "language": "Python",
  "readme_contents": "# physoce\n\nPython tools for physical oceanography. Includes functions for plotting, time series analysis, statistics and surface wave dynamics.\n\nDocumentation can be found at https://physoce.github.io/physoce-py\n\nTo install the latest version using pip, run the following the terminal/command prompt:\n\n`pip install git+https://github.com/physoce/physoce-py`\n\nThis package can also be installed from the Python Package Index (PyPI), which is updated less frequently:\n\n`pip install physoce`\n\nFeedback and ideas for future work are welcome. Please feel free to submit an issue.\n"
 },
 {
  "repo": "ocefpaf/descriptive_oceanography",
  "language": "TeX",
  "readme_contents": "# Curso de Oceanografia F\u00edsica Descritiva lecionado na unimonte no primeiro semestre de 2014.\n\n## Reposit\u00f3rios com aulas, slides, notas e exerc\u00edcios para Oceanografia F\u00edsica Din\u00e2mica\n~ Filipe Fernandes ~\n\nPara compilar as aulas use o script `make_lecture.py`\n\n```python\nMake lecture slides, handout and homework.\n\nUsage:\n    make_lecture (DIR) [--compile=DOC]\n    make_lecture (-h | --help | --version)\n\nExamples:\n    make_lecture Aula_01 --compile=slides\n    make_lecture Aula_01 --compile=handout\n    make_lecture Aula_01 --compile=homework\n    make_lecture Aula_01 --compile=all\n\nArguments:\n  DIR      Lecture directory.\n\nOptions:\n  -c --compile=DOC   slides, handout, homework or all [default: slides]\n  --version   Show version.\n  ---help     Show this screen.\n```\n\n### 1. Plano de Aula\nDisciplina: Oceanografia Descritiva (Curso: Oceanografia / Ciclo/mod. 1/1B)\n\nProfessor: Filipe Fernandes\n\nTurno: Matutino\n\nSemestre: 2\u00ba -- 2013\n\nTurma: OCE1BM-VMA\n\n#### Ementa\nIntrodu\u00e7\u00e3o \u00e0 Oceanografia F\u00edsica. Conceitos, estrutura e caracter\u00edsticas gerais\ndos oceanos.\n\n#### Bibliografia b\u00e1sica\n- GARRISON, Tom; MIYAJI, C\u00edntia (Trad.). Fundamentos de oceanografia. S\u00e3o Paulo: Cengage Learning, 2009. 426 p. ISBN 9788522106776.\n- MIRANDA, L. B., CASTRO, B. M., KJERFVE, B. 2002. Princ\u00edpios de Oceanografia F\u00edsica de Estu\u00e1rios. 1\u00aa. ed. S\u00e3o Paulo : Edusp, 414p.\n- SVERDRUP, K. A., Duxbury A. B., Duxbury, A. C. 2006. Fundamentals of Oceanography. McGraw Hill, 5th edition. N\u00famero de Chamada: 551.46 S968f 2006.\n\n\n#### Bibliografia complementar\n- STEWART, R. H. 2002. Introduction to Physical Oceanography. Department of Oceanography. Texas A. M. University.\n- SOUZA, Maria Cristina de Arruda. A Corrente do Brasil ao largo de Santos: medi\u00e7\u00f5es diretas. 2000. 169p. Disserta\u00e7\u00e3o (mestrado). Instituto Oceanogr\u00e1fico. USP. Dispon\u00edvel em: http://www.teses.usp.br/teses/disponiveis/21/21132/tde-10092003-094250/pt-br.php\n- The Open University. 1989. Ocean Circulation, Pergamon Press, 2nd edition.\n- TIPLER, Paul Allen.  F\u00edsica para cientistas e engenheiros.  4. ed. Rio de Janeiro:  LTC,  2006. v.1.\n- TOMCZAK, Matthias.  Physical oceanography.   Australia  The Flinders University of South Australia,  2002. 1 CD-ROM:  son., color  Dispon\u00edvel em: http://www.es.flinders.edu.au/~mattom/regoc/pdfversion.html.\n\n### Cronograma\n| Aula | Data          | Conte\u00fado                              | Lista/Prova         |\n|:----:|:--------------|:--------------------------------------| -------------------:|\n| 01   | 2013-08-09    | Introdu\u00e7\u00e3o \u00e0 Oceanografia F\u00edsica 1    |                     |\n| 02   | 2013-08-16    | Introdu\u00e7\u00e3o \u00e0 Oceanografia F\u00edsica 2    | Lista 1             |\n| 03   | 2013-08-23    | Dimens\u00f5es e formas dos oceanos 1      | Lista 2             |\n| 04   | 2013-08-30    | Dimens\u00f5es e formas dos oceanos 2      | Lista 3             |\n| 05   | 2013-09-06    | Propriedades f\u00edsicas da \u00e1gua do mar 1 | Lista 4             |\n| 06   | 2013-09-13    | Propriedades f\u00edsicas da \u00e1gua do mar 2 | Lista 5             |\n| 07   | 2013-09-20    | Distribui\u00e7\u00e3o espacial e temporais 1   | Lista 6             |\n| 08   | 2013-09-27    | Distribui\u00e7\u00e3o espacial e temporais 2   | Lista 7             |\n|      | 2013-10-04    |                                       | Prova 1             |\n| 11   | 2013-10-11    | Leis de conserva\u00e7\u00e3o 1                 | Revis\u00e3o Prova 1     |\n| 12   | 2013-10-18    | Leis de conserva\u00e7\u00e3o 2                 | Lista 8             |\n| 13   | 2013-10-25    | Circula\u00e7\u00e3o e massas d'\u00e1gua 1          | Lista 9             |\n| 14   | 2013-11-01    | Circula\u00e7\u00e3o e massas d'\u00e1gua 2          | Lista 10            |\n| 15   | 2013-11-08    | Oc. costeira e estuarina 1            | An\u00e1lise de dados    |\n|      | 2013-11-15    |                                       | Feriado             |\n| 16   | 2013-11-22    | Oc. costeira e estuarina 2            | Lista 11            |\n|      | 2013-11-29    |                                       | Semin\u00e1rio           |\n|      | 2013-12-06    |                                       | Prova 2             |\n|      | 2013-12-13    |                                       | Revis\u00e3o             |\n|      | 2013-12-20    |                                       | Prova Alternativa   |\n"
 },
 {
  "repo": "BjerknesClimateDataCentre/QuinCe",
  "language": "Java",
  "readme_contents": "![Status](https://img.shields.io/uptimerobot/status/m778932366-17f73ee77c432e68e22f5195)\n![Uptime](https://img.shields.io/uptimerobot/ratio/m778932366-17f73ee77c432e68e22f5195)\n\n[![Version](https://img.shields.io/github/v/release/BjerknesClimateDataCentre/QuinCe)](https://quince.bcdc.no)\n![Activity](https://img.shields.io/github/commit-activity/m/BjerknesClimateDataCentre/QuinCe)\n\n[![build](https://github.com/BjerknesClimateDataCentre/QuinCe/workflows/build/badge.svg)](https://github.com/BjerknesClimateDataCentre/QuinCe/actions)\n[![junit](https://github.com/BjerknesClimateDataCentre/QuinCe/workflows/junit/badge.svg)](https://github.com/BjerknesClimateDataCentre/QuinCe/actions)\n[![codecov](https://codecov.io/gh/BjerknesClimateDataCentre/QuinCe/branch/master/graph/badge.svg)](https://codecov.io/gh/BjerknesClimateDataCentre/QuinCe)\n\n[![Issues](https://img.shields.io/github/issues-raw/BjerknesClimateDataCentre/QuinCe)](https://github.com/BjerknesClimateDataCentre/QuinCe/issues)\n[![Bugs](https://img.shields.io/github/issues/BjerknesClimateDataCentre/QuinCe/bug?color=red&label=known%20bugs)](https://github.com/BjerknesClimateDataCentre/QuinCe/issues?q=is%3Aissue+is%3Aopen+label%3Abug)\n[![Pull Requests](https://img.shields.io/github/issues-pr/BjerknesClimateDataCentre/QuinCe)](https://github.com/BjerknesClimateDataCentre/QuinCe/pulls)\n\n[![License](https://img.shields.io/github/license/BjerknesClimateDataCentre/QuinCe)](https://www.gnu.org/licenses/gpl-3.0)\n\n# QuinCe\nQuinCe is a system for the automatic QC of data collected from ship-board scientific instruments.\n\nSee Documentation/Specification for more details of what the project does.\n\n"
 },
 {
  "repo": "grrrizzzz/numerical_modeling",
  "language": "FORTRAN",
  "readme_contents": "# numerical_modeling\nSome oceanographic numerical modeling code in Matlab and Fortran\n\n* I wrote the lake model in its entirety.  It's based on Lake Kauhako on the island of Molokai.\n* I also wrote much of the barotropic ocean model.\n* The Zebiak-Cane ENSO model was written back in the 80's and is essentially the first numerical model to capture the physics of ENSO realistically.  I modified a few hundred lines of code for my thesis experiments."
 },
 {
  "repo": "cchdo/exchange",
  "language": "Python",
  "readme_contents": "exchange\n========\n![Docs Build](https://github.com/cchdo/exchange/workflows/Docs%20Build/badge.svg)\n[![Documentation Status](https://readthedocs.org/projects/exchange-format/badge/?version=latest)](https://exchange-format.readthedocs.io/en/latest/?badge=latest)\n\n\nDescription of the WHP (CCHDO) Exchange Format for CTD/Hydrographic Data\n"
 },
 {
  "repo": "chouj/JPO_CloudofKeywords",
  "language": "Matlab",
  "readme_contents": "![wordcloud_jpo_keywords_2013-2017](https://github.com/chouj/JPO_CloudofKeywords/blob/master/wordcloud_jpo_keywords_2013-2017.jpg)\n\n# a MATLAB script for generating cloud of keywords of the Journal of Physical Oceanography\n\n## WARNING\nDo not use this code illegally !\n\n## Note\nWindows code.\n\nMATLAB R2017b or newer and associated Text Analytics Toolbox are required.\n\n## Acknowledgements\n\n#### Inspired by \n\n[\u5982\u4f55\u8fc5\u901f\u79ef\u7d2f\u4e24\u4e07\u8bcd\u6c47\u91cf\u5e76\u6d41\u7545\u9605\u8bfb\u7ecf\u6d4e\u5b66\u4eba\uff1f](https://zhuanlan.zhihu.com/p/20713896)\n\n[R2017b Text Analytics Toolbox:\u82f9\u679c\u7684\u5341\u5e74](https://zhuanlan.zhihu.com/p/31054652)\n\n\n#### Thanks to\n\n[free-proxy-list](https://github.com/a2u/free-proxy-list)\n\n[MATLAB \u4e0e \u722c\u866b](https://zhuanlan.zhihu.com/p/35372205)\n\n[How can I save the content of the MATLAB web browser window programmatically? Not the same as WEBREAD....](https://ww2.mathworks.cn/matlabcentral/answers/276583-how-can-i-save-the-content-of-the-matlab-web-browser-window-programmatically-not-the-same-as-webrea)\n\n[How do I set a proxy server to use with the URLREAD and URLWRITE functions in MATLAB?](https://ww2.mathworks.cn/matlabcentral/answers/94117-how-do-i-set-a-proxy-server-to-use-with-the-urlread-and-urlwrite-functions-in-matlab)\n\n## \u4e2d\u6587\n#### \u722c\u5b66\u672f\u520a\u7269JPO\u8bba\u6587\u7684\u5173\u952e\u8bcd \u6839\u636e\u8bcd\u9891\u751f\u6210\u6807\u7b7e\u4e91\n\n## \u6350\u8d60\n[![Donate](https://img.shields.io/badge/Donate-PayPal-green.svg)](https://www.paypal.me/Mesoscale)\n[![Donate](https://img.shields.io/badge/Donate-WeChat-brightgreen.svg)](https://github.com/chouj/donate-page/blob/master/simple/images/WeChatQR.jpg?raw=true)\n[![Donate](https://img.shields.io/badge/Donate-AliPay-blue.svg)](https://github.com/chouj/donate-page/blob/master/simple/images/AlipayQR.jpg?raw=true)\n"
 },
 {
  "repo": "pvthinker/argopy",
  "language": "Python",
  "readme_contents": "# argopy\nPython tools to perform statistics with [Argo data](http://www.argo.ucsd.edu/) and produce global atlases of statistics. The tools allow to brew the over two millions of profiles in the Argo database with hundreds of cores. \n\nThe resulting atlas is about to be published on [SEANOE](https://www.seanoe.org)\n\n![ScreenShot](/images/compensated_density_1000m.png)"
 },
 {
  "repo": "chouj/MoveEddiesIntoAtlantic",
  "language": "M",
  "readme_contents": "![19921014_MoveEddiesIntoAtlanticOcean](https://github.com/chouj/MoveEddiesIntoAtlantic/blob/master/MovingAllEddiesIntoAtlantic_19930101.gif?raw=true)\n\n# What If moving all oceanic mesoscale eddies into the vast domain of the Atlantic Ocean ?\n\nThis idea results from a discussion with one of my colleagues who did not believe oceanic mesoscale eddies cover nearly a third of the ocean surface at any time. The data calculation such as summing up all the areas of eddies is effective but sometimes boring. Thus, how about visualizing it by gathering them together in one place ? The vast Atlantic Ocean covers about 29 percent of the Earth's water surface area according to [Wikipedia](https://www.wikiwand.com/en/Atlantic_Ocean), making it the perfect domain for this kind of demonstration. \n\n## Data\n[Global Mesoscale Eddy Track Atlas Product released by AVISO](https://www.aviso.altimetry.fr/en/data/products/value-added-products/global-mesoscale-eddy-trajectory-product.html) (See its breif introduction [here](http://cioss.coas.oregonstate.edu/eddies/), including data structures.)\n\n## Eddy Moving Strategy\n- Generate property matrix for eddies already in the Atlantic Ocean. Use it as a eddy list for both the original ones that not need to move and those after moved.\n- Generate property matrix for eddies outside the Atlantic Ocean which have to moved. So, a moving waiting list.\n- Randomly generate a lon/lat coordinate for eddy center after moved.\n- Diagnose whether the generated position fall within the region of the Atlantic Ocean and whether it is on land or not.\n- Calculate the distances from generated coordinate to 1) the boundary of the Atlantic ocean, 2) the coastline and 3) all existing eddy circles (not eddy centers).\n- Continue if the generated coordinate does not fall within any existing eddy circles.\n- Find whether there are eddies in the moving waiting list that can be fit into the minimum distance calculated above.\n- If so, move the largest one to the generated position. Then, keep moving until all eddies outside the Atlantic Ocean domain have been moved into the domain.\n\n## Dependency\n- Lon/Lat coordinate data for [the boundary of the Atlantic Ocean](http://www.marineregions.org/gazetteer.php?p=details&id=1902), including islands. Here's a [*.mat* version](https://github.com/chouj/MoveEddiesIntoAtlantic/raw/master/AtlanticOceanBoundary.mat).\n- **[The M_Map toolbox](https://www.eoas.ubc.ca/~rich/map.html)** and associated Lon/Lat coordinate data **m_coasts.mat**\n- [land_or_ocean.m](https://ww2.mathworks.cn/matlabcentral/fileexchange/45268-land_or_ocean-m)\n\n## Compatibility\n#### MATLAB Release Compatibility\nCreated with R2015a\n\n## Acknowledgement\n- [\u5982\u4f55\u751f\u6210\u591a\u4e2a\u4e92\u4e0d\u91cd\u53e0\u7684\u4e0d\u540c\u534a\u5f84\u5706\uff1f](https://www.zhihu.com/question/53012468)\n\n## Final Figure\n##### Take 1993-01-01 for instance.\n![19930101_MoveEddiesIntoAtlanticOcean](https://github.com/chouj/MoveEddiesIntoAtlantic/blob/master/19930101_EddiesMovedIntoAtlanticOcean_r300.png?raw=true)\n\n## Support ME !\n\n[![Donate](https://img.shields.io/badge/Donate-PayPal-green.svg)](https://www.paypal.me/Mesoscale)\n[![Donate](https://img.shields.io/badge/Donate-WeChat-brightgreen.svg)](https://github.com/chouj/donate-page/blob/master/simple/images/WeChatQR.jpg?raw=true)\n[![Donate](https://img.shields.io/badge/Donate-AliPay-blue.svg)](https://github.com/chouj/donate-page/blob/master/simple/images/AlipayQR.jpg?raw=true)\n"
 },
 {
  "repo": "truedichotomy/dg_ocean_toolbox",
  "language": "MATLAB",
  "readme_contents": "# dg_ocean_toolbox\nA set of matlab scripts and functions used for the loading, processing, and visualization of oceanographic data.\n"
 },
 {
  "repo": "Davidatlarge/ggTS",
  "language": "R",
  "readme_contents": "---\ntitle: \"create a TS diagram as a ggplot\"\nauthor: \"David Kaiser\"\ndate: \"2018/01/16\"\noutput: github_document\n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\nsource(\"ggTS_DK.R\")\n```\n\n### Description\n\nA TS diagram, Temperature-Salinity-diagram, plots the **potential temperature** of water over **salinity**. Many water masses have characteristic shapes in a TS diagram, [which is used in physical oceanography to identify water masses and their mixing.](https://doi.org/10.1016/S0422-9894(08)71172-3)\n\nThis function requires the input of vectors of **potential temperature** and **salinity**. The *gws* package is used to calculate potential density for plotting of isopycnals (contours of the same density). This calculation requires a **reference pressure**, which defaults to 0, the sea surface.\n\nA third vector can optionally be supplied to **col.par** to be plotted in color, and can be named with a string supplied to **col.name**.\n\nUse [![DOI](https://zenodo.org/badge/112729228.svg)](https://zenodo.org/badge/latestdoi/112729228) to cite a permanent archived version of the function.\n\n\n### Arguments\n\n  *sal* -- vector of salinity values\n  \n  *pot.temp* -- vector of potential temperature values in degree C\n  \n  *reference.p = 0* -- reference pressure which was also used to calculate potential temperature\n  \n  *col.par = NA* -- optional vector of a parameter to be displayed as color of the TS-pairs\n  \n  *col.name = \"col.par\"* -- optional name of the \"col.par\" to be used on the color bar\n\n### Plot\n\n```{r example_data}\nexample <- read.csv(\"example_data/example_data.csv\")\n\nhead(example)\n```\n\nThe data will be plotted using the *ggplot2* package.\n\n```{r plot_result, echo=TRUE, message=FALSE, warning=FALSE}\nggTS(sal = example$salinity, \n        pot.temp = example$potential.temperature, \n        reference.p = 0,\n        col.par = example$depth, \n        col.name = \"depth [m]\")\n```\n\n**NOTE**: the special \"\u00c2\" character does not show when the function is used in R, this seems to be a markdown/knitr problem\n\nSince the result is a ggplot, it can be altered and amended:\n```{r extend_plot, echo=TRUE, message=FALSE, warning=FALSE}\np1 <- ggTS(sal = example$salinity, \n        pot.temp = example$potential.temperature, \n        reference.p = 0,\n        col.par = example$depth, \n        col.name = \"depth [m]\")\np1 + scale_color_gradient(low = \"grey\", high = \"black\", name = \"something\\nelse\") +\n      annotate(geom = \"text\", x = 15, y = 6, color = \"red\", size = 14, label = \"ADD\\nSTUFF\")\n```\n\nPlot a TS diagram with isopycnals running more horizontally\n```{r example_data1}\nexample1 <- read.csv(\"example_data/example_data1.csv\")\n\nhead(example1)\n```\n\n```{r plot_result1, echo=TRUE, message=FALSE, warning=FALSE}\nggTS(sal = example1$salinity, \n        pot.temp = example1$potential.temperature, \n        reference.p = 0,\n        col.par = example1$depth, \n        col.name = \"depth [m]\")\n```"
 },
 {
  "repo": "ArgoCanada/bgcArgoDMQC",
  "language": "Python",
  "readme_contents": "{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"# Argo Canada BGC Quality Control\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 1,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# import package\\n\",\n    \"import bgcArgoDMQC as bgc\\n\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Oxygen DMQC\\n\",\n    \"\\n\",\n    \"For delayed mode quality control, we need to (1) visually inspect the data as a DMQC operator and (2) compute a gain by comparing the data to a reference source, typically WOA, NCEP, or ERA5 data. For both of these tasks, we will load the data via the [synthetic profile file](https://archimer.ifremer.fr/doc/00445/55637/). `bgcArgoDMQC` can load these files given a WMO number, and has functions for visualizing and performing QC on the data within.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 3,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stderr\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"c:\\\\Users\\\\GordonC\\\\Documents\\\\projects\\\\bgcArgoDMQC\\\\bgcArgoDMQC\\\\unit.py:36: RuntimeWarning: invalid value encountered in log\\n\",\n      \"  Ts = np.log((298.15 - T)/(273.15 + T))\\n\"\n     ]\n    },\n    {\n     \"data\": {\n      \"image/png\": \"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\",\n      \"text/plain\": [\n       \"<Figure size 432x288 with 3 Axes>\"\n      ]\n     },\n     \"metadata\": {},\n     \"output_type\": \"display_data\"\n    }\n   ],\n   \"source\": [\n    \"# define a WMO number you want to look at\\n\",\n    \"wmo = 4900869\\n\",\n    \"# load into a synthetic profile object\\n\",\n    \"syn = bgc.sprof(wmo)\\n\",\n    \"# look at the current state of QC flags for T, S, and DO\\n\",\n    \"g1 = syn.plot(kind='qcprofiles', varlist=['TEMP', 'PSAL', 'DOXY'])\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"We can see above that most T/S points are good, with a few obvious outliers in red. THe oxygen data also look all good. This is an old float, but for current floats the QC flag for unadjusted oxygen should be 3 (probably bad). If we want to see just good data (QC flags of 1, 2, 5, 8) so that the scale on the salinity plot isn't so bad, we can run:\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 4,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"data\": {\n      \"image/png\": \"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\",\n      \"text/plain\": [\n       \"<Figure size 432x288 with 3 Axes>\"\n      ]\n     },\n     \"metadata\": {},\n     \"output_type\": \"display_data\"\n    }\n   ],\n   \"source\": [\n    \"# get rid of bad flags\\n\",\n    \"syn.clean()\\n\",\n    \"# re-make above plot\\n\",\n    \"g2 = syn.plot(kind='qcprofiles', varlist=['TEMP', 'PSAL', 'DOXY'])\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"As a QC operator it would be out job to set the flags for `DOXY` to 3 in the D-mode file. Since none of the \\\"good\\\" points look wrong (note - its hard to tell in a big cloud of points like this, I usually look at profiles indiviually but that would be overkill in this notebook), our visual inspection is done and we can calulate the gain. Since this is an older float, there are no in-air measurements made by the optode. Therefore we will calcualte the gain by comparing surface values to WOA data.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 5,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"Extracting WOA data for Jan\\n\",\n      \"Extracting WOA data for Feb\\n\",\n      \"Extracting WOA data for Mar\\n\",\n      \"Extracting WOA data for Apr\\n\",\n      \"Extracting WOA data for May\\n\",\n      \"Extracting WOA data for Jun\\n\",\n      \"Extracting WOA data for Jul\\n\",\n      \"Extracting WOA data for Aug\\n\",\n      \"Extracting WOA data for Sep\\n\",\n      \"Extracting WOA data for Oct\\n\",\n      \"Extracting WOA data for Nov\\n\",\n      \"Extracting WOA data for Dec\\n\",\n      \"Bounding climatological profile(s) missing data - taking simple average of available data.\\n\",\n      \"Bounding climatological profile(s) missing data - taking simple average of available data.\\n\",\n      \"Bounding climatological profile(s) missing data - taking simple average of available data.\\n\",\n      \"Bounding climatological profile(s) missing data - taking simple average of available data.\\n\",\n      \"Bounding climatological profile(s) missing data - taking simple average of available data.\\n\",\n      \"Bounding climatological profile(s) missing data - taking simple average of available data.\\n\",\n      \"Bounding climatological profile(s) missing data - taking simple average of available data.\\n\",\n      \"Bounding climatological profile(s) missing data - taking simple average of available data.\\n\",\n      \"Bounding climatological profile(s) missing data - taking simple average of available data.\\n\",\n      \"Bounding climatological profile(s) missing data - taking simple average of available data.\\n\",\n      \"Bounding climatological profile(s) missing data - taking simple average of available data.\\n\",\n      \"Bounding climatological profile(s) missing data - taking simple average of available data.\\n\",\n      \"Bounding climatological profile(s) missing data - taking simple average of available data.\\n\",\n      \"Bounding climatological profile(s) missing data - taking simple average of available data.\\n\",\n      \"Bounding climatological profile(s) missing data - taking simple average of available data.\\n\",\n      \"Bounding climatological profile(s) missing data - taking simple average of available data.\\n\",\n      \"Bounding climatological profile(s) missing data - taking simple average of available data.\\n\",\n      \"Bounding climatological profile(s) missing data - taking simple average of available data.\\n\",\n      \"Bounding climatological profile(s) missing data - taking simple average of available data.\\n\",\n      \"Bounding climatological profile(s) missing data - taking simple average of available data.\\n\",\n      \"Bounding climatological profile(s) missing data - taking simple average of available data.\\n\",\n      \"Bounding climatological profile(s) missing data - taking simple average of available data.\\n\",\n      \"Bounding climatological profile(s) missing data - taking simple average of available data.\\n\",\n      \"Bounding climatological profile(s) missing data - taking simple average of available data.\\n\",\n      \"Bounding climatological profile(s) missing data - taking simple average of available data.\\n\",\n      \"Bounding climatological profile(s) missing data - taking simple average of available data.\\n\",\n      \"Bounding climatological profile(s) missing data - taking simple average of available data.\\n\",\n      \"Bounding climatological profile(s) missing data - taking simple average of available data.\\n\",\n      \"\\n\",\n      \"Calculating gains using WOA surface data and float O2 percent saturation...\\n\"\n     ]\n    },\n    {\n     \"name\": \"stderr\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"c:\\\\Users\\\\GordonC\\\\Documents\\\\projects\\\\bgcArgoDMQC\\\\bgcArgoDMQC\\\\interp\\\\interp.py:207: RuntimeWarning: Mean of empty slice\\n\",\n      \"  woa_interp[:,i] = np.nanmean(np.nanmean(D3, axis=2), axis=1)\\n\",\n      \"c:\\\\Users\\\\GordonC\\\\Documents\\\\projects\\\\bgcArgoDMQC\\\\bgcArgoDMQC\\\\core\\\\core.py:1005: RuntimeWarning: Mean of empty slice\\n\",\n      \"  mean_float_data[i,2] = np.nanmean(subset_o2sat)\\n\",\n      \"c:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\py39\\\\lib\\\\site-packages\\\\numpy\\\\lib\\\\nanfunctions.py:1670: RuntimeWarning: Degrees of freedom <= 0 for slice.\\n\",\n      \"  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"# calculate the gains\\n\",\n    \"gains = syn.calc_gains(ref='WOA')\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"The `ref` keyword argument sets the reference dataset. In this case we set it to WOA data. By default `ref` is set to `'NCEP'`, but in this case that would return all `NaN` values since there is no in-air data for this float.\\n\",\n    \"\\n\",\n    \"We can also visualize the gains in a single line:\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 6,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"data\": {\n      \"image/png\": \"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\",\n      \"text/plain\": [\n       \"<Figure size 432x288 with 2 Axes>\"\n      ]\n     },\n     \"metadata\": {},\n     \"output_type\": \"display_data\"\n    }\n   ],\n   \"source\": [\n    \"# plot gains over time, show source data\\n\",\n    \"g3 = syn.plot('gain', ref='WOA')\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"I find this plot particularly useful, as sometimes it is a good indicator to go back and inspect certain profiles. We see a couple large spikes in the suface float data. Are those spikes real, or should we go back and have a closer look at those profiles? This could change our mean gain a little, and perhaps we would flag some data as bad that we didn't notice before.\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"interpreter\": {\n   \"hash\": \"8571e7f3e92f6e490cddd84ef78d4e4e0b96a1f565959148b10a39523fba88f5\"\n  },\n  \"kernelspec\": {\n   \"display_name\": \"Python 3.9.7 ('py39')\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.9.7\"\n  },\n  \"orig_nbformat\": 4\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 2\n}\n"
 },
 {
  "repo": "truedichotomy/MAB_ocean_climate",
  "language": "MATLAB",
  "readme_contents": "# MAB_ocean_climate\n\nCitation: \n\nWallace, E. J., Looney, L. B., & Gong, D. (2018). Multi-decadal trends and variability in temperature and salinity in the Mid-Atlantic Bight, Georges Bank, and Gulf of Maine. Journal of Marine Research, 76(5-6), 163-215.\n\nAbstract:\n\nIncreasing attention is being placed on the regional impact of climate change. This study focuses on the decadal scale variabilities of temperature and salinity in the Mid-Atlantic Bight (MAB), Georges Bank (GB), and Gulf of Maine (GOM) from 1977 to 2016 using hydrographic survey data from the National Oceanic and Atmospheric Administration (NOAA) Northeast Fisheries Science Center. The MAB (as defined by the shelf regions from Cape Hatteras to Cape Cod) experienced warming rates of 0.57 \u25e6C per decade during the Winter/Spring season (Jan\u2013Apr) and 0.47 \u25e6C per decade during the Fall/Winter season (Sep\u2013Dec). The GOM and GB, on the other hand, warmed at approximately half the rate of the MAB over the same time span (1977\u20132016). We found that rates of warming vary on decadal time scales. From 1977 to 1999, significant temperature increases (>0.6 \u25e6C/decade) were found in the southern regions of the MAB during the Winter/Spring season. During the same period, significant freshening (stronger than \u22120.2/decade) was found in GB and the northern regions of the MAB during the Winter/Spring and Summer seasons. From 1999 to 2016, on the other hand, we found no significant trends in temperature and few significant trends in salinity with the exceptions of some northern MAB regions showing significant salting. Interannual variability in shelf salinity can in part be attributed to river discharge variability in the Hudson River and Chesapeake Bay. However, decadal scale change in shelf salinity cannot be attributed to changes in river discharge as there were no significant decadal scale changes in river outflow. Variability in along-shelf freshwater transport and saline intrusions from offshore were the likely drivers of long-term changes in MAB shelf-salinity.\n\nData Analysis:\n\nSoftware required:\n- MATLAB\n- MATLAB Mapping Toolbox\n- Gibbs SeaWater (GSW) Oceanographic Toolbox (http://www.teos-10.org/software.htm)\n\nAanalysis of CTD data:\nRun 'dg_grid_regions2D.m' first to load CTD data & calculate grids.\n\nAnalysis of river discharge data:\nRun 'dg_load_hudson_chesapeake_discharge.m' first to load river discharge data.\n"
 },
 {
  "repo": "apaloczy/InnerShelfReynoldsStresses",
  "language": "Jupyter Notebook",
  "readme_contents": "# InnerShelfReynoldsStresses\n\n[![DOI](https://zenodo.org/badge/255769835.svg)](https://zenodo.org/badge/latestdoi/255769835)\n\nThis repository contains codes and processed datasets for a manuscript entitled **\"Subtidal to supertidal variability of Reynolds stresses in a mid-latitude stratified inner-shelf\"**, by A. Pal\u00f3czy, J. A. MacKinnon and A. F. Waterhouse, [published on the Journal of Physical Oceanography](https://journals.ametsoc.org/view/journals/phoc/aop/JPO-D-20-0140.1/JPO-D-20-0140.1.xml). This [Jupyter notebook](https://nbviewer.jupyter.org/github/apaloczy/InnerShelfReynoldsStresses/blob/master/index.ipynb) provides an overview of the contents.\n\nThe directory **plot_figs/** contains the Python codes used to produce the figures in the manuscript (Figures 1-14). The codes depend on the data files in the **data_reproduce_figs/** directory. Some of these are too large to be included in this repository, but are available for download from the links listed on the accompanying README files. Please contact [Andr\u00e9 Pal\u00f3czy](mailto:apaloczy@ucsd.edu) if you have issues downloading the files.\n\n## Abstract\nWe describe the spatio-temporal variability and vertical structure of turbulent Reynolds Stresses (RSs) in a stratified inner-shelf with an energetic internal wave climate. The RSs are estimated from direct measurements of velocity variance derived from bottom-mounted Acoustic Doppler Current Profilers. We link the RSs to different physical processes, namely internal bores, mid-water shear instabilities within vertical shear events related to wind-driven subtidal along-shelf currents; and non-turbulent stresses related to incoming Nonlinear Internal Wave (NLIW) trains. The typical RS magnitudes are O(0.01 Pa) for background conditions, with diurnal pulses of O(0.1-1 Pa), and O(1 Pa) for the NLIW stresses. A NLIW train is observed to produce a depth-averaged vertical stress divergence sufficient to accelerate water 20 cm/s in 1 hour, suggesting NLIWs may also be important contributors to the depth-averaged momentum budget. The subtidal stresses show significant periodic variability and are O(0.1 Pa). Conditionally-averaged velocity and RS profiles for northward/southward flow provide evidence for down-gradient turbulent momentum fluxes, but also indicate departures from this expected regime. Estimates of the terms in the depth-averaged momentum equation suggest that the vertical divergence of the RSs are important terms in both the cross-shelf and along-shelf directions, with geostrophy also present at leading-order in the cross-shelf momentum balance. Among other conclusions, the results highlight that internal bores and shoaling NLIWs may also be important dynamical players in other inner-shelves with energetic internal waves.\n\n## Authors\n* [Andr\u00e9 Pal\u00f3czy](https://apaloczy.scrippsprofiles.ucsd.edu/) (<apaloczy@ucsd.edu>)\n* [Jennifer A. MacKinnon](https://jmackinnon.scrippsprofiles.ucsd.edu/)(<jmackinnon@ucsd.edu>)\n* [Amy F. Waterhouse](https://awaterhouse.scrippsprofiles.ucsd.edu/)(<awaterhouse@ucsd.edu>)\n\n## Acknowledgments\nThe authors gratefully acknowledge funding from the Office of Naval Research (ONR), grants grants N00014-15-1-2885, N00014-15-1-2633 and N00014-5-1-2631. Input from two anonymous reviewers substantially improved the manuscript. Thanks to Stephen Monismith, Jack McSweeney, Johannes Becherer, Jim Lerczak and Anthony Kirincich for helpful discussions during the writing of this manuscript, and to Johanna Rosman for helpful discussions and assistance in implementing the Adaptive Filtering Method. We thank Pieter Smit and Tim Janssen of Sofar Technologies for providing the wave data, and Jim Thomson and Merrick Haller for providing the land-based meteorological data. Thanks also to Eric Terrill and the technical team at the Coastal Observing R&D Center at Scripps Institution of Oceanography for providing the marine meteorological observations (miniMET buoy). We also thank the Captain and crew of the R/V *Sally Ride*, Christian McDonald and Brett Pickering for successfully deploying and recovering the moorings used in this study.\n\n## Data availability statement\nCodes and reduced datasets required to reproduce the results are available at [https://github.com/apaloczy/InnerShelfReynoldsStresses](https://github.com/apaloczy/InnerShelfReynoldsStresses), archived under DOI [10.5281/zenodo.4601716](http://doi.org/10.5281/zenodo.4601716). The full inner shelf dataset with the raw mooring data required to derive the reduced datasets is archived under DOI [10.6075/J0WD3Z3Q](https://doi.org/10.6075/J0WD3Z3Q).\n"
 },
 {
  "repo": "Esri/ecological-marine-units-explorer",
  "language": "CSS",
  "readme_contents": "# Exploring Ecological Marine Units\n\n[![N|Solid](https://github.com/ArcGIS/ecological-marine-units-explorer/blob/master/images/splash-screen.png)](http://livingatlas.arcgis.com/emu/)\n\n\nThe [Ecological Marine Units Explorer] is a web-based application that is used to explore different water characteristics available from NOAA's World Ocean Atlas. \n\n## Features\n* The interactive map allows you to zoom and pan and interact with the data by clicking on the map. Clicking the points on the map enables you to explore the depth (vertical) profile and associated oceanographic information for the selected location.\n\n## Instructions\n1. Fork and then clone the repo. \n2. Navigate to the home directory of the repo on your local machine with the web server running and view the app\n\n## Requirements\n* Notepad or your favorite HTML editor\n* Web browser with access to the Internet\n\n## Resources\nThe Ecological Marine Units project is an innovative public-private partnership led by Esri and the USGS.\n[![N|Solid](http://www.esri.com/~/media/Images/Content/Ecological-Marine-Units/logos)](http://www.esri.com/~/media/Images/Content/Ecological-Marine-Units/logos)\n\n[Ecological Marine Units] website\n\n## Issues\n\nFind a bug or want to request a new feature?  Please let us know by submitting an issue.\n\n## Contributing\n\nEsri welcomes contributions from anyone and everyone. Please see our [guidelines for contributing](https://github.com/esri/contributing).\n\n## Licensing\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\nA copy of the license is available in the repository's [license.txt]( https://raw.github.com/Esri/ecological-marine-units-explorer/master/LICENSE.txt) file.\n\n[//]: # (These are reference links used in the body of this note and get stripped out when the markdown processor does its job. There is no need to format nicely because it shouldn't be seen. Thanks SO - http://stackoverflow.com/questions/4823468/store-comments-in-markdown-syntax)\n\n   [Ecological Marine Units]: <http://www.esri.com/ecological-marine-units>\n   [Ecological Marine Units Explorer]: <http://livingatlas.arcgis.com/emu/>\n"
 },
 {
  "repo": "CyprienBosserelle/xbeach_gpu",
  "language": "Cuda",
  "readme_contents": "# XBeach_GPU\nLightweight version of [XBeach](https://oss.deltares.nl/web/xbeach/ \"XBeach Homepage\") that uses CUDA C to run on the GPU. The code is used to simulate coastal waves, currents, sediment transport and beach morphology changes. \n***\n[![License: GPL v3](https://img.shields.io/badge/License-GPL%20v3-brightgreen.svg)](https://www.gnu.org/licenses/gpl-3.0)\n[![Codacy Badge](https://api.codacy.com/project/badge/Grade/688613b28d894c73bcc47274bb936713)](https://www.codacy.com/project/CyprienBosserelle/xbeach_gpu/dashboard?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=CyprienBosserelle/xbeach_gpu&amp;utm_campaign=Badge_Grade_Dashboard)\n"
 },
 {
  "repo": "IrishMarineInstitute/erddap-leaflet-velocity-demo",
  "language": "JavaScript",
  "readme_contents": "# erddap-leaflet-velocity-demo\nA demonstration of connecting [Erddap](https://github.com) to [Leaflet-Velocity](https://github.com/danwild/leaflet-velocity) maps\n\nSee [here](https://irishmarineinstitute.github.io/erddap-leaflet-velocity-demo/) for an online demonstration.\n\nSee [here](https://irishmarineinstitute.github.io/erddap-leaflet-velocity-demo/time-dimension-example/) for example with time dimension control.\n\n## Contents\n   - [Including the Code](https://github.com/IrishMarineInstitute/erddap-leaflet-velocity-demo/blob/main/README.md#including-the-code)\n   - [Example Usage](https://github.com/IrishMarineInstitute/erddap-leaflet-velocity-demo/blob/main/README.md#example-usage)\n   - [Function Inputs](https://github.com/IrishMarineInstitute/erddap-leaflet-velocity-demo/blob/main/README.md#function-inputs)\n   - [Grib2JSON Output format](https://github.com/IrishMarineInstitute/erddap-leaflet-velocity-demo/blob/main/README.md#grib2json-output-format)\n   - [JavaScript Boilerplate](https://github.com/IrishMarineInstitute/erddap-leaflet-velocity-demo/blob/main/README.md#javascript-boilerplate)\n\n## Including the Code\n\nTo include the code in your own website, you will need to first add the [Leaflet.js](https://leafletjs.com/) mapping library, Leaflet-Velocity and then include errdapToLeafletVelocity, e.g.:\n\n```html\n<script src=\"https://irishmarineinstitute.github.io/erddap-leaflet-velocity-demo/erddapToLeafletVelocity.js\"></src>\n```\n\n## Example Usage\n\n```javascript\nL.erddapVelocityLayer({\n\terddapBaseUrl: 'https://erddap.marine.ie',\n\tdatasetID: 'IMI_Model_Stats',\n\tuParameter: 'sea_surface_x_velocity',\n\tvParameter: 'sea_surface_y_velocity',\n\tminLat: 48.5,\n\tmaxLat: 58.5,\n\tminLon: -17.0,\n\tmaxLon: -2.0,\n\trefTime: '2019-12-15T00:00:00Z',\n\tstrideLon: 10,\n\tstrideLat: 10,\n\tminVelocity: 0,\n\tmaxVelocity: 1}).addTo('map');\n```\n\n## Function Inputs\n\nOnce you have added the [erddapToLeafletVelocity.js](https://github.com/IrishMarineInstitute/erddap-leaflet-velocity-demo/blob/main/erddapToLeafletVelocity.js) script to your webpages, you can use the erddapToLeafletVelocity function or the L.erddapVelocityLayer function with the following inputs:\n\n- _erddapBaseUrl_ `String`: The base URL of the Errdap server to be called - e.g. 'http://erddap.marine.ie'\n- _datasetID_ `String`: The dataset name to be accessed from the Erddap server - 'e.g. IMI_Model_Stats'\n- _uParameter_ `String`: The eastwards velocity parameter name to be accessed from the Erddap dataset - e.g. 'sea_surface_x_velocity'\n- _vParameter_  `String`: The northwards velocity parameter name to be accessed from the Erddap dataset - e.g. 'sea_surface_x_velocity'\n- _minLat_ `Number`: The southernmost latitude to be accessed from the Erddap dataset\n- _maxLat_ `Number`: The northernmost latitude to be accessed from the Erddap dataset\n- _minLon_ `Number`: The westernmost longitude to be accessed from the Erddap dataset\n- _maxLon_ `Number`: The easternmost longitude to be accessed from the Erddap dataset\n- _refTime_ `String`: The dateTime for the data to be returned from Erddap\n- _strideLon_ `Number`: The Erddap stride to use on the grid for the longitude axis as an integer value\n- _strideLat_ `Number`: The Erddap stride to use on the grid for the latitude axis as an integer value\n- _minVelocity_ `Number`: The minimum velocity to use on the Leaflet Velocity display\n- _maxVelocity_ `Number`: The maximum velocity to use on the Leaflet Velocity display\n- _mapID_ `String`: The Leaflet map ID to add the velocity layer to\n\n## Grib2JSON output format\n\nLeaflet-Velocity expects input in the form of a JSON object conforming to the output of [Grib2JSON](https://github.com/cambecc/grib2json). Some of the fields are documented below:\n\n- Header\n    - _dx_ The grid spacing on the x-axis in degrees\n    - _dy_ The grid spacing on the y-axis in degrees\n    - _la1_ The northernmost latitutde of the grid\n    - _la2_ The southernmost latittude of the grid\n    - _lo1_ The westernmost longitude of the grid\n    - _lo2_ The easternmost longitude of the grid\n    - _nx_ The number of grid points on the x axis\n    - _ny_ The number of grid points on the y-axis\n    - _parameterCategory_ Use the integer 2\n    - _parameterNumber_ Use the integer 2 for the x-axis speed and the integer 3 for the y axis speed\n    - _parameterNumberName_ A plain text label for the parameter\n    - _parameterUnit_ The units of measure for the parameter\n    - _refTime_ The date and time for this parameter and grid combination\n- Data\n    - A one-dimensional array of the parameter's data values for this point in time on the grid. Data are west-to-east across the grid, repeating north-to-south down the grid\n\n## JavaScript boilerplate\n\nThe following should allow you to build the expected output from any Erddap instance, using a griddap enabled dataset. Contains input from [@abkfenris](https://github.com/abkfenris).\n\n```javascript\nconst erddapBaseUrl = 'https://erddap.marine.ie';\nconst datasetID = 'IMI_Model_Stats';\nconst uParameter = 'sea_surface_x_velocity';\nconst vParameter = 'sea_surface_y_velocity';\nconst minLat = 48.5;\nconst maxLat = 58.5;\nconst minLon = -17.0;\nconst maxLon = -2.0;\nconst refTime = '2019-12-15T00:00:00Z';\nconst strideLon = 20;\nconst strideLat = 20;\n\nfunction jsonp(url) {\n    return new Promise(function(resolve, reject) {\n        let script = document.createElement('script')\n        const name = \"_jsonp_\" + Math.round(100000 * Math.random());\n        //url formatting\n        if (url.match(/\\?/)) url += \"&.jsonp=\"+name\n        else url += \"?.jsonp=\"+name\n        script.src = url;\n\n        window[name] = function(data) {\n            resolve(data);\n            document.body.removeChild(script);\n            delete window[name];\n        }\n        document.body.appendChild(script);\n    });\n}\n\njsonp(erddapBaseUrl \n\t\t\t+ '/erddap/griddap/' \n\t\t\t+ datasetID \n\t\t\t+ '.json?' \n\t\t\t+ uParameter + '[(' + refTime + '):1:(' + refTime + ')][(' +  String(minLat) + '):' + String(strideLat) + ':(' + String(maxLat) + ')][(' + String(minLon) + '):'+ String(strideLon) +':(' + String(maxLon) + ')],' \n\t\t\t+ vParameter + '[(' + refTime + '):1:(' + refTime + ')][(' + String(minLat) + '):' + String(strideLat)+ ':(' + String(maxLat) + ')][(' + String(minLon) + '):' + String(strideLon) + ':(' + String(maxLon) + ')]')\n.then(\n\tdata => [{\n\t\t'header':{\n\t\t\t'la1': Math.max(...Array.from([...new Set(data.table.rows.map(x => x[1]))])),\n\t\t\t'la2': Math.min(...Array.from([...new Set(data.table.rows.map(x => x[1]))])),\n\t\t\t'lo1': Math.min(...Array.from([...new Set(data.table.rows.map(x => x[2]))])),\n\t\t\t'lo2': Math.max(...Array.from([...new Set(data.table.rows.map(x => x[2]))])),\n\t\t\t'dx': (Math.max(...Array.from([...new Set(data.table.rows.map(x => x[2]))])) - Math.min(...Array.from([...new Set(data.table.rows.map(x => x[2]))]))) / ([...new Set(data.table.rows.map(x => x[2]))].length - 1),\n\t\t\t'dy': (Math.max(...Array.from([...new Set(data.table.rows.map(x => x[1]))])) - Math.min(...Array.from([...new Set(data.table.rows.map(x => x[1]))]))) / ([...new Set(data.table.rows.map(x => x[1]))].length - 1),\n\t\t\t'nx': [...new Set(data.table.rows.map(x => x[2]))].length,\n\t\t\t'ny': [...new Set(data.table.rows.map(x => x[1]))].length,\n\t\t\t'parameterCategory': 2,\n\t\t\t'parameterNumber': 2,\n\t\t\t'parameterUnit': 'm.s-1',\n\t\t\t'parameterNumberName': data.table.columnNames[3],\n\t\t\t'refTime': refTime.replace('T', ' ').replace('Z','')\n\t\t}, \n\t\t'data': data.table.rows.sort((a,b) => b[1]-a[1]).map(x => x[3])\n\t},{\n\t\t'header':{\n\t\t\t'la1': Math.max(...Array.from([...new Set(data.table.rows.map(x => x[1]))])),\n\t\t\t'la2': Math.min(...Array.from([...new Set(data.table.rows.map(x => x[1]))])),\n\t\t\t'lo1': Math.min(...Array.from([...new Set(data.table.rows.map(x => x[2]))])),\n\t\t\t'lo2': Math.max(...Array.from([...new Set(data.table.rows.map(x => x[2]))])),\n\t\t\t'dx': (Math.max(...Array.from([...new Set(data.table.rows.map(x => x[2]))])) - Math.min(...Array.from([...new Set(data.table.rows.map(x => x[2]))]))) / ([...new Set(data.table.rows.map(x => x[2]))].length - 1),\n\t\t\t'dy': (Math.max(...Array.from([...new Set(data.table.rows.map(x => x[1]))])) - Math.min(...Array.from([...new Set(data.table.rows.map(x => x[1]))]))) / ([...new Set(data.table.rows.map(x => x[1]))].length - 1),\n\t\t\t'nx': [...new Set(data.table.rows.map(x => x[2]))].length,\n\t\t\t'ny': [...new Set(data.table.rows.map(x => x[1]))].length,\n\t\t\t'parameterCategory': 2,\n\t\t\t'parameterNumber': 3,\n\t\t\t'parameterUnit': 'm.s-1',\n\t\t\t'parameterNumberName': data.table.columnNames[4],\n\t\t\t'refTime': refTime.replace('T', ' ').replace('Z','')\n\t\t}, \n\t\t'data': data.table.rows.sort((a,b) => b[1]-a[1]).map(x => x[4])\n\t}]\n)\n```\n"
 },
 {
  "repo": "uwdb/istc_oceanography",
  "language": "Jupyter Notebook",
  "readme_contents": "# istc_oceanography\n\nPlaceholder for datasets, todo items for the ISTC hackathon and demo.\n\nAccess the TX-E1 cluster via `ssh USERNAME@txe1-login.mit.edu`.\nMake sure your user account is in the `istcdata` group.\n\nMain folder is `/home/gridsan/groups/istcdata/datasets/ocean_metagenome`\n\nWe need to find a link between the metadata and the genomics data that has been processed in the `overlapped_trimmed_data` folder.\n\n\n\n\n## README file on TX-E1\n\nSimons metagenomics samples\n\nDirectories and sample processing steps:\n\n1) `raw_data`: directory containing the raw Illumina NextSeq data from BioMicro. Each subfolder (150122Chi, etc) represents the BioMicro project name.\n2) `renamed_raw_data`: directory with symbolic links to the original raw file. Links are renamed in the format S0010_1_sequence.fastq etc, where \"S0010\" would be sample #10 from Maddie's master extraction spreadsheet. Done to simplify naming and make bulk preprocessing simple. Renaming is based on the `simons_mg_samplekey.txt` file, using the script `renamefiles.sh`\n3) `overlapped_trimmed_data`: Output of the \"metagenome_preprocessing.sh\" script. Steps are:\n   a: run `cutadapt_1.8.1` to remove any adapter sequences in the raw sequence data\n   b: run `clc_overlap_reads` to overlap any paired-end data (min 75 bp output)\n   c: run `clc_quality_trim` on nonoverlapping reads to remove low-quality reads/sections of reads (min 75 bp output)\n\n\n"
 },
 {
  "repo": "dankelley/dal-oce-thesis",
  "language": "TeX",
  "readme_contents": "## Overview\n\nThis directory contains materials that can help with preparing theses at\nDalhousie University, using latex. It is provided \"as is\" by a professor who is\ntrying to help his students. It is not an official product of the university,\nand its results are not claimed to meet the formatting requirements.  \n\nSee http://dankelley.github.io/dal-oce-thesis/index.html for documentation.\n\n## Installation\n\nIf you want to use the ocethesis package like any regular LaTeX package without\ncopying the provided style sheets into the directory of your document, you may\ninstall the package in a root path of your TeX distribution. The procedure\ndepends on the operating system, and so some steps listed below are divided\ninto Linux and MacOS (formerly OSX) variants; a procedure on Windows may be\nadded if a user explains it to the developers.\n\n### Step 1: set up directories\n\nOpen a terminal and type\n```\nkpsewhich -var-value=TEXMFHOME\n```\n\nto find the root path of your TeX distribution. On linux and unix-like systems,\nthis is typically `/home/<username>/texmf`, where `<username>` is the name of\nyour user account on your computer. On MacOS (or the previous OSX) this will be\n`/Users/<username>/Library/texmf`.  It is possible that this directory (and\nnecessary subdirectories) does not exist. In this case use the terminal and\ncreate the requisite subdirectories, with\n```\nmkdir -p ~/texmf/tex/latex\nmkdir -p ~/texmf/bibtex/bst\n```\non Linux or\n```\nmkdir -p ~/Library/texmf/tex/latex\nmkdir -p ~/Library/texmf/bibtex/bst\n```\non MacOS.\n\n\n### Step 2: clone the Dalhousie thesis repository\n\nAt this stage, you are set up for doing work with Latex, and you will not need\nto repeat these steps for any later updates to the Dalhousie thesis style sheet.\n\nThe next step is to clone the dal-oce-thesis repository. In the terminal, type\n```\ncd ~/texmf/tex/latex\ngit clone https://github.com/dankelley/dal-oce-thesis.git ocethesis\n```\non Linux, or \n```\ncd ~/Library/texmf/tex/latex\ngit clone https://github.com/dankelley/dal-oce-thesis.git ocethesis\n```\non MacOS.\n\nFinally, set up the bibliography style sheet, with\n```\ncd ~/texmf/bibtex/bst\nln -s ../../tex/latex/ocethesis/ocethesis.bst .\n```\non Linux or\n```\ncd ~/Library/texmf/bibtex/bst\nln -s ../../tex/latex/ocethesis/ocethesis.bst .\n```\non MacOS.\n\n\n### Step 3. testing the setup\n\nYou can test if your TeX distribution is able to find the class file and style\nsheets by typing in the terminal:\n```\nkpsewhich ocethesis.cls\nkpsewhich ocethesis.bst\n```\nand verifying that both commands report the full path to the respective files,\nas you've set them up.\n\n## Updating the installation\n\nIf `dal-oce-thesis` gets updated, you may update your installation by doing\n```\ncd ~/texmf/tex/latex/ocethesis\n```\non Linux or\n```\ncd ~/Library/texmf/tex/latex/ocethesis\n```\non MacOS, and then typing\n```\ngit pull\n```\n\n\n"
 },
 {
  "repo": "UCSD-E4E/Smartfin",
  "language": "Jupyter Notebook",
  "readme_contents": "# Smartfin [Visit Here!](https://smartfin.org)\n  \nSee our work-in-progress: [HERE](https://github.com/hwanggit/Smartfin/wiki)\n\nThe Smartfin Project aims to unite the surfing community and the research community in an effort to fill this gap in our oceanographic data. It began years ago with the collaborative vision of Scientists and Researchers working with the Scripps Institute of Oceanography, who hoped to innovate a new way to model the behaviour of our oceans. \n\n### Click the Image below to play the video\n\n[![image](https://github.com/hwanggit/Smartfin/blob/master/Images/Screen%20Shot%202019-06-13%20at%203.17.25%20PM.png)](https://youtu.be/wi0JMVOShCc)\n\n![text](https://github.com/hwanggit/Smartfin/blob/master/Images/longboard-smartfin-logo_web1920x1335.jpg)\n\n![text](https://github.com/hwanggit/Smartfin/blob/master/Images/Screen%20Shot%202019-05-08%20at%202.01.41%20PM.png)\n\n"
 },
 {
  "repo": "biofloat/biofloat",
  "language": "Jupyter Notebook",
  "readme_contents": "biofloat\n--------\n\n[![Build Status](https://travis-ci.org/biofloat/biofloat.svg?branch=master)](https://travis-ci.org/biofloat/biofloat)\n[![Coverage Status](https://coveralls.io/repos/biofloat/biofloat/badge.svg?branch=master&service=github)](https://coveralls.io/github/biofloat/biofloat?branch=master)\n[![Code Health](https://landscape.io/github/biofloat/biofloat/master/landscape.svg?style=flat)](https://landscape.io/github/biofloat/biofloat/master)\n[![PyPI version](https://badge.fury.io/py/biofloat.svg)](https://badge.fury.io/py/biofloat)\n[![DOI](https://zenodo.org/badge/21375/biofloat/biofloat.svg)](https://zenodo.org/badge/latestdoi/21375/biofloat/biofloat)\n\nPython module biofloat is designed to simplify using \n[Bio-Argo data](https://en.wikipedia.org/wiki/Argo_(oceanography)) \nin a Python/Jupyter Notebook programming environment.\n\n#### Installation\n\nFrom Anaconda, Canopy, or Unix prompt:\n\n    pip install biofloat\n\nFor plotting data on a map [basemap](http://matplotlib.org/basemap/users/installing.html) needs to be installed.\n\n#### Usage\n\nSee example [Jupyter Notebooks](notebooks) and [scripts](scripts) that demonstrate specific analyses and \nvisualizations.\n\n"
 },
 {
  "repo": "PyHOGS/pyhogs-code",
  "language": "OpenEdge ABL",
  "readme_contents": "# pyhogs-code\n\nRepository of data, code, and notebooks for the UW Python Hour for Oceanography and GeoSciences\n"
 },
 {
  "repo": "regeirk/atlantis",
  "language": "Python",
  "readme_contents": "Atlantis\n========\n\nPython library for atmospheric, oceanographic and hydrographic data management\nanalysis and visualization.\n\nAccording to Wikipedia:\n\n    Atlantis (in Greek, \u1f08\u03c4\u03bb\u03b1\u03bd\u03c4\u1f76\u03c2 \u03bd\u1fc6\u03c3\u03bf\u03c2, \"island of Atlas\") is a legendary \n    island first mentioned in Plato's dialogues Timaeus and Critias, written \n    about 360 BC. According to Plato, Atlantis was a naval power lying \"in \n    front of the Pillars of Hercules\" that conquered many parts of Western \n    Europe and Africa 9,000 years before the time of Solon, or approximately \n    9600 BC. After a failed attempt to invade Athens, Atlantis sank into the \n    ocean \"in a single day and night of misfortune\".\n\nThis softwares is intended to apply the \"don't repeat yourself\" (DRY) principle\nto atmosphere, ocean and hydrology scientists, and reduce time spent developing\ncode to analyse and make diagrams speak. It makes use of code from different\nsources and references are given to them where appropriate.\n\n\nDISCLAIMER\n----------\n\nThis software may be used, copied, or redistributed as long as it is not sold \nand this copyright notice is reproduced on each copy made. This routine is \nprovided as is without any express or implied warranties whatsoever.\n\n\nLICENSE\n-------\n\nThis software is licensed under a Creative Commons Attribution-ShareAlike 3.0\nUnported License. More information on this license can be obtained at\nhttp://creativecommons.org/licenses/by-sa/3.0/deed.en_US\n\n\nAUTHOR\n------\n\nThe library was created by Sebastian Krieger (solutions@nublia.com)\n\n\nINSTALLATION\n------------\n\n\nTO-DO\n-----\n.. Write book / manual: \"Data management and analysis in oceanography\"\n... leanpub.com\n.. Data browser:\n.... Log of information\n.. Cruise/expedition planner:\n... Tasks\n... Standard procedures per tasks\n... Map of stations\n... List of variables / instruments\n... Casts per station\n... Identification\n\n\nCONTRIBUTING\n------------\n\nDo you want to contribute by improving code performance or increasing \ncapabilities? Great! Your help is very welcome. Give a look at the source code\nand share your contributions by\n\n1. Fork it.\n2. Create a branch (git checkout -b my_atlantis)\n3. Commit your changes (git commit -am \"Added capabilities\")\n4. Push to the branch (git push origin my_atlantis)\n5. Open a Pull Request\n6. Enjoy a refreshing lemonade and wait\n\n\nREVISION\n--------\n\n. 3 (2017-02-22 13:37 -0300)\n. 2 (2016-06-28 21:49 -0300)\n. 1 (2014-06-10)\n. 0 (2013-05-20 20:32 -0300)\n"
 },
 {
  "repo": "ocefpaf/dynamical_oceanography",
  "language": "TeX",
  "readme_contents": "# Curso de Oceanografia F\u00edsica Din\u00e2mica lecionado na unimonte no primeiro semestre de 2014.\n\n## Reposit\u00f3rios com aulas, slides, notas e exerc\u00edcios para Oceanografia F\u00edsica Din\u00e2mica\n~ Filipe Fernandes ~\n\nPara compilar as aulas use o script `make_lecture.py`\n\n```python\nMake lecture slides, handout and homework.\n\nUsage:\n    make_lecture (DIR) [--compile=DOC]\n    make_lecture (-h | --help | --version)\n\nExamples:\n    make_lecture Aula_01 --compile=slides\n    make_lecture Aula_01 --compile=handout\n    make_lecture Aula_01 --compile=homework\n    make_lecture Aula_01 --compile=all\n\nArguments:\n  DIR      Lecture directory.\n\nOptions:\n  -c --compile=DOC   slides, handout, homework or all [default: slides]\n  --version   Show version.\n  ---help     Show this screen.\n```\n\n### 1. Plano de Aula\nDisciplina: Oceanografia Descritiva (Curso: Oceanografia / Ciclo/mod. 1/1B)\n\nProfessor: Filipe Fernandes\n\nTurno: Matutino\n\nSemestre: 2\u00ba -- 2013\n\nTurma: OCE1BM-VMA\n\n#### Ementa:\nIntrodu\u00e7\u00e3o \u00e0 Oceanografia F\u00edsica. Conceitos, estrutura e caracter\u00edsticas gerais\ndos oceanos.\n\n#### Bibliografia b\u00e1sica:\n- GARRISON, Tom; MIYAJI, C\u00edntia (Trad.). Fundamentos de oceanografia. S\u00e3o Paulo: Cengage Learning, 2009. 426 p. ISBN 9788522106776.\n- MIRANDA, L. B., CASTRO, B. M., KJERFVE, B. 2002. Princ\u00edpios de Oceanografia F\u00edsica de Estu\u00e1rios. 1\u00aa. ed. S\u00e3o Paulo : Edusp, 414p.\n- SVERDRUP, K. A., Duxbury A. B., Duxbury, A. C. 2006. Fundamentals of Oceanography. McGraw Hill, 5th edition. N\u00famero de Chamada: 551.46 S968f 2006.\n\n\n#### Bibliografia complementar:\n- STEWART, R. H. 2002. Introduction to Physical Oceanography. Department of Oceanography. Texas A. M. University.\n- SOUZA, Maria Cristina de Arruda. A Corrente do Brasil ao largo de Santos: medi\u00e7\u00f5es diretas. 2000. 169p. Disserta\u00e7\u00e3o (mestrado). Instituto Oceanogr\u00e1fico. USP. Dispon\u00edvel em: http://www.teses.usp.br/teses/disponiveis/21/21132/tde-10092003-094250/pt-br.php\n- The Open University. 1989. Ocean Circulation, Pergamon Press, 2nd edition.\n- TIPLER, Paul Allen.  F\u00edsica para cientistas e engenheiros.  4. ed. Rio de Janeiro:  LTC,  2006. v.1.\n- TOMCZAK, Matthias.  Physical oceanography.   Australia  The Flinders University of South Australia,  2002. 1 CD-ROM:  son., color  Dispon\u00edvel em: http://www.es.flinders.edu.au/~mattom/regoc/pdfversion.html.\n\n### Cronograma\n| Aula | Data          | Conte\u00fado                              | Lista/Prova         |\n|:----:|:--------------|:--------------------------------------| -------------------:|\n| 01   | 2013-08-09    | Introdu\u00e7\u00e3o \u00e0 Oceanografia F\u00edsica 1    |                     |\n| 02   | 2013-08-16    | Introdu\u00e7\u00e3o \u00e0 Oceanografia F\u00edsica 2    | Lista 1             |\n| 03   | 2013-08-23    | Dimens\u00f5es e formas dos oceanos 1      | Lista 2             |\n| 04   | 2013-08-30    | Dimens\u00f5es e formas dos oceanos 2      | Lista 3             |\n| 05   | 2013-09-06    | Propriedades f\u00edsicas da \u00e1gua do mar 1 | Lista 4             |\n| 06   | 2013-09-13    | Propriedades f\u00edsicas da \u00e1gua do mar 2 | Lista 5             |\n| 07   | 2013-09-20    | Distribui\u00e7\u00e3o espacial e temporais 1   | Lista 6             |\n| 08   | 2013-09-27    | Distribui\u00e7\u00e3o espacial e temporais 2   | Lista 7             |\n|      | 2013-10-04    |                                       | Prova 1             |\n| 11   | 2013-10-11    | Leis de conserva\u00e7\u00e3o 1                 | Revis\u00e3o Prova 1     |\n| 12   | 2013-10-18    | Leis de conserva\u00e7\u00e3o 2                 | Lista 8             |\n| 13   | 2013-10-25    | Circula\u00e7\u00e3o e massas d'\u00e1gua 1          | Lista 9             |\n| 14   | 2013-11-01    | Circula\u00e7\u00e3o e massas d'\u00e1gua 2          | Lista 10            |\n| 15   | 2013-11-08    | Oc. costeira e estuarina 1            | An\u00e1lise de dados    |\n|      | 2013-11-15    |                                       | Feriado             |\n| 16   | 2013-11-22    | Oc. costeira e estuarina 2            | Lista 11            |\n|      | 2013-11-29    |                                       | Semin\u00e1rio           |\n|      | 2013-12-06    |                                       | Prova 2             |\n|      | 2013-12-13    |                                       | Revis\u00e3o             |\n|      | 2013-12-20    |                                       | Prova Alternativa   |\n"
 },
 {
  "repo": "kkats/physical-oceanography",
  "language": "Haskell",
  "readme_contents": "# physical-oceanography\nTools for physical oceanographic data analysis.\n\n## oceanogr\n\nModules for IO (binary IO, netcdf), special data handling (SAM index, ETOPO5 topography, WGHC climatology),\nlinear data analysis (EOF, low-pass filters, leastsquare fit), power spectra (PSD),\nparallel matrix computation (RepaExpand), and others (Misc).\n\n## observation\n\nFor visualising CTD data from [GO-SHIP](http://www.go-ship.org)-style, full-depth observation.\n\n1. It is necessary to provide `CTDfileRead`, a set of header information for the CTD output (e.g. `example/SBEproc.hs` -- this is for output from Sea-Bird Scientific CTD process programs).\n2. Build a list of CTD station to plot using `readCTD` (e.g. `example/MakeList.hs`).\n3. Section data along the list can be output by `sectionCTD` (e.g. `example/PlotSection.hs`).\n\n## gamma-n\n\nCalculate an approximation of the [neutral density](http://www.teos-10.org/preteos10_software/neutral_density.html) surfaces, *gamma-n* from measured temperature, salinity, and pressure.\n\n## GSW\n\nIs now in a separate [repo](https://github.com/TEOS-10/GSW-Haskell.git) under [TEOS-10](https://github.com/TEOS-10).\n\n"
 },
 {
  "repo": "mvdh7/biogeochem-phi",
  "language": "MATLAB",
  "readme_contents": "# biogeochem-phi\n\n[![DOI](https://zenodo.org/badge/95231697.svg)](https://zenodo.org/badge/latestdoi/95231697)\n\n**Documentation:** [biogeochem-phi.readthedocs.io](https://biogeochem-phi.readthedocs.io/en/latest/)\n\nThese scripts should be cited as:\n\nHumphreys MP, Daniels CJ, Wolf-Gladrow DA, Tyrrell T, & Achterberg EP (2017): \"On the influence of marine biogeochemical processes over CO<sub>2</sub> exchange between the atmosphere and ocean\", *Marine Chemistry* 199, 1-11, <a href=\"https://doi.org/10.1016/j.marchem.2017.12.006\">doi:10.1016/j.marchem.2017.12.006</a>.\n\n## bgc_sliso()\n  * Calculates isocapnic quotient (Q) from *p*CO<sub>2</sub>, DIC, temperature and salinity\n  * Requires functions from https://github.com/mvdh7/oceancarb-constants\n\n## bgc_phi()\n  * Calculates Phi from Q (output from `bgc_sliso()`) and biogeochemical process vector\n\n## bgc_normalise()\n  * Optionally normalises a biogeochemical process vector to unit length\n"
 },
 {
  "repo": "redouanelg/qgsw-DI",
  "language": "Jupyter Notebook",
  "readme_contents": "***** for QGNET the PyTorch version of the code https://arxiv.org/abs/1911.08856 please check the QGNET folder ******\n\n# qgsw-DI\nQuasigeostrophic shallow-water model for Dynamic Interpolation of SSH\n\nThis is the supplementary material of the publication \"Dynamic Interpolation of Sea Surface Height and Potential Applications for Future High-Resolution Altimetry Mapping\", from C. Ubelmann et al., [pdf](https://journals.ametsoc.org/doi/abs/10.1175/JTECH-D-14-00152.1), please cite this article if you use the code in your works.\n\nThe backbone of the codes was written by Cl\u00e9ment Ubelmann (ubelmann@cls.fr). I commented and prepared the codes for sharing, I also added a Jupyter notebook for illustration.\n\n# Example\n\nPlease check the folder \"notebooks\" for more details\n\n![](example_qgSSH.png)\n"
 },
 {
  "repo": "seaflow-uw/seaflowpy",
  "language": "Python",
  "readme_contents": "# Seaflowpy\n\nA Python package for SeaFlow flow cytometer data.\n\n## Table of Contents\n\n1. [Install](#install)\n1. [Read EVT/OPP/VCT Files](#evtoppvct)\n1. [Command-line Interface](#cli)\n1. [Configuration](#configuration)\n1. [Integration with R](#rintegration)\n1. [Testing](#testing)\n1. [Development](#development)\n\n<a name=\"install\"></a>\n\n## Install\n\nThis package is compatible with Python 3.7 and 3.8.\n\n### Source\n\nThis will clone the repo and create a new virtual environment `seaflowpy`.\n`venv` can be replaced with `virtualenv`, `conda`, etc.\n\n```sh\ngit clone https://github.com/armbrustlab/seaflowpy\ncd seaflowpy\n[[ -d ~/venvs ]] || mkdir ~/venvs\npython3 -m venv ~/venvs/seaflowpy\nsource ~/venvs/seaflowpy/bin/activate\npip3 install -U pip setuptools wheel\npip3 install -r requirements-test.txt\npip3 install .\n# Confirm the seaflowpy command-line tool is accessible\nseaflowpy version\n# Make sure basic tests pass\npytest\n# Leave the new virtual environment\ndeactivate\n```\n\n### PyPI\n\n```sh\npip3 install seaflowpy\n```\n\n### Docker\n\nDocker images are available from Docker Hub at `ctberthiaume/seaflowpy`.\n\n```sh\ndocker pull ctberthiaume/seaflowpy\ndocker run -it ctberthiaume/seaflowpy seaflowpy version\n```\n\nThe Docker build file is in this repo at `/Dockerfile`. The build process for the Docker image is detailed in `/build.sh`.\n\n<a name=\"evtoppvct\"></a>\n\n## Read EVT/OPP/VCT Files\n\nAll file reading functions will return a `pandas.DataFrame` of particle data.\nGzipped EVT, OPP, or VCT files can be read if they end with a \".gz\" extension.\nFor these code examples assume `seaflowpy` has been imported as `sfp`\nand `pandas` has been imported as `pd`, e.g.\n\n```python\nimport pandas as pd\nimport seaflowpy as sfp\n```\n\nand `*_filepath` has been set to the correct data file.\n\nRead an EVT file\n\n```python\nevt = sfp.fileio.read_evt_labview(evt_filepath)\n```\n\nRead an OPP file as an Apache Arrow Parquet file, select the 50% quantile, and subset columns.\nVCT files created with `popcycle` are also standard Parquet files and can be read in a similar fashion.\n\n```python\nopp = pd.read_parquet(opp_filepath)\nopp50 = opp[opp[\"q50\"]]\nopp50 = opp50[['fsc_small', 'chl_small', 'pe']]\n```\n\n<a name=\"cli\"></a>\n\n## Command-line interface\n\nAll `seaflowpy` CLI tools are accessible from the `seaflowpy` executable.\nRun `seaflowpy --help` to begin exploring the CLI usage documentation.\n\n### SFL validation workflow\n\nSFL validation sub-commands are available under the `seaflowpy sfl` command.\nThe usage details for each command can be accessed as `seaflowpy sfl <cmd> -h`.\n\nThe basic worfkflow should be\n\n1) If starting with an SDS file, first convert to SFL with `seaflowpy sds2sfl`\n\n2) If the SFL file is output from `sds2sfl` or is a raw SeaFlow SFL file,\nconvert it to a normalized format with `seaflowpy sfl print`.\nThis command can be used to concatenate multiple SFL files,\ne.g. merge all SFL files in day-of-year directories.\n\n3) Check for potential errors or warnings with `seaflowpy sfl validate`.\n\n4) Fix errors and warnings. Duplicate file errors can be fixed with `seaflowpy sfl dedup`.\nBad lat/lon errors may be fixed with`seaflowpy sfl convert-gga`,\nassuming the bad coordinates are GGA to begin with.\nThis can be checked with with `seaflowpy sfl detect-gga`.\nOther errors or missing values may need to be fixed manually.\n\n5) (Optional) Update event rates based on true event counts and file duration\nwith `seaflowpy sfl fix-event-rate`.\nTrue event counts for raw EVT files can be determined with `seaflowpy evt count`.\nIf filtering has already been performed then event counts can be pulled from\nthe `all_count` column of the opp table in the SQLITE3 database.\ne.g. `sqlite3 -separator $'\\t' SCOPE_14.db 'SELECT file, all_count ORDER BY file'`\n\n6) (Optional) As a check for dataset completeness,\nthe list of files in an SFL file can be compared to the actual EVT files present\nwith `seaflowpy sfl manifest`. It's normal for a few files to differ,\nespecially near midnight. If a large number of files are missing it may be a\nsign that the data transfer was incomplete or the SFL file is missing some days.\n\n7) Once all errors or warnings have been fixed, do a final `seaflowpy validate`\nbefore adding the SFL file to the appropriate repository.\n\n\n<a name=\"configuration\"></a>\n\n## Configuration\n\nTo use `seaflowpy sfl manifest` AWS credentials need to be configured.\nThe easiest way to do this is to install the `awscli` Python package\nand go through configuration.\n\n```sh\npip3 install awscli\naws configure\n```\n\nThis will store AWS configuration in `~/.aws` which `seaflowpy` will use to\naccess Seaflow data in S3 storage.\n\n<a name=\"rintegration\"></a>\n\n## Integration with R\n\nTo call `seaflowpy` from R, update the PATH environment variable in\n`~/.Renviron`. For example:\n\n```sh\nPATH=${PATH}:${HOME}/venvs/seaflowpy/bin\n```\n\n<a name=\"testing\"></a>\n\n## Testing\n\nSeaflowpy uses `pytest` for testing. Tests can be run from this directory as\n`pytest` to test the installed version of the package, or run `tox` to install\nthe source into a temporary virtual environment for testing.\n\n<a name=\"development\"></a>\n\n## Development\n\n### Source code structure\n\nThis project follows the [Git feature branch workflow](https://www.atlassian.com/git/tutorials/comparing-workflows/feature-branch-workflow).\nActive development happens on the `develop` branch and on feature branches which are eventually merged into `develop`.\n\n### Build\n\nTo build source tarball, wheel, and Docker image, run `./build.sh`. This will\n\n* create `seaflowpy-dist` with source tarball and wheel file (created during Docker build)\n\n* Docker image named `seaflowpy:<version>`\n\nTo remove all build files, run `rm -rf ./seaflowpy-dist`.\n\n### Updating requirements files\n\nCreate a new virtual environment\n\n```sh\npython3 -m venv newenv\nsource newenv/bin/activate\n```\n\nUpdate pip, wheel, setuptools\n\n```sh\npip3 install -U pip wheel setuptools\n```\n\nAnd install `seaflowpy`\n\n```sh\npip3 install .\n```\n\nThen freeze the requirements\n\n```sh\npip3 freeze | grep -v seaflowpy >requirements.txt\n```\n\nThen install test dependencies, test, and freeze\n\n```sh\npip3 install pytest pytest-benchmark\npytest\npip3 freeze | grep -v seaflowpy >requirements-test.txt\n```\n\nThen install dev dependencies, test, and freeze\n\n```sh\npip3 install pylint twine\npytest\npip3 freeze | grep -v seaflowpy >requirements-dev.txt\n```\n\nLeave the virtual environment\n\n```sh\ndeactivate\n```\n"
 },
 {
  "repo": "chouj/POCalendar",
  "language": null,
  "readme_contents": "# POCalendar\nA (Physical) Oceanography Calendar based on <strike>Airtable</strike> [Notion](https://notion.so).\n\n## Address: [Here](https://www.notion.so/pocalendar/Calendar-for-Physical-Oceanographer-814d0e162904439a932fac0b4ee9b87a)\n\n### Why I create and maintain a public calendar while everyone has his/her own ?\n\n- Event seeking of course. Especially, deadlines are marked on the calendar.\n- might be useful to community. Just like another project of mine: [POPapers](https://github.com/chouj/popapers)\n- A test whether community is willing to collaborate on such kind of project. \n- convenient to revisit / recall all the events in the past.\n\n### Who might be interested ?\n\n- Researchers in the fields of (Physical) Oceanography and Climate Change.\n- Media / Press chasing scientific achievements and academic events.\n- Those who cares about or study history of ocean sciences.\n\n### Special Features\n\n- Associated deadlines are marked, not only events.\n- - There are \"Event list\" in which you can sort according to certain `Property` and the \"Calendar view\" is absolutely instinctive and convenient.\n- Events with livestream capability are labeled with different color.\n\n### Content Sources\n\n- [\u6d77\u6d0b\u77e5\u5708](https://mp.sohu.com/profile?xpt=c29odW1wOGh2a2NnQHNvaHUuY29t&_f=index_pagemp_1&spm=smpc.content.author.2.15680372448367pEIA27)\n- [Conferences and Meetings on Oceanography](https://www.conference-service.com/conferences/oceanography.html)\n- [https://usclivar.org/us-clivar-newsgrams](https://usclivar.org/us-clivar-newsgrams)\n- [https://www.solas-int.org/activities/conferences.html](https://www.solas-int.org/activities/conferences.html)\n- [https://www.egu.eu/meetings/calendar/](https://www.egu.eu/meetings/calendar/)\n- [https://www.jcomm.info/index.php?option=com_oe&task=eventCalendar&start=2021-03-01&end=2021-06-30&headGroupID=78](https://www.jcomm.info/index.php?option=com_oe&task=eventCalendar&start=2021-03-01&end=2021-06-30&headGroupID=78)\n- [GEOMAR events](https://events.geomar.de/)\n- [North Pacific Marine Science Organization meetings](https://meetings.pices.int/meetings)\n\n### Notice\n\n- Time zone issue exists. Please pay attention to. Any suggestions about this are appreciated.\n- Any adjustment about event time, for example, deadline getting extended, would not be modified/revised accordingly, if I might not be aware of it.\n\n### Seeking collaborators and financial support.\n\nTeamwork and cooperation will make its management a lot easier. However, as defined by Notion, more collaborators, more charges. Thus, please use my [referral link](https://www.notion.so/?r=d94e7c7a384f486185f8296abe5ea15b) to register (You will get $10 credit while I will get $5).\n\n### \u4e2d\u6587\u7b80\u4ecb (Introduction in chinese)\n\n[(\u7269\u7406)\u6d77\u6d0b\u9886\u57df\u65e5\u5386](https://xuchi.name/calendar-for-oceanographer/)\n\n### Changelog\n\n- Moved from [Airtable](https://airtable.com/shrc9MSotEPF7zYAv/tbl4cC2OM65kinAka/viwxx55XjBsRGEeIj?blocks=hide&date=2020-04-17&mode=month) to Notion on Mar 11, 2020. [Airtable](https://airtable.com/shrc9MSotEPF7zYAv/tbl4cC2OM65kinAka/viwxx55XjBsRGEeIj?blocks=hide&date=2020-04-17&mode=month) is such an elegante tool but simply costs too much for full features.\n\n#### Buy me a cup of coffee\n\n[![Donate](https://img.shields.io/badge/Donate-PayPal-green.svg)](https://www.paypal.me/Mesoscale)\n[![Donate](https://img.shields.io/badge/Donate-WeChat-brightgreen.svg)](https://github.com/chouj/donate-page/blob/master/simple/images/WeChatQR.jpg?raw=true)\n[![Donate](https://img.shields.io/badge/Donate-AliPay-blue.svg)](https://github.com/chouj/donate-page/blob/master/simple/images/AlipayQR.jpg?raw=true)\n"
 },
 {
  "repo": "EPauthenet/fda.oceM",
  "language": "MATLAB",
  "readme_contents": "# fda.oceM\nFunctional Data Analysis of oceanographic profiles\n\n**Functional Data Analysis** is a set of tools to study curves or functions. Here we see vertical hydrographic profiles of several variables (temperature, salinity, oxygen,...) as curves and apply a functional principal component analysis (FPCA) in the multivaraite case to reduce the dimensionality of the system. The classical case is done with couples of temperature and salinity. It can be used for front detection, water mass identification, unsupervised or supervised classification, model comparison, data calibration ...\nThis repository is also available in an R package [fda.oce](https://github.com/EPauthenet/fda.oce) with a doi [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.4073123.svg)](https://doi.org/10.5281/zenodo.4073123).\n\n*Dependencies*:\nThe method uses the fdaM Toolbox by Jim Ramsay.\nhttp://www.psych.mcgill.ca/misc/fda/downloads/FDAfuns/Matlab/\nYou will need to install this toolbox and add it to the matlab path to use this software\n\n*References*: \n- Pauthenet et al. (2019) The thermohaline modes of the global ocean. Journal of Physical Oceanography, [10.1175/JPO-D-19-0120.1](https://doi.org/10.1175/JPO-D-19-0120.1)\n- Pauthenet et al. (2018) Seasonal meandering of the Polar Front upstream of the Kerguelen Plateau. Geophysical Research Letters, [10.1029/2018GL079614](https://doi.org/10.1029/2018GL079614)\n- Pauthenet et al. (2017) A linear decomposition of the Southern Ocean thermohaline structure. Journal of Physical Oceanography, [10.1175/JPO-D-16-0083.1](http://dx.doi.org/10.1175/JPO-D-16-0083.1)\n- Ramsay, J. O., and B. W. Silverman, 2005: Functional Data Analysis. 2nd Edition Springer, 426 pp., Isbn : 038740080X.\n\n\n# Demo\nHere is an example of how to use these functions. We compute the modes for temperature and salinity profiles of the reanalysis [GLORYS](http://marine.copernicus.eu/services-portfolio/access-to-products/) in the Southern Ocean for December of 2015.\n\nFirst we load the data and fit the Bsplines on the 1691 profiles of the example. By default the function fit 20 Bsplines. It returns a fd object named 'fdobj' :\n``` Matlab\nload GLORYS_SO_2015-12.mat\nfdobj = bspl(Pi,Xi);\n```\n\nThen we apply the FPCA on the fd object :\n``` Matlab\nfpca(fdobj);\n```\n\nThe profiles can be projected on the modes defined by the FPCA, to get the principal components (PCs) :\n``` Matlab\nproj(fdobj,pca);\n```\n\nVisualisation of the 2 first PCs :\n``` Matlab\npc_plot(pca,pc);\n```\n<img src=\"https://github.com/EPauthenet/fda.oceM/blob/master/figures/pc_plot.png\" alt=\"drawing\" width=\"1000px\"/>\n\nVisualisation of the 2 first eigenfunctions effect on the mean profile (red (+1) and blue (-1)) :\n``` Matlab\neigenf_plot(pca,1);\neigenf_plot(pca,2);\n```\n<img src=\"https://github.com/EPauthenet/fda.oceM/blob/master/figures/eigenf1.png\" alt=\"drawing\" width=\"370px\"/> <img src=\"https://github.com/EPauthenet/fda.oceM/blob/master/figures/eigenf2.png\" alt=\"drawing\" width=\"370px\"/>\n\nThe profiles can then be reconstructed with less PCs than the total number, removing the small variability. For example with only 5 modes :\n``` Matlab\nte = 5;\nfdobj_reco = reco(pca,pc,te);\n```\n\nTo transform fd objects back in a the variable space, we use the function eval.fd (\"fda\" package) :\n``` Matlab\nX = eval_fd(Pi,fdobj);\nX_reco = eval_fd(Pi,fdobj_reco);\n```\n\nAnd finally we can represent the profiles reconstructed compared to the original data :\n``` Matlab\ni = 600  %index of a profile\nfigure(1),clf\nfor k = 1:pca.ndim    %Loop for each variable                        \n  subplot(1,2,k)\n  plot(Xi(:,i,k),-Pi,'ko')              %Plot of the raw data\n  xlim([min([Xi(:,i,k);X_reco(:,i,k)]) max([Xi(:,i,k);X_reco(:,i,k)])])\n  ylim([-max(Pi) 0]);\n  xlabel(char([pca.fdnames{3}{k}]))\n  ylabel(pca.fdnames(1))\n  hold on\n  plot(X(:,i,k),-Pi,'r')           %Plot of the B-spline fit\n  plot(X_reco(:,i,k),-Pi,'g')     %Plot of the reconstructed profiles\nend\nlegend('raw','spline','reconstructed')\n```\n<img src=\"https://github.com/EPauthenet/fda.oceM/blob/master/figures/reco_prof600.png\" alt=\"drawing\" width=\"1000px\"/>\n\n"
 },
 {
  "repo": "KingSeaStar/Oceanography-Underwater-Glider-Survey-Amelia-2016-Wilimington-Canyon",
  "language": "MATLAB",
  "readme_contents": "# Oceanography\n\nScripts, tools, data sources for analyzing and visualizing ocean data from:\n\n- Slocum underwater gliders. (underwater temperature, conductivity, pressure, dissolved oxygen, optical properties etc.)\n- Marine buoys (winds)\n- Moorings and vertical profilers\n- Satellites (sea surface temperature, sea surface height)\n- Ocean models (most oceanographic parameters)\n- Atmospheric models (winds)\n\nThis repository contains the MATLAB scripts I have written in my graduate school research (M.S. & Ph.D.).\nI plan to translate some of the scripts from MATLAB to Phython.\nI have learned from or directly used the scripts written by others in the oceanography community, meteology community, earth science community, and on the internet. \nBugs might exist in some of the scripts.\n"
 },
 {
  "repo": "max-simon/master-thesis",
  "language": "Jupyter Notebook",
  "readme_contents": "# On the Impact of Submesoscale Fronts on Mesoscale Eddies and Biological Productivity in the California Current System - Masterthesis in Physics\n\n[![CC BY 4.0][cc-by-shield]][cc-by]\n\n**Title**: On the Impact of Submesoscale Fronts on Mesoscale Eddies and Biological Productivity in the California Current System\n\n**Author**: Max Simon\n\n**Institute**: _Department of Environmental Systems Science_, ETH Z\u00fcrich and _Department of Physics and Astronomy_, University of Heidelberg\n\n**Submission date**: 18.12.2020\n\n**Supervisors:** [Prof. Dr. Norbert Frank (University of Heidelberg)](https://www.iup.uni-heidelberg.de/de/institut/mitarbeiter/prof-norbert-frank), [Prof. Dr. Nicolas Gruber (ETH Z\u00fcrich)](https://usys.ethz.ch/personen/profil.nicolas-gruber.html) and [Dr. Matthias M\u00fcnnich (ETH Z\u00fcrich)](https://up.ethz.ch/people/person-detail.NDY0NDk=.TGlzdC8xMDg5LC0zMDYxNTA1MjU=.html)\n\n**Abstract**: Submesoscale motions are often not resolved in numerical models, although recent studies suggest that they interact with mesoscale processes. This might be particularly relevant for regions like the California Current System (CCS) where mesoscale processes redistribute nutrients and organic matter to offshore regions. In this study, the impact of submesoscale fronts on mesoscale eddies and biological productivity is examined by comparing two models of the CCS with different horizontal resolutions: a conventional (7.0 km) and a front-permitting resolution (2.8 km). A novel detection algorithm was developed which allows quantifying the area covered by submesoscale fronts. The algorithm reveals that fronts occur more often in anticyclones than in cyclones. This results in a weakening of the density anomaly associated with anticyclones by 40% during winter for the increased resolution. Further, the energy cascade of mesoscale eddies is better resolved contributing to the seasonal evolution of eddy kinetic energy. Finally, the biological productive band at the coast broadens, presumably driven by enhanced lateral transport of nutrients. The results demonstrate that submesoscale and mesoscale motions are inextricably linked and that regional numerical models should aim to resolve submesoscale fronts for future studies.\n\n**Full text**: [Digital Version](thesis/print/document_signed.pdf)\n\n\n**License**: This work is licensed under a\n[Creative Commons Attribution 4.0 International License][cc-by].\n\n[![CC BY 4.0][cc-by-image]][cc-by]\n\n[cc-by]: http://creativecommons.org/licenses/by/4.0/\n[cc-by-image]: https://i.creativecommons.org/l/by/4.0/88x31.png\n[cc-by-shield]: https://img.shields.io/badge/License-CC%20BY%204.0-lightgrey.svg"
 },
 {
  "repo": "SalishSeaCast/SalishSeaNowcast",
  "language": "Python",
  "readme_contents": "****************\nSalishSeaNowcast\n****************\n\n:License: Apache License, Version 2.0\n\n.. image:: https://img.shields.io/badge/license-Apache%202-cb2533.svg\n    :target: https://www.apache.org/licenses/LICENSE-2.0\n    :alt: Licensed under the Apache License, Version 2.0\n.. image:: https://img.shields.io/badge/python-3.10-blue.svg\n    :target: https://docs.python.org/3.10/\n    :alt: Python Version\n.. image:: https://img.shields.io/badge/version%20control-git-blue.svg?logo=github\n    :target: https://github.com/SalishSeaCast/SalishSeaNowcast\n    :alt: Git on GitHub\n.. image:: https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&logoColor=white\n   :target: https://github.com/pre-commit/pre-commit\n   :alt: pre-commit\n.. image:: https://img.shields.io/badge/code%20style-black-000000.svg\n    :target: https://black.readthedocs.io/en/stable/\n    :alt: The uncompromising Python code formatter\n.. image:: https://readthedocs.org/projects/salishsea-nowcast/badge/?version=latest\n    :target: https://salishsea-nowcast.readthedocs.io/en/latest/\n    :alt: Documentation Status\n.. image:: https://github.com/SalishSeaCast/SalishSeaNowcast/workflows/sphinx-linkcheck/badge.svg\n      :target: https://github.com/SalishSeaCast/SalishSeaNowcast/actions?query=workflow:sphinx-linkcheck\n      :alt: Sphinx linkcheck\n.. image:: https://github.com/SalishSeaCast/SalishSeaNowcast/workflows/CI/badge.svg\n    :target: https://github.com/SalishSeaCast/SalishSeaNowcast/actions?query=workflow%3ACI\n    :alt: pytest and test coverage analysis\n.. image:: https://codecov.io/gh/SalishSeaCast/SalishSeaNowcast/branch/master/graph/badge.svg\n    :target: https://codecov.io/gh/SalishSeaCast/SalishSeaNowcast\n    :alt: Codecov Testing Coverage Report\n.. image:: https://github.com/SalishSeaCast/SalishSeaNowcast/actions/workflows/codeql-analysis.yaml/badge.svg\n      :target: https://github.com/SalishSeaCast/SalishSeaNowcast/actions?query=workflow:CodeQL\n      :alt: CodeQL analysis\n.. image:: https://img.shields.io/github/issues/SalishSeaCast/SalishSeaNowcast?logo=github\n    :target: https://github.com/SalishSeaCast/SalishSeaNowcast/issues\n    :alt: Issue Tracker\n\nThe ``SalishSeaNowcast`` package is a collection of Python modules associated with running\nthe SalishSeaCast ocean models system of daily nowcasts and forecasts.\nThe runs use as-recent-as-available\n(typically previous day)\nforcing data for the western boundary sea surface height and the Fraser River flow,\nand atmospheric forcing from the four-times-daily produced forecast results from the\nEnvironment and Climate Change Canada High Resolution Deterministic Prediction System\n(HRDPS) operational GEM 2.5km resolution model.\n\nThe model runs are automated using an asynchronous,\nmessage-based architecture that:\n\n* obtains the forcing datasets from web services\n* pre-processes the forcing datasets into the formats expected by NEMO and the other models\n  in the automation system\n* uploads the forcing dataset files to the HPC or cloud-computing facility where the runs\n  will be executed\n* executes the run\n* downloads the results\n* prepares a collection of plots from the run results for monitoring purposes\n* publishes the plots and the processing log to the web\n\nDocumentation for the package is in the ``docs/`` directory and is rendered at https://salishsea-nowcast.readthedocs.io/en/latest/.\n\n.. image:: https://readthedocs.org/projects/salishsea-nowcast/badge/?version=latest\n    :target: https://salishsea-nowcast.readthedocs.io/en/latest/\n    :alt: Documentation Status\n\n\nLicense\n=======\n\n.. image:: https://img.shields.io/badge/license-Apache%202-cb2533.svg\n    :target: https://www.apache.org/licenses/LICENSE-2.0\n    :alt: Licensed under the Apache License, Version 2.0\n\nThe SalishSeaCast ocean model automation system code and documentation are copyright 2013 \u2013 present\nby the `SalishSeaCast Project Contributors`_ and The University of British Columbia.\n\n.. _SalishSeaCast Project Contributors: https://github.com/SalishSeaCast/docs/blob/master/CONTRIBUTORS.rst\n\nThey are licensed under the Apache License, Version 2.0.\nhttp://www.apache.org/licenses/LICENSE-2.0\nPlease see the LICENSE file for details of the license.\n"
 },
 {
  "repo": "CoherentStructures/OceanTools.jl",
  "language": "Julia",
  "readme_contents": "# OceanTools.jl\n\n[![build status][build-img]][build-url] [![coverage][codecov-img]][codecov-url]\n\nThis package was previously known as `CopernicusUtils.jl`.\nUtilities for working with oceanographic datasets from the [Copernicus](http://marine.copernicus.eu/)\nproduct family. This package can be used (amongst other things) to generate large test\ncases for use with [CoherentStructures.jl](https://github.com/CoherentStructures/CoherentStructures.jl).\n\n## Main features\n\nAbility to load velocity fields of arbitrary space-time cubes from Copernicus CMES data files.\n\nFast, allocation free interpolation on regular grids supporting periodic boundaries.\n\n## Documentation\n\n[![stable docs][docs-stable-img]][docs-stable-url] [![dev docs][docs-dev-img]][docs-dev-url]\n\n## Example Pictures\n\nFTLE field for a 90 day period off the coast of Japan.\n\n![FTLE field off the coast of Japan](https://raw.githubusercontent.com/CoherentStructures/OceanTools.jl/master/examples/ftle_plot.jpg)\n\nCalculating ``material barriers'' on freely choosable domain, more details in the\ncorresponding [documentation page](https://coherentstructures.github.io/OceanTools.jl/dev/example/):\n\n![material barriers](https://github.com/natschil/misc/raw/master/images/oceantools3.png)\n\n## Misc\n\nThis package is in no way affiliated with the Copernicus Marine Environment Monitoring Service.\n\n[build-img]: https://github.com/CoherentStructures/OceanTools.jl/workflows/CI/badge.svg?branch=master\n[build-url]: https://github.com/CoherentStructures/OceanTools.jl/actions?query=workflow%3ACI+branch%3Amaster\n\n[codecov-img]: http://codecov.io/github/CoherentStructures/OceanTools.jl/coverage.svg?branch=master\n[codecov-url]: http://codecov.io/github/CoherentStructures/OceanTools.jl?branch=master\n\n[docs-dev-img]: https://img.shields.io/badge/docs-dev-blue.svg\n[docs-dev-url]: http://coherentstructures.github.io/OceanTools.jl/dev\n\n[docs-stable-img]: https://img.shields.io/badge/docs-stable-blue.svg\n[docs-stable-url]: http://coherentstructures.github.io/OceanTools.jl/stable\n"
 },
 {
  "repo": "oceanhackweek/ohw19-projects-Trackpy",
  "language": "Jupyter Notebook",
  "readme_contents": "# Trackpy\n\n## Connecting animal tracking and mesoscale oceanographic data\n***\n\n#### Collaborators\n\n+ Hannah Blondin (hblondin at stanford dot edu)\n+ James Fahlbusch (musculus at stanford dot edu)\n+ Will Oestreich (woestreich at stanford dot edu)\n\n***\n\n### Background\n\nUnderstanding the underlying oceanographic drivers of the movements of pelagic marine organisms has been a major area of research interest for both primary and applied marine ecologists in recent years. Several recent studies leveraging animal tracking data and remotely-sensed oceanographic data have elucidated the importance of mesoscale oceanographic features, including eddies and filaments, to movements of highly-mobile pelagic organisms (e.g. Braun et al., 2019; Gaube et al., 2018; Abrahms et al., 2018). Open-source tools for linking such animal movements to oceanographic remote sensing data are of interest (see Dodge et al., 2013 for an R-based tool), yet simple tools for linking animal movements to databases of value-added mesoscale oceanographic features derived from remote sensing products are not readily available.\n\n#### Resources\n\n- https://www.aviso.altimetry.fr/fileadmin/documents/data/tools/hdbk_eddytrajectory_2.0exp.pdf\n\n### Goals\n\nProvide an illustrative example for Python beginners to link animal movement data with remotely sensed oceanographic data (SST) and databases of mesoscale oceanographic features and visualize these interactions.\n\n### Datasets\n1. Simulated loggerhead turtle (Caretta caretta) satellite tag tracks.\n\n2. AVISO+ Mesoscale Eddy Trajectory Atlas Product \n\n3. Sea surface temperature, blended, global, 2002-2014, EXPERIMENTAL 5-day composite\n \n### Workflow\n1. Upload animal track data\n\n2. Link animal movements in space and time to remotely-sensed SST\n\n3. Access AVISO+ mesoscale eddy data product\n\n4. Slice mesoscale eddy dataset for spatiotemporal domain of animal tracks\n\n5. Visualize animal-feature interactions in space and time\n\n\n### References\nAbrahms, B., Scales, K.L., Hazen, E.L., Bograd, S.J., Schick, R.S., Robinson, P.W. and Costa, D.P., 2018. Mesoscale activity facilitates energy gain in a top predator. Proceedings of the Royal Society B: Biological Sciences, 285(1885), p.20181101.\n\nBraun, C.D., Gaube, P., Sinclair-Taylor, T.H., Skomal, G.B. and Thorrold, S.R., 2019. Mesoscale eddies release pelagic sharks from thermal constraints to foraging in the ocean twilight zone. Proceedings of the National Academy of Sciences, p.201903067.\n\nChelton, D.B., Schlax, M.G. and Samelson, R.M., 2011. Global observations of nonlinear mesoscale eddies. Progress in Oceanography, 91(2), pp.167-216.\n\nDodge, S., Bohrer, G., Weinzierl, R., Davidson, S.C., Kays, R., Douglas, D., Cruz, S., Han, J., Brandes, D. and Wikelski, M., 2013. The environmental-data automated track annotation (Env-DATA) system: linking animal tracks with environmental data. Movement Ecology, 1(1), p.3.\n\nGaube, P., Braun, C.D., Lawson, G.L., McGillicuddy, D.J., Della Penna, A., Skomal, G.B., Fischer, C. and Thorrold, S.R., 2018. Mesoscale eddies influence the movements of mature female white sharks in the Gulf Stream and Sargasso Sea. Scientific reports, 8(1), p.7363.\n\n\n\n### Required Packages\nInstall:\nconda install package -c conda-forge\n\nPackages:\nfolium\nmatplotlib\nftplib\ngetpass\nnumpy\nnetCDF4\ndatetime\npandas\nos\nxarray\n\n\n"
 },
 {
  "repo": "albertcodes/albertcodes",
  "language": "Common Workflow Language",
  "readme_contents": "<p>\n  <img align=\"left\" alt=\"contact\" src=\"assets/images/welcome-rounded.png\" width=\"141\" height=\"125\" />\n  <img align=\"center\" alt=\"streak\" \n       src=\"https://github-readme-streak-stats.herokuapp.com/?user=albertcodes&hide_border=true&stroke=ffffff00&background=ffffff&ring=fe3f40&fire=fe3f40&currStreakLabel=fe3f40\" \n       width=\"390\" height=\"125\" />\n  <img align=\"center\" alt=\"gif\" src=\"assets/images/i-love-rounded.gif\" width=\"265\" height=\"100\" />\n</p>\n<img align=\"right\" alt=\"contact\" src=\"assets/images/contact-decoration.png\" />\n<br>\n<h3 align=\"left\">Toolbox:</h3>\n<p align=\"left\">\n  <a href=\"https://www.typescriptlang.org/\" target=\"_blank\">\n    <img src=\"https://raw.githubusercontent.com/devicons/devicon/master/icons/typescript/typescript-original.svg\"\n      alt=\"typescript\" width=\"40\" height=\"40\" />\n  </a>\n  <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript\" target=\"_blank\">\n    <img src=\"https://raw.githubusercontent.com/devicons/devicon/master/icons/javascript/javascript-original.svg\"\n      alt=\"javascript\" width=\"40\" height=\"40\" />\n  </a>\n  <a href=\"https://kotlinlang.org/\" target=\"_blank\">\n    <img src=\"https://raw.githubusercontent.com/devicons/devicon/master/icons/kotlin/kotlin-original.svg\" alt=\"kotlin\"\n      width=\"40\" height=\"40\" />\n  </a>\n  <a href=\"https://www.figma.com/\" target=\"_blank\">\n    <img src=\"https://raw.githubusercontent.com/devicons/devicon/master/icons/figma/figma-original.svg\" alt=\"figma\"\n      width=\"40\" height=\"40\" />\n  </a>\n  <a href=\"https://www.python.org/\" target=\"_blank\">\n    <img src=\"https://raw.githubusercontent.com/devicons/devicon/master/icons/python/python-original.svg\" alt=\"python\"\n      width=\"40\" height=\"40\" />\n  </a>\n  <a href=\"https://dart.dev\" target=\"_blank\">\n    <img src=\"https://www.vectorlogo.zone/logos/dartlang/dartlang-icon.svg\" alt=\"dart\" width=\"40\" height=\"40\" />\n  </a>\n  <a href=\"https://git-scm.com/\" target=\"_blank\">\n    <img src=\"https://www.vectorlogo.zone/logos/git-scm/git-scm-icon.svg\" alt=\"git\" width=\"40\" height=\"40\" />\n  </a>\n  <a href=\"https://firebase.google.com/\" target=\"_blank\"> \n    <img src=\"https://www.vectorlogo.zone/logos/firebase/firebase-icon.svg\" \n      alt=\"firebase\" width=\"40\" height=\"40\"/>\n  </a>\n  <a href=\"https://couchdb.apache.org/\" target=\"_blank\">\n    <img src=\"https://raw.githubusercontent.com/devicons/devicon/master/icons/couchdb/couchdb-original.svg\"\n      alt=\"couchdb\" width=\"40\" height=\"40\" />\n  </a>\n    <a href=\"https://www.meteor.com/\" target=\"_blank\">\n    <img src=\"https://raw.githubusercontent.com/devicons/devicon/master/icons/meteor/meteor-original.svg\" alt=\"meteor\" width=\"40\" height=\"40\" />\n  </a>\n  <a href=\"https://appwrite.io/\" target=\"_blank\">\n    <img src=\"https://raw.githubusercontent.com/devicons/devicon/master/icons/appwrite/appwrite-original.svg\"\n      alt=\"appwrite\" width=\"40\" height=\"40\" />\n  </a><br>\n  <a href=\"https://angular.io/\" target=\"_blank\">\n    <img src=\"https://raw.githubusercontent.com/devicons/devicon/master/icons/angularjs/angularjs-original.svg\"\n      alt=\"angularjs\" width=\"40\" height=\"40\" />\n  </a>\n  <a href=\"https://www.cplusplus.com/\" target=\"_blank\">\n    <img src=\"https://raw.githubusercontent.com/devicons/devicon/master/icons/cplusplus/cplusplus-original.svg\" alt=\"c++\"\n      width=\"40\" height=\"40\" />\n  </a>\n  <a href=\"https://www.w3.org/html/\" target=\"_blank\">\n    <img src=\"https://raw.githubusercontent.com/devicons/devicon/master/icons/html5/html5-original-wordmark.svg\"\n      alt=\"html5\" width=\"40\" height=\"40\" />\n  </a>\n  <a href=\"https://www.w3schools.com/css/\" target=\"_blank\">\n    <img src=\"https://raw.githubusercontent.com/devicons/devicon/master/icons/css3/css3-original-wordmark.svg\"\n      alt=\"css3\" width=\"40\" height=\"40\" />\n  </a>\n  <a href=\"https://flutter.dev/\" target=\"_blank\">\n    <img src=\"https://raw.githubusercontent.com/devicons/devicon/master/icons/flutter/flutter-original.svg\" alt=\"flutter\"\n      width=\"40\" height=\"40\" />\n  </a>\n  <a href=\"https://vuejs.org/\" target=\"_blank\">\n    <img src=\"https://raw.githubusercontent.com/devicons/devicon/master/icons/vuejs/vuejs-original.svg\" alt=\"vue\"\n      width=\"40\" height=\"40\" />\n  </a>\n  <a href=\"https://sass-lang.com\" target=\"_blank\">\n    <img src=\"https://raw.githubusercontent.com/devicons/devicon/master/icons/sass/sass-original.svg\" alt=\"sass\"\n      width=\"40\" height=\"40\" />\n  </a>\n  <a href=\"https://aws.amazon.com/\" target=\"_blank\">\n    <img\n      src=\"https://raw.githubusercontent.com/devicons/devicon/master/icons/amazonwebservices/amazonwebservices-original.svg\"\n      alt=\"aws\" width=\"40\" height=\"40\" />\n  </a>\n  <a href=\"https://www.mongodb.com/\" target=\"_blank\"> \n    <img src=\"https://raw.githubusercontent.com/devicons/devicon/master/icons/mongodb/mongodb-original-wordmark.svg\" \n      alt=\"mongodb\" width=\"40\" height=\"40\"/>\n  </a>\n  <a href=\"https://reactjs.org/\" target=\"_blank\">\n    <img src=\"https://raw.githubusercontent.com/devicons/devicon/master/icons/react/react-original-wordmark.svg\"\n      alt=\"react\" width=\"40\" height=\"40\" />\n  </a>\n  <a href=\"https://coffeescript.org/\" target=\"_blank\">\n    <img src=\"https://raw.githubusercontent.com/devicons/devicon/master/icons/coffeescript/coffeescript-original.svg\"\n      alt=\"coffeescript\" width=\"40\" height=\"40\" />\n  </a>\n</p><br>\n<h3 align=\"left\">Social:</h3>\n<p align=\"left\">\n  <a href=\"https://twitter.com/albertcodes_dev\" target=\"blank\">\n    <img src=\"https://raw.githubusercontent.com/devicons/devicon/master/icons/twitter/twitter-original.svg\"\n      alt=\"twitter\" width=\"40\" height=\"40\" />\n  </a>\n  <a href=\"https://albertcodes.dev\" target=\"blank\">\n    <img src=\"assets/images/world-vector.png\" alt=\"webpage\" width=\"40\" height=\"40\" />\n  </a>\n  <a href=\"https://dribbble.com/albertcodes\" target=\"blank\">\n    <img src=\"assets/images/dribbble.svg\" alt=\"dribble\" width=\"40\" height=\"40\" />\n  </a>\n</p><br>\n<p>\n  <img align=\"left\" height=\"180em\"\n    src=\"https://github-readme-stats.vercel.app/api?username=albertcodes&custom_title=GitHub Stats:&bg_color=ffffff&show_icons=true&title_color=fe3f40&icon_color=000000&hide_border=true&&count_private=true&include_all_commits=true\" />\n  <img align=\"center\" height=\"180em\"\n    src=\"https://github-readme-stats.vercel.app/api/top-langs/?username=albertcodes&custom_title=Codebase:&bg_color=ffffff&show_icons=true&title_color=fe3f40&hide_border=true&layout=compact&exclude_repo=sociee&langs_count=10&hide=Common Workflow Language\" width=\"350\" />\n</p>\n"
 },
 {
  "repo": "hvillalo/satin",
  "language": "R",
  "readme_contents": "# satin\r\n\r\n[![CRAN\\_Status\\_Badge](http://www.r-pkg.org/badges/version/satin)](http://cran.r-project.org/package=satin)\r\n[![](https://cranlogs.r-pkg.org/badges/satin)](https://cran.r-project.org/package=satin)\r\n\r\nVisualisation and Analysis of Ocean Data Derived from Satellites\r\n\r\nDescription: With 'satin' functions, visualisation, data extraction and further analysis like producing climatologies from several images, and anomalies of satellite derived ocean data can be easily done.  Reading functions can import a user defined geographical extent of data stored in netCDF files.  Currently supported ocean data sources include NASA's Oceancolor web page <https://oceancolor.gsfc.nasa.gov/>, sensors VIIRS-SNPP; MODIS-Terra; MODIS-Aqua; and SeaWiFS.  Available variables from this source includes chlorophyll concentration, sea surface temperature (SST), and several others.  Data sources specific for SST that can be imported too includes Pathfinder AVHRR <https://www.ncei.noaa.gov/products/avhrr-pathfinder-sst> and GHRSST <https://www.ghrsst.org/>.  In addition, ocean productivity data produced by Oregon State University <http://sites.science.oregonstate.edu/ocean.productivity/> can also be handled previous conversion from HDF4 to HDF5 format.  Many other ocean variables can be processed by importing netCDF data files from two European Union's Copernicus Marine Service databases <https://marine.copernicus.eu/>, namely Global Ocean Physical Reanalysis and Global Ocean Biogeochemistry Hindcast.\r\n"
 },
 {
  "repo": "iuryt/gaussian_bump",
  "language": "Jupyter Notebook",
  "readme_contents": "# Flow Over Gaussian Bump\n\nThis is the MITgcm simulation of a flow over a gaussian bump based on [exp2](https://github.com/MITgcm/MITgcm/tree/master/verification/exp2) from MITgcm documentation. \n\n\n<img src=\"https://github.com/iuryt/gaussian_bump/blob/main/notebooks/img/Init_BAT.png\" data-canonical-src=\"https://github.com/iuryt/gaussian_bump/blob/main/notebooks/img/Init_BAT.png\" width=\"600\" height=\"auto\" />\n\n## Context\n\nThis is part of a mentoring that I am giving to the BSME student Alan Andonian from prof. Amit Tandon's [laboratory](https://tandonlab.sites.umassd.edu/people/) (UMassD). I created this repository to organize the numerical experiments I am running for him to analyze. \n\nThe main idea is to familiarize him to basic Geophysical Fluid Dynamics problems and Python programming. And, of course, give him an opportunity to learn more about control version on GitHub. One of the first problems is the [Taylor-Proudman Theorem](https://en.wikipedia.org/wiki/Taylor%E2%80%93Proudman_theorem). This experiment is perfect to understand this theorem, as even being 400m above the top of the gaussian bump, the flow cannot \"jump it\", which results into a [Taylor column](https://en.wikipedia.org/wiki/Taylor_column) above the bump. \n\n<img src=\"https://github.com/iuryt/gaussian_bump/blob/main/notebooks/img/quiver_zeta_13m_N0.png\" data-canonical-src=\"https://github.com/iuryt/gaussian_bump/blob/main/notebooks/img/quiver_zeta_13m_N0.png\" width=\"600\" height=\"auto\" />\n\nThe same does not happen to an stratified fluid, in which thermal-wind balance allows vertical shear.\n\n\n<img src=\"https://github.com/iuryt/gaussian_bump/blob/main/notebooks/img/quiver_zeta_13m_NC.png\" data-canonical-src=\"https://github.com/iuryt/gaussian_bump/blob/main/notebooks/img/quiver_zeta_13m_NC.png\" width=\"600\" height=\"auto\" />\n\nI also believe that this repository can help other students that are beginning to setup and run their own numerical simulations using MITgcm. \n\n\n## How to run an experiment\n\n1. Follow the [Getting Started](https://mitgcm.readthedocs.io/en/latest/getting_started/getting_started.html) section on MITgcm documentation to set up the model;\n2. Clone this experiment to the `MITgcm/exp` folder (you can also download the repository and extract it to MITgcm folder.);\n3. Configure [`compile.sh`](https://github.com/iuryt/gaussian_bump/blob/main/compile.sh) for the designated `optfile`;\n4. Run [`compile.sh`](https://github.com/iuryt/gaussian_bump/blob/main/compile.sh). If everything is correctly configured, you will have the `mitgcmuv` executable inside `run_<experiment>` folders; \n5. Create a symbolic link for the files in `input` and `input_<experiment>` to `run_<experiment>` folder.\n6. Run the executable `mitgcmuv` inside `run_<experiment>` folder (You can base your submit file on [`submit.lsf`](https://github.com/iuryt/gaussian_bump/blob/main/input/submit.lsf)).\n\nThe current configuration on [`code/SIZE.h`](https://github.com/iuryt/gaussian_bump/blob/main/code/SIZE.h) works in parallel using 4 cores (see [Documentation](https://mitgcm.readthedocs.io/en/latest/) to learn how to set up for a different number of cores).\nIf the experiment is already configured you just have to run `mpirun -np 4 ./mitgcmuv` in `run_<experiment>` folder.\nIf you are using a supercomputer, check with support how to setup a batch script for your experiment.\n\n## How to generate the initial conditions\n\nIn `notebooks` there is a file called [`00-Init.ipynb`](https://github.com/iuryt/gaussian_bump/blob/main/notebooks/00-Init.ipynb) that creates the initial conditions\nThe data will be saved to `input` folder. You may have to change the grid spacing in `input_<experiment>/data` or number of points in [`code/SIZE.h`](https://github.com/iuryt/gaussian_bump/blob/main/code/SIZE.h) if you change the code in the notebooks.\n\nWe define the Barotropic velocity from the free surface using the geostrophic balance <img src=\"https://render.githubusercontent.com/render/math?math=u = -\\dfrac{g}{f}\\dfrac{d\\eta}{dy}\">:\n\n<img src=\"https://github.com/iuryt/gaussian_bump/blob/main/notebooks/img/Init_ETA.png\" data-canonical-src=\"https://github.com/iuryt/gaussian_bump/blob/main/notebooks/img/Init_BAT.png\" width=\"auto\" height=\"200\" />\n\nWe defined three options for the initial mass field: homogeneous (N0), constant stratification (NC) and with a thermocline defined by a `tanh` (NZ).\n\n<img src=\"https://github.com/iuryt/gaussian_bump/blob/main/notebooks/img/Init_Temp.png\" data-canonical-src=\"https://github.com/iuryt/gaussian_bump/blob/main/notebooks/img/Init_BAT.png\" width=\"auto\" height=\"300\" />\n\n\n## How to read the data from the output\n\nThe notebook [`notebooks/01-Analysis.ipynb`](https://github.com/iuryt/gaussian_bump/blob/main/notebooks/01-Analysis.ipynb) its a tutorial that explains how to read and plot the output from this experiment.\n\n\n\n"
 },
 {
  "repo": "ctroupin/AlborEx-Data-Python",
  "language": "Jupyter Notebook",
  "readme_contents": "This repository contains the python code in the form of [jupyter](https://jupyter.org/) notebooks to reproduce the figures of the paper:\n\n> Troupin, C.; Pascual, A.; Ruiz, S.; Olita, A.; Casas, B.; Margirier, F.; Poulain, P.-M.; Notarstefano, G.; Torner, M.; Fern\u00e1ndez, J. G.; R\u00fajula, M. Mu\u00f1oz, C.; Allen, J. T.; Mahadevan, A. & Tintor\u00e9, J. The AlborEX dataset: sampling of submesoscale features in the Alboran Sea Earth System Science Data Discussions, 2018, 1-21.\n\nand to provide examples of the data from the netCDF files can be read, processed and displayed.\n\n![](./figures/fig00.png)\n\n## Directories\n\n* __data__: contains data files used for the plots and not available through [OPeNDAP](https://www.opendap.org/) protocol.\n\n* __figures__: default directory where the figures are saved.\n\n* __leaflet__: contains the source (html) to generate a [Leaflet](https://leafletjs.com/) interactive map showing the deployments and the sea surface temperature in the area of study.\n\n* __python__: contains\n1. the main module [`alborexdata.py`](./python/alborexdata.py) that defines the different classes\n2. the jupyter notebooks and\n\n## Usage\n\n### Configuration\n\nThe file [`alborexpaths.py`](./python/alborexpaths.py) stores the OPEnDAP URLs of all the data files (so they don't have to be downloaded locally), the extension of the domain and the path to the figure directory.\n\n__Note:__ the URLs were hard-coded but can be obtained using the [SOCIB Data API](http://api.socib.es/home/) (see the [API Examples](https://github.com/socib/API_examples)).\n\nThe SST files are not available in this repository but are automatically download from [OceanColor](http://oceancolor.gsfc.nasa.gov/) web when needed.\n\n### Reading files\n\nEach platform (drifters, Argo floats, gliders, ...) has a class defined in [`alborexdata.py`](./python/alborexdata.py), and the corresponding class has a method to read the data from the netCDF file.\n\nFor example, for the CTD data:\n1. We create an object CTD:\n```python\nctd = alborexdata.CTD()\n```\n2. We read the data from the correspond files, whose URL comes from `alborexpaths`:\n```python\nctd.get_from_netcdf(alborexpaths.ctdfile)\n```\n\n### Plotting data\n\nThere are many different types of plots used here.     \nFor the maps, we used the [Basemap Toolkit](https://matplotlib.org/basemap/) but in the future we will switch to [Cartopy](https://scitools.org.uk/cartopy/docs/latest/), once all the Basemap's features have been implemented.\n\nWe also work with the [cmocean](https://matplotlib.org/cmocean/) module in order to obtain perceptually uniform colormaps.\n\n## How to cite?\n\nThis module:\n\n> Charles Troupin. (2019, January 12). ctroupin/AlborEx-Data-Python: V1.1.0 (Version V1.1.0). Zenodo. DOI: [10.5281/zenodo.2538348](http://doi.org/10.5281/zenodo.2538348)\n\nThe AlborEx paper in [Earth System Science Data](https://www.earth-system-science-data.net/) (the reference will be updated):\n\n> Troupin, C.; Pascual, A.; Ruiz, S.; Olita, A.; Casas, B.; Margirier, F.; Poulain, P.-M.; Notarstefano, G.; Torner, M.; Fern\u00e1ndez, J. G.; R\u00fajula, M. Mu\u00f1oz, C.; Allen, J. T.; Mahadevan, A. & Tintor\u00e9, J. The AlborEX dataset: sampling of submesoscale features in the Alboran Sea. *Earth System Science Data Discussions*, 2018, 1-21. DOI: [10.5194/essd-2018-104](https://doi.org/10.5194/essd-2018-104)\n\nOther related papers:\n\n> Pascual, A.; Ruiz, S.; Olita, A.; Troupin, C.; Claret, M.; Casas, B.; Mourre, B.; Poulain, P.-M.; Tovar-Sanchez, A.; Capet, A.; Mason, E.; Allen, John T..; Mahadevan, A. & Tintor\u00e9, J. A multiplatform experiment to unravel meso- and submesoscale processes in an intense front (AlborEx). *Frontiers in Marine Science*, 2017, **4**, 1-16. DOI: [10.3389/fmars.2017.00039](https://doi.org/10.3389/fmars.2017.00039)\n\n> Olita, A.; Capet, A.; Claret, M.; Mahadevan, A.; Poulain, P. M.; Ribotti, A.; Ruiz, S.; Tintor\u00e9, J.; Tovar-S\u00e1nchez, A. & Pascual, A. Frontal dynamics boost primary production in the summer stratified Mediterranean Sea. *Ocean Dynamics*, 2017, **67**, 767-782. DOI: [10.1007/s10236-017-1058-z](https://doi.org/10.1007/s10236-017-1058-z)\n\n\n## Acknowledgments\n\nThe 3 anonymous reviewers made an outstanding work with a lot of comments that helped us improve\nthe initial manuscript, and also motivated me to improve and update the present project.\n\nI am also thankful to the SOCIB Data Center for corrections and improvements made on the\ndata file. Their [Data API](http://api.socib.es/home/) is very cool and I really invite everybody to try and use it.  \n"
 },
 {
  "repo": "apaloczy/AntarcticaCircumpolarIntrusions",
  "language": "Jupyter Notebook",
  "readme_contents": "# AntarcticaCircumpolarIntrusions\n\n[![DOI](https://zenodo.org/badge/91201018.svg)](https://zenodo.org/badge/latestdoi/91201018)\n\nThis repository contains codes and processed datasets for a manuscript entitled [**\"Oceanic heat delivery to the Antarctic continental shelf: Large-scale, low-frequency variability\"** (2018)](https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2018JC014345), by A. Pal\u00f3czy, S. T. Gille and J. L. McClean (Journal of Geophysical Research \u2013 Oceans). This [Jupyter notebook](http://nbviewer.jupyter.org/github/apaloczy/AntarcticaCircumpolarIntrusions/blob/master/index.ipynb) provides an overview of the contents.\n\nThe directory **plot_figs/** contains the Python codes used to produce all figures in the main manuscript (Figures 1-10) and Figures S3-S10 and S1 in the Supplementary Materials. The codes depend on the data files in the **data_reproduce_figs/** directory. Some of these are too large to be included in this repository, but are available for download from the links listed on the accompanying README files. Please contact [Andr\u00e9 Pal\u00f3czy](mailto:apaloczy@ucsd.edu) if you have issues downloading the files.\n\n## Key Points\n* The contributions from eddy advection, eddy stirring and mean flow advection to the total onshore heat transport vary regionally.\n* The time-mean component governs the seasonal variability of the total heat transport and largely cancels the eddy component.\n* Circumpolar heat transports and total heat content of the Antarctic continental margin follow SAM, but ENSO prevails in West Antarctica.\n\n## Abstract\nOnshore penetration of oceanic water across the Antarctic continental slope (ACS) plays a major role in global sea level rise by delivering heat to the Antarctic marginal seas, thus contributing to the basal melting of ice shelves. Here, the time-mean ($\\Phi^\\text{mean}$) and eddy ($\\Phi^\\text{eddy}$) components of the heat transport ($\\Phi$) across the 1000 m isobath along the entire ACS are investigated using a 0.1\\textdegree global coupled ocean/sea ice simulation based on the Los Alamos Parallel Ocean Program (POP) and sea ice (CICE) models. Comparison with \\textit{in situ} hydrography shows that the model successfully represents the basic water mass structure, with a warm bias in the Circumpolar Deep Water layer. Segments of on-shelf $\\Phi$, with lengths of O(100-1000 km), are found along the ACS. The circumpolar integral of the annually-averaged $\\Phi$ is O(20 TW), with $\\Phi^\\text{eddy}$ always on-shelf, while $\\Phi^\\text{mean}$ fluctuates between on-shelf and off-shelf. Stirring along isoneutral surfaces is often the dominant process by which eddies transport heat across the ACS, but advection of heat by both mean flow-topography interactions and eddies can also be significant depending on the along- and across-slope location. The seasonal and interannual variability of the circumpolarly-integrated $\\Phi^\\text{mean}$ is controlled by convergence of Ekman transport within the ACS. Prominent warming features at the bottom of the continental shelf (consistent with observed temperature trends) are found both during high-SAM and high-Ni\u00f1o 3.4 periods, suggesting that climate modes can modulate the heat transfer from the Southern Ocean to the ACS across the entire Antarctic margin.\n\n## Plain Language Summary\nIn the Southern Ocean, warm Circumpolar Deep Water (CDW) is carried by the Antarctic Circumpolar Current (ACC), which flows in the open ocean usually far from cooler Antarctic coastal waters. When the ACC approaches the Antarctic coast, CDW can supply heat to the ice shelves. We use a realistic computer model of the global ocean and sea ice to study these processes. The model indicates that winds, ocean eddies and current interactions with the seafloor all contribute to moving heat toward Antarctica. Seasonal changes in the winds around Antarctica affect the heat transport near the surface and due to current-seafloor interactions, but not so much the heat transport due to eddies. Over multi-year periods, some of the changes in the heat transport are related to climate variability taking place in the tropics and around Antarctica. Both the processes responsible for bringing heat to the Antarctic coast and the variability of this heat delivery represent knowledge needed to improve computer model simulations of the melting of the Antarctic ice cap. In turn, improving these simulations is likely to reduce uncertainties in projections of sea level rise, allowing for the development of adaptation and mitigation policies that better address global societal impacts.\n\n## Authors\n* [Andr\u00e9 Pal\u00f3czy](http://scrippsscholars.ucsd.edu/apaloczy) (<apaloczy@ucsd.edu>)\n* [Sarah T. Gille](http://scrippsscholars.ucsd.edu/sgille)\n* [Julie L. McClean](http://scrippsscholars.ucsd.edu/jmcclean)\n\n## Funding\nThis work was funded by the U.S. Department of Energy (DOE, grant \\# DE\u2013SC0014440) in the scope of the \"Ocean and Sea Ice and their Interactions around Greenland and the West Antarctic Peninsula in Forced Fine\u2013Resolution Global Simulations\" project and high-performance computing support from Yellowstone (ark:/85065/d7wd3xhc) provided by National Center for Atmospheric Research (NCAR)'s Climate Simulation Laboratory (CSL), sponsored by the National Science Foundation (NSF) and other agencies. J.L. McClean was supported by the U.S. DOE Office of Science grant entitled \"Ultra-High Resolution Global Climate Simulation\" via a Los Alamos National Laboratory subcontract to carry out the POP/CICE simulation. The analyses of the model output performed in this study were enabled by computing resources provided by Oak Ridge Leadership Computing Facility (OLCF).\n"
 },
 {
  "repo": "apaloczy/ADCPtools",
  "language": "MATLAB",
  "readme_contents": "# ADCPtools\nBeam to Earth coordinate transformations and other utilities for Acoustic Doppler Current Profiler (ADCP) data.\n\n## Convert profiles of along-beam velocity to Earth coordinates (east-north-up).\nConvert raw along-beam velocity (positive toward transducer) time series (ntimes x nbins) to Earth-referenced velocity:\n```\n[u, v, w, w5] = janus5beam2earth(head, ptch, roll, theta, b1, b2, b3, b4, b5)\n```\n\nor, with all optional arguments specified,\n```\n[u, v, w, w5] = janus5beam2earth(head, ptch, roll, theta, b1, b2, b3, b4, b5, ...\n                                 'uvwBeam5', true, 'Gimbaled', true, 'Binmap', 'linear', r, r5)\n```\n\nWhere\n\n* ```b[1-5]``` are the along-beam velocity time series [ntimes x nbins] (positive toward transducer face).\n\n* ```u```, ```v```, ```w``` are the Earth-referenced (eastward, northward, upward) velocity time series [ntimes x nbins].\n\n* ```w5``` is the vertical velocity calculated from the vertical beam only.\n\n* ```head```, ```ptch``` and ```roll``` are the heading (rotation about the z-axis (beam 5), positive clockwise), pitch (rotation about the x-axis (beam 1), positive clockwise) and roll (rotation about the y-axis (beam 3), positive clockwise) angles measured by the ADCP, in degrees.\n\n* ```theta``` is the beam angle measured from beam 5's direction (vertical if the instrument is level), in degrees.\n\n* ```'uvwBeam5'``` (```true```/```false```) indicates whether to use the vertical beam data to calculate ```u```, ```v``` and ```w```.\n\n* ```'Gimbaled'``` (```true```/```false```) indicates whether the pitch and roll sensors were gimbaled (mounted on a free-swiveling platform as opposed to a rigid frame).\n\n* ```'Binmap'``` (```'none'```/```'nn'```/```'linear'```) indicates whether or not to perform bin-mapping on the raw beam velocities. Bin-mapping here means interpolating the velocities from all beams to the same horizontal plane before converting to instrument and then Earth coordinates. Defaults to ```'none'```, _i.e._, the assumption that pitch and roll are zero at all times. ```'linear'``` and ```'nn'``` specify linear and nearest-neighbor interpolation, respectively.\n\n* ```r``` and ```r5``` are vectors with the along-beam positions of the center of each bin for the Janus (\\#1-4) and vertical (\\#5) beams. Required only if ```'Binmap'``` is not ```none```.\n"
 },
 {
  "repo": "ksen0/diss",
  "language": "TeX",
  "readme_contents": "# FAQs\n\n*Q: Is there a peer-reviewed article based on this work?*\n\nA: Yes!\n\nK Kuksenok, C Aragon, J Fogarty, C P Lee, Gina Neff. [Deliberate Individual Change Framework for Understanding Programming Practices in four Oceanography Groups](https://ora.ox.ac.uk/objects/uuid:63c75827-6e45-4736-b35d-ac3b936eac4e/download_file?safe_filename=Neff%2Bet%2Bal%252C%2BDeliberate%2Bindividual%2Bchange%2Bframework%2Bfor%2Bunderstanding%2Bprogramming%2Bpractices%2Bin%2Bfour%2Boceanography%2Bgroups.pdf&file_format=application%2Fpdf&type_of_work=Journal+article). *Computer Supported Cooperative Work* (2017). doi:10.1007/s10606-017-9285-x.\n\n*Q: How about a short blog post?*\n\nA: [This one is a 6-minute read, with the key findings and figures](https://medium.com/hci-design-at-uw/code-work-in-science-how-it-changes-and-why-it-matters-how-we-talk-about-change-fecd33471b0).\n\n*Q: Which of these files is downloadable dissertation?*\n\nA: [output.pdf](output.pdf)\n\n*Q: How can I contact you to talk more about this?*\n\nA: ksenok at protonmail dot com\n"
 },
 {
  "repo": "apaloczy/dewaveADCP",
  "language": "Python",
  "readme_contents": "# dewaveADCP\nRemove surface gravity wave signal from ADCP velocity data in along-beam coordinates.\n"
 },
 {
  "repo": "Kadam-Tushar/Eddy-Analyser",
  "language": "Jupyter Notebook",
  "readme_contents": "<p align=\"center\">\n  <a href=\"\" rel=\"noopener\">\n <img width=200px src=\"./img/logo.png\" alt=\"chintak-logo\"></a>\n\n<h1 align=\"center\">Eddy-Analyser</h1>\n<h3 align=\"center\">Real-Time Eddy Visualisation and Analysis.</h3>\n\n\n\n- **Eddies** are clockwise or counter-clockwise circular movements of water that play a major role in transporting energy and biogeochemical particles in the ocean.\n<p align=\"center\">\n  <a href=\"\" rel=\"noopener\">\n <img width=500px src=\"./img/Perpetual_Ocean.gif\" alt=\"NASA\"></a>\n\n\n<p align=\"center\"> NASA Visualisation Studio</p>\n\n------------------------------------------\nThis project does visualization of eddies present in red sea and finds interesting statistics to study behavior of eddies.\n\n- Given the narrow nature of the basin, many eddies can occupy more than half of the Red Sea width, providing rapid transport of organisms and nutrients along the coastline and between the african and Arabian Peninsula coasts. \n- These marine `whirlpools` are much more frequent than what had been previously thought, profoundly affecting the social and economic lives of people living in the surrounding countries.\n\n\n<p align=\"center\">\n<a href=\"\" rel=\"noopener\">\n<img src=\"./img/red_sea_map.png\" alt=\"redsea\" width=\"350px\"/></a>\n\n<p align=\"center\">Red Sea - Google Maps</div>\n\n- This is a course project of [Graphics and Visualization](https://www.csa.iisc.ac.in/~vijayn/courses/Graphics/index.html) course.\n- Problem statement for this project is taken from [IEEE SciVis Contest 2020](https://kaust-vislab.github.io/SciVis2020/index.html)\n- This project solves `Eddy Visualization in 3D ` and `Interesting Eddy statistics`  tasks of contest. It tracks eddies over multiple time intervals and derive important statistics like births/deaths or path of eddies.\n  \n------------------------------------------\n## Contributors:\n- [@Kadam-Tushar](https://github.com/Kadam-Tushar)\n- [@TanayNarshana](https://github.com/TanayNarshana)\n\n------------------------------------------\n\n## Red Sea Dataset: \n[Ensembled](https://kaust-vislab.github.io/SciVis2020/data.html) data of red sea is provided on contest webpage.\n\n------------------------------------------\n\n## Visualisations Softwares / Dependencies : \n- [Inviwo](https://github.com/inviwo/inviwo) - Open Source Scientific Visualization tool.\n- `Matlab` for fast Data processing.\n- Visualization libraries like `py-plot`,`networkx`.\n- Data processing libraries in matlab/python like pandas.\n- `OpenCV` for Image Processing. \n  \n\n## Brief Explanation of our Approach:\n\nNote : To view images in high resolutionn goto `img/Outputs` \n- Detected meso-scale eddies using [Okuba-weiss parameter](https://github.com/inviwo/inviwo) from `velocity field` in  dataset.\n<p align=\"center\">\n<a href=\"\" rel=\"noopener\">\n<img src=\"./img/Outputs/falseDetections3D.png\" alt=\"redsea\" width=\"500px\"/></a>\n<p align=\"center\">3D Eddy Visualisations</p>  \n<br>\n   \n- Filtered out `false postitive` eddies using Gemotrical constraints mentioned in this [paper](https://www.researchgate.net/publication/260722486_Gulf_Stream_eddy_characteristics_in_a_high-resolution_ocean_model_Gulf_Stream_Eddy_Characteristics).\n<p align=\"center\">\n<a href=\"\" rel=\"noopener\">\n<img src=\"./img/Outputs/ens1T45withGeomConstr.png\" alt=\"redsea\" width=\"500px\"/></a>\n<p align=\"center\">Surface Level Meso-scale Eddies</p>  \n<br>\n\n- Applied `Bredth First Search` algorithm on detected eddy centers from above step  over 3D region till threshhold value of Okuba-weiss parameter is reached.\n- Generated possible regions of Eddies in 3D space and then this new data is visualized using `Inviwo` tool.\n \n Here in Inviwo we can interactivly study shapes of these eddies in 3D. \n\n ```\nGreen : Eddies.\nPink : Sea Bed from Bathy metry data.\n ```\n\n<p align=\"center\">\n<a href=\"\" rel=\"noopener\">\n<img src=\"./img/Outputs/eddy_vis_3d.png\" alt=\"redsea\" width=\"500\"/></a>\n<p align=\"center\">3D Eddy Visualisations with Bathymetry Data at Timestep 15</p>  \n\n<br>\n\n- Calculating this eddy information over all ensembles we plotted count of eddies graph.\n\n<p align=\"center\">\n<a href=\"\" rel=\"noopener\">\n<img src=\"./img/Outputs/numberOfEddies.png\" alt=\"redsea\" width=\"500px\"/></a>\n\n<p align=\"center\">Eddy Count over 7 ensembles</p>\n<br>\n\n- To validate our detected eddies we tried `stream-line-plots` over same regions:\n \n<p align=\"center\">\n<a href=\"\" rel=\"noopener\">\n\n<img src=\"./img/Outputs/stream_with_eddy_radii%20(1).png\" alt=\"redsea\" width=\"500px\"/>\n</a>\n<p align=\"center\">Streamlines and eddy detection on same graph</p>\n<br>\n\n- To establish relationship between eddies i.e identify  particular eddy is same in timestep X and X+1 we developed algorithm (see report for details) for eddy-tracking using area - overlap method. \nAlso tried  matlab's `SURF` feature to identify same eddies over different time interval:\n<p align=\"center\">\n<a href=\"\" rel=\"noopener\">\n  <img src=\"./img/Outputs/SURF.png\" alt=\"redsea\" width=\"500px\"/></a>\n\n<p align=\"center\">SURF from matlab</p>\n\n<br>\n\n\n- To make this anaylysis as real time as posible we used `Image Processing` to track eddies between images of surface eddies.\n\n<p align=\"center\">\n<a href=\"\" rel=\"noopener\">\n<img src=\"./img/Outputs/eddy_centroid.png\" alt=\"redsea\" width=\"500\"/></a>\n<p align=\"center\">Eddy Detection from images and their centroids </p>\n<br>\n\n- Applied object detection algorithms on images of eddies using `Contour Detection`\u200b.For each detected objects we can assign features like centroid of eddy, shape of contours, area of contours, After that  we can search for eddies with same features in next few timesteps.\n- Once tracking is done from previous step now to Visualise eddy-relationships we tried to show it graph format as below. \n  \n<p align=\"center\">\n<a href=\"\" rel=\"noopener\">\n<img src=\"./img/Outputs/eddy_life_cycle.png\" alt=\"redsea\" width=\"500\"/></a>\n<p align=\"center\">Eddy Life Graph </p>\nEach Column of nodes represents eddies from that timestep and edge between node represents these two eddies are same,just images  at different timestep.\n<br>\n<br>\n\n- From this network we visualised Deaths/Births graph and splits/merges of eddies.\n  \n<p align=\"center\">\n<a href=\"\" rel=\"noopener\">\n<img src=\"./Outputs/../img/Outputs/BirthsEnsemble1.png\" alt=\"redsea\" width=\"500px\"/></a>\n<p align=\"center\"> Eddy Births</p>\n\n----\n- PR's are welcome, \n- Detailed [ Report](./Report.pdf) of Work.\n- [PPT](https://indianinstituteofscience-my.sharepoint.com/:p:/g/personal/tanayn_iisc_ac_in/EVK5ONb0xZxPhPe2M9f-Kk0Bxvx2NOX_0W1nUUdQd0rmXQ?e=XhJACi) for better understadning.\n"
 },
 {
  "repo": "gvoulgaris0/WaveRIC",
  "language": "HTML",
  "readme_contents": "# WaveRIC: Wave Radar Inversion Code ![image](https://user-images.githubusercontent.com/48567321/126871081-83260317-951c-478d-a180-f007d3c0c8c0.png)\n\n(see Alattabi, Cahl and Voulgaris (2019), JTECH)  \n\nThis is the code described in Alattabi et al. (2019) for the inversion of the 2nd-order of a Doppler spectrum from an HF/VHF radar system. This is a hybrid, empirical radar wave inversion technique that treats swell and wind waves separately. Prior to the inversion, the 2nd order spectrum is normalized using Barrick\u2019s (1977b) weighting function as this removes harmonic and corner reflection peaks from the inversion and improves the results. \n\nAlattabi et al. (2019) presented calibrations coefficiens for the wind and swell parts of the Doppler spectrum. However, not clear if these coefficients are universal as this is under verification at this time. \n\n- Code Citation:  \nDouglas Cahl, George Voulgaris, & Zaid Alattabi. (2019, April 17). Wave Radar Inversion Code (WaveRIC) V1.0.1 (Version V1.0.1). Zenodo. http://doi.org/10.5281/zenodo.2643696\n\n- Method Citation:  \nZaid Alattabi, Z., D. Cahl, and G. Voulgaris (2019). Swell and Wind Wave Inversion Using a Single Very High Frequency (VHF) Radar. Journal of Oceanic and Atmospheric Technology, 36, 987\u20131013, https://doi.org/10.1175/JTECH-D-18-0166.1\n\nCopyright 2019(c) Zaid Alattabi, Douglas Cahl, George Voulgaris\n\nRadarWIC is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.\n\nThis program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License along with this program. If not, see https://www.gnu.org/licenses/.\n\nThe files are located in three directories  \n- WaveRIC  \n-It contains the main functions used to run the inversion (ConfigRWIC.m, masterRadarWIC.m, RadarWIC.m);  \n-file RWIC_Contents.html contains inflormation about the functions included in the package;  \n-master_testing.m is an example using the functions to recreate the spectra in the paper, figure 10;;\n- WaveRIC/lib  \nIt contains a number of functions called by the main function\n- WaveRIC/html and WaveRIC/lib/html  \nIt contains explanations for each function in html files. These are called from RWIC_Contents.html  \n- WaveRIC/data  \nIt contains the data files used by master_testing.m for running examples from the Alattabi et al (2019) paper; Events A to H. It recreates Figure 11 in the paper.\n\nList of Files  \n\nMain Functions:  \n\n  ConfigRWIC.m  \n  masterRadarWIC.m  \n  RadarWIC.m\n\nLibrary Functions (../lib):\n\n  Bragg_peak.m  \n  ConditionDopRWIC.m  \n  findSwellRWIC.m  \n  Gauss_fit.m  \n  hfr_noise.m  \n  plot_swell.m  \n  plot_wind.m  \n  PXYint.m  \n  PXYsideband.m  \n  specSwellRWIC.m  \n  specWindRWIC.m  \n  spectral_noise.m  \n  waveparams.m  \n  weightf_barrick.m  \n  wn2ndRWIC.m  \n  wspecRWIC.m\n"
 },
 {
  "repo": "gher-ulg/SeaDataCloud",
  "language": "Jupyter Notebook",
  "readme_contents": "# SeaDataCloud\nTools and interfaces to work with DIVA interpolation software tool. \n\n## Re-gridding\n\nRe-gridding is the action of interpolation a field from a grid to another grid, usually with a higher resolution.     \n[nco](http://nco.sourceforge.net) provides a tool [`ncremap`](http://nco.sourceforge.net/nco.html#ncremap) for this purpose. \n\n### Installation\n\n1. Download and compile [ESMF](https://earthsystemmodeling.org/download/)\n2. Download a recent version of nco.\n\n### Usage\n\n```bash\nncremap -i data.nc -d grid.nc -o data_regridded.nc\n```\nwhere:\n- *data.nc* is the original file containing the fields,\n- *grid.nc* is another file containing the new grid\n- *data_regridded.nc* is the final output file with the fields interpolated onto the new grid.\n\nThe grid file looks like:\n```\ndimensions:\n\tlongitude = 851 ;\n\tlatitude = 321 ;\nvariables:\n\tfloat longitude(longitude) ;\n\t\tlongitude:units = \"degrees east\" ;\n\tfloat latitude(latitude) ;\n\t\tlatitude:units = \"degrees north\" ;\n}\n```\n\nNote that the input file needs to have lon and lat as coordinates. If that's not te case, one can always use [`ncrename`](https://linux.die.net/man/1/ncrename), for instance:\n```bash\nncrename -d x,lon -v x,lon in.nc\n```\n* the dimension *x* is renamed *lon*\n* the variable *x* is renamed *lon*\n\n### Troubleshooting\n\n#### NetCDF \n\nIf you get errors related to netCDF, define the environment variables as in:     \nhttp://www.earthsystemmodeling.org/esmf_releases/last_built/ESMF_usrdoc/node9.html#SECTION00093200000000000000\n\n#### Grid file\n\nIt seems that the dimensions should be named *longitude* and *latitude*, otherwise the result is strange.\n"
 },
 {
  "repo": "jonnakutip/Oxygen_3D-visualization",
  "language": "MATLAB",
  "readme_contents": "# 3D-visualization\n3D Visualization of Ocean model data of Indian Ocean.\n"
 },
 {
  "repo": "janosh/ocean-artup",
  "language": "Svelte",
  "readme_contents": "<p align=\"center\">\n  <img src=\"static/favicon.svg\" alt=\"Favicon\" width=150>\n</p>\n\n<h1 align=\"center\">Ocean artUp</h1>\n\n<h3 align=\"center\">\n\n[![Netlify Status](https://api.netlify.com/api/v1/badges/209f70e1-0a07-4d82-b642-55c77f2af40f/deploy-status)](https://app.netlify.com/sites/ocean-artup/deploys)\n[![Lighthouse](https://github.com/janosh/ocean-artup/actions/workflows/lighthouse.yml/badge.svg)](https://github.com/janosh/ocean-artup/actions/workflows/lighthouse.yml)\n[![pre-commit.ci status](https://results.pre-commit.ci/badge/github/janosh/ocean-artup/main.svg)](https://results.pre-commit.ci/latest/github/janosh/ocean-artup/main)\n![GitHub repo size](https://img.shields.io/github/repo-size/janosh/ocean-artup?label=Repo+Size)\n\n</h3>\n\nOcean artUp is a research project funded by an [Advanced Grant](https://cordis.europa.eu/project/rcn/205206_en.html) of the European Research Council. It aims to study the feasibility, effectiveness, associated risks and potential side effects of artificial upwelling in increasing ocean productivity, raising fish production, and enhancing oceanic CO<sub>2</sub> sequestration.\n\nThis site is built with [Svelte](https://github.com/sveltejs/svelte) and [Contentful](https://contentful.com).\n\n## Setup\n\nRunning this site locally requires [`git`](https://git-scm.com) and [`yarn`](https://yarnpkg.com) (or [`npm`](https://npmjs.com)). With those installed, do:\n\n1. Clone the repo and change into its directory.\n\n   ```sh\n   git clone https://github.com/janosh/ocean-artup && cd ocean-artup\n   ```\n\n2. (optional) Setup [`pre-commit` hooks](https://pre-commit.com).\n\n   ```sh\n   pre-commit install\n   ```\n\n3. Install dependencies.\n\n   ```sh\n   yarn\n   ```\n\n4. Copy `.env.example` to `.env`.\n\n   ```sh\n   cp .env.example .env\n   ```\n\n   Then open `.env` and insert your [Contentful space ID and access token](https://contentful.com/developers/docs/references/authentication). These are found in the settings menu of a Contentful space under 'API keys'.\n\n5. Start the dev server.\n\n   ```sh\n   yarn dev\n   ```\n\n## Deploy\n\nTo publish this site to netlify:\n\n1. Create an account with [netlify](https://netlify.com).\n2. Install the [`netlify-cli`](https://netlify.com/docs/cli).\n3. Login to your account.\n\n   ```sh\n   netlify login\n   ```\n\n4. Connect your GitHub repo with your netlify account for [continuous deployment](https://netlify.com/docs/cli/#continuous-deployment).\n\n   ```sh\n   netlify init\n   ```\n\n5. Create a production build.\n\n   ```sh\n   yarn build\n   ```\n\n6. Finally deploy the site with\n\n   ```sh\n   netlify deploy\n   ```\n"
 },
 {
  "repo": "gher-ulg/gher-ulg.github.io",
  "language": "JavaScript",
  "readme_contents": "# GHER\n\nGeoHydrodynamics and Environment Research\n"
 },
 {
  "repo": "thomasjo/nemo",
  "language": "Python",
  "readme_contents": "# Project Nemo\n\nForaminifera (forams for short) classification via deep feature extraction.\n\n## Image dataset\n\nAll models have been trained on a dataset of large, high-resolution images of\nforams. The dataset has been produced by our research group, and will be made\npublically available in the near future. Each of the source images consist of\na single class of forams. From these images, patches of _224x224_ pixels are\nextracted using combinations of Gaussian smoothing, binary image generation\nvia thresholding, and connected components. The first step removes the metallic\nborder present in all source images, and the second step extracts candidate\npatches. Each patch that passes a defined selection critera is extracted by\nplacing a _224x224_ crop at the centroid of the candidate region. The entire\nprocess is automated in the `preprocess_data.py` script.\n\nOnce the source images have been preprocessed by extracting patches, datasets\nfor training, validation, and testing are generated automatically by using the\n`build_datasets.py` script.\n\n### Caveat regarding `raw-halves` source images\n\nThe `raw-halves` source images are slightly different in nature, and requires\nthat the `preprocess_data.py` script be invoked with `--border-threshold=50`.\nPatches from this dataset must be manually copied to the `preprocessed` folder\nbuilt by process outlined above. In the future, we should find a way to fully\nautomate this step as well.\n"
 },
 {
  "repo": "USGS-CMG/stglib",
  "language": "Python",
  "readme_contents": "# stglib - Process data from a variety of oceanographic instrumentation\n\n[![Documentation Status](https://readthedocs.org/projects/stglib/badge/?version=latest)](http://stglib.readthedocs.io/en/latest/?badge=latest)\n![stglib](https://github.com/dnowacki-usgs/stglib/workflows/stglib/badge.svg)\n[![Anaconda-Server Badge](https://anaconda.org/conda-forge/stglib/badges/version.svg)](https://anaconda.org/conda-forge/stglib)\n\nThis package contains code to process data from a variety of oceanographic instrumentation, consistent with the procedures of the USGS [Coastal/Marine Hazards and Resources Program](https://marine.usgs.gov) (formerly Coastal and Marine Geology Program).\n\nCurrently, this package has at least partial support for:\n\n- Nortek Aquadopp profilers, in mean-current and wave-burst modes\n- RBR pressure (including waves) and turbidity sensors\n- YSI EXO2 water-quality sondes\n- SonTek IQ flow monitors\n- WET labs sensors, including ECO NTUSB and ECO PAR\n- Onset HOBO pressure sensors\n- Vaisala Weather Transmitter WXT sensors\n- In-Situ Aqua TROLL sensors\n- RD Instruments ADCPs\n- Moving-boat ADCP data processed using [QRev](https://hydroacoustics.usgs.gov/movingboat/QRev.shtml), for use in index-velocity computation\n\nThis package makes heavy use of [NumPy](http://www.numpy.org), [xarray](http://xarray.pydata.org/en/stable/), and [netCDF4](http://unidata.github.io/netcdf4-python/). It works on Python 3.7+.\n\n[Read the documentation](http://stglib.readthedocs.io/).\n"
 },
 {
  "repo": "OceanOptics/MISCToolbox",
  "language": "MATLAB",
  "readme_contents": "MISC Lab Toolbox\n================\n\n**Matlab tools for oceanographic analysis focusing on the inherent optical properties (IOPs) of the open ocean.**\n\nFeatures\n--------\nThe functions available are:\n* compute_bbp.m: Compute the particulate backscattering b_bp from the backscattering bb\n* correct_npq.m: Correct for non photochemical quenching using Xing et al. (2012) and/or Sackmann et al. (2008)\n* etimate_mld.m: Estimate the mixed layer depth (MLD) with one of the following method: fixed temperature threshold, fixed density threshold, variable density threshold or fixed density gradient.\n* meshprofile.m: Interpolate data between profile (often used with scatter3m)\n* need_npqc.m: Determine if need a non photochemical quenching correction\n* scatter3m.m: 4D visualization with earth map (latitude, longitude, depth and measure)\n\nRequirements\n------------\nTo work properly the toolbox need those features in matlab path (addpath):\n* betasw_ZHH2009.m from Xiaodong Zhang available [here](https://github.com/ooici/ion-functions/blob/master/ion_functions/data/matlab_scripts/flort/betasw_ZHH2009.m)\n* gsw_matlab_v3_04 from TEOS-10 available [here](https://github.com/TEOS-10/GSW-Matlab/releases)\n* lr2.m a robust linear regression type II, can substitute it by regress() instead\n"
 },
 {
  "repo": "juoceano/lecture_figures",
  "language": "Jupyter Notebook",
  "readme_contents": "# Notebook gallery\n\nFigures, and the notebooks used to create them, for lectures in oceanography.\n\n\n| Platform       | Status                                                                                                                                             |\n| -------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------|\n| Linux and OS X | [![Build Status](https://travis-ci.org/juoceano/lecture_figures.svg?branch=master)](https://travis-ci.org/juoceano/lecture_figures)                    |\n| Windows        | [![Build status](https://ci.appveyor.com/api/projects/status/o66cvyp766w5bd10?svg=true)](https://ci.appveyor.com/project/juoceano/lecture-figures) |\n\n\nSee the rendered version at https://juoceano.github.io/lecture_figures\n\nTo suggest a notebook or ask questions please open an issue at: https://github.com/juoceano/lecture_figures/issues\n\n## License\n\n<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-sa/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\">Creative Commons Attribution-ShareAlike 4.0 International License</a>.\n\n## Citation\n\n[![DOI](https://zenodo.org/badge/84759695.svg)](https://zenodo.org/badge/latestdoi/84759695)\n"
 },
 {
  "repo": "jamespatrickmanning/pyocean",
  "language": "Python",
  "readme_contents": "# pyocean\ncollection of tested code related to physical oceanographic observations and modeling \nUnfortunately, it has not been updated in years so there are likely better versions available that what is posted here. \n"
 },
 {
  "repo": "briochemc/OceanographyCruises.jl",
  "language": "Julia",
  "readme_contents": "# OceanographyCruises.jl\n\n*An interface for dealing with oceanographic cruises data*\n\n<p>\n  <a href=\"https://github.com/briochemc/OceanographyCruises.jl/actions\">\n    <img src=\"https://img.shields.io/github/workflow/status/briochemc/OceanographyCruises.jl/Mac%20OS%20X?label=OSX&logo=Apple&logoColor=white&style=flat-square\">\n  </a>\n  <a href=\"https://github.com/briochemc/OceanographyCruises.jl/actions\">\n    <img src=\"https://img.shields.io/github/workflow/status/briochemc/OceanographyCruises.jl/Linux?label=Linux&logo=Linux&logoColor=white&style=flat-square\">\n  </a>\n  <a href=\"https://github.com/briochemc/OceanographyCruises.jl/actions\">\n    <img src=\"https://img.shields.io/github/workflow/status/briochemc/OceanographyCruises.jl/Windows?label=Windows&logo=Windows&logoColor=white&style=flat-square\">\n  </a>\n  <a href=\"https://codecov.io/gh/briochemc/OceanographyCruises.jl\">\n    <img src=\"https://img.shields.io/codecov/c/github/briochemc/OceanographyCruises.jl/master?label=Codecov&logo=codecov&logoColor=white&style=flat-square\">\n  </a>\n</p>\n\nCreate a `Station`,\n\n```julia\njulia> using OceanographyCruises\n\njulia> st = Station(name=\"ALOHA\", lat=22.75, lon=-158)\nStation ALOHA (22.8N, 158.0W)\n```\n\na `CruiseTrack` of stations,\n\n```julia\njulia> N = 10 ;\n\njulia> stations = [Station(name=string(i), lat=i, lon=2i) for i in 1:N] ;\n\njulia> ct = CruiseTrack(stations=stations, name=\"TestCruiseTrack\")\nCruise TestCruiseTrack\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Station \u2502 Date \u2502  Lat \u2502  Lon \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502       1 \u2502      \u2502  1.0 \u2502  2.0 \u2502\n\u2502       2 \u2502      \u2502  2.0 \u2502  4.0 \u2502\n\u2502       3 \u2502      \u2502  3.0 \u2502  6.0 \u2502\n\u2502       4 \u2502      \u2502  4.0 \u2502  8.0 \u2502\n\u2502       5 \u2502      \u2502  5.0 \u2502 10.0 \u2502\n\u2502       6 \u2502      \u2502  6.0 \u2502 12.0 \u2502\n\u2502       7 \u2502      \u2502  7.0 \u2502 14.0 \u2502\n\u2502       8 \u2502      \u2502  8.0 \u2502 16.0 \u2502\n\u2502       9 \u2502      \u2502  9.0 \u2502 18.0 \u2502\n\u2502      10 \u2502      \u2502 10.0 \u2502 20.0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\nAnd make a `Transect` of `DepthProfiles` along that `CruiseTrack`\n\n```julia\njulia> depths = [10, 50, 100, 200, 300, 400, 500, 700, 1000, 2000, 3000, 5000] ;\n\njulia> idepths = [rand(Bool, length(depths)) for i in 1:N] ;\n\njulia> profiles = [DepthProfile(station=stations[i], depths=depths[idepths[i]], values=rand(12)[idepths[i]]) for i in 1:N] ;\n\njulia> t = Transect(tracer=\"PO\u2084\", cruise=ct.name, profiles=profiles)\nTransect of PO\u2084\nCruise TestCruiseTrack\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Station \u2502 Date \u2502  Lat \u2502  Lon \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502       1 \u2502      \u2502  1.0 \u2502  2.0 \u2502\n\u2502       2 \u2502      \u2502  2.0 \u2502  4.0 \u2502\n\u2502       3 \u2502      \u2502  3.0 \u2502  6.0 \u2502\n\u2502       4 \u2502      \u2502  4.0 \u2502  8.0 \u2502\n\u2502       5 \u2502      \u2502  5.0 \u2502 10.0 \u2502\n\u2502       6 \u2502      \u2502  6.0 \u2502 12.0 \u2502\n\u2502       7 \u2502      \u2502  7.0 \u2502 14.0 \u2502\n\u2502       8 \u2502      \u2502  8.0 \u2502 16.0 \u2502\n\u2502       9 \u2502      \u2502  9.0 \u2502 18.0 \u2502\n\u2502      10 \u2502      \u2502 10.0 \u2502 20.0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\njulia> t.profiles[3]\nDepth profile at Station 3 (3.0N, 6.0E)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Depth \u2502              Value \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502   50.0 \u2502  0.519255214063679 \u2502\n\u2502  300.0 \u2502 0.6289521421572468 \u2502\n\u2502  500.0 \u2502 0.8564006614918445 \u2502\n\u2502 5000.0 \u2502 0.7610393670925686 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n"
 },
 {
  "repo": "lukecampbell/gsw-teos",
  "language": "C",
  "readme_contents": "TEOS-10 V3.0 GSW Oceanographic Toolbox in C\n\nThis is a translation of the original Fortran-90 source\ncode into C. You should download the documentation from http://teos-10.org.\nThe functions gsw_saar and gsw_delta_sa_ref\nhave been modified from the original to not use an external\ndata file for global absolute salinity anomaly and absolute\nsalinity anomaly ratio data. The data are instead incorporated\ninto static tables that are used directly.\n\nManifest:\n\nREADME                                                          -- this file.\ngsw_check_functions.c                                           -- C implementation of the check functions\ngsw_data_v3_0.dat.gz                                            -- global absolute salinity anomaly data\ngsw_format.c                                                    -- program to create gsw_saar_data.c from\n                                                                   gsw_data_v3_0.dat\ngsw_oceanographic_toolbox.c                                     -- The C GSW library less gsw_saar\ngsw_saar.c                                                      -- gsw_saar and gsw_delta_sa_ref (modified)\ngsw_saar_data.c                                                 -- static global absolute salinity anomaly data\n\t\t\t\t                                                   used by gsw_saar.c and created by gsw_format\ngswteos-10.h                                                    -- GSW function prototypes\nMakefile                                                        -- basic make file to build gsw_check_functions\n\t\t\t\t                                                   and libgswteos-10.so\n\nYou'll probably want to build gsw_oceanographic_toolbox.c, and gsw_saar.c\ninto a library. \"make library\" will attempt to build a shared library for\ngcc/GNU Linux platforms.\n\nC programs using the GSW Oceanographic Toolbox should include the\nsupplied header file:\n\n#include <gswteos-10.h>\n\nChangeLog:\n\n2012-10-07:\tgsw-3.0.1 New gsw_check_functions.c based on revised f90.\n2011-09-23:\tgsw-3.0 Initial creation.\n\nFrank Delahoyde <fdelahoyde@ucsd.edu>\nShipboard Technical Support, Computing Resources <sts-cr@ucsd.edu>\nScripps Institution of Oceanography\nNimitz Marine Facility, Point Loma\nSan Diego, Ca. 92106-3505\n"
 },
 {
  "repo": "tompc35/oceanography-notebooks",
  "language": "Jupyter Notebook",
  "readme_contents": "# oceanography-notebooks\nIPython/Jupyter notebooks showing examples of oceanographic data analysis.\n\n### Physical oceanography\n\n[Line P hydrographic data](Line_P/plot_line_p_profiles.ipynb)\n\n[Abyssal recipes](Abyssal_Recipes/Abyssal_Recipes.ipynb)\n\n### Loading and plotting data\n\n[MLML\\_seawater.ipynb](MLML_seawater.ipynb)\n\n[Monterey Bay bathymetry](montereybay_bathymetry.ipynb)\n\n[ocean\\_color\\_netcdf.ipynb](ocean_color_netcdf.ipynb)\n\n### Time series analysis\n\n[sst\\_harmonic\\_fit.ipynb](sst_harmonic_fit.ipynb)\n\n### Statistics and probablity distributions\n\n[generating\\_t_and\\_chi2.ipynb](generating_t_and_chi2.ipynb)\n\n[central\\_limit\\_theorem.ipynb](central_limit_theorem.ipynb)"
 },
 {
  "repo": "evb123/oceanography-visualizations",
  "language": "MATLAB",
  "readme_contents": "# oceanography-visualizations\nExamples of data visualizations created for my master's dissertation (2018).\n\nData engineering, analysis and visualisation for dissertation work was performed in MATLAB with use of the Thermodynamic Equation of Seawater 2010 Gibbs-SeaWater Oceanographic [Toolbox](http://www.teos-10.org/software.htm) \n\nPlease navigate to the [wiki](https://github.com/evb123/oceanography-visualizations/wiki) for easy viewing of visualizations and code snippets.\n"
 },
 {
  "repo": "lesommer/2017-lectures-godae-ocean-view",
  "language": "HTML",
  "readme_contents": "# 2017-lectures-godae-ocean-view\nLecture material for GODAE Ocean View summer school\n"
 },
 {
  "repo": "NOC-MSM/SEAsia",
  "language": "Fortran",
  "readme_contents": "******************\n# Relocatable NEMO\n******************\n\nAn example configuration of SE Asia, demonstrating how to setup new regional domains in the NEMO framework.\nThis model configuration has been developed through the ACCORD (Addressing Challenges of Coastal Communities through Ocean Research for Developing Economies) Project, funded by [Natural Environment Research Council, under a National Capability Official Development Assistance](http://gotw.nerc.ac.uk/list_full.asp?pcode=NE%2FR000123%2F1).\n\n*************************************************\n## NEMO regional configuration of South East Asia\n*************************************************\n\n### Model Summary\n\nA specific region of focus includes exploring South East Asia (75E to 135E and -20N to +25N)\n\nThe model grid has 1/12&deg; lat-lon resolution and 75 hybrid sigma-z-partial-step vertical levels. Featuring:\n\n* FES2014 tides\n* Boundary conditions from ... (in prog.)\n* Freshwater forcing (in prog.)\n* ERA5 wind and sea level pressure (in prog.)\n\n![SE Asia bathymetry](https://github.com/NOC-MSM/SEAsia/wiki/FIGURES/ACCORD_SEAsia_bathy.png)\n\n### Model Setup\n\n\nThe following process is followed to build and get started with this configuration\n\n``git clone https://github.com/NOC-MSM/SEAsia.git``\n\nThen follow descritptions in: https://github.com/NOC-MSM/SEAsia/wiki\n\nThe example is based on NEMO v4.0.6 and XIOS v2.5:\n\n\n\n### Experiment Summary\n\n* ``EXP_barotropicTide``\nOnly tidal forcing. Constant T and S\n\n* ``EXP_unforced``\nNo forcing. Stratification varies only with depth. Start from rest.\n\n\n...\n\n### Repository structure\n\n* ...\n"
 },
 {
  "repo": "asascience-open/QARTOD",
  "language": "Python",
  "readme_contents": "**Library has been archived; new projects wishing to use QARTOD and other QC algorithms should use https://github.com/ioos/ioos_qc instead.**\n\n.. image:: https://travis-ci.org/asascience-open/QARTOD.svg?branch=master\n   :target: https://travis-ci.org/asascience-open/QARTOD\n   :alt: build_status\n\n\nQARTOD\n======\n\nCAVEAT: this module is deprecated!\nWe recommend https://github.com/ioos/ioos_qc as an alternative QARTOD python implementation.\n``ioos_qc`` is actively developed and supported by IOOS.\n\nCollection of utilities, scripts and tests to assist in automated\nquality assurance and quality control for oceanographic datasets and\nobserving systems.\n"
 },
 {
  "repo": "nitrogenlab/oceanography_colab_notebooks",
  "language": "Jupyter Notebook",
  "readme_contents": "# oceanography_colab_notebooks\nFor oceanography-related colab notebooks\n"
 },
 {
  "repo": "gher-ulg/Ocot-notebook",
  "language": "Jupyter Notebook",
  "readme_contents": "# Coastal Oceanography\n\nClick on the \"launch binder\" icon to start the notebooks. Setting-up the working environement on the binder service can take a couple of minutes. Binder will automatically shut down user sessions that have more than 10 minutes of inactivity.\n\n* Data processing [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/gher-ulg/Ocot-notebook/master?filepath=Data\\_processing\\_blank.ipynb)\n\n* Linear regression [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/gher-ulg/Ocot-notebook/master?filepath=LinearRegression\\_blank.ipynb)\n\n* Air-Sea heat fluxes [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/gher-ulg/Ocot-notebook/master?filepath=OceanHeatFluxes.ipynb)\n\n## Downloading\n\nTo download a file (like a figure), select `File -> Open...` in the menu. This opens a new window with a file manager. Tick the check-box of the file and select `Download`.\n\n![download1](Figures/download1.png)\n\n![download2](Figures/download2.png)\n\nMore information about Binder is available in the [Binder FAQ](https://mybinder.readthedocs.io/en/latest/faq.html).\n"
 },
 {
  "repo": "janjaapmeijer/oceanpy",
  "language": "Python",
  "readme_contents": "# OceanPy\nPython package for Oceanography\n\n```\noceanpy\n\u251c\u2500\u2500 docs\n\u251c\u2500\u2500 models\n\u251c\u2500\u2500 obs\n\u2502   \u251c\u2500\u2500 ctd.py\n\u2502   \u2514\u2500\u2500 satellite.py\n\u251c\u2500\u2500 plot\n\u251c\u2500\u2500 stats\n\u251c\u2500\u2500 tools\n\u2502   \u251c\u2500\u2500 contour.py\n\u2502   \u251c\u2500\u2500 netcdf.py\n\u2502   \u2514\u2500\u2500 projections.py\n```\n\n## Polynomials\n\n\\begin{equation}\nL_t = e^x\n\\end{equation}\n\n\n<script type=\"text/javascript\" async\n  src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML\">\n</script>\n"
 },
 {
  "repo": "Sam-Saarinen/WHOI-ML",
  "language": "Jupyter Notebook",
  "readme_contents": "# WHOI-ML: About This Repository\nThis Repository contains notes, resources, and assignments for a Machine Learning Bootcamp for the Woods Hole Oceanographic Institution.\n\n# About This Document\nThis ReadMe outlines the content of the bootcamp, provides installation instructions for the required software packages, and suggests background reading material for extended study.\n\n# Course Structure\nThe course will run over 3 days in six 4-hour sessions. Each session will be broken up into 3-4 subsections consisting of presentation content and a practical component. Practical components are designed so that the minimal learning goals can be comfortably met within the allotted time, but stretch goals will allow participants to gain additional practice and deepen their understanding.\n\nThe six sections are:\n1. **The Machine Learning Pantry** - Machine Learning comes in several different flavors (supervised, unsupervised, reinforcement, etc.) and there are several ready-made libraries that will give us 80% of the functionality in 20% of the time. In this section, you will use scikit-learn and matplotlib to make predictions.\n2. **Neural Networks and Supervised Training** - Artificial Neural Networks have been used to great effect in almost every area of machine learning, thanks to their flexibility, their simplicity of use, and the explosion of available training data. In this section you will use pytorch to define and train several types of neural networks to solve several types of machine learning problem. Although why they work is still something of a mystery, this section will elucidate when they work, and what to try when they don't.\n3. **Structured and Spatio-Temporal Data** - Data often have structure that constrains what is possible or what is likely. In this section, you will learn to use clustering algorithms on discrete spatial categories, represent continuous functions using Gaussian Processes, represent sequential and temporal data, and encode translational symmetry using convolutional structures.\n4. **Unsupervised Learning and Anomaly Detection** - Unsupervised (and self-supervised) techniques can be used to summarize data, providing interpretability, computational simplification, and measures of typicality. In this section, you will use PCA, t-SNE, and mixture models to summarize data, and you will use data reconstruction error and likelihoods to identify outliers.\n5. **Reinforcement Learning and Automation** - Machine Learning can be used to optimize decisions and improve them over time. In this section you will apply several versions of Thompson Sampling to decision problems with uncertainty, and you will apply REINFORCE to decisions with long-term effects.\n6. **Machine Learning in the Real World** - In this section you will apply what you've learned to a dataset of your choice. This section will also describe some approaches to scalability, addressing time, memory, and data constraints.\n\n# System Requirements\nAny machine with an up-to-date popular desktop operating system (Mac OS, Windows, Ubuntu, e.g.) and a modern web browser installed will be sufficient for this course. That said, machine learning can be computationally intensive, so higher-end CPU's will save time, and machines with NVidia GPUs _may_ be able to leverage them for faster training of neural networks, in the section of the course pertaining to that. However, modest hardware will still be sufficient for the purpose of the course.\n\n# Software Packages\nThis program uses many software packages, but fortunately will only require two installs, as a large number of the packages we will use come packaged with Anaconda.  \n1. [Install Anaconda](https://www.anaconda.com/distribution/). If offered a choice, download and install the package with the most recent version of Python (3.7). Anaconda is a scientific computing package that bundles many software packages and libraries we will be using in the course, including Python, Jupyter Notebooks, the conda package manager, matplotlib, numpy, pandas, scikit-learn, tqdm, and many more.\n2. [Install PyTorch](https://pytorch.org/). PyTorch is a deep-learning library (comparable to Tensorflow) with a shallow learning curve and high flexibility. This is easiest to do through the command line, using the conda package manager. In the \"Getting Started\" section, you can choose the install command that fits your environment. Note that if you are not running on a CUDA-supported NVidia GPU, you should choose \"none\" to install only the CPU version of PyTorch. Windows users may need to run the installation command through Power Shell or by going through Anaconda Navigator > Environments > base (root) > Open Terminal.\n3. Test the installation by opening a new Jupyter notebook and running  \n`import torch`  \n`print(torch.__version__)`    \nJupyter notebooks can be opened by either running `jupyter notebook` from a shell/terminal or by going through the Anaconda Navigator > Jupyter Notebooks. Either option should open a page in your web browser that will allow you to browse files and create a new notebook by clicking new > Python 3. Type the code above into a cell, and then execute the cell by pressing Shift+Enter.\n\n# Background Reading:\nNo prior knowledge of Machine Learning is assumed in this course, but a strong programming background is assumed. The course will be exclusively in Python, and participants may want to review some of the following pages so that they can focus exclusively on Machine Learning once the course begins.  \n- [Python Cheat Sheet](https://perso.limsi.fr/pointal/_media/python:cours:mementopython3-english.pdf). Feel free to search or follow a tutorial if anything needs clarification.\n- [Python Classes/Objects](https://www.w3schools.com/python/python_classes.asp)\n- [Python Special Methods](https://micropyramid.com/blog/python-special-class-methods-or-magic-methods/) (`__init__`, `__str__`, `__repr__`, etc.). There is a gentler introduction [here](https://dbader.org/blog/python-dunder-methods).\n- [Functional Programming](https://kite.com/blog/python/functional-programming/)\n- [Debugging in Python](https://realpython.com/python-debugging-pdb/). Although Jupyter notebooks make interactive programming and introspection through `print` statements very flexible, `pdb` may still be useful to developers aiming at 10x engineer status.\n\n# Additional Resources:\nThese resources will not be used in this course, but those hoping to explore some areas in greater detail may find them worth looking into.\n\n### Troubleshooting Neural Networks\n- Josh Tobin - [Troubleshooting Deep Neural Networks](http://josh-tobin.com/troubleshooting-deep-neural-networks.html)\n- Andrej Karpathy - [A Recipe for Training Neural Networks](http://karpathy.github.io/2019/04/25/recipe/)\n    - See also: Karpathy's [tweet thread](https://twitter.com/karpathy/status/1013244313327681536) on common neural net mistakes\n- Matt Holt & Daniel Ricks - [Practical Advice for Building Deep Neural Networks](https://pcc.cs.byu.edu/2017/10/02/practical-advice-for-building-deep-neural-networks/)\n\n### Parallel/Distributed Training\n- Thomas Wolf - [Training Neural Nets on Larger Batches: Practical Tips for 1-GPU, Multi-GPU & Distributed Setups](https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255)\n\n### Recurrent Networks\n- Andrej Karpathy - [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n- Chris Olah - [Understanding LSTMs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n\n### Reinforcement Learning\nFor those interested in doing more reinforcement learning experiments (on Markov Decision Processes, not Bandit Problems), the libraries `gym` and `simple_rl` may be helpful. These can be installed through a python package manager, such as `conda` or `pip`.\n\n### Other Courses\n- fast.ai - [Practical Deep Learning for Coders](https://course.fast.ai/index.html)\n- Stanford - [Convolutional Neural Networks for Visual Recognition](http://cs231n.github.io/)\n\n# License Information\nAll content in this repository is \u00a92019 (S)am. It has been made available for educational purposes, but copying or redistributing this content is not permitted. If you would like to suggest changes to this repository or correct errors, please contact the owner of this repository or submit a pull request.\n\n# Instructor Contact:\n`sam_saarinen@brown.edu`\n"
 },
 {
  "repo": "cgentemann/python4oceanography.github.io",
  "language": "SCSS",
  "readme_contents": "## Introduction\nThis template utilizes Jekyll, an open source static website generator, as well as a theme based largely off of the Minimal Mistakes theme by Michael Rose. The purpose of this template is to provide you with a simple, well designed website that is optimized for hosting on Github pages. We aim to reduce the technological know-how and time that is usually required for maintaining a personal or professional website.\n\n#### Why Should I Use This?\nBy using this template you will have a website that is well designed, easy to maintain, free to host and easy to update. While there are many options out there for personal and professional websites, most are dependant on the platform on which they were built, and cannot be easily migrated. This template, while built for Github Pages integration, provides flexibility should you choose to host it elsewhere.\n\n## [Getting Started Guide](https://ncsu-libraries.github.io/jekyll-academic-docs/)\nComplete documentation for getting started as well as advanced features of Jekyll Academic can be found at [https://ncsu-libraries.github.io/jekyll-academic-docs/](https://ncsu-libraries.github.io/jekyll-academic-docs/).\n\n## Migrating to a new default branch name\nWe've decided to change this project's default branch name to 'main'.  If you've forked this repository prior to July 20th, 2021, then you should a message with update instructions when you go to your fork in github: \n\n![fork renamed message](https://user-images.githubusercontent.com/3514165/126372022-ae4c07fa-dec7-427c-a4b5-cdd73aec75eb.png)\n\nIn your fork on GitHub, go to the branches view, and click on the edit icon next to the 'master' branch.  Change the branch name to main.  Underneath the input box where you change the name you will be presented with the commands that you will need to run on your local copy of your fork.\n\n![local instructions for default branch name change ](https://user-images.githubusercontent.com/3514165/126372635-208fbc4b-698e-4938-bdae-5ff19eed2c96.png)\n\n\n## Upgrade Notes for June 2021 release\nIf you are running a fork of Jekyll Academic before June 2021, we made some breaking changes to upgrade the underlying Jekyll version and to address the constant github/dependabot security notices mentioned in issue #4.\n\nWe have updated Jekyll to version 4 and removed reveal.js as an included library. We still want to support reveal.js presentations, so we have taken the suggestion from issue #4 and made the reveal.js directory a [git submodule](https://git-scm.com/book/en/v2/Git-Tools-Submodules). If you are running Jekyll Academic as a Github Page, this should hopefully be a minor change.\n\nIf, however, you are running Jekyll Academic locally or on a custom server, after merging this repo's commits in to your fork, you will need to go to the command line in your local or custom instance and perform the following command:\n\n  `$ git submodule update --init`\n\nIf you have any reveal.js presentations posted, you may need to make some updates for them to display properly using reveal.js version 4.  See [Jekyll's documentation](https://revealjs.com/upgrading/) for details.\n\n## Keeping reveal.js up to date\nMoving forward, if you'd like to update reveal.js you will need to run the following commands:\n\n  `$ git submodule update --remote`\n"
 },
 {
  "repo": "pmlmodelling/oceandata",
  "language": "Python",
  "readme_contents": "# oceandata\nMethods to access oceanographic data available through the likes of thredds servers.\n\n\n\n"
 },
 {
  "repo": "SalishSeaCast/tools",
  "language": "Jupyter Notebook",
  "readme_contents": "***********************\nSalish Sea MEOPAR Tools\n***********************\n:License: Apache License, Version 2.0\n\nThis is a collection of tools for working with the Salish Sea MEOPAR NEMO model,\nits results,\nand associated data.\n\nDocumentation for the tools is included in the docs/ directory and is rendered at https://salishsea-meopar-tools.readthedocs.io/en/latest/.\n\n.. image:: https://readthedocs.org/projects/salishsea-meopar-tools/badge/?version=latest\n   :target: https://readthedocs.org/projects/salishsea-meopar-tools/?badge=latest\n   :alt: Documentation Status\n\n\nLicenses\n========\n\nUnless otherwise specified,\nthe Salish Sea MEOPAR tools and documentation are copyright 2013-2021 by the `Salish Sea MEOPAR Project Contributors`_ and The University of British Columbia.\n\nThey are licensed under the Apache License, Version 2.0.\nhttps://www.apache.org/licenses/LICENSE-2.0\nPlease see the LICENSE file for details of the license.\n\n.. _Salish Sea MEOPAR Project Contributors: https://github.com/SalishSeaCast/docs/blob/master/CONTRIBUTORS.rst\n\nThe `salishsea_tools.namelist` and `salishsea_tools.tests.test_namelist` modules are based on https://gist.github.com/krischer/4943658.\nThey are copyright 2013 by Lion Krischer <krischer@geophysik.uni-muenchen.de> and are licensed under the GNU Lesser General Public License, Version 3 (http://www.gnu.org/copyleft/lesser.html).\n"
 },
 {
  "repo": "BarshisLab/CBASS-vs-RSS-Physiology",
  "language": "R",
  "readme_contents": "Electronic notebook for \"Remarkably high and consistent tolerance of a Red Sea coral to rapid and prolonged thermal stress\"\n\nOpen Access publication accessible here: https://aslopubs.onlinelibrary.wiley.com/doi/full/10.1002/lno.11715 \n\nContains two folders: one with all the main figures and one with the supplementary figures\n\nEach Folder contains a PDF of the figure, the data and R code used to produce the figure, and the R code detailing how the statistical analyses were conducted.\n\nNote that figures 3, 4, and S2 were produced with Prism by Graphpad (v8) using the provided data.\n"
 },
 {
  "repo": "fcarvalhopacheco/CODAS",
  "language": null,
  "readme_contents": "# ``CODAS`` ADCP PROCESSING GUIDE\n\nThis is a guide to complement and/or simplify some steps required to install [CODAS software](https://currents.soest.hawaii.edu/docs/adcp_doc/codas_setup/index.html)\n\n>*A **CODAS** (Common Oceanographic Data Access System) database is a way to store and access oceanographic data. CODAS was developed in the late 1980's as a portable, self-describing format for oceanographic data, with emphasis on processed ADCP data.*\n\n\n>*The \"**CODAS processing**\" refers to the ADCP data processing software and procedures that were developed around the CODAS format. The processing steps have become increasingly automated, but human judgement is still required for the final product, and the software is highly flexible in allowing manual configuration and execution of individual steps.* [(Hummon, 2009)](https://currents.soest.hawaii.edu/docs/adcp_doc/)\n\n\n## For detailed  information, please visit:\n- [Currents Group at the University of Hawaii](https://currents.soest.hawaii.edu/home/)\n\n- [UHDAS+CODAS Documentation](https://currents.soest.hawaii.edu/docs/adcp_doc/)\n\n\n\n## Installation Options:\n\n***Recommended***\n\n- [Virtualbox](https://currents.soest.hawaii.edu/docs/adcp_doc/codas_setup/virtual_computer/index.html)\n\n\n**Harder/Tricky**\n\n- [Ubuntu - Anaconda3 - Python 2.7 Environment](https://github.com/fcarvalhopacheco/CODAS-installation/blob/master/installation/anaconda3_py27.md)\n\n- [Ubuntu - Anaconda3 - Python 3.6 Environment](https://github.com/fcarvalhopacheco/CODAS-installation/blob/master/installation/anaconda3_py36.md)\n\n\n## Processing Example:\n\n- [Shipboard ADCP Processing](https://github.com/fcarvalhopacheco/CODAS/blob/master/processing/kilo_moana_processing.md)\n\n\n"
 },
 {
  "repo": "metno/moxml",
  "language": null,
  "readme_contents": "============================================================\nMOX - Meteorological and Oceanographic XML\n============================================================\n\nNorwegian Meteorological Institute (met.no)\nBox 43 Blindern\n0313 OSLO\nNORWAY\n\nemail: wdb@met.no\n\nThis is a meteorological/oceanographic data exchange format \ndeveloped at met.no. It is developed as an application \nschema of the Geographic Markup Language (GML) version \n3.2.1. The mox schema is used in the wdb weather and water \ndatabase.\n\n\n"
 },
 {
  "repo": "project-hermes/hermes-firebase",
  "language": "Vue",
  "readme_contents": "# hermes-firebase\n\n[![Join the chat at https://gitter.im/hermes-firebase/Lobby](https://badges.gitter.im/hermes-firebase/Lobby.svg)](https://gitter.im/hermes-firebase/Lobby?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n[![Build Status](https://travis-ci.org/sonyccd/hermes-firebase.svg?branch=master)](https://travis-ci.org/sonyccd/hermes-firebase)\n[![Codacy Badge](https://api.codacy.com/project/badge/Grade/dd6fcab8566444d486ccb79b8ec91494)](https://app.codacy.com/app/sonyccd/hermes-firebase?utm_source=github.com&utm_medium=referral&utm_content=sonyccd/hermes-firebase&utm_campaign=badger)\n[![Dependencies](https://david-dm.org/sonyccd/hermes-firebase.svg)](https://david-dm.org/sonyccd/hermes-firebase)\n\nFirebase code for project hermes\n\n## To devel\n\n```\n$ npm install -g firebase-tools\n$ firebase serve   # Start development server\n$ firebase deploy  # Deploy new version of everything\n```\n\n## To deploy only the functions\n```\n$ firebase deploy -P staging --only=functions\n```\n\n## Databse\n\nAll data is layed out in Google Firestore document databse\n```\n{\n    \"Sensor\": {\n        \"s2d4t6rdw46yew3f\": {\n            \"Name\": \"hermes1\",\n            \"Id\": 12334,\n            \"FirmwareV\": 0.05,\n            \"BuildV\": 0.01,\n            \"LastUpdate\": 1543742463,\n            \"Mode\": \"debug\",\n            \"Dive\": {\n                \"3d3r5fw32r45fgr56yne46ewg\": {\n                    \"Time\": 153453533,\n                    \"Start Lat\": 23.24323432,\n                    \"Start Long\": -31.2423424,\n                    \"End Lat\": 23.456435,\n                    \"End Long\": -31.564564,\n                    \"Duration\": 1234345,\n                    \"Name\": \"key largo\",\n                    \"Diver\": \"Brad Bazemore\",\n                    \"Data\": {\n                        \"2s23d43654f4yef53g43q\": {\n                            \"time\": 154575673,\n                            \"temp\": 89.34,\n                            \"depth\": 6547\n                        },\n                        \"4r43fr5tr44rfw34tfe4t\": {\n                            \"time\": 154575679,\n                            \"temp\": 89.42,\n                            \"depth\": 6553\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n```\n"
 }
]